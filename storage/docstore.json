{"docstore/metadata": {"158b4de7-8a7e-4aeb-b7bf-c5d216ed1a23": {"doc_hash": "2e4811c04895a1bae117316c0163a7da940b6682744556d0a8435eee773cceb6"}, "f17a44af-b268-42bc-9cc7-b4d76930c2ab": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "803f0f88-0565-4a83-9050-ce9030447b37": {"doc_hash": "d0ee4426e44ffa1d1be9b5d164d9f6b48a2c79c5c40261272cd1ebd79a99322c"}, "d962c8b5-54cc-4680-be29-9586eb3e7519": {"doc_hash": "3dc6510aac91b0220e1d2c406ed540fcc21df97f350df597578402e7e52b0f1f"}, "8be8a3ec-7583-40d3-ac97-2a239839d10b": {"doc_hash": "c18e2e0007de7d6001e052ea6840ad3cb2a5d58774accddf1b2158473db191b0"}, "7278b6a0-f9c7-49bd-a0e3-deaffbf982e7": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "59e1a370-fd70-40e6-8186-1014e376068f": {"doc_hash": "990b94352dc6f87f40d6cb2af6ea2bd49422be46e839b7ae18128d3c24e8b20d"}, "45466ecd-1ae1-48a2-90fd-2398a596695a": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "b5661d06-2aae-4f13-8056-4e2db3b68406": {"doc_hash": "6864bc21fe255caf84b986871bad6b4aca0af87941f540b154ff1d416c897121"}, "38b20bf6-887c-4e02-bb3b-ece2d2f397ce": {"doc_hash": "e77d79d7a9c441a382b8769e99c4a0dac83a076c89b712c6eecc888877c2c279"}, "53b4b53a-1edc-428e-a3d5-7e6fc768608e": {"doc_hash": "143a54d50d779208c3ca2ca17894c4aff4ca2079920ad2be64e488e14656c5fa"}, "5ee656fb-8bfe-4b51-822d-4eb09a6d2e06": {"doc_hash": "f13b68256424e4f1aebd4a003eee8f4c686bda3420a912a9b01ab3302b556813"}, "96780b3c-3e8a-49ae-85b9-10e089161a90": {"doc_hash": "117b1c32beb902c8c52f9214b8e149501385395770dc148619052cb0ab9ce525"}, "7f70a001-7676-4bd5-9b3d-aeaa32f2cf4e": {"doc_hash": "806926220877a1040e0aff53686de46bcc21cf862471bc9212d9c09ebeb55000"}, "c9ccaf6f-00ac-4d62-aad7-7f976149b58f": {"doc_hash": "e6414793f73a9eec88ac93fcce7188c2f0d12a55ac807fb14ffc6a5951018ec6"}, "57b2c53d-1303-4874-a654-481e669c432c": {"doc_hash": "98d84111582c36e74ebb42b5b96a5b945c57e3d6f48b256d39303f491bbb202e"}, "4fd3a217-c3bc-47a7-a2cc-3660c00ad8dd": {"doc_hash": "3c53f3be36b5c779d02d642c150cb7c48bb92162fd383ea752fbc945efc4aa7f"}, "e36b77e3-bf09-4bdd-9aed-9e9095f7e345": {"doc_hash": "78cb05b730e9f30d4736514243e486a4d498f124f7d92e5a53c98c03dd597a8c"}, "949be309-81f7-4b2d-b8fb-b6e1c8243b8a": {"doc_hash": "11d0cbb69e34199832748273380a98cc3f2a68d361414f9fcdcca097c07c8718"}, "333b3eb1-c28f-40bd-92b5-e0e2fa35310d": {"doc_hash": "f48314a45f3286f88444cf834a2cbcb96f0fa4afff420c331213de28a53a1a1f"}, "dede16a6-53e1-44ac-8b35-395e6e4fbf07": {"doc_hash": "ce0e8e9a7872379ca2a1f3e46e82019310a4706c53acb3d271124510eae7283f"}, "2c728a22-0319-46d7-a8ba-9fd84fe074df": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "175f7a36-82be-4a56-81c6-4679d8ad2aa4": {"doc_hash": "ad98cd2273f467eb3bfa97e720bf8d07ca356025ff252aad5a3f4d2de9235d68"}, "cefe1f2c-0a0c-4644-a493-f1785b91d7fa": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "b3cdb196-2131-48e5-a27a-a6244c693ba5": {"doc_hash": "4abe886d1eb1794c6434ea5dbf91d79fa5530bfb4d80c9bd218f4b2588024ace"}, "800c85e3-444a-4195-a9de-d5aabbe2cc68": {"doc_hash": "bd2f2b4222e44ec68dde2eb6fd90e53dc44fab1f95df0dc1587f6887f8f9b0c5"}, "0c065824-2202-4ddc-b76e-8b087036ad99": {"doc_hash": "35ffc3f0bb339f2737f39746bf30144dbc9db89f2cddb421bce5a006b43a91c0"}, "e90b83f0-de99-49a7-b817-3e17ed0ea68f": {"doc_hash": "8be4f7739d2cde6ffb3baeb3de144cf27b9bd47822da9622cdf0f7ee9e086977"}, "5d5e453c-0641-47c2-aaf0-7d96236a2634": {"doc_hash": "cb69c993fb13a11a4d49deaa817c66a49a7113125d85bcd15f79c17f3dde8a89"}, "9975b226-070a-4ce9-8252-29c09b570218": {"doc_hash": "e90ad1698417692e8509a2d3b63151de09afe487e53c19d2e6b8cd4b5c88af95"}, "6ae163d3-8fb6-45ff-9723-3aead34f0341": {"doc_hash": "594fa6fca9f26b0108d846866e605c3b223823f15cf966f35270fd0c0ee0bae5"}, "8f618ba9-af3d-40ac-9cf7-a150ad794244": {"doc_hash": "0a1f83b80bfc73ad4e0ac9660ddd820e4b02de602f807a2ef3cf8433ce1b08e4"}, "3d84d26b-4153-49da-a775-5697c59a786e": {"doc_hash": "457bafdcd01859dcda04556965a65a58c1a07bf6a0abec91032b876dcdb82a60"}, "8cde927c-fa2c-4f44-bbb3-117e7192373d": {"doc_hash": "ab0105b8c978b930c6f48f181ee619414ac0462b9ca9168520c938549922bd5a"}, "6ddd9787-8809-4b44-83d8-73f4e4c605bb": {"doc_hash": "6c3cde551b5ad8cb822f2514b3e571277431e5d05664db08ec5c2dfd9f938d1f"}, "82681f19-cfd0-4373-8c02-5a140c7d4416": {"doc_hash": "67c67f9cd6df7169821f641f3b3b0a5e260fc3955ee4d27ed16506095c5ef197"}, "33c010c0-cde3-4d06-8dae-aa630c182a01": {"doc_hash": "e6c6d4a0fc55f8594aed6a5e038892743d59516efec240c2b345ed2705a6cbc9"}, "ebca9fb4-f907-467b-a919-92a54c84d9ae": {"doc_hash": "6c9c9c5c46b6c8de0472ed0ec70d5bf4e48a352de10d346ba57c0f88b50aff75"}, "026a0aff-325d-4816-a4c7-ac3210891bba": {"doc_hash": "0a508a09bab60e964bcbab4235ae5c12b3f24c0b399b29c07f09c32e102913e2"}, "65c8d38a-117b-4a97-b221-12ec138c1fe0": {"doc_hash": "be8500ddca9df77069120374a61ed0ecc9f255efd1d00adc31afcfb826830b19"}, "0521757f-09a7-4b3f-a7d5-4ed986a399fc": {"doc_hash": "67f5cdcf16d9812766b22df1f9e83304c0f360c2f38b72e9ceff23aedf3c666b"}, "78dc8c45-d28c-44dd-8bbe-b1ac778d145e": {"doc_hash": "07caab51d34891b4de236a66bc0db0e83544732db36c6d0df96ccbc29730e94c"}, "4b4da5b9-4280-4507-ae6b-0236588afcbb": {"doc_hash": "494c5bdf1e7a00d3ca2736134735f1093c0050f34188f0cb8fcfb0f75a1b55bb"}, "86f26097-22cc-419d-90db-a1d6c342bcf3": {"doc_hash": "fc83fa8c4ba35a258ca2c0066f599911fd0db3fb08ac0339b3254085c101382d"}, "a3907149-1b10-4755-9b11-62c174602753": {"doc_hash": "207aa3c1319ebade7d081d154f37e04073dabb3e5d4cd8885ebccb6fe6ac5c33"}, "53f880cd-cd8b-4f03-b685-8e26a3ca7dae": {"doc_hash": "4ec0b7809b39323bd753c04101184dfc28f685035fc38949e621960aec6f4a72"}, "b47728d4-e10c-490a-9da9-712f188da937": {"doc_hash": "9842c49e7ac53fac3b6776bacd74117bc594a73e6c3815d1c834db1f3af3168e"}, "3a343cc4-40f9-437b-8868-bccb9e00f012": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "accfafda-224c-4f91-a224-3ded2f116879": {"doc_hash": "c763277575c3aa455078f142cca624ad367948d7d444da0132d31b64b2275038"}, "1cad1385-7571-4f49-a8d3-aa0846d34b47": {"doc_hash": "a5c8200ec81b223a534c1cdb957fabd834e9cd0c20e294448d0e68ea19d1aa9b"}, "ce0ab60f-d163-449b-b741-2eb1c7578f18": {"doc_hash": "887aefe8280da5daab0b97e8eddb81be33f199145dcade218f273e9057b80781"}, "ebf38025-9a55-4f3b-8b77-d934a8affa83": {"doc_hash": "a2a9d8364ce7c7f743dddce6240c0be7d8fa8cdb3a66ae49356e20e382bfd7e6"}, "4b0b820e-2840-455b-88fe-8136912889f5": {"doc_hash": "4406390e156ca00e522ee0c1cf5a7c3949dd8a5198af1531adac61c1dd90aa2c"}, "eea40c6e-3053-4fa0-9ae7-4ce87614052a": {"doc_hash": "0d5e46d336ee3c2d6419ee50c06e0de5913c70330cd45d31f1173b6e4850f413"}, "62a15cf9-434d-4e3b-aa87-461f55460f67": {"doc_hash": "f0a19a783897646dcfbb6dcce2e6a83907c9d80eead201ffdf1b3aa35fbbe702"}, "07b91301-91b9-4d5d-bee0-114c22b370c6": {"doc_hash": "d8bbd56d97c284d48834a5b5e174e9fe0c49128e24e000df3ea395d4d21409e9"}, "196ab068-bce1-4c40-bd5c-a25420c412d4": {"doc_hash": "7411ac7208ca63517848ba4314d1662678024974c005932831101621290647f3"}, "13a1e471-4782-488c-88ae-ae8a07d45805": {"doc_hash": "ddb464a87965cd05220af5bfef785cd83e1bae5a8b6a80c449e43c1265865526"}, "4174e529-5bac-40a6-a42c-71adf86f2e2b": {"doc_hash": "fe55e3b92c2d49ae6f7827dbeb7f3864a3f135f0bf89fd8bb2943d6e2c08e2de"}, "498e5d68-001a-486e-945e-7a4144989417": {"doc_hash": "5076dca470d1900fc270053f3a49f13847a4d36b84632cda4d96b911e70b7376"}, "30c6e9df-7d12-4968-b529-45020c041df0": {"doc_hash": "feca1f786338c66e658b18badb46fbba4ad40a8de6d12cdb85127bb88e53c533"}, "2b3a19c5-1ef0-4e37-b166-9dc1db7d6a37": {"doc_hash": "66902bc1024d8567e1c3e814fc242d5c5f5aeb1f2cec7ac8b9913437e6f2085c"}, "3332c923-efb8-4443-a217-d8d349fe2a6d": {"doc_hash": "25e9e6b3a36fa3b3406192679c153f2a9fa65f1dce0107340e0271a9346c0c0d"}, "64a28575-6889-4150-a276-f2b9ca6e08d7": {"doc_hash": "9641628789f8872b05cd875d68fc124bd582421411147a6198418ff3d2942bfd"}, "bc608379-63df-4bb9-a540-7ade84f76ec3": {"doc_hash": "2414f747c5214a010cbd5fb435ec3fc2ee65992a204e1cbd0b9cb63ad6efa0b7"}, "e3a94c32-7875-476e-af9a-fd1f1a3ae230": {"doc_hash": "c760c147f41105530dcbc6aa2d980284d7f88e3651c1d3224a5298d0107f9566"}, "5c165f2d-39ac-4820-9d2f-05b833b94bc5": {"doc_hash": "f37b256d6c915c632014d85e5117fb9f6057a81b944bcac44b2ef676f943b523"}, "39b200c9-1f24-4ed5-8ab3-1d71e1cd8ccd": {"doc_hash": "77abb73070bae8db55b13b61237337b900d08c166b3b8a24fafe08ec98aa47b9"}, "72638843-dc45-402d-9618-a3a03b43bf9e": {"doc_hash": "caab8b29d6734faf639b4e5b6f19d1b73868d4079d46988227424e67b2bb9e91"}, "e8aa1a36-af32-4dd5-a440-42a0ffc1fa99": {"doc_hash": "1ce5abf43bb3f1f04e956fe222af19343d012540e1c4837b32f925a18377cacb"}, "bf5a425c-53a0-43a8-812a-14d1e8963cbb": {"doc_hash": "f550289525db05866d5c221630a11a39ccd14ae89b8caa3d3311c23ccbe57cc9"}, "10fe7c4e-0bda-4546-abe0-4b71ad91472f": {"doc_hash": "33a77213d13d6c543018c9fce51f21d8577773e4914165ac65f2533ccc6fc19b"}, "6b885a6a-9e08-44ed-a3aa-5aaca44778f1": {"doc_hash": "bd97ceccf493344fb7caace22c915f69b9ab11d7587d5f796b2693017ee5f09a"}, "4e62801d-7a35-4ff1-aa17-642319b35c71": {"doc_hash": "989f7d2461a75eb683196dd320e712b17c1bb1d25415a833a8f008ade5921782"}, "2ff38da6-8d1f-4878-bc32-af2d9b203592": {"doc_hash": "9452105db8064b93c0ae4e118d786b041caf404257c4fb369889ae322301d76c"}, "a6527f2a-9a0c-425e-8591-58e749edeb0c": {"doc_hash": "8938b0f0b9e37d657c11049f3bec0760504dd52bd9eb977d478513ca247a723c"}, "67a82af4-295d-4baa-88b8-dc93e18b293b": {"doc_hash": "6b90180544476179a13ab48b45e471176b988ef54bedaf3d4575b71ac704734e"}, "47957cf9-b8c1-4fce-9d12-1767f143269e": {"doc_hash": "86cf07c1f5a7f31ce869bad5cb8b9448fb96f9a9673a4ecaaef828770c4c0b7b"}, "85c5bd4e-946d-4cd8-a653-51fec6567eb5": {"doc_hash": "60c9f9e6b0766d0c0b6fe7d7d0a8dc9ddac3cd417128a558201ec4193e675a6f"}, "d22e775e-a19e-4e7f-b00e-06df2a4a163a": {"doc_hash": "b140008536a6137cec6b5c684430d391309d775153c79d39f2ce430310bdd842"}, "0f92a0d6-1409-4419-a9c9-9ebed828fea4": {"doc_hash": "d984c2a0ea56645a8f60a91a40ab62066b085c97174f84abbe48ac4bc328ea71"}, "58a41555-a20a-42a9-9b7b-9f4ab164f94f": {"doc_hash": "0edd3601e8b6cdf1ab0d8f449fd46b51b85d62004eb157186c6c064896de76b3"}, "1fb6742f-57e9-41d5-8c22-274b9d3c669d": {"doc_hash": "2358ea03bcaa79617896563d91c512ee19abd2cfb05119888ebceee57ea7fcf5"}, "bd6c2965-d1ba-482e-aee1-51aaa8fca60b": {"doc_hash": "52a9d738a718bb4f92c73a266d022600c453ef184cf163eaf4c84467dddce935"}, "75fe0343-1e02-4bcc-81bb-85c584e638dd": {"doc_hash": "0d08678131d0d0d0883504badb1aa1362f4f6d6fae42135cbc90e0e06a7e431a"}, "59494823-b19d-403b-ad05-1360189fd0e7": {"doc_hash": "bfc6df80bdbb7faa12d6019f80b167f317deea8ef6112fcc046ef14e34ce69e4"}, "2d0fecaa-1342-4773-9153-ce1f0673bb71": {"doc_hash": "5f7e592d25b83d679942d5d3df83e8d831ba98bfcad25dc5d35621b9984b0894"}, "a0202601-d35a-4778-97e8-1416cb1d40b0": {"doc_hash": "d68860def14560b2ad8f2815c9a32177ce48ff8b28c0a7944e6a5516ce1f34c3"}, "47496805-f1bd-4ac5-9b88-61e65eab326f": {"doc_hash": "6c8ec8384d53b42e1d680f0e7b9954cb8eb90164e1c4a5f0e6563b8cc9cd7372"}, "8016b10d-8686-4b02-aabf-7264c1e5dd51": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "23d8dc60-6c7d-444b-bd39-f2695060e598": {"doc_hash": "96855862a2113a7e1eba881587d5960fa155d614d7a690fa4db8bd1e727b216c"}, "c15c24fc-48d4-4048-8a31-5b13804b3eac": {"doc_hash": "ec5c69d731af74c27cba9946f7ae4553b975dbbf1911f4e67e524cf85e2ee07e"}, "2d95fa0b-5348-4e94-9d2b-ec09375e4c0a": {"doc_hash": "8fd317bff629d73a8ed835470ac4a666207b300952ff7c35d92334f8567ee506"}, "29ce8a71-ebdf-4fe4-90a7-df77cadc0597": {"doc_hash": "23378401374110293e4976a2fb49f2c2684d6932ca574f9193f945a28826aaf9"}, "491ea2be-1772-4dce-a89d-bca86f9f0a1b": {"doc_hash": "133db8d981f707e0475ec37b5e7a2447e51096b6ab6d808cf29dc7f2a53b6862"}, "73f18ff8-6c82-4fec-9751-419c75de6f77": {"doc_hash": "9718ae6b563e8b9470e4e4cbc11a6029bec80f515904c18e8d1ae012f4d7aaf9"}, "874ecc9e-2088-40a3-8c9b-a0149c34740b": {"doc_hash": "e0feadfe6be01164bde7fc6e985a2e87ed33f78a13d8272ccc531b10a9efcf81"}, "537bc11f-d29d-40de-973f-98eaf9b3e3bf": {"doc_hash": "011153cc607b565da8ec964fd094fe2558233c9ead631e07d77e44b56e588713"}, "6476d238-12ee-4cf3-9af7-69a88bfcd53f": {"doc_hash": "8a29538f0fd5fca96b8a6ed1ec09fbe64eeca947af3ef10a2f51bbfa5af69862"}, "52691c9d-5a8d-49b2-88b6-bf5d28fff13f": {"doc_hash": "35ad75176489ec8f0e0c3732abe32af5631eb3224caf47707e6753f2adb536c5"}, "362504aa-2f4a-460d-b953-e469544af73b": {"doc_hash": "dbcf90dcdd06febc539f3949dac24fd530ffd77e8ee5f7a3fdcf15e9a606ebf5"}, "d08ab820-f6d7-4188-9b8a-3d4b18ad6fc2": {"doc_hash": "4a57dd0d4637e599085708ccab3a51289a2111642e41bc8eec9812eb110158da"}, "33be2da0-97eb-4239-9ff7-d7d9e5a8de60": {"doc_hash": "45019c61043a94a9811e2ba21eafc937389cbcdfa141ac89282d9a380d396e21"}, "749d3916-1057-456f-b57e-a87090801b78": {"doc_hash": "7a8f1804d3f0ccd4ce3293aafde5f7f121c8e274f3ba2cee1fd3b29eecad408b"}, "879c0ea2-9517-4705-ad8e-e422d4f0c535": {"doc_hash": "7ccc994ff780ee9363649364bd9672103abe68ae05c5b03080722fd938d74e7b"}, "5f18ff42-25b2-4874-b9c1-711b6730375d": {"doc_hash": "4e3c74830787b714d65070836ad558dc34a7be5646ec95a4a91da992f09b306e"}, "21d53f44-d41a-43a1-943c-f709044405f4": {"doc_hash": "d41b0365f6ad1cfc9174e3970551ce668e26e3f510b868e2a2d1235a233d54bf"}, "7c087c30-a793-40a8-98b7-f2b2709f5e77": {"doc_hash": "c2eb9d063b7c910aae6f0efa856fc6f29e228723359daec45c17b37b10f35893"}, "77cf12e8-62e4-4986-af61-c9c8c1c412a0": {"doc_hash": "b7e9018f8f1e660406addebe300ca7e7aeddb6a397d24f389dd80af7d42cd10f"}, "d92d8bdb-8825-4a32-801b-f4742dcb6c39": {"doc_hash": "d432fac5a1153836366e1ff3218b80fee475a65e2c79dce43f7e620a4befedb9"}, "83d23e43-aee3-46b0-8eab-920371d96849": {"doc_hash": "4d5675abe8dfde413179060715999828edcc9ab873092c271bdbac2c22586f50"}, "d6624e4f-7dbd-4716-b413-cd66f0055a69": {"doc_hash": "cf6c97d6f2801f3cda1a62a69421c9a0647ba2fe88adae1b7930dd97a968bd6f"}, "951c4c49-2d38-4604-a115-0f8dff56ed80": {"doc_hash": "72096dc610aef49c0b325db4e1170d03193acf0e6d4b6f30053189ad8676fe25"}, "73b29697-d1cb-4732-a6d9-815a63da0d05": {"doc_hash": "1da3c8dba6c9116b664388960224a48fde61316f37cc5021eb4a758e830bb28f"}, "25ec912a-b6f7-45d4-b7d3-b03cefd03e07": {"doc_hash": "5c651de06a0b844677bf11bc0a0faadf6a8f51ca98f0cd9683c40133e8be4048"}, "eb1b3245-8f85-498b-8c99-acfb26d5c969": {"doc_hash": "ffb0fbb94361b31b601856e5736cd5ff46b49f0a4b88c7d26ba595646e7cc68b"}, "dfd9bfb1-094d-4f66-be09-a912dcd9a4dc": {"doc_hash": "a1ef3480bf244985c5232024b01c438b6062e533e5842fa31dd9fbb9e6187ba9"}, "59e17e04-9380-4f53-9ac7-e9f2a0385485": {"doc_hash": "98e07c6fa0ca0eea14893f119c9cd2ab2860d066e1718650c7c08aed2827c361"}, "4c8ef361-db9c-4853-a030-522ec6535aee": {"doc_hash": "cc33f2169b1b555490639ec9f9c123ffdfbb0c4e87e4fe698ed136489efe2fd1"}, "d001133d-911d-4af4-9a0a-7ba952670862": {"doc_hash": "6c5d205160f35f606599d0e0bd327ad5ffdf4a032bef09807a4125f9644bf805"}, "4f1b0618-4642-4184-9aeb-70e4323fa0fd": {"doc_hash": "8c6926791d7da10a8f5dabffdf95b66a14b28fc91daab7a068264d889d42166a"}, "dc798519-767a-430c-a81a-dc7c36638aa6": {"doc_hash": "92171d41a3331f11c509acfcc38a127c365442b4db9f03f96b48893834d033b8"}, "124b4c50-fdd5-4ebe-856d-839d9f1538b1": {"doc_hash": "6618e7720bb63a07fadd3625fd75cc73de1af30c341d6ae6935a660b98cb5e39"}, "7fb080f3-6d65-484c-9181-880033258288": {"doc_hash": "6aca7b4ac8df92b2d83733e01a3daac0ca768179673c4ccf7adc1c87305ddf3b"}, "3302a6dd-22f8-4137-8488-8a627e163b02": {"doc_hash": "d183f22deab112ce2513a1a97e37535b28ebd6aeb01bdaf147bc89c466b7f071"}, "f8a9a860-2380-4bd7-842d-71fe3fdb8f6c": {"doc_hash": "3bb8a72e152794dd7c168a21c118a65e07d34e84c6011f04df835ab2cb5d9884"}, "12290d7f-3841-4666-a602-db5ddba1c7c2": {"doc_hash": "89547e9dc400c22c64fc227f6cd35658c09f77bf84eb5dc9242a58f098dce6ac"}, "8de76ca1-1478-4cf3-b08c-af18a5ae2ff6": {"doc_hash": "735d0001199a6c266307820aacada19e2861783d15d523e4400407866d2a8c3c"}, "63fa029d-cf64-41f9-bd0d-d4bfc46210db": {"doc_hash": "7e1bd2f6c61cb8d607295d75c1dec9918080c066bd79a2eef06b55471a1ce50a"}, "35dfe2e9-f655-49f3-9b1c-fc17d09e9a01": {"doc_hash": "1df5cdedad991a08baa0c2a75f915668993319ef52fe114f6cdd62ec7017ebe6"}, "eda0303e-0beb-49be-8d08-3ee5d9c8aea6": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "b73c1a03-565b-4223-baa0-5d9d0417dcc7": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "345a8a3d-4e15-4d26-82b8-ec307d36e7ba": {"doc_hash": "d608246226b90fc0eeb4e0514d88f274c79946586d848204030843dda8dc8eff"}, "eed8740c-838b-4ea7-8d9e-24766f5de35a": {"doc_hash": "d4c09c57b4a493199aff5c194d4a796eda57cd9d2dba051cc93622cefa74a7f8"}, "e186f9ad-7aeb-4bb8-839d-39009f37fbd4": {"doc_hash": "80bd13eb333b54fb32ec2f59f6311b73253cfadd2fcbd60c52c1733e6885135c"}, "4a598210-0d54-44a9-9502-ec10dfed78e1": {"doc_hash": "2a3066227969f1c2ccdddd08e010743a67a71fecc119b2766d9dc7395ff130b1"}, "0936adf8-4efe-4e42-a51a-30e6254e307d": {"doc_hash": "d32e55e8d45db1f8961272137359a0920aac4e77a64b3d3ee86e61d46c2502e6"}, "1c9b9f51-7f61-466d-b092-3e61f64ca6e4": {"doc_hash": "617faaac945168159936018f621a3fb312accfbb6467cb081d0c68e5384eba2d"}, "f63d7ccc-a7dd-4fea-9c51-4d82b8b42c59": {"doc_hash": "e7253550f07ccbbd8d3ae4a6d5c20cede7f01deee868440d99c46e1fc6471ce9"}, "c2ae9afc-ed09-4c1e-a1bf-ee09e7130048": {"doc_hash": "bead9a3b0c52ecf5acca182a074a6cc0bd455805b820615b63e6704529a920e6"}, "3ea71c8f-e875-4f17-9a99-d87468767bd7": {"doc_hash": "85574e57321e58c7d28e345d058fdc0db85006923300388f1edd28a88c9f3433"}, "88dbeb51-81c5-46a7-8629-780d2e47b287": {"doc_hash": "60c0976500a0b35aeee16e5b68bdb70ccaacee53dd12d2fb68949e4ab0fa17ce"}, "f6e43daa-7ddc-4891-a997-03a0736954bc": {"doc_hash": "bb72f388e725bce5544a7ff302c5533d9e250f616ededeb3f337fbaef313b5a9"}, "60f0ca57-3352-4ff5-86df-948bd5358ff7": {"doc_hash": "6be75b05765a2f31560489ae085843d2d39ad56a21c5138640184e6f352a254d"}, "95b20aef-433d-4a25-874a-ebe075d8b4b5": {"doc_hash": "ac1c7314ad1844603e7f2126f67c18b25403aca518439185697ddce5157228cb"}, "c4c09667-167e-4ce0-8055-26b96db88d24": {"doc_hash": "c497860618fde8f4dcf02c4b4669b83ecece38cd1ddde0a5272ed8f995f9cff0"}, "c0037b18-1040-4e5d-ad92-d82bc18c1858": {"doc_hash": "c10ec0b7ab90e672bd02db9f09239ba77d4ca04979b94bec361dc6c428bd8c77"}, "0b90b9ca-ce91-44cc-a62a-2d3c5b43aa71": {"doc_hash": "25a6f840415a8e57e8b2b3c9d2164be6176111333a12fdf109471893137a9e5c"}, "d66b00b1-e2da-4ec0-8683-65c1b2ea9b5b": {"doc_hash": "cedfcae1a809e876b36f8c926618976d0069b51616b63bfd9b5d338d7fe54144"}, "8c559614-084b-43ff-9cfc-7ead488ee6cc": {"doc_hash": "6c1f05e7f1b7c2876437440ddf9a0024e1a19cc9946cd670387afbca02b4930f"}, "4ce965e6-4a71-4430-a8ef-acb6044f9a42": {"doc_hash": "280b5574e9a01aace11b0096b12de4a0642916c2191f432e4c22884ccb5d7900"}, "67076165-aa9b-411b-9022-003bf9b5318a": {"doc_hash": "3bc9b116bcae6840be49e065098959057c27108392285bbbec68d3a635bdd9f5"}, "af130a67-c49e-43bf-9b58-50a950c1ff08": {"doc_hash": "d82ecf051691dc5d643980363f56c94ed5bbe9a9afb712349fe3deae2a7b339f"}, "eb13fee7-e50f-4d0d-9f2a-4ba29c7bc046": {"doc_hash": "ec166b8411d540a92563c4b06a1efc8e04969c82c1deeb5e365767306b969a50"}, "e73266c0-a56a-46f5-aca6-5ae3e27fa699": {"doc_hash": "81e33e5488240317462306bbb716966e14c7f1549b7f43f847319d45a987fc7a"}, "0e8b2615-740c-4739-9cad-9d4bc88a465b": {"doc_hash": "3776d8a44f0f89210f9796c57aea2a1f01e407ab96f51957ae27f173bba9186d"}, "714d2d41-4756-48a3-86a5-64b8f6f54d97": {"doc_hash": "d9d75f1f47b6a46caff1c3be7c7369490fc2de73778173c0376cfc2899f2c5e9"}, "db0cebe4-09ec-4fcf-9523-bb6f5fe36294": {"doc_hash": "49bb643a7c9458cef15222ca3d8b7cc8ba89df229fe2b3f895e33c43631b0d93"}, "d35650d0-7ec1-403f-b4fc-8b39878e25f7": {"doc_hash": "9e83ddcbe38dcfd54d75f73b219a651cb68dceb5f463dab302a30ecfffdc7081"}, "75584461-ea9d-4cc5-92f7-49cc25cdd0e9": {"doc_hash": "8d69e49511326e485f537b8fc7193a03ca7e52c28cf2afb4f067318d932f7472"}, "85b9f8fe-8a24-432d-b7db-a45a78223a81": {"doc_hash": "9ba3dd467b0a18b9057861959f32bec91ecce3ece98cc53711f0f402e36946bf"}, "46b57060-f7ac-4571-a868-c6e91749782b": {"doc_hash": "dcce4cd18082563e4b5e7d59399339c09c6351fc3a2af127b2bbe13ea809e43c"}, "eb09f4e1-d0af-4f9d-9124-6ef7f7280147": {"doc_hash": "4b022190d4242b181062e3060fd3168fa8fe560562fb56e7599d377aec7bf083"}, "919841c8-0688-4df9-acc7-e86a7f2ae6d8": {"doc_hash": "b7403d8d1645c41b03ac990279dfc219289643b89c526a4ab32558c3fce9fd19"}, "03f2be03-5506-49cd-b146-24fcbe01f957": {"doc_hash": "a8e53c90e7eb52ac96fa3598b1804aeba5cdccbf837062486c95db7a2a25ef2d"}, "b40ad265-d181-4cef-a479-eaec0cd332c6": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "593a8ed3-5fa6-493c-b5ad-513122c2e444": {"doc_hash": "27a204114b43ae7cccd5fe70a338fc4f2ec2afe5152f1d574839f6615f7a8ff6"}, "95768b69-1d3e-4936-b120-59e38eabf579": {"doc_hash": "d92d8e5ee51615bd2dde91f6e168e1e432940dd11a671fed36fc53aa53cde9ea"}, "8b6f7b7b-52dc-477e-ae8b-97277996c19d": {"doc_hash": "7adbc61b89623cbb381086eb5c109017f13dbbd8bf897ce85620476006dfd439"}, "195164b3-d902-4e34-ab9c-488282e01f97": {"doc_hash": "b9e4a9d6c778ebbaf005bd6162b161d5776cea15c1c4ba979dc64111bca70d0b"}, "c921a356-bdc5-4096-942b-d82378df18b4": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "8e8bb51e-8cbe-4634-ac6c-98bd7211de12": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "b38ed499-9796-446d-ad50-333c5414f666": {"doc_hash": "778eef1dbb2935c4dd8bdf2b7e9fc466be1dc6531aca6421ad429af0cb502fc9"}, "1d2bf0bf-8cb5-4476-aae5-3b1d76c63966": {"doc_hash": "4dc4dd40d18e791bcb90ec8a855e477a2463c010053abfa3b9ae645a98d25938"}, "9131b24f-6d97-4bd8-a5a0-355b25b4db76": {"doc_hash": "08d8c535becaf05266bdf65886014d466ee6b86e372f106417dbfa594403efdc"}, "bec3700a-1655-4ff5-9f03-c325b45911aa": {"doc_hash": "fd6e4c7051996e2ec551c8b79336d8d3f3c16bbd0b38364edc2ac448b7596474"}, "b31b14b2-30c3-4d47-ae4d-c5fa2eeef456": {"doc_hash": "6b71f80b1bdf7a41d2262aac1a2c5918436b241965c23f7a225552e486c5c6dc"}, "a7a48cd7-8ab2-4550-9ebf-8f7baa43058a": {"doc_hash": "2f9d200d8ad0bb346e4401fd889aa0ea7d54bb1bed9d8d938c2b481442f7d453"}, "b2c8bbd8-bcad-479e-816e-199bff35e224": {"doc_hash": "74ee7276d64933a5ecb62778fc1939d45e6e9fd9ce36cf5b04894ce3de175b85"}, "5605c88d-26f9-4dd9-ae25-29adf6c839c9": {"doc_hash": "23e7a018b6a042beaae95d897e2216de67846a4da58c45fb37745e01cde92b2c"}, "de4c517d-cab1-4e88-9e98-32bd65f36774": {"doc_hash": "3769a73e6a6a9acab2fc444a0de7bdab672beedb4f0a70699d91b6e66ba6a97c"}, "d5b9410c-6060-461f-80c9-1a079d431dff": {"doc_hash": "806101500f2201dd4026dd1a7c9a643aff5133a54e08e57fa3bcaab6d950cf46"}, "9ab61b82-4266-45c7-a720-45f93c985cf6": {"doc_hash": "b5bf242a562a97b394caf38f8ca858972ab5f1c11afbb428b23daa3cb14e88a4"}, "0f778f31-41aa-4d34-a48e-9e3b4cf2a339": {"doc_hash": "88bff7f35bb343cccc9e105ccbf382d393517b41afda24c9a45083ab0c4669b6"}, "b6911630-7e84-41ec-8d57-941199bb45ea": {"doc_hash": "8e87a41dea633d2397569ee9fceb5783f96b7bc201732ad5b30a4601c12fad60"}, "843e01eb-cd13-46f1-9318-12dab5e46341": {"doc_hash": "c91d7d2e8b5125ad502d88a6b49eb3555b5fb48631d39c8434e4c0a3022c53dd"}, "f9f956fb-1413-4767-b5c3-3c2a1ea68001": {"doc_hash": "3cdc5d61924f9e1fd877d6b9f7a377ae8acbf27ccf0665674951f3ef7cd7329b"}, "11e11b6a-99b7-4449-9e82-3691af2e3ae6": {"doc_hash": "6586e1405ede3351f729965bf60b5493eb3ffea954771b667475d7384bdaa5c6"}, "1b5388e7-9b69-478c-9d74-5deece8363bd": {"doc_hash": "7a095df300531c30ceacee25e7a1cf3563d1c2108fa2f14635c05527ad4f4279"}, "59c620fa-c73e-44a5-8f52-bcc51834e567": {"doc_hash": "a2dbfb906228e8531f837acec688ff1f32ad6a0cfd618c41a7883c38752605cd"}, "384aa737-d647-432e-a3e6-687a03a38726": {"doc_hash": "3376083f6b178da5f685ea6967ff28a77fffda3867151b07304fdbb956de1898"}, "7505b684-5e5d-4c1c-a0dd-c21bb425195d": {"doc_hash": "8b0f576d98d21bf1634282d1942437bec1455e63d1886120a89a13553e987382"}, "95399a02-52c2-46bb-8915-1ae0772afbb1": {"doc_hash": "2abc3c7a8653f3749490e89440ab348ef933a0084237b4b0b45cd44bba3d92fe"}, "8a0593a2-5395-4885-b6a2-735b2177eb55": {"doc_hash": "4d93a0ef3ee7081c72032a5adfb427fd17444b1785812771f0f3525e0d40e333"}, "d6d5b278-e94f-4115-8c0f-1bdf21f37202": {"doc_hash": "d569f2201f7fa8c2e0344132e454ff50438a122450ebf119f3dfa5e55616618e"}, "483ae7a9-035f-4a75-a3e1-fbe28e3109a8": {"doc_hash": "eb7eef0a5efb312921b84d0c1a6d8e65d8718ec0d8ce032fa867fd29293866ba"}, "504a606a-c84d-49ae-8b52-47873bd0cb30": {"doc_hash": "30352f0514c1a493c1bf201c6ecc682228f190bdcf88d929d630008abde5bc86"}, "d45a1def-af70-4f6a-8155-9f98e7e5ca47": {"doc_hash": "4d2b0babbd1e2fd16bb7e50fe0c8f398e620f79fb6b7665755e3bf45d2a52bc0"}, "2a9c34b7-7bfb-4d69-82cd-92ea5ce978ad": {"doc_hash": "273e8a457ebce4d86a74d215a14bb58baaa3abda355add950878241e191c08b8"}, "b7389cd1-68cc-4697-b77c-eb76d0ddcba1": {"doc_hash": "6438597c6e753302d8e9ff4c17679309cafdaf818a1a3db59d73f93f137d039f"}, "d52ea854-7804-4c8a-81d1-d3545322ed7a": {"doc_hash": "4ce1d1aed7b05d6477f06817f8e62f9772324f2e444da0ff69b753af91db18d5"}, "00f86e02-8911-4ead-8cc7-13ffb0337b37": {"doc_hash": "49fc069429abe6cbf5d8d55123bf0f9849b8aa27ee4849c3159c8502b6e6a885"}, "05ea65a2-5eeb-4a89-b854-7eafe2180cf9": {"doc_hash": "4377183b8ff0c46707a34cbe526eb425e6308eaa93e295b65b3977cd6a0e5f0e"}, "034d0216-d8c6-4196-9f1e-82d36a15ad8c": {"doc_hash": "dbec8584b371d163458044054a2aeecfce6096ab3cc65532313f90c79d2cc3da"}, "8c4fab6e-1390-46bd-a4d6-2fd115973002": {"doc_hash": "3bc591f9d4d8881a254bed0e0763eab2c387609749a9a69ebfc1eadf345baedc"}, "0dbcf08e-010c-4586-abb4-bc3f8e1b7972": {"doc_hash": "a372d865a8b62120826add4baa166a1a8420616803bc18ca4bf836fc7976d548"}, "26bcc9c5-b2c5-439a-bcd5-59f0d7b6314e": {"doc_hash": "5242dfc8eb3ad7be1a4bfcd248bfe7d4e43ae1adc661b70ec07775c18825b023"}, "dadd8d88-008b-4a35-b3ce-9c1d6a089e5c": {"doc_hash": "c43f073f9e9b2ef95daa3111ba495318768389499546f33f7383b4ac65417eb5"}, "e23216a8-c2b3-421d-a992-d9008946d8e6": {"doc_hash": "d3c61f89eded8ae5500573f7953132818df2b9b9d5a49a6e8a38ab9202056035"}, "8f549920-ed69-436a-9b02-bef620480b9d": {"doc_hash": "9a68a4ea4f049d1eb761b07af2d88b5df74e00ed55d054c2fe9a3255ad89e8ae"}, "388cb89f-2a82-46df-a496-441d6df6e64c": {"doc_hash": "15589700308a3436a771d05f98b57925d834e1482177b5c0466d889762008706"}, "2c683c1c-e3e0-4220-932a-51301c1b07e4": {"doc_hash": "9d996668bddf88f0e355286a3cb998f92c19ace0c1c3405c62b588089f23f0ef"}, "a5561d24-1aca-4b2b-b322-dd5ea7226001": {"doc_hash": "d4e2b364723164ef636ff7f79f452321d5e215724ccf72c0edf676860efa3d55"}, "79809051-c57e-4978-bfee-b8fc6ca852b1": {"doc_hash": "063cdbd150822b50b57cb3c3887d9f901f50111e0a09e21d881a91af483a552f"}, "8d237f0d-f526-4cdd-a73f-52644e70bd0b": {"doc_hash": "ed448cb595690fb587d9e8ed5c0565a5e5e274a4b6b96079298d1b248a8649ab"}, "b2bd46fd-96a9-4435-bd28-c07e274925c1": {"doc_hash": "cecb67aa825768c28bfaa3224a8b027b476a0e8843d10ad94c6a0f57981ed2ab"}, "017d6d9c-ed45-4169-95a5-59ccd6a35de0": {"doc_hash": "0c9cd7c1a03c37aead3c1932d6d6bc036790e70e474b7b0915e0aefeb6ee92c1"}, "1aff4c98-de25-4051-800d-7b57d2ae66ab": {"doc_hash": "2176bb1e5609d2d0c6954845f44c87d4475c40dbff467cf6eb0ad99705d7e1ad"}, "e514d2da-7c09-41be-b0a7-46492578fed4": {"doc_hash": "57f57f4cd35d6b3d10094525e1ce5220aee92eedd80a03415b82e027544c266e"}, "c6aad94f-5348-48cd-b55f-99dca572b39e": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "e3ec671b-22ae-47e2-bb4b-fda3fc2fcfe0": {"doc_hash": "45cd4ab69aeb1e081f473c305a3c706b2549c57930868cf3affb7ea73caa74c2"}, "2d55a1f2-1d5b-467e-92ec-332f0adc5247": {"doc_hash": "e41e4c5cce07e8f64b49d994573c80151699e57fe01e631f48a72546ec757439"}, "f372b5ec-85ea-449c-9e78-4f9d9ed6f908": {"doc_hash": "16210ec63078e56e0a54daf62fa393f2581fbe4df9496e5376f76dc64809787a"}, "9bce9237-fefe-40d4-a719-c3f2e335cbf8": {"doc_hash": "4551b13d24c9cd1cd92c8e4cf561417ea64795a20ddd70ce54c20a62db9d2ef6"}, "1818347e-aa20-4baa-a8b6-c64d667cc55e": {"doc_hash": "81f99ca9b03c1fc8e62e8ef92ec9ea36b0bc7ba6320402bb0eaecb4e2f331794"}, "d3cf5a39-ad8a-4105-abd2-261a9d97db68": {"doc_hash": "3903dd68455b00644b3e1a8b96d218a18f0f4384ccf92babec6a820eed6f73f7"}, "3c384dbd-602b-40bb-b715-e2917af954e6": {"doc_hash": "2e588dd6d40494cc125feae25112c59ca83f9c52faf38ad96d0e32687ecce139"}, "d641463c-1bef-4129-a1d2-fcca7062f840": {"doc_hash": "40bd9cd110ad4fcfda91d449f8fb11e677d505159a16ae8b4723377a743b2081"}, "b3d6b3e9-d57e-47e8-b6ca-d763851c7509": {"doc_hash": "3241c79f50d953773629bbb8c130a8bcf094ce59a728f49f80f970c906c03bac"}, "0ae2a488-63eb-424d-a1bb-1bb5fc66abb8": {"doc_hash": "7451add99048f19c335e6a1b0be53c26b27bfd6fc99ac63b1f9caefa2db389f0"}, "ae70fef7-8300-4626-9199-205f444fe720": {"doc_hash": "ca2052478dc8a2fa40ed69a909f7413d32c3710b2804b1ed6448a54f9ae9e9fa"}, "a63d4b5a-cd72-4508-bd4e-3bab18a77731": {"doc_hash": "8cec8a1ae644b105f3fbfce7bfca31db314edefc245a0c118cfb84d59094d068"}, "5f282b10-aaf4-4c9e-a7ea-ba2fbe971e76": {"doc_hash": "6cf4f1a1fa910e3c4dd6d7e6e935b96c2b2e517c6c6e4800a9558844160cd68f"}, "b73212b3-5dbc-4053-8fd5-6ec06b7ea30f": {"doc_hash": "61ab35af0d6b89c6773b99d7900065b387deaf5369edd36dccb5c6b33a1b6b7c"}, "b5fe18cb-8e22-4ebc-b789-315eb7098b4a": {"doc_hash": "aaeda53c7035187d65e89e7d9227d297ccce3f42d446f3213cd887a7317843a8"}, "4f8184cb-8aaa-4941-9835-ca2235fc1473": {"doc_hash": "b18f579c054151a30012e0fcc3b7a394079ee6b239ee92c6d0fc53dcc92f6cc7"}, "55e64ba1-3626-4432-9553-4a725df12809": {"doc_hash": "f3214769b44fd383a1aa83d1648b375e4362d53f87f28b84be0fe723dd74bc07"}, "d4d55bfe-7b55-41c1-b30b-0a02e339b3e9": {"doc_hash": "7db9a6e22accf11a073a8a49f090287e5cddfe046d77eda2147a8c8658e79073"}, "ddea5aad-c97b-4e50-823c-791b6faea3a6": {"doc_hash": "7de67693665f34099e401fbb97b6a353aa0992c0a5fea454b9365d46944962b8"}, "807f5503-36e6-4f89-a513-2bbfbeaba8c5": {"doc_hash": "82a953543fd441c41f173ef05a7ecdf55d2fbd0237f39a75edc57194a222b78a"}, "12d93309-5e41-4d01-9f3f-025abf8714aa": {"doc_hash": "b67240a82c6a2149e1ac9037ec6cf67233149c94f346653da14c46c87fabee6c"}, "7e02434b-e831-4ec2-807a-1f1eee14ba9f": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "4b60672d-8eb8-43c9-b9de-d82902f8df0f": {"doc_hash": "36c459f952c16ab58fffce6b33b6ad5b952122834aa5cf59c19c5b158751cb07"}, "dfebe986-0279-4307-aa33-6308ee7cd67f": {"doc_hash": "f5efcf3d309bc12255b70493fd901282ba53846f76de17f50fc9c633a0812701"}, "86ffb901-20ea-4f43-a051-a25a91325180": {"doc_hash": "09987a852bff3c9a4e942380c80ac1a4b9de892bd437dcc79e7bfe309d2453bc"}, "91cc06b7-7f6d-4bc6-92e5-e9ec0d649a14": {"doc_hash": "5a71834a056fcc2a40930fe82de0fa9f46706777d74614c4ae232ef1571ba8b9"}, "5d3fd03e-4fd8-44d8-b7ef-393253e96071": {"doc_hash": "565e496d19d1673f14cfd2addaeca395e8e231dfa01e220544b7393d226d78c9"}, "9438b325-5078-4032-ab75-23aee677f786": {"doc_hash": "2e63eb3eb38c4550fa6a875f669156f7c61840d0656b4429db779800334787d6"}, "a9b1a814-2bd1-467e-a2f8-90eea72933a2": {"doc_hash": "ab16142c4301c99a1da2b7a0b16c9d9ffb5d7988241ef999a1c500446ccac951"}, "db1a1697-d138-4b24-9e51-4abbeb69f6e5": {"doc_hash": "690c58eae1667c01e0126f1887b47db85ef6ed0860884a785fc60e2df36a1605"}, "078ecd0c-4fe1-44ee-ad86-5bb08b4acd9e": {"doc_hash": "80f64707fd2cdfc776e01bc34db38a8290f39ebc0e7534580c4af3d43248a378"}, "3a7c549f-524d-4575-9fcc-ef049f75fbe7": {"doc_hash": "f7be7416dc6dc76de9671653ec8deb0b01d47b088d717d2a7bb5e7051875a203"}, "dc153b65-8fe2-4645-88a1-dcd4f44b9695": {"doc_hash": "7f120d32fdbf9ed7a7705d35338402dd8007870514cdd56819b7a9d321da1fee"}, "439bd428-700d-4a3c-b961-ea2246b1e6b3": {"doc_hash": "ceabd31b6405626a6c3b74cbcc5108479ffeeceb3bf230ab6b1a9f9a2e1c7ba2"}, "609dc07e-9863-456e-8249-5e6d274ad36c": {"doc_hash": "83cdc4162f2a60f73f52c06671c36ca05c16b49b7c8e8b08a7b6418be570e15a"}, "9bb75d5e-fa94-47d1-b1fc-6778db6f4dcd": {"doc_hash": "9beaf4a107b1132cad7f83fd883aee0138dfd34141d2324027f40b6572f35a12"}, "dc8b4ab0-8511-4f24-a921-7cbf29a03d9f": {"doc_hash": "4e1bfd0cb880189e16b4f30bc49b2e712b9bcd6611627101195fd43b85eb4f71"}, "a4ab655f-0313-44ca-ad7c-a45e7c2cf1e0": {"doc_hash": "98b6d9c7c4030d63858f3eda6efa286a81b9f9c25cbddaf13b03249cf6091e01"}, "5e8d4c61-96a6-4108-b38e-c57469a5e0d2": {"doc_hash": "eaf182657d6bb575ae53aac74166bc20fb07f6e7887f3f2f104a12070433956d"}, "344fb342-dcd1-41fc-8635-c0b301550fa2": {"doc_hash": "5dc36f501f646001fb88784aa476f54566ed82fa4d92b25f5a6331e3939a7132"}, "faff5807-f23f-4e13-ab66-53e25c1bb2df": {"doc_hash": "bb615fced979830a99ab90c4f46cd3445beeebe1d8f85ce0d64b2bf89471f0df"}, "faa21e24-2335-4321-8210-7f4d58b666b1": {"doc_hash": "4ef34af096aff8ac61fb8e09d8cf0eda3db07b0598b07c511081b15ed98c781b"}, "6c0350b0-7685-43e6-bcd0-765f0a95c059": {"doc_hash": "8d7725d76427a0956ad6772c4872dcc809255a60262f68bf5b0730b6c7012dbd"}, "54c98d2f-258a-4d13-a0cd-a7f9fb29d5e7": {"doc_hash": "b03b583859eea4e9e0d15121925344684cd181afab42bfc6a70827cbaae67680"}, "17c8aff2-c905-4674-98cd-f21304fe72a4": {"doc_hash": "9038c941f8b7faeff86d318994de6e277cdc75f5ee4f5b65782bf0c75619c414"}, "37142b41-f8f0-4d73-beff-2c98ebb4e9b2": {"doc_hash": "819c11314edd7d2788306d3538c28e57eb4623220f463b31ceb07bf7076a3572"}, "f64c40cb-e6a8-4648-b6ec-2747dffc2df9": {"doc_hash": "f7d2e5d5aba91aebbf1f88e961822e072052ca05079f5c680873f5184cd1cca1"}, "9e97586c-d232-4cec-831c-c5e0892d54a4": {"doc_hash": "08cd4a12b63e3e334e90b994ec53540d45236396b2b9f7d3b2c0f8b49c0317be"}, "0868aa4e-7f15-4586-bbca-f7524768c0bd": {"doc_hash": "211a93cff0fd7ed1595fdbb6336d628df78b17af89df29f22b8ca84487d64aa7"}, "5425a85e-b5ad-4dd6-953e-464a22a789ad": {"doc_hash": "952a05a3012a26577742574126bbc933732bb82a56b99697ac9de1f624b36062"}, "25d505de-fcd3-4b28-b6c5-c61c3059ccf7": {"doc_hash": "f81389549eba4a73975c51c65453aef42ea8d2bdbee5a524d44facd68b484cec"}, "0f4c1cd9-a48c-49d5-a4cd-9da444550806": {"doc_hash": "f11895b255cc6a47f1f56b954e14e01b90c70cf9acaf50cb05e966da6bb45579"}, "692e4d1a-9030-4f99-97c5-d881fdd6131f": {"doc_hash": "eb3e25b4deda47f0e5cc23a2b7a92194b48062929163292d80eaab609ce71adb"}, "efd61919-c942-4d87-9e00-f944444079ef": {"doc_hash": "241b502f6e36f7c869f751ec9aa4481c01f3b62a0af929aa67d5e4a0f8c30aaa"}, "26e77fce-5744-440a-839e-633e524ad6b9": {"doc_hash": "c036edfd3e7eb89eef4683b33415afc4b7b0be52f6111336f377717b40b4eeb7"}, "86643081-82a7-408c-ba32-a5e5badd9400": {"doc_hash": "ce6d6e0eb44bec283416bfb891901d53f50487925f2a6630caee286129d92bb8"}, "aafe8ec4-cf51-4792-b20c-6d0042ba4a13": {"doc_hash": "1708513345821b13e99c673e275424ce4a9def31d3aaf02c9d0794ff3bb5e7b2"}, "ffd9175d-2101-4323-a1cf-1487f936a32a": {"doc_hash": "a013368c9448e56feb897aa8b7ccc5b04409544bdd6bd9b01d50f62dec9de823"}, "43d79507-4bfd-4089-b9da-cfca60c33aa1": {"doc_hash": "e3aa8b1a64952947fd2b0f5f6ffc196baee5a83ed6f4bd49283ce2a4efc8a404"}, "72f75733-9d8e-4758-95ad-ef2bc7cfcab0": {"doc_hash": "8c7f76ba88486c2eae5143f083551d66f68364ff3069781b3b08a86453d1dfc8"}, "297a91aa-162b-478d-bd1b-0e14265aa459": {"doc_hash": "78d786d43d1d65cdb5fe6597030c36e61c68c79f518f98eb7b040ef2753be9be"}, "a458f7be-3ba2-4ac7-a214-36d7e556600f": {"doc_hash": "de4e506cabc52a705922cd996b9c06583591be2e1fa3f3b1f117146a37007806"}, "0cc3edc6-2603-4fdf-934a-f264119a3e9b": {"doc_hash": "0d03087d1979e538f0db495ebab91e851a5f54b254f16ecd913a691fcd2979c1"}, "90c905e1-5ea6-446c-b280-4061f7830663": {"doc_hash": "cbd2af0edcf8be19b5ff95477b088bd5eb00db9ea825b7a4a63ac03e6ec243ac"}, "6794b52d-e16b-472e-ac32-00d8f30f0870": {"doc_hash": "9bce37080b7d17c9d8e2073c73cfef5cde187bda70063575bda98a3a2af3768b"}, "b65208a1-da2e-44d9-b943-449420968989": {"doc_hash": "eed0004dca7972881acd57b06f5f596a11bb84069654d7266ec668a3a94bd72e"}, "17433cef-5ad8-4e43-b827-07d57be06b58": {"doc_hash": "a61be2f27c8002fe0b61061728c6439b5dd8f23532323a296482040fc5d95cbf"}, "f084b401-eb4c-4664-968c-ecff59af92d2": {"doc_hash": "732c85be8a05d1a999326de655a00767b09573181225ed509211114dcf70f527"}, "a1984c0f-7d75-46a0-b135-b7d25cebc7cc": {"doc_hash": "b4ec68d07b68bd03a276f31f04236257ef969edc6b0b3aff1a2a3b285ace6b29"}, "24f49a3e-a823-43a1-8e11-4eb53efcd002": {"doc_hash": "da797f0963e8556ddebedc059b41737d653d24e1ad6c402fac3a1533be1fe72e"}, "9b0bd982-d606-4186-9c30-2700f52e4f78": {"doc_hash": "0c91f5361f99ed002c486a7abc1dc7f2b947d855b55def86aa21f711998d8b78"}, "5062fc32-8fcc-4657-8dd9-32aa692b11a0": {"doc_hash": "70a7c123f7a8026e5745af48e8b79518cb60f0a607dff45482db7f25d853c98a"}, "1d50e18f-ab9b-496e-b029-a462597b901e": {"doc_hash": "509ea37fa24e558758bfa840b1010b26203ac4b2b28ae8a10a6fde1b4b419927"}, "6f0c8856-047f-44ad-b6a1-c854209e5789": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "a901a4b1-b8a5-4ae0-8c4c-2a48d5d7b51d": {"doc_hash": "9a77239b4918f2e38c51c89da1312356da8c0358368d9d376ec5947a8a4a6871"}, "3cf8bd25-df63-4537-bd80-9fafe18103fa": {"doc_hash": "9d997206ee7c65a14b1ddf6d9dc38166df3f13af84f1886e68f9bb239c4c7493"}, "d7029599-2a8b-4350-aa5a-429aa86669be": {"doc_hash": "700074d80ae1ab198892d9b526093da3259b0245f5bc780dd992ae895c40a0cd"}, "afebad18-6729-4eb7-aa94-e1a408169132": {"doc_hash": "8555edf1d143884c66c9e58da079ada12f3cfaebb06233ece21402ff549f40f4"}, "1922822c-7f61-41e8-882c-d455ea7dd004": {"doc_hash": "f2d1e24788faca30983d397fa47ec3451be8cec9c8761f2eff528601b277a7bd"}, "06f08744-8672-4555-93ff-8b27740a7105": {"doc_hash": "ef7cd7fb2618ec194c4acb3e201ec1445ee016dd75f467c78c95f997f6d82e32"}, "e8369408-a7a0-46cb-9641-fe7b484dfb39": {"doc_hash": "81ae3ce0e6f4ba8cd07eed98a8665803f2d1b416068a2a40e8af3b2c7beed2e9"}, "6463ed32-d90e-4ae3-abde-fa5cfcfc1305": {"doc_hash": "d739ddd17fc912198956d2c6e1b9fe0ca9482b887fcd2094ca0badb5155d1a9e"}, "08274d27-5b3c-4259-85d1-00e40ab9e6db": {"doc_hash": "57ea403569849364af074f4b75809a148ab277f73d53125ac4adedf24b536b29"}, "c16c5a69-4a10-455c-8808-8cfdb8891d86": {"doc_hash": "47128aeebe6220052c462524bd8a95b3efd725f10d2503d29cf156dac6617280"}, "6bc6e1ea-2743-49a3-8414-fff1a177faeb": {"doc_hash": "ff067445212c9424d6b80bd003fb3535d4a2fecb88cb3104e4d3ac82353c0f7a"}, "76901f59-946a-4a22-ad06-71d826f3bdb7": {"doc_hash": "03e21095aefb33efe073c2dfcbd16ca01264c86fe41a4b7331a0a39e626e921d"}, "280c5167-53c5-4c84-ae47-536a354972ab": {"doc_hash": "93a5072cea45771fa1bd9218980d499b959ecae667a9724e1cbb9a57f7f32863"}, "b6689f3b-3150-45b0-852c-663332516c6b": {"doc_hash": "8008d6f3a5a8a64b2a09f75c6b6c85751a7a66fe83fd14f56d519db38a64a1ea"}, "6fe95aaa-bd99-4cad-bd5f-b5171fdf5793": {"doc_hash": "a1564dff255ec30d3ca4ee9414338dc606007d6828d2b22e88fb7c6a1b69bd70"}, "285e2dbb-5bba-47eb-826a-aa493ce6035e": {"doc_hash": "f1e639a9943cf5a2a16dbd89700f65f956a3001118108a27545bcf966f4e8593"}, "fba866dc-60b3-4fad-b3de-64ec560e7a0e": {"doc_hash": "677e81434cddd75821a4f81276e303082d7d87bd515d79419f5a205e6c113e3c"}, "3d919f57-9e0c-4cb2-ab1a-a404211b3290": {"doc_hash": "9b4871bb937a7d17ac2e5e6dbe3ee3fea8e55504ac9567a4e10b918bbf70d6d7"}, "a1a825b3-44fe-429f-848a-655dc6e4c2df": {"doc_hash": "6424547b2eb253b3c73bfdebf341431ec91f722a393fd13424a6da74ffff6ea0"}, "870e626b-c339-4276-beb3-5c1352e52297": {"doc_hash": "180a1175842f159551034d6680dccd1d0e190601a1f97f2a06efba0262ed9926"}, "0ed1ed0e-1db8-4616-b55b-d5e6628152f4": {"doc_hash": "feca62c1d242da38083b67696ab0deeb315c651ba445851b89a0f271954ede2d"}, "4964c129-c94a-456e-916a-33e51414535b": {"doc_hash": "b13c7f5b82a284bb40e31c7dd37b8c07b348ada3bf5adaaaddb331dc64ce2e3b"}, "8c17705d-01dd-4013-b142-da1c13c79ed6": {"doc_hash": "1ef50bc16be737d41193e313ab0931358d7bb08742ee5a18b13daebc82a8293c"}, "64fffc0d-b0ac-4198-a2b3-fc555b5d8eee": {"doc_hash": "538db98c9261acdaad4bbfbff454e8f611e9a74a96ab948853f1468662c85bf9"}, "7ec1b229-ea8d-49e2-810f-43774a11dadc": {"doc_hash": "6fd71e7f4d78dd29465cfb99b71c7107f67b9ab325f85b845457dc4bbef8e987"}, "8326b6d6-b9da-4823-bb9a-6206d306e680": {"doc_hash": "8836d92fb65d665b2e2f68afa0f1f8726910647aa01c6541aeb483ac67118108"}, "d3dd3086-f639-4b29-b85f-44b42d7311b8": {"doc_hash": "e46511e8864947be21c7453b645b03ed655ab922f643dcf8ae187d3fea920b05"}, "53b432d7-598d-4280-bb3e-bd446d7cc82e": {"doc_hash": "b3368dce0001ce1c7f2407c254a6322d91198a7a4a37482b39176e2ed481a262"}, "3337c29f-6428-4c88-bba8-26230533f988": {"doc_hash": "220d93ae1b8557a23ed7e9cf2181f31b19cfaac7bcea1b5ff0bef45d256010a0"}, "da4f8257-8431-4e24-8e8d-8189011b9d3e": {"doc_hash": "1f55ab77eebd2b8c4f1987a29c86172c1a218a2916149cfee93b765f23c6496b"}, "805aea54-bc7d-433e-90be-4d8e421a1b25": {"doc_hash": "65face958dc719f14eb725b0f8775e6e43aeb11e8d52239382041dfc4069a9db"}, "dd11957a-d0e9-46f1-a2de-544a1101cb70": {"doc_hash": "327a97b3fb152ab7ae1455999dc8eac82b1ee2fc7e428576337272d1367bd005"}, "e011e8d8-0f71-4918-b56e-0cf1f9df4e58": {"doc_hash": "5a3cdfc4efb8ab2fd111bc03a4f41355a4ffb64b9d2ed5cc144a144236259064"}, "cc5b80f8-5814-414b-8e4d-4a3056d6b9f3": {"doc_hash": "3923cc8733f3178a88fc802b8547991bcdbc445706cb50c559cc075cac9497b3"}, "b7185b27-654a-45aa-80e2-62b8cd9e817e": {"doc_hash": "9f46c94541d71dff3e282f12f87094f2c5e43d130c707035917668109eaf688f"}, "7da5b45b-7598-45d9-998d-2531afcfb62b": {"doc_hash": "1cb6c429010143712c12c932a3d6ea8f8ff5a83e9393ec40cf9ea627ef38807a"}, "2ee31911-576c-44eb-a412-8552d0ffb9c9": {"doc_hash": "06d7da3e818ee6a1ce9e455e5af978bf83e632b72b97610ebd7e959b460d713f"}, "3f512eb2-6e49-49e9-b014-22933af6692b": {"doc_hash": "7bab17b9dab309d1c51547cb1e9c4c8e45d3b65c7f63d1bb6f4232718b783489"}, "802d2266-c773-464d-b233-cec2ba0e113f": {"doc_hash": "ef6ef8d607a311ab5b62614be579e28bcc8d51106da1685dd47f72183856afba"}, "aa37e8b2-13a0-4eb4-98b4-c84698883a2c": {"doc_hash": "0fb6896c763c33c49d5d177c533ba64bb90e1baf84ef44643b2e1ceed740b175"}, "3f9f5dc8-7542-499b-a32f-e9f3fc19cd02": {"doc_hash": "efa2e0d149d6ab1df847ea84a8f0ac961b428874bff30c6969335fd9b2ccd4d6"}, "e301a2a6-da2f-4aa7-b180-c9e47279cad3": {"doc_hash": "5543e5f88962336ec2c11969e02a1aece534230ad0372b2cab3a85ca3ea7bb33"}, "90da39fc-2439-4d31-958f-025eb82fee62": {"doc_hash": "aaba2ae121464e9ef50359ca0640aeaa4a813fe12af15ba3765d4e403dc40f9c"}, "4294a845-4635-4233-9e19-aa7d389dce76": {"doc_hash": "134bfd404e27d0bfb496d36f341149efdc8aaeca8adb707c575ef68bc4c53984"}, "2c45232a-fe8b-4edd-a0b7-20fadcd4f31f": {"doc_hash": "2aafda66a9ae43c62fd84aad3778c0a052742953bd51bf430d8c12dd7990dc99"}, "77802f9b-ccc2-44ef-9254-8700cc5a8c4b": {"doc_hash": "a6fdfb871ccb20a19031ee5d196deee1562b2468b105007c09b4bed737910e28"}, "4abe58bf-6574-4b43-8a84-bd5af396f967": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "065d5d0c-83ba-429e-bb5f-98493f53da54": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "696d5cd1-317e-41a2-b635-958f4ac7a36c": {"doc_hash": "5d396f1c2dbd7ada27c9d3a331bda7daa0c5317c06cf871ef208ca4fdca308fc"}, "a0a95923-7449-46b7-8ecc-0066c504ec5e": {"doc_hash": "c3423c0cbc634acece1df49c993a76712debbfe2ea18c865db26c70a780aff04"}, "9ec43339-e699-40ca-ba21-a5b5428a097b": {"doc_hash": "be18497103635fe22ef96d68bcef9bfa5a27345fff35d490b5c4854736f65d77"}, "f1a57b63-993f-4ce6-b6ec-86385572464f": {"doc_hash": "be5aa74677e8fc62b97fd1421a1bbf3a9efda073985e913c762f5912bcacbc23"}, "29d45a3f-475e-4287-93cd-c7a0cf01754b": {"doc_hash": "dfb31a1b1ade6139b3abbdf32daff45e1792df5eefe99050c559f1236a98e548"}, "d9f7158e-35f8-4d8e-bb54-e30a6ea0b366": {"doc_hash": "344dfb68691219102fe80e64e837bdedd33ee187358a256e80c0134a16ee3ee8"}, "c8de2432-1598-47fa-a541-d114fdaa0991": {"doc_hash": "34a4e7477b2185186bd4a371b439c01507c0a1586fedc0955829b3ca00862327"}, "cdb01ea5-7da9-47a0-998f-79c69f9f42c1": {"doc_hash": "cb5f0fffa844f5f3f8f27fcbfe8da4f2b56829d6482e787e3c2ea69f99c81c74"}, "ca285005-75a7-4c7e-b2b5-46e62f3530de": {"doc_hash": "5c4ace2ab14b8865ae60507c73a6896aa4958bcb695eb9d4686def446fde8574"}, "6be3961a-8656-4d71-86da-09c94d726dfc": {"doc_hash": "d702b8f1081b8596af725e478423a34b4e51a0e88558372edb6f4f8a6bab4216"}, "99e4b054-2d92-4585-b778-a7ca930409c5": {"doc_hash": "c91c7e184c7bd1301860829deb7f921fa8fe0ee97e3e9b5abfd7bc6ceb1366b0"}, "3d8a5ef6-a6bc-4498-a913-a6ae3d1c049a": {"doc_hash": "e7d79f49f9dcf5e8b522d2b2981101587bcbd2407fa9404c94b00fd0198a5ac5"}, "7683e053-388f-42b1-98b3-b6d1eb9d9636": {"doc_hash": "33e578260cd29919703853bb2bdfb74dffd1ae2fa17b78dd2290b963f653842a"}, "e4048736-6fb3-4e67-8272-69948092b646": {"doc_hash": "a1ffe8c2d2c437849dbbb765156e803249c39d0423c8cd645a3e964af462932b"}, "7c02a25a-494f-43c6-a565-446fb458179d": {"doc_hash": "75d3e4e0ae488a8563d97062c990b28ea7556b2a73b18cda7455fb7807545043"}, "6bf757dc-44bc-414b-ba9b-2eb96f452623": {"doc_hash": "f2223543a921a2de4318af0dda329fc84fdadadcfb8e788b0ab12f20470698ed"}, "6aa317e1-ae0b-4749-a6db-c673a5e7b5a4": {"doc_hash": "b6bc35fb488e684a9a7443533b5e8b58d8d431552b9294baebb3cbb5a0b40703"}, "61c9d363-6a29-41fd-b772-95058130ac42": {"doc_hash": "0bf6bea63426b815bddfd4dfc3ee95eeb5d615a6b2fa9d4279385058b00ecc43"}, "3129500b-7b60-4971-9871-95973d3d53e0": {"doc_hash": "75971e6cfea16607e73b6750841160a7978af373d8d96546bf32810afd6cc34e"}, "5544f9d2-9915-4226-abf0-c3ee6454199e": {"doc_hash": "a09fec2fd245aec8a56e83e703b4da0cdd8f1cbaff8f8cabba038b875e3a8871"}, "287fdc18-ac7c-488d-9a6b-63a934d73724": {"doc_hash": "49b1e78ce6098737b0dd177f20e5c1a8f5d86deace8abb7af60cafb420df8ef7"}, "03d2d7df-6831-485d-bdcd-76b4ec0bfb08": {"doc_hash": "67d9fb3291f8f5d7db959e4c492c8710d2293b6f3aac07c63ff5dfd5b9e10fce"}, "2af1954a-ab08-44b4-8593-3b714352b5dd": {"doc_hash": "ddd090ee704ff054d09ae009fbcb147a12419badf560977f975a7e636cfd4792"}, "cf0d539a-a3bf-4b8f-b151-88cba71d43b6": {"doc_hash": "1ba41c65bd083c329e4b794d7ca14a4ee1d838097f956b4b1ced72badd43c52b"}, "01073416-15d2-4ed3-9569-c3a7758c2fbf": {"doc_hash": "08efdcf31660a6c897fa16ca00efbebb59aa7c5f8294cdf2be5a9ce5fb7f6cd6"}, "054e3658-ddc7-49ec-810d-a6de0f47add0": {"doc_hash": "1fc7a1337aedb62f8cd685c5f18b7977219fb2f0a6cde579ac4befa9e9312bac"}, "bd6e70eb-c1a9-4d6e-8a91-e9a0845d9ee7": {"doc_hash": "af89bfaec3140e94c0dd65432ec01789f20c7f33ce5c6cae4963e6fcddecebf6"}, "95dc23d6-cde0-4dba-8678-b0dec0c87ff1": {"doc_hash": "3be2ef50187b55c64aeb20e0ee3b80559993b767300ec95848a9717e08a756aa"}, "e25f08ce-c3aa-4165-a5f5-bc0a06378411": {"doc_hash": "753439ad99efb9675d629e222d342133736021ee1591b7c7e54d8ae69b65dbf9"}, "4a5e6544-5550-4555-85be-40e06f5acc5a": {"doc_hash": "e49b4288a0e774790b6e2895252ee013ada0c28a6c93957d7b11b0f70df88d03"}, "bcad1dc8-f51f-4443-9685-1add2b409bf5": {"doc_hash": "8e57b45b7abbddf89f0af472c36bc01e3a63d84ea452251e805c037c60909158"}, "75efab75-6103-4a86-a65a-95b3f5f6803c": {"doc_hash": "9811a7ec27f9a5012c380a719f1cfe844718aa8eecada7b3f5ff8dfa27258175"}, "f3bca2b0-a931-4def-a018-aaa835af6265": {"doc_hash": "dc71c5d6646b43739feec773793cd040f25cf11f1ad5c70a7f9cf32ebd391700"}, "4dea3af1-89f0-4cff-9fe0-e62c4e20be25": {"doc_hash": "4973aed0ca6fb4855aec9f2832a2326f8d7b586de5445ec3985c2b8dd72da66b"}, "84ad2d4d-3861-4c5b-ab04-05459126052d": {"doc_hash": "b5f23c1ba83c7e408d8ff0521d04d1772c6b9e1e728fb1a43f4e1a1d51f22d5a"}, "c5ad3909-7bc2-4fbf-b861-0ed12ef9cf7a": {"doc_hash": "29d799094d75b7e0d975238fc3fc19743a717611de198e3b0419889ded5d2cfe"}, "a7bd547d-9bdc-41b4-bd95-b27fba8ebf35": {"doc_hash": "bf26e66ec280eb163734403fd81ba3abaa23c7191c7bdee1f01ae10095a05d78"}, "2900c8c7-cf10-455b-9d3a-3370dad43dac": {"doc_hash": "806779d4377ea7e216ea563d48ad13a37b9b72536da1f029e5df5b3593e854c1"}, "6a96b9b3-1198-446e-b8d9-6255eebfb3be": {"doc_hash": "84f110968a4e4f9b5f034c6b709a488fb97a05a46f2a5d11dba2cc5172c4d69c"}, "97eb87f5-8826-4de8-93ba-962fba5e0936": {"doc_hash": "322e969d0230b6ce0c299c3b4d200e21efcccbb8b413547e51eaa74e8a76a243"}, "af67df2a-d9d9-4735-8d02-691dbb378710": {"doc_hash": "8f8669c8bab9bc0ee50c2f1fa9a732489c2979fea6a24c848f7698f0de02aa47"}, "0921d14d-082c-4af2-b132-e553b872873e": {"doc_hash": "3a4cd84623b7e877bb37397cd77fa62ac36f6cec3441bc2011a655ff4a27f113"}, "ae0da04c-ccf6-4d95-9801-a037af371b77": {"doc_hash": "ffa8a6590a3f8d07d5b6232b87bf3d2dbc0a2377a2513139c2a0751dda4d5ba2"}, "df11fac4-a4f4-4a88-80c5-b88544cd60b5": {"doc_hash": "550034f1b8ff38e13ae5036dee816fae000ca635318b3e288364fb4e56eb7c7c"}, "addf1f2d-18a5-486d-b004-995e5ed621ce": {"doc_hash": "2f09426bcc395036e9abcb0d38562702760be6925cce28927096f3416bd1b4d9"}, "0c6532e6-85bb-49fb-9e38-c7e180fae61a": {"doc_hash": "f2ff778d5db007ce4c87b1d87bc4895566e90ae8d8d13e9fe5cc15aaa1dd5f0d"}, "45f61d53-fdd2-45e0-b68b-43316ab82c64": {"doc_hash": "2bb10ba37016490a1f9753f5a3e75c5774f21358ddaa530eea2cf0fc7195c6b7"}, "bea78591-7b96-4433-9d19-b7fe32c97193": {"doc_hash": "c87a17a5b3c9edd33a3806e4fe520386e04afe7d8e698e1325250aca708e2367"}, "9c855298-579f-4f17-b6be-3cf70cd81aa5": {"doc_hash": "109b77ad962905cb796175fa1cf013ce8a7223ad7c353dfe3211f4ed7fccd159"}, "54f93970-accf-4166-ae70-b619f6fab915": {"doc_hash": "5546c80f699b02c863a616088e7de279d0b570f962ebddeea578c4c63fd6079a"}, "adf2fa83-68e0-4d59-a937-052769668a2b": {"doc_hash": "aa6650b8419b8aa346d1345261549ff786056ff2eaf6c903ab6fb7f4b9005d1c"}, "7dd88692-0b1e-45b3-88c4-0ccb4fbaeeae": {"doc_hash": "abb018523e8e09468d2ae2f08aa9e1ba9954c5eb8ed615e52a74e083c94d05fc"}, "6e80d002-31b9-401d-aabe-ab84c20d52ab": {"doc_hash": "fbba6cf52d8b52e277932cd67a7cd1a47e60de46dea9869225f3e90c024a5d35"}, "a22269d0-f9ea-4172-a34e-41b479aacce3": {"doc_hash": "99e6127878258ad1b30b2f0f52393070e0d6d94967267c51153bb66614cf890f"}, "e4621f15-7b4d-4bd3-8711-6dbad76f7731": {"doc_hash": "71d8e8bebf2decf6bcb9bc4672322bf947d96b0e207e22bffa0108e2152d3bfc"}, "f8d59f6e-509c-48ef-8805-84c3cfa94d23": {"doc_hash": "b55c468da0e779cb85f408e6164019e2720527acb664ebe21464499fd964b028"}, "80e080f4-3428-4c42-ba30-5a7c9d710361": {"doc_hash": "797a7c91add69eadb06320c0d208694de3a6f71f56439af82aa381ba841e1278"}, "c1b36253-3fdb-4b05-9ed9-ac2f7a5fb212": {"doc_hash": "79b5733e03c034162b1f31588804894960e78e6a5cca0ecce4b9fbb50237bfda"}, "32076889-1c93-4889-b785-b1203477eab4": {"doc_hash": "100453ef226e122db8076a2106f9d1fdd20d7c5c3b61194903c23c0688e5aee0"}, "3311af09-25a9-43a2-8f45-c4edf1d0a72e": {"doc_hash": "72b905ee3339e249c87fcfb6327be735850f6e40496b704501852c55cf475c55"}, "1a06ebf6-0823-4253-bc6c-7edc3aa13df6": {"doc_hash": "84ef38cb7f8a009e0eca306d58ac87f93adb7141e9b2a6fc57bc44858f7e8e56"}, "8c440d27-4be8-4565-b4bb-f853c17399df": {"doc_hash": "d51205f0545e99795a58bfc1e55ab9f2d3db9aa8964b8b74869119d7c373f706"}, "f719e8f8-2109-4fc1-a46c-145ac5021b50": {"doc_hash": "83815badfdfb427ff01e721aa8f77150a8a9558c7c7ad19dd6b1dc7f7ad4d112"}, "7055f343-6d9f-4afc-a566-1fe19af54aa3": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "28772a37-b87f-4a6f-ad17-d1081cd2900b": {"doc_hash": "f873f7ac2a5a120b5aa7abbfa3ebb8cfcd5cbc388a198f74563b6b476b8171db"}, "3e1a3ecb-9dae-4847-965d-598c86a43865": {"doc_hash": "537ea3e243184055ca75bf4416a585c90ac745bb3cacd49a4c5a8574ffc1a63b"}, "0842b10d-a6fa-4dc2-b017-5b14af8da28c": {"doc_hash": "009100eb7f5f03f0c06263a3fc7543db751d5b03d121d8af2a1792058aa4f475"}, "3dc193bd-bba1-4922-8a69-fdcd6642501b": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "4389bd7a-45a2-497a-8acb-3175ba8ef500": {"doc_hash": "21da96198ffdd21ec94c94743ad22471794c460647e43d91c14290bc22eacd41"}, "cf2a28be-b1bc-4be7-a797-b894e1a9e90c": {"doc_hash": "c2611425d135acab694f60aa60a3a57e21e53768567b5b53f5e055e3fc08459f"}, "684cb30f-884f-4c91-a90e-11279924a187": {"doc_hash": "8d1a118cb64b54f82bd74754756483f85467582c03a3f65a0c35360d99d887ee"}, "1bbe6e75-21e8-4dcc-95be-cba9027560b7": {"doc_hash": "f8cc0bf2bc901d9629dbb5cde16ef152faf3c1f2f27c33fe83244d183d38b93c"}, "2e8afd3e-c770-415a-838c-60b15b8e1b78": {"doc_hash": "12d8e2c7ff9c39c1d19ce51b017b93f6930d0ed4aa294af798da356c76767b42"}, "9566c3f6-07c9-40a0-9738-9c200d9de840": {"doc_hash": "5614cb81775d7635c3c1e0e8902c060ad62d58cb30a8f14cb8d1b88cdcdadcd3"}, "9a76d14f-c044-4894-a9ae-b358f3e8b501": {"doc_hash": "a3058eebfe2bd6cdeae95aaf9d8d9d72c7c61f73d357e40d5783509c0479b466"}, "5672d9cb-a86b-4556-ac6c-ea32f804b33e": {"doc_hash": "ae867891d5bf61fab94729ed1fc01352d0d2e35432637cb0a93deece079e893a"}, "f11afa31-403d-4507-907c-10cd9b848534": {"doc_hash": "b6e106be2b1dc260cf571134aa75caee41190faf8457230a27a0a07a58babb29"}, "7d53a1cb-7313-430c-b35f-76e44bfe5a0c": {"doc_hash": "dca4dc275274ea2ff7276b2aad1a1d205e2aff676ae4a8133fb6160fef08adfe"}, "98137909-efe0-49e2-95a9-82cab4f68d19": {"doc_hash": "c1d379030dea395ad2a04920ce97b201f8d2dcdf0516f9fa1583288dee9b0fe0"}, "a0a8ae2d-72c8-4247-8b63-60632d8febcc": {"doc_hash": "7df70765b091ff8f85836ecdb684355801b8aab30a958de295eeb00aace0e02d"}, "311873fc-c662-42de-a871-fde4739a3499": {"doc_hash": "3eb53958fcc68114c2c4a8054e059d2dcdb6239f14a57e9245b6b1a16f5ef481"}, "b4b40766-a5ea-486a-941c-f53d4e5c928f": {"doc_hash": "811567d346a2d65139904b5b802001ad78f4324807abb4640023792fc442e947"}, "dbd1239c-7778-443c-b15d-589a0124e1da": {"doc_hash": "adaccd94a33e9d056b2ed958eda7b8d8024cb39e805a6312f8ea14452515c6cc"}, "4ccfaec2-a9b1-4d76-87f8-dae6ba181373": {"doc_hash": "c015acbf138d48f31c35767046f3cbdd0dd9f0bdcdce2069b7ff7bb3ad102625"}, "6fc83394-5da9-4f8a-84eb-0148832b3c55": {"doc_hash": "77f916c1550efe2f793e5f1b08585433715167ffa8f36c7a3b7b1acfd9d22c4d"}, "b2de1ac1-0d16-453e-841a-20d054c85f8b": {"doc_hash": "7185948a52310f20305b3b67b8037f43df149ff956507c34726f123fa4f1ae46"}, "5a800053-18d3-4b3f-9ae6-2b083f34a392": {"doc_hash": "872a77c360a31922ecee060581de4515649f55bb5e07de78e86f2dbed3dea920"}, "a713f84e-7055-42f0-a623-2ff9653d3ed4": {"doc_hash": "f66669805e83c8ff54313d39604e8918535edee8e11a4eee16ed49c8f7a1c492"}, "9b1cc965-6768-4867-9a27-cfc85f41de16": {"doc_hash": "7c78d9a6eda7080f1787dc052ba5d61c1850d063f37dd371ad1d032c7e112da6"}, "0bd41b34-a1d2-49a6-b1b8-83ea4323ba5d": {"doc_hash": "e5964ac380f2d8cae2a724b013e7dd19d017e6843785565f3ef747898a4a15e4"}, "c7a5009b-c4a1-420b-8a9c-e3ca17a7bba4": {"doc_hash": "98ce2ebc9fb6ee7fa73a845d64838b558767eeedb6440820a93c7e0f5f85f974"}, "ddec3343-bef6-4269-b65e-e767294dc820": {"doc_hash": "48608f57358edd5fc835b61bc79194d1c66ed5671a87fe4c6176fcde37a2a653"}, "1b142e80-6c3c-4b3f-88f7-2b38b6d9f797": {"doc_hash": "10896a32a27c6784a62b4d0328920ee115a4aedd60caf341c006459aec24ef17"}, "162430e6-5ea3-4f9a-85f4-5d89a5385c8e": {"doc_hash": "ea36c17d30f790f075d4558fff6925702e54498e6a6b2b15947cc15101c57a97"}, "141c6cc9-ab51-4ded-8fe2-cac73bc41f75": {"doc_hash": "05d353251cddec3c92833327e8e2dc054d274d1924b572985c057c0bc8a2a39d"}, "a513c3fe-3f32-42db-b7bd-f3d23a9fc402": {"doc_hash": "a17a74ab992da7aed738e22643aed0950bd6e12a22413b88e621fe94a29b3399"}, "a1a3367d-07f0-4546-84da-b64d32f5628d": {"doc_hash": "dcd8cbcc6c7657a9b0e5023a810688b819897510e48d13af57dd9f2ada0cc8c9"}, "f04e1ae7-0130-4947-927f-b0daf049078d": {"doc_hash": "b16f485317a5445ab6c615fbe0a3453ec0f73882829b3f38ebb7ff0269e11fc8"}, "aa41a984-149b-411e-a492-2ff95084de3f": {"doc_hash": "4174c33c4222d287928cc44e2c8f0522d6d710c4959ff6960eae40f7a452fc1c"}, "f9de3e58-a408-4d1d-b213-c6f2e02648af": {"doc_hash": "2d13beb924fd1c17f4dbc993004ae02adc250505982531270daf6b5fdb5a7fc8"}, "3813571e-45be-49ca-90b3-54b9b4ab303d": {"doc_hash": "39d2079d1efafce1dbf7a58959a04fb09a360b954035b225f2b0f3c9618fe0a1"}, "b1aa33c2-fce2-425d-82a6-352a0eed3b42": {"doc_hash": "fd590c3147fe914735a4872bf2b2bbe16a86da30177e9eb485e965a93e18cb50"}, "e7457fbf-ec07-4c45-959b-e13f8e0968f9": {"doc_hash": "d6f48b0f88c2856d57f98e521314f3af877f6350e07804f7f4cf281b0804b330"}, "ec2ad01f-991e-4a1b-acac-7109e76e8137": {"doc_hash": "717feed36419fdd0b8766881e6a7f3057b346d5eace244af5c919d67b5fb7f9c"}, "46f07b8b-004b-4221-8799-c9224afb7e58": {"doc_hash": "90136ccec6141bcd3d69c6afd00f3afb9239129dcffd4c15e3dc3f657368c530"}, "d90a2264-16d4-468b-9ae4-806739cca1b2": {"doc_hash": "807d7d38914f105fcff4bac26992cd2f7972d5d4ed7fdd4b9cad779a1d1bc2ef"}, "5827da53-13ed-415b-b577-ca6514d5b27f": {"doc_hash": "591c33b3027c5115880e21ad45951827bda185c406dd0b34872b976a053d80c8"}, "365bcf18-7006-498b-9573-a2c1c1405f7e": {"doc_hash": "3046de597f30cf9117abab6e680add85a66754d6f4bb37fb8c3ddbc1102a27ba"}, "8210b0c6-b22d-4537-bbd3-72854118a090": {"doc_hash": "b9c55dde042b36efea48ac7bce1f00ff5773ed2eee0de0171d927b165e878b17"}, "934eb9d2-92c3-4f32-b94b-7d6e7428cf41": {"doc_hash": "870e16db0745f3b322757ca7e1baef6c50b62b8692e7a55c0f46ef0f3a46e26a"}, "14773323-390a-4edd-9ff0-adc79735bac4": {"doc_hash": "fab401e66030635ebd22c059fd99e646312b1df3622e175c53c70420b1abdbd1"}, "a99d9ab7-1192-4469-a10c-332562365e28": {"doc_hash": "aca20be60d87dc64c9a8006f402acd4f438bc81113e8c4c5e058495801b013a8"}, "84d75a17-6aba-4591-86fb-bc5d19de2eb0": {"doc_hash": "9803376310769c8a1a5c6213680dded05ffbe720eb28d8b2edb5d5039b0d4c67"}, "fa5099dd-36e0-4dc9-ab6c-546dd398816e": {"doc_hash": "7c56c88b7628b66779a4c693e6c82d4d9daf5862860d08aaed552cfe7c89f4cc"}, "7ee217a0-d49c-4749-8e89-5697487841db": {"doc_hash": "2b1820019841e0e96f3d30ecb2e9a8f28d9c4cd0e0d0c96dfa3663f5f0dd8036"}, "2e1f0b58-7bb1-414c-b934-c5850fe5bb4a": {"doc_hash": "ee097a3ca966e492ba26946bac259f1ad3060ec473e99c64d611ad299a7d09e6"}, "416d6001-a1eb-44bd-b44c-853d0dbd64b0": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "b9eab09e-4d9f-45b7-bb51-ea4432e9b921": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "5b2022a6-0497-44a4-a48d-d2e6146eaa10": {"doc_hash": "b93cbc895c16925725c1e6d6da37a8baf09cbb1f017a4d1137849076c6f2dd38"}, "9d534fcc-46ec-440c-8a4f-19cc53ae45a5": {"doc_hash": "4a6f083acf9db10c9979bea3d4102e259ee05eed62068fc3895a78c2c4da3184"}, "84a15434-eb42-4ff2-bfa6-9e1039197932": {"doc_hash": "8b662413ad463ffb3fef220b1896e112fc4a2aa02d709c57ff744e2c7f04c8f8"}, "bf89a88f-b93b-4995-8968-125b7cb57ead": {"doc_hash": "b07abfc7b447a2c2ba8ce42205bd2cb960373810ae9eb5ac96c79496f7146aa1"}, "8e7cd6b6-9acf-47b8-8558-24cbde3b5e21": {"doc_hash": "88af830d7599e2aeeb2bc600b55f2657580e73e35376d1e11870a533742df79f"}, "5ede781f-94cc-4ec1-a0ac-79734c148224": {"doc_hash": "eae540a619e118f4520c5426a7d71135cce4135c324c0bf0224913a43569dbcb"}, "b06ace18-964e-4f5b-a098-c0a394666dc9": {"doc_hash": "a88e97cd29774811b0c68419561b81a6bde3d7dcf538e647b68c54b2aadaeed1"}, "1b346ba6-c764-432b-9006-7a833ff633d0": {"doc_hash": "241bccf93cdc8215f63cdbf68efeb3fae3e816fa2658cbdb98bfa78a201f604d"}, "0a68d8ca-712f-400f-b87f-270898937ad7": {"doc_hash": "8aa11ddbede6853d2ca56e1d78a8ec36452d75876d897943bd6ac5ed414c510b"}, "6ba51c12-507c-4e81-9f86-55b53a2f7f69": {"doc_hash": "5d515d6c05578d3676a7e8368d1a1a115817e75610af5e35fc5da07e4e4dd3df"}, "cd2be06c-6fe0-4823-8285-44c5689b8258": {"doc_hash": "7ea27e98247a7396bf4f0edf45775a430b97014ca0141357dd3ab7a74c149dcc"}, "968c7f06-2151-472f-8649-79563383d892": {"doc_hash": "1675f5edc409d61dd862bcb818cbb41ac87317e0bb24b8b7ba840a33bf469a19"}, "a8695a0c-5403-4b4a-b00c-8b131d041388": {"doc_hash": "61ddaede323d002837baa0e1d9d808a74a581cd75451f73467882f930ff57807"}, "77511991-81d6-4abd-b506-97de6f31b92b": {"doc_hash": "fedd558ffc2a215178f270851211ab7d288ba1c7563d3dce9e956b2bdc0aad8b"}, "1c595d90-86f7-4117-9d03-12cebbd672fa": {"doc_hash": "57bbae3e8703ff686d17c4972f37f8e1d0cecdf503455e69f27f455463caea25"}, "be45592f-7e7f-4683-9a84-d84d94fbfa9a": {"doc_hash": "fce7ce8caf37717b66ec8aa1a5ab268b5624b892f1b364e2f8506c10647906d5"}, "7e14ee7e-a8dc-4a4f-914e-f5418c518135": {"doc_hash": "9e6a21b83a5d573bca5cb4a6fcef926cb45f573108977f28679b68f6f3c60c97"}, "b137b1d1-76eb-4c28-8474-f9d9cb9fb11f": {"doc_hash": "9dfcd2cbd65b1e1180567956e90ae64f635324c51a4fdc064c734af27540c5c4"}, "4738e651-60f3-49d9-9f0e-c6e80fc363a0": {"doc_hash": "42fc94657f5f96b6af9021c7d4eefb5fd7ea01ec2bce98e7fee00098886efdd8"}, "f07ef0e3-bac2-4112-a550-4aa731fd8236": {"doc_hash": "d050c427434ecd7cfd11fcce83cb1d713b891ded63d1c52b075bc1620ddcc897"}, "d4b6e2d4-c5e9-412c-a263-4dd7ac67138e": {"doc_hash": "33e4c1e5b57d85caa781065fbb1fd1c6a4a8a46f771be5b05f711ceb08f43681"}, "268e3d23-d175-460f-9b0c-3c63224b0b97": {"doc_hash": "76b8ac3f1c81535392d4deb95d2866cd752826be3c6e02d622bd96956266aad0"}, "fe45250b-25e0-46f3-be4b-8b26d7ee8be7": {"doc_hash": "95db9a95c00dd53a8b778c94a11f26a032ed92cd466ff948db48de5898d15533"}, "a33f9f31-d173-4f26-ab64-b78e454e6e7b": {"doc_hash": "9c0907e5293d547ca711e24d381daa91acaed77b6e23ccd716404b4a2d1167cc"}, "759b35bf-9700-4feb-a710-198a75996616": {"doc_hash": "b5d7208ba85b1c2a1d567adea9ddf6f7f9ad50373aac9a25002b54a103196ec6"}, "0cda95b9-3300-4296-bd13-f0ce7b223d37": {"doc_hash": "96f1d2eee20d55fa36fc9ee5b7c0820086d82c18d35a91ba01e7e9290d2a93e2"}, "e0325c47-3f7e-46e9-aaca-b5618ebf055c": {"doc_hash": "704cedef6a1834bfb8cb840d505a1ade9f38d4952d3eb8070b23b52914e68b1e"}, "556f65ab-88fb-4aba-8e8c-7829a9de24a6": {"doc_hash": "034a08624287f9e39dce032580e520d07e1e7ac27c9228ef177e5f12931d0e96"}, "dea88df1-71db-48ff-861f-cfd6d384bd06": {"doc_hash": "bc4ee8542b08f274e531879e7137be3c2aa419c68415d3aec4b017b3a6090944"}, "89a5411c-aa01-4c2b-9d00-04d430ac929b": {"doc_hash": "6c04d151671cdd4ed466a999c897c2e339f0284bf5d0083dbc07658d2a667e56"}, "4fae5da1-37ad-4a3c-8804-fdd107bbcb39": {"doc_hash": "b16810d4275a2efa5b32a3fd2d4b984ff4919ce093d538104e17f08be5c175dc"}, "cb91518e-ccca-4a97-8afa-b65ddedc40c8": {"doc_hash": "35709c6b8f8bdca8fe34d94dc1b623f25f147c0bed61eacbf80975e3ab7b1b86"}, "3e74ee4f-2c37-4c3a-9094-67291e7aed0c": {"doc_hash": "6ee306258139026a0fd97ee36c9912f18e2ed4b0e6543bbddc87c48cb37fe271"}, "e6ed040d-4599-4f63-9713-21643c08d14b": {"doc_hash": "b7b128b4e2c5f54602b7b319aae31601d847c31e66a736cf3e0d5fb68bec3bc1"}, "2507874c-147f-4ba2-b52d-0bce0275324f": {"doc_hash": "210f158a315e3919c4ab09c6a9250a172e879431458e5f3c31a2dd2864cacc70"}, "aae6d196-35aa-4723-a43d-12ee2e12dad4": {"doc_hash": "6f4f1253584b4ad4472e4597ebf2e0ce5148684e64f2f6b8faa241bc853ff24b"}, "885ccbb4-34ac-456b-87b4-03cf1b17db25": {"doc_hash": "f82d179a75578c19a981f8938880ccc467f975edc1dda7eaeda9aae22b64617f"}, "ff564d44-d7e6-474f-abba-fd234ef7df00": {"doc_hash": "c5f3f9b453d45dce7377a2f9dada8ba6505eff9170574dc665828520d438b07e"}, "5e22e822-31c6-4813-82e3-3b0dd629067a": {"doc_hash": "535ed5cee9c7ae6dce46e72bbc438d4f1ad2bf9738866f9bfc88d34dc67b1130"}, "13238669-c93a-448f-8db4-b4ad15071394": {"doc_hash": "3e839f259f87e47b98290be29f4aba8e0f59278e4bb9c3633765cced591348f5"}, "9fc8790d-3459-4432-9a9a-2dc4513c3c3f": {"doc_hash": "4cbf8333b40ecf58666d0b052b57c16fad54c69ce8580117cf66076a0e856ffa"}, "b3ae67c8-10a6-492d-98fd-caf2df38d93a": {"doc_hash": "5877b646117d024d66b376f0227430f6a93bcc61528dbae258e1da0e1a49a501"}, "2c973714-1327-4f17-a8ad-3e6bbbe6c8c7": {"doc_hash": "411f911b46cffe57ba832707819df2963d3f76758936d00d3bdd2503e2c65104"}, "2463e0f9-f295-4325-9a22-a2489ed55899": {"doc_hash": "cbada9f054ab9b63bc84e21e61b85ee9876420f5f278dda521c15a2e93e988f7"}, "8aa29d93-878f-441e-a49d-089ece12d7b0": {"doc_hash": "adce5d3e89834d2d4845d8ce88fbc43299d7803e4d00d56e256a25fa355cd6fc"}, "8d290785-3a12-4b06-a821-4e7adf4d3912": {"doc_hash": "62bf4b592c5dfcc7c821c0d87f591960586379375857228914a360576eb7e89a"}, "4aed5dff-ca5c-43dd-bde8-1c4102935cbf": {"doc_hash": "9177f01f40a1cabab8e680a1d582d0328e17ed877d2ac4071aa3377d5fb5b261"}, "6c80f8f5-2c34-444f-9cd2-a89f9e0e7de9": {"doc_hash": "68084632b015c9eb49b69ad40f648530bbc954b2d14e76de8932dd987e02f2c3"}, "22110560-cf2e-47d5-9618-c6eb6b2be016": {"doc_hash": "0b2d20ada6da8e3eecc2db710d65c7895a7e4e2c996772bec0f4bf7772421a01"}, "9d027828-b439-4704-b10d-9ef1051c3c31": {"doc_hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}, "7bb47ef6-b3b7-45ff-9da9-479566d83fd7": {"doc_hash": "8d206db89a24fce53865d373f22aadc29f4688d13c0b8eebfe000767afe885b5"}, "b3e6e739-1c89-4f43-8ce7-77ea9514c8cd": {"doc_hash": "d5371745f7e651a2a9373dc8a0f54f310b3a93c8fe17416ef7a0d6704011ab6b"}, "c08874de-13ed-4097-ba87-dff6e43fb537": {"doc_hash": "8487c02ec2001c2f8df86825fbf66b0597340ab46eac85c1f1dcd64647f12ebe"}, "8705eb3a-1f5f-4bad-9654-edfbb8f88068": {"doc_hash": "706e7e7eeb17370134ffa0c4893b08544bd5376a81dafad2ee73aed17f3f2196"}, "ae7cf2de-36d9-4a7a-9387-66cb81cb2068": {"doc_hash": "f9d1fbc1b6897288a8908c31e6a4ac566579959b7da046add905513934989ac3"}, "2bcde958-2fa2-4bd8-88e1-f60f9966fa94": {"doc_hash": "af32e4c90257e6e3d006816a1afaa649507d56799507c1487ad8601ee6f7cb45"}, "2daa6184-1982-45da-bfee-483064f41798": {"doc_hash": "18b9688f3512d21c81de9b9596ab471afc247ce40dc378cf6ffb563bdfd756da"}, "66cdc326-000e-4d3d-acfe-9d46440b8b04": {"doc_hash": "49c5edd3becb19a1da9b52ccd04c18fb02ad1dc835a7d283017bece61c3975f5"}, "ed03184b-4e03-462b-9ec0-193a951c8487": {"doc_hash": "f492e493d3140abf25c276bade9e48a96e4789b609937ec1e41a39149f1fdc7e"}, "65421512-2153-418f-a356-5f80f4d63fdc": {"doc_hash": "f5edfeda16b16b6090059cd3ffb8fdac33266b7a4f090c60af979a868db82b9f"}, "bf6206f9-d55a-4a4f-9245-71b7ee4a85a6": {"doc_hash": "df7959413b6196b3e2d284ddc3cec3caca166c53fc4995bfbba0b58955a87b42"}, "fe9ef192-dd43-40d4-9660-73382dbb3fa4": {"doc_hash": "173ce5702eba187161abc044adf834255a91f8f26461253efed0ec8b1af17669"}, "3f655cb7-3c82-4f42-af76-6975240df378": {"doc_hash": "e1d3f103940d88c5e47cd55e68059023ebecd8ee7c8ec61f2fd32d52524e595f"}, "0984b0e1-8825-4bab-a777-5b25903afb80": {"doc_hash": "d7fc63a89604c61444fa96aa145b443c436ee94678f8148ddc2fccab86b3bbe8"}, "16dabdeb-269c-4d97-b25d-1602538fe2ae": {"doc_hash": "58f25c8cb9f927b2c8bae9edb2ed52c718d4644037c9fdcb583dc11fe1d2491c"}, "1e73ea35-227f-4865-afba-b5325315169b": {"doc_hash": "bbd4cee66db8644b3ebc6548a7f5e9995098710a6d7a31b3210867e610d96b3e"}, "54cee247-9a64-4b4a-9526-013be196d873": {"doc_hash": "47df230819e8ac63c1cdd16ba16e3e76514412bc111710b994b8153a055f4157"}, "6abe6038-5b67-4bf4-bb6e-47c4a3e0f197": {"doc_hash": "4c3fde21304bd326602a1215355f741ddbc715d286e85ac983236bca7d726a74"}, "f53dcc64-eb02-4465-b9ed-0625a409b519": {"doc_hash": "6b917c87499f3bac78d5050906c28b59216970a7ea03600d935d07d945aecf5e"}, "681c7631-815a-4a12-affe-c1cdfff0ab8f": {"doc_hash": "35ee20af1ff95a9bb11f32490c734145a8d9e05b2875140fbb54432747e25e94"}, "b50947cc-b197-4b41-a716-a767606c578f": {"doc_hash": "d2016be37cd3ee2b7fcd6e2fdd85a271425a9ee14cd73b691c506d39da8b4338"}, "1240fab6-21e1-4f4c-9dbe-a28e69663082": {"doc_hash": "4972246f531f849e1e26c9fad4b194c79b22306daaaf7a2ed1c04fa0eabc7e12"}, "60e0c99c-3ccb-4d86-9315-ce2b8bece4af": {"doc_hash": "4fe2d36f6135071b7dcab59b7f7fe5db95f41eca20d74fad292b5d9e8c4c198b"}, "e8c598b6-7706-4683-9167-3704832bf3c4": {"doc_hash": "54558659d0235b321126c84eb89b0305b4edcd6cae8e740a2786bc8af667c332"}, "9fa80710-f940-4a97-b983-d00dfb02adb5": {"doc_hash": "25fa8db34836171820a6f48ab010dce7984e784296a2523f4b8c2ab2b84a946d"}, "11c9715d-dc9d-40a0-86f4-4aa11d06ac66": {"doc_hash": "f65acef174a077821b0dc88fc154107129973bf054e338b94e2d78f22fd74570"}, "ac15fe04-145d-4f53-ac4c-7f00a54a4cf3": {"doc_hash": "17dcc4019cc83fc022153022b16de85f758cb3326cecd572aa0bbf193f34d077"}, "f2e4f884-77fd-49e8-a4c5-c53f6fb4bdb5": {"doc_hash": "8252a139803ff1bab4f565e03b3a867ac5c254813b46d837167543ed0c5d25ac"}, "eb48d5c0-da60-4aa4-8734-06294983fb1c": {"doc_hash": "4bb34b26c9bf9d0952112160ed1ca324f49fe2d3152c4e225d0e489694852222"}, "c2201a65-c24f-4b78-9c57-543a46df7785": {"doc_hash": "6d5f154af87b702f795865b8866882ff21d1627ac2643ed64c849bcd6d6150a9"}, "70070691-a812-46e1-820a-72b8d72a191c": {"doc_hash": "9db3c1f9f831cd815117386dc4e2a9bb3e8eaa1d0d9cfd33fc7e8b988af20087"}, "6bfd0d76-acb5-4a41-8f60-295f26f0ba1a": {"doc_hash": "f4219f76f84e6924dd1aeeb4a8b984e58604aa647bd592ac47829ab7210a7d99"}, "1e65ec52-ec77-4f0b-b62d-36ec7a0b7aa3": {"doc_hash": "76286e4a953349d54351e5c58dd88c514c0fbf5bf980325d9e51739032e9a41f"}, "f27bd5ff-f5f6-4e01-9e67-5ec2795a3dd4": {"doc_hash": "4fa2c382b7392d5f3cc391d796b3f2070c4c8d13c2828f77bfc1d19ed94ced3f"}, "82b6e5e3-0b77-4480-9b8f-ecf6fac3e528": {"doc_hash": "17881cb8c8224d0df92a9b4c79befbb257f230dc314c00870094fd9c256d7c0e"}, "6fc1f10d-3311-47cb-b4e6-d101dec4e87e": {"doc_hash": "07baaa36723e0ac3deb8e11feaebe1aeddbcafaea94341b52231a8732d119db6"}, "d7738e21-a5b4-416b-8377-e673f3f8f5f6": {"doc_hash": "cbc8646ae801f30c3ce154d49ec3c5be1b8311a30394bebb761bf5bad0aefc67"}, "80f0582b-5836-4bdc-81a1-01afee9c253a": {"doc_hash": "459b046fc27f321355c8e1e5af754e329c87c017ebf2814a6b398b02ae3f1fdc"}, "c6ce5391-1394-43ec-87de-97316d6008ea": {"doc_hash": "0ccf258dc83247d8283eb32d183def673f926c78c9fb463ff4d8bce1358bb255"}, "435f96e8-f705-466b-8e3c-a20eba1a0ab6": {"doc_hash": "86b32744df70f3252f6b525793a52a5fd3c055b7074cf2b2cb7b31cb6c8fd124"}, "a270f694-a4c1-44c4-9b76-b362fdd7be7a": {"doc_hash": "c619c6c8244d79ee6402af0cfb956387fd5051cd7adcabdd01ab488a1f88cba6"}, "0e697497-e117-450e-96db-567cc1fd941f": {"doc_hash": "025d4ea9eda9ba9eb5d3024e744c69e8812bd169802f5ff148da3071a776aebc"}, "632b6f14-3965-45d9-a77d-e92181f5bb8c": {"doc_hash": "dc8e59f31d33b7566be5a06161b6ecdf20aaeb43df989147b89fccf671c319d1"}, "8262359e-29ed-4003-bae3-37dc012c80a6": {"doc_hash": "b0d627eea4631fba76bb1bf03bad4f8ff7932a371c670ce003278500ec897b3d"}, "18624e86-96dd-4ca1-80fd-cbf4788d4b7d": {"doc_hash": "84123f1243d5b7b381f58beae837872b57a1bb6332a99389538ea7a39e24a456"}, "b3808f54-f124-4d0c-ab1f-618eda95593a": {"doc_hash": "6b159182de8022576e99a5619457a6dfe75bd87bf19c21c2bb27c26c3fa08e8d"}, "1f1c5137-1bbc-4a40-bd25-db7996926143": {"doc_hash": "cd6e1040002e534088116939f3b2da1b5dca3f29bc237976a0818d1b78dfd018"}, "3e90d93d-5469-493c-ac99-d2cd7e80f725": {"doc_hash": "496d450d082e93892db452519ac2235d751f75e56ad602b77dc75d0a6007b0d9"}, "bda2501a-784e-484b-a1df-bfdea29358a2": {"doc_hash": "18db34359ebe877fef77da785afb488f8756977272a22dad60961628e821de7b"}, "7cc10ee2-2872-43b0-a6dc-059019cbfc7c": {"doc_hash": "0a505e8ee10d2d40b8c0b6b4567e6ea9ebe212cd20252a2df55831a9cd707452"}, "985383d5-8688-4d9c-92c5-0e27ee0b0112": {"doc_hash": "e37b5318064305fdba794f6aee8197970a4fbfc17344eebd7c1c82009c3668d5"}, "3ec9c0c5-b5f6-4e0a-96e3-4cdff960da1f": {"doc_hash": "11d3ea3194ff24c93c425e13cb8957c3aae25bb154f0ed4483a276f3b1a0e1d2"}, "de17f2e6-650c-4858-9fa3-216f1eb249e7": {"doc_hash": "a3824c72fdbe69ccea0106561b95f1aa011b44af88228682dfeb6236d311343d"}, "d98de347-796a-46d5-9155-f3aaa2f3a921": {"doc_hash": "3cee590b10a7127b679427673e7a3a0e6dc3988123c0d07a497f75b1911d9f6d"}, "6eeeb315-2816-4bf0-92b5-f5d92eed548b": {"doc_hash": "e29954cd40519deacaf32e572081ef959721652da1e88ca37bb5a512e8b6a336"}, "a4d3d7bd-e13d-4f3a-a5cc-c53e97a1b090": {"doc_hash": "541073140fd01d982af83895d8ffddf89e5856afe377b861bed6585b682e22e3"}, "570bc122-9b98-430e-9c45-81f50eca26e4": {"doc_hash": "af6e70775738cc185eefcab4a1a3f1139b9eca05cf994c86bc82ad39a48fe66e"}, "85a17d89-f7f9-4f69-a1e4-4a7f196609a6": {"doc_hash": "6ac0da2f1bf6dc94ee23020bbe8216ea885be31e0306ab1d0c696b706777d995"}, "9836168d-450a-4873-be3c-64d13d4707c9": {"doc_hash": "7daec71c45c49550d10bb4dbe7772aabbaf8dc723eefec3465d41cdbba280b3b"}, "8861b292-6a4e-462f-bf12-ebeaf3cb795b": {"doc_hash": "c42680d1150b2cd31fcd0de6a956fe59b5dab048c766a2652e96e1631d278c01"}, "19053491-90c1-4900-b3df-cec4d042a334": {"doc_hash": "88bd624329bbff0970b9d32dbd7a266811720bb636d3496864efa570b56519d0"}, "85fc3415-8fbd-4e4d-9f77-7ed86812eba3": {"doc_hash": "d0b43d9708493361309da60c81a2527b15f625f1c41c26966e0eb8fbacb05dd9"}, "7be73b89-e40e-4d0a-bcdf-76a5aa7b79cb": {"doc_hash": "a82a93877f8df9826a35f1847796b63339c3c32fb6ffaa095531e065b9f7c5cb"}, "02648872-446a-41e0-a655-eca95044163c": {"doc_hash": "d1b3599f606655188e0283da89831863bbafd4b6c06748bd8f9691fd4875c620"}, "87aa9734-c741-46ea-844c-5abe89d376b7": {"doc_hash": "21d0c38f7e28487e9d7b19d78684fc31cb876c1a02b2186d229ff903a8a0fbbd"}, "703cc9b9-e2b0-48a5-95cc-1ea42d282b60": {"doc_hash": "dfbb690d9bc07859b8c71600fc55b0924d0ef6f44a4257cbe7687e79948844e5"}, "6935f477-88a3-44dd-98aa-f92408087518": {"doc_hash": "93695ba4c499e953f94abeac5ba76f908ee993f3e5172dd40703171a41d7f1a3"}, "3509afe1-f76a-4f0b-adfa-80cffbcd902c": {"doc_hash": "a825d86c7b28f3a74e1a5f84a9aa2902a79bf20c53cd003774dcce5873d1770c"}, "9718f708-f716-43c9-aef5-3d79f9a41216": {"doc_hash": "41da5170b37cdfd230c531acbca20495656f1cc7d7c0d971b41ec0e0379eda8e"}, "6d8940cf-4a56-40c8-a14d-65b8e7a452c6": {"doc_hash": "1ecc94716c166527666a801d697eb12b62cbbf570929092c49eca4b978254219"}, "18692ed0-051a-40f8-9c9d-5b4f5776e51d": {"doc_hash": "ff3fc03782e1b0d5229d70ce06633acf1eefdb2225be2c0d272c4a7767940f30"}, "c93bc452-1746-464d-a1d6-1aca61539cf8": {"doc_hash": "5851e561ff6b9ebb4d368c13f526163af5f424df3abdce90e3e48dffc57382ea"}, "891e1a39-3797-4686-aac2-2324a61fac11": {"doc_hash": "0ca4a5ea3d3ed965c3963a1fd49cb099b885c66162ed7aa84e75f0edf14666b7"}, "a3e1383c-3df4-4f5a-851c-fc3f75c8497e": {"doc_hash": "2491e9f3cfe6f054f5ad47be19e9e0e61a6b70ec720b6d022996d4d52cd583a8"}, "506e2a77-43ad-4b8c-aeaa-5b97f719292e": {"doc_hash": "7779fc54e78dcb27eb1331f061456519027e8ea5f20360a93ebe8d00288b0a71"}, "d33cdea7-d2d2-4530-95b1-a6c2bddeb32b": {"doc_hash": "16b6245a311c320b072128bc8dd2992681da3de25f2c6e40c656883c264bae28"}, "e16615fa-db1b-4c60-9dfd-398cb857304f": {"doc_hash": "0ebebba941d51a936967e1516ec0d0b087f4f0e60c4bced6651a3987974709a9"}, "e74c6fc9-111f-44b9-a5c1-040c52410f6c": {"doc_hash": "a00b35fbb8bb110d1a8260661f4f57d0e627455e408db0ba368a6b046d402595"}, "d93faed3-ba2e-4915-8026-8365d2d14423": {"doc_hash": "c0a7bb2b499e087d3f1a35d60e83f455258955b9cad18b6b0e5c16a40fba21ac"}, "48477fcc-691f-463f-b45b-2849a53d6aa5": {"doc_hash": "59e914ab7b0af7ee30616641189fcb593e56f1c26c255b9176571fbe411ad114"}, "80287f50-da5d-4e30-bcc2-86fa7f642a27": {"doc_hash": "f63b320534aee27975d765cb2cbcd520696dbe062f1a3423df0ce9ab2118e8ac"}, "3e1922ff-baa8-48e7-b731-94c1daca510b": {"doc_hash": "45cfbd8fb1c39a9472cf48403c527ef785dc74f650328987edfb5cff1e4cd8b2"}, "36a7f4b5-af65-4b7e-a0ea-10d884b2f7b7": {"doc_hash": "17c90097d08d076f68eb8f9e4c03ef7074b7fb4e6be0f673369a1d4e0318955f"}, "63ee8a48-1f9a-4aa6-9ecf-14c016d732d2": {"doc_hash": "15604b393f5329eea6cce0ba4420068b49f3830077492d4889acb2ac70c55ad1"}, "749e268c-6685-49d1-8911-06d614772d2c": {"doc_hash": "656813989b4d12035443718ae72fb908a0af5afa714731bae36c2fb06ea2e660"}, "0273c2ed-70a8-46c6-a209-dec0015be041": {"doc_hash": "5416dea9c6a495d478ebc4b6b0f975fcc6236cc4499ad3dc318e5842f23a0d45"}, "5c9636ae-360b-4680-a22c-b48120ff1284": {"doc_hash": "61a2d06868dea22946d81063d4acc1a7bb3b8dea5ea707967eda63ded3c93143"}, "4c2a7d84-7bb9-4173-aba2-b8fd1e180b45": {"doc_hash": "6bb9ec21001342f52d716e8b349db6786659d2b37dda30e7b72399951b82bc4c"}, "be85ea15-9607-435f-a738-b0994566d1c8": {"doc_hash": "cd55f9a6808fdbf5837d9e8f13c187e1baf522517485120abd08078ac299747e"}, "3b92e74d-d69a-4e0d-9bc2-0cd9b49943bc": {"doc_hash": "a3824f99502c5aa81b50a6d1918fbd2cc72b000564c9a098fe5afe5c4a25f2e7"}, "2b276a36-4177-4614-90f4-ced07f7eb874": {"doc_hash": "eed352f38fc34d12faa1e973ddbdc4c5f28062fd99fc72cb26f7fa629a4f2935"}, "ec61542d-8b98-4f03-8581-edc47443e7d7": {"doc_hash": "b8cec29aa25e8a3b076ea3b26df9806ac68364c20fb2762aecf38b74db826838"}, "adeb9a86-3040-4f7e-aeff-6c399f3e1d05": {"doc_hash": "ba8adc525aa7e62ff3809767a1ed01d0c35132d9ec21bdda14b0b2fedd0b70e9"}, "e81d1485-4097-498e-ae45-4dcca4c7a01f": {"doc_hash": "89f89e682bda47a4e50bbca9ba38e0edf1d215bb24b08f3b38eec91566ea74ee"}, "3631ed6d-7b27-46ce-beb3-4fc6b4b8447c": {"doc_hash": "0a21361a0c6b5a08f1c02d5604a87514f76f9065d04fe1d62950e87f8f861c75"}, "65680103-60a1-43a6-8038-837d7875d63d": {"doc_hash": "c7603c48ae72747d881d1e842653e95169b6fe07c6bb56b0875bf31c2d7a0bd3"}, "59074bad-f21b-4fb0-8e24-f280fb071a9a": {"doc_hash": "80a121914eef16e80b242a4d126f941742ab7075699d658410345bd61e407e04"}, "788fe1cc-44ad-46ab-b7e0-8d390ceb0fea": {"doc_hash": "fabf3ab4c4b6e5e6e8f09878fe27cf4a7a478297045705b547cf9d0eef22a27b"}, "521a8690-5463-439b-8c35-4cb6b8345bc9": {"doc_hash": "7288649ed1c4615aa88b4581857b86c7c719422656d62283cdc670a36b267b4d"}, "29c94398-ca63-42fa-bdc2-da01c5a7d075": {"doc_hash": "748072ebc0ab41a7224239dedfebce054530980f5697aae3c663aea23348dc40"}, "d299fd8f-72ea-4fc8-a8a8-82f0fff1328c": {"doc_hash": "76c9b19aed89115d2efef011e5cccb93cdf45d025f22ef375a17fde22fed00f8"}, "5ed38320-35a0-4b26-9be7-8d5a4eef7519": {"doc_hash": "f772d61c32c1d1ee1acb214da8e3228d043683fbbf132e5e3f2fad3fd563991e"}, "a3197b74-fa14-40df-8acc-33bd968f10b6": {"doc_hash": "d5b852a05c94517f2600ad04ca848bb33b86d890546fa78b4e04b6983ab8d06e"}, "da934f90-7e2a-4913-94f7-f4f693618e6a": {"doc_hash": "7423ec4a3944d96443f285a077c373107cace4842ae48613f4bc84d94db39e3d", "ref_doc_id": "158b4de7-8a7e-4aeb-b7bf-c5d216ed1a23"}, "64c4f981-23df-4f51-8e58-9e7b062924b2": {"doc_hash": "0982d04b427b782eed2bcc538a730a2c4f8774f044a29067c8a005e2d73f7e57", "ref_doc_id": "f17a44af-b268-42bc-9cc7-b4d76930c2ab"}, "5b27adc0-5f42-4f41-beee-07f2029f8e40": {"doc_hash": "7da1d2f32c40e9acb97e4ff1dd51b09e7812ccde61f9c8112d09c04c8d5f04d6", "ref_doc_id": "803f0f88-0565-4a83-9050-ce9030447b37"}, "e2814700-a0d0-4423-b231-7fc15bd83b17": {"doc_hash": "4c88a802e0c432bd2ceb5478c9568adad87b5271a0305755483812ce1a643c32", "ref_doc_id": "d962c8b5-54cc-4680-be29-9586eb3e7519"}, "561613d5-88da-4b28-843d-e6694db14db9": {"doc_hash": "2f8547293a87ff7e9674ca24a91dd2463213de7ef680e49caef20ffb0910ac37", "ref_doc_id": "8be8a3ec-7583-40d3-ac97-2a239839d10b"}, "f6be9233-0bee-466e-b872-30cd57e71566": {"doc_hash": "e1c674cd8e52fcbd57f422185c5803a113759ca68b8743cfbad1bd4206a732d7", "ref_doc_id": "7278b6a0-f9c7-49bd-a0e3-deaffbf982e7"}, "da7b41a6-830f-4c6a-895c-1d1cfaf0c24a": {"doc_hash": "4c17ec859ea1f89f035b9c19593039389f9e8e7c655e7a2c19ad802be88c67b2", "ref_doc_id": "59e1a370-fd70-40e6-8186-1014e376068f"}, "6f11c00a-0d78-4788-a7ca-2f2ed2ee24e8": {"doc_hash": "f8c2d2d3d91cf09919a72706694cfc141971b5f823c84d63f6692c54dab5a9a1", "ref_doc_id": "45466ecd-1ae1-48a2-90fd-2398a596695a"}, "c5d9c598-3a7e-4c64-889b-701c8c1d0a2c": {"doc_hash": "a8a17a024c1d60c7bd6e442ff3235c31917d10cb0e08dc48ee76cd4703fc074a", "ref_doc_id": "b5661d06-2aae-4f13-8056-4e2db3b68406"}, "32a1f12f-099a-4716-bce1-d9e47a3e63d1": {"doc_hash": "d5b580066719e43e77f7561789632399ecb08e6976582900c53d5bb97b2c2031", "ref_doc_id": "38b20bf6-887c-4e02-bb3b-ece2d2f397ce"}, "2aff21b3-b096-470d-beaf-771591da1674": {"doc_hash": "862b6e0d55cbe07e0b25b2d11627ec3708c482d4721bd7fd69411330c12fc7ac", "ref_doc_id": "53b4b53a-1edc-428e-a3d5-7e6fc768608e"}, "b9d4215b-0830-4e7e-9fe9-f865f50c2640": {"doc_hash": "9a972ea50264697c85bca461ca7713a2f13dd28e6fc09c50a21a177ef7535665", "ref_doc_id": "5ee656fb-8bfe-4b51-822d-4eb09a6d2e06"}, "33292fe8-84ee-4ad3-8b7e-3e2edba505ed": {"doc_hash": "bf9d6e42414d4846996b331d349308980273d55b71c0dcff387b33acad5bec27", "ref_doc_id": "96780b3c-3e8a-49ae-85b9-10e089161a90"}, "feb08288-990c-4c69-880e-27b35286a12e": {"doc_hash": "a490857e34539a364c0ce9d809cd1326338e2bd204c38a6aa4cdd5992b99bb56", "ref_doc_id": "7f70a001-7676-4bd5-9b3d-aeaa32f2cf4e"}, "c70be966-a9dc-4884-b846-69f450cc34c3": {"doc_hash": "9c18236195f08f7f34f04bd8e117e0bdb06b6aa6e41df7b7f47e325c60646298", "ref_doc_id": "c9ccaf6f-00ac-4d62-aad7-7f976149b58f"}, "771adf29-419d-466e-b73c-400a09313b67": {"doc_hash": "1b2d12ca63a0cc6890463b0a8a990e87424e33c2960d45a290aa58600a4423dc", "ref_doc_id": "57b2c53d-1303-4874-a654-481e669c432c"}, "ec261c0b-7e21-4e43-ae4a-e8c393e6a089": {"doc_hash": "adf9f402c13c89dc1b0c2b95582c881282154938a7bd2b420bba8a08f317ec7e", "ref_doc_id": "4fd3a217-c3bc-47a7-a2cc-3660c00ad8dd"}, "afb1c572-c830-49e6-953c-91b5838bb6ce": {"doc_hash": "7470d9c9a662d3eea943981b8cdf3ba66e2e70f05cd847860d0e399c86478ab5", "ref_doc_id": "e36b77e3-bf09-4bdd-9aed-9e9095f7e345"}, "da232409-ed36-4e1b-9e7a-ee174dab1226": {"doc_hash": "9049c13ab7c44ff5f8905213220413907b2fe1e8e56ee4747c2513aca48472f9", "ref_doc_id": "949be309-81f7-4b2d-b8fb-b6e1c8243b8a"}, "ecd6489a-f60b-4f7f-91c3-93c2c2fb22f4": {"doc_hash": "b5ded5790e12a7be6581c6eab79728327a3df433dd27b187d6f04ee99a852658", "ref_doc_id": "333b3eb1-c28f-40bd-92b5-e0e2fa35310d"}, "8f710380-b0c9-44d0-b96e-77bd42e5b0af": {"doc_hash": "0a5872ce532866ea05f1782f1478294f8e167904058a4e0f57c0bcd44860da56", "ref_doc_id": "dede16a6-53e1-44ac-8b35-395e6e4fbf07"}, "276e76e1-f1dd-45a0-bea9-caf7cc81111a": {"doc_hash": "5c1792252b790de1e6772f2989743a7c6d08769e9dd76897d1c7198f22147529", "ref_doc_id": "2c728a22-0319-46d7-a8ba-9fd84fe074df"}, "ecd76de6-5648-489f-a243-15c87e1cced5": {"doc_hash": "0a2a4379855f962d05e6742aed7b3b5413cc7173e61ad6265146c398edc1e204", "ref_doc_id": "175f7a36-82be-4a56-81c6-4679d8ad2aa4"}, "3574d821-e7e4-4d0e-9f7b-352aabc0a303": {"doc_hash": "56fc905df6057787d5d30d30ca88cb40dac323ab6e0a5be45bc03e5ee1a8ffe9", "ref_doc_id": "cefe1f2c-0a0c-4644-a493-f1785b91d7fa"}, "a005d3f4-4167-41ab-b374-ed60eda103cb": {"doc_hash": "220a45ebee15efd1c8cec9b760c9492ef815abb213086e38e377180d1326121e", "ref_doc_id": "b3cdb196-2131-48e5-a27a-a6244c693ba5"}, "94f1f223-a88a-4ad1-94bb-6700d9f4b46f": {"doc_hash": "1a89d999b42d028784e9a1e0dd9a93fba0b468818e581e66409d837ec62e6e9b", "ref_doc_id": "800c85e3-444a-4195-a9de-d5aabbe2cc68"}, "20fee458-a49a-4c7e-ba21-0e5ea8c8609e": {"doc_hash": "1cd12c1dbe818e93813de8f4641f6054df407f482fb93e703086c4af5105bc14", "ref_doc_id": "0c065824-2202-4ddc-b76e-8b087036ad99"}, "a1afc7f8-e083-414b-9da6-a028894cf4fc": {"doc_hash": "249ad7b7f2e8b4bc969f0997c67c87a07149c7004c2c79d57533180fc3b9ca60", "ref_doc_id": "e90b83f0-de99-49a7-b817-3e17ed0ea68f"}, "8d6071a6-9c88-40da-89ed-b72ca452bc97": {"doc_hash": "d9121a60f9f9e5393251c97264911443f83b83b95fbb41181107f1eb1c98c052", "ref_doc_id": "5d5e453c-0641-47c2-aaf0-7d96236a2634"}, "5c9bc4b4-f952-457c-9fc0-74541858758d": {"doc_hash": "3e77c1737f48209ab1e1c8e45aee6d2efe0309bdbd9691b258b15f221908c50d", "ref_doc_id": "9975b226-070a-4ce9-8252-29c09b570218"}, "12e1958a-06c6-4067-8be1-9a189801485e": {"doc_hash": "13f8a8660774d5fefe21852db76b85289c689695bcfbae9e1403b7769c25f8c0", "ref_doc_id": "6ae163d3-8fb6-45ff-9723-3aead34f0341"}, "c40765cc-eb46-4419-b9d0-9d55e71571d2": {"doc_hash": "5c0cb0d55cb601fb78eda02b327f3ff2b7572b8401940c5567bf10ede4ced941", "ref_doc_id": "8f618ba9-af3d-40ac-9cf7-a150ad794244"}, "a04a40f0-fca5-43da-9033-4ee60387574f": {"doc_hash": "95d5a4c3a0d9d0da8574a9a1f267d63e6d204ea0a96b62349a020c2945a26c30", "ref_doc_id": "3d84d26b-4153-49da-a775-5697c59a786e"}, "cb32bdbd-f078-418e-a3b2-aa4e86a9b38d": {"doc_hash": "855c618485ecceb2004bbbd62b273086c098abc476cd7b10a54abdbf95d53887", "ref_doc_id": "8cde927c-fa2c-4f44-bbb3-117e7192373d"}, "49c4f477-90d5-4d38-a73e-1c7874cc47f3": {"doc_hash": "f76a81a95ae8373fae3090bc8650b3184ab1a2a30ce3f08fb19271f5c5d81a42", "ref_doc_id": "6ddd9787-8809-4b44-83d8-73f4e4c605bb"}, "6d13c574-23b7-4096-a223-9732f4846259": {"doc_hash": "55c03e50305ce2c179b212d828221c10b6c03aadd8bec06be9c16aaef318044c", "ref_doc_id": "82681f19-cfd0-4373-8c02-5a140c7d4416"}, "df49c6f6-dd50-4d70-9128-2a075a8de127": {"doc_hash": "4d9437214efc0008e66fc92ec42549cd9357316e053f09024c79fa35548d3d78", "ref_doc_id": "33c010c0-cde3-4d06-8dae-aa630c182a01"}, "c9ff7186-5177-4874-a324-5b1949726998": {"doc_hash": "c2b0b5537493177de72863b5e90bb8fe4c603fc9c15dbb5f5c2663cf2e48a090", "ref_doc_id": "ebca9fb4-f907-467b-a919-92a54c84d9ae"}, "fbcd03c3-2428-48a8-a09e-2d45e3f8c6d1": {"doc_hash": "a6a6b2c05aa9868e4ca8f99a4a63a644d5ffd21037f9752241bbc068f8b23621", "ref_doc_id": "026a0aff-325d-4816-a4c7-ac3210891bba"}, "71416b74-ecd1-4437-b035-685abbf85a6f": {"doc_hash": "b84787466e10624091405ae4205d32cd795187eca1e889fa50369179bf2c047a", "ref_doc_id": "65c8d38a-117b-4a97-b221-12ec138c1fe0"}, "e3098fd5-4a3f-44f0-92b6-4f521b827017": {"doc_hash": "aa1591b769351a6191298df0536d11b51751223de75b6965b9a22685844a7d39", "ref_doc_id": "0521757f-09a7-4b3f-a7d5-4ed986a399fc"}, "ed6d9857-bf66-4f99-b70c-dd2e83e927c9": {"doc_hash": "f39f9aef184311d2afb0bf8d86a3616da3da963a008fa8810e63f326ef423176", "ref_doc_id": "78dc8c45-d28c-44dd-8bbe-b1ac778d145e"}, "e2a5b2f4-0b4d-48fd-87fe-68fc06298680": {"doc_hash": "2b5c3ac43d5e5322a5a5f0ec9e667991a0da64ee4fceda1dd165f17b01b6586a", "ref_doc_id": "4b4da5b9-4280-4507-ae6b-0236588afcbb"}, "505b222f-a61e-4bf8-828b-92566105bba7": {"doc_hash": "017cc37d35de14ab86b99f70cb2ec97260f8962a33d2a2a4f87eb8515b31dc9b", "ref_doc_id": "86f26097-22cc-419d-90db-a1d6c342bcf3"}, "e2ad2570-f543-4a8e-807b-6d145ccac168": {"doc_hash": "0d8b2672ec303714c9fe34494919416785d8f007d97880fa1e886b1f8d3400aa", "ref_doc_id": "a3907149-1b10-4755-9b11-62c174602753"}, "251474d9-8435-431b-89fc-9e2d6cfac3b7": {"doc_hash": "a8f7ff1677148b1d0e7f8575d02fe264a7e5a2c3299981cfb527c44feebc8301", "ref_doc_id": "53f880cd-cd8b-4f03-b685-8e26a3ca7dae"}, "e2499712-9e36-4c91-a963-0eb58f9750a1": {"doc_hash": "15243cadd2064cba56174bee6c8ccc2e01d8828ec0b6e17594b48840602c9511", "ref_doc_id": "b47728d4-e10c-490a-9da9-712f188da937"}, "4adafff2-cc87-42de-bd97-cb62071b6080": {"doc_hash": "457a89e4e4f866c53e6e74e998d39166d4afa77b666e836a6a59d97f53d37b65", "ref_doc_id": "3a343cc4-40f9-437b-8868-bccb9e00f012"}, "0a7f0a37-cef9-4724-8a49-6a5490c29d32": {"doc_hash": "f156bfd6486ee345e3fa6a48cdc0ceca725c6596e1b7d7779a8d4659dabeacef", "ref_doc_id": "accfafda-224c-4f91-a224-3ded2f116879"}, "c8456caa-1e90-468b-b26e-ddaeda24a612": {"doc_hash": "e92c283659bf85777b46ade1914374fd49e4c4db18c6e352fc5f559db3a32e71", "ref_doc_id": "1cad1385-7571-4f49-a8d3-aa0846d34b47"}, "54819b5a-2f66-4d5a-bb5d-cba105077fa3": {"doc_hash": "ba2fa58296b4af71dd86c234f4214f7a5306f24fa732cf1fbaf63bef3c548537", "ref_doc_id": "ce0ab60f-d163-449b-b741-2eb1c7578f18"}, "b7a641ac-aecf-42c1-bc58-751b2b3cbde9": {"doc_hash": "35375ea11cc7bb18682535a5462d632c21ba5b50dc2e28f4d12114f89ca2e402", "ref_doc_id": "ebf38025-9a55-4f3b-8b77-d934a8affa83"}, "ef1fc31f-7cdc-4696-9560-d4d585421c35": {"doc_hash": "5c4122101f730e882f250eb5f48927fa570a62f9131963389089ea521b55b74b", "ref_doc_id": "4b0b820e-2840-455b-88fe-8136912889f5"}, "8fd81cef-b9fc-4bab-926f-63ea33a0c8cc": {"doc_hash": "a8b9dcbc677a15ef74b7e65f4b79fd533230b7ee04e25af3f5d7e177a2e35954", "ref_doc_id": "eea40c6e-3053-4fa0-9ae7-4ce87614052a"}, "11f2614e-7e73-49e8-85e2-498ed3156c00": {"doc_hash": "6af9a329fe087e68e3b408e9f1a2b6a611f1a8ad779e9f26b6f6b20b6cd450c8", "ref_doc_id": "62a15cf9-434d-4e3b-aa87-461f55460f67"}, "a4ca28c6-d6eb-4d68-80e9-a5920132d524": {"doc_hash": "608e19d55aa1bfce3d55a5b6ecd1fb7e4eaced7db64acef2fd066552220adf0a", "ref_doc_id": "07b91301-91b9-4d5d-bee0-114c22b370c6"}, "dfddffbb-e575-42ad-b1ac-4ad7a436df4a": {"doc_hash": "6763d831599406deb65c80e68fdcd196339ea540ec82f0be07d3a2102bccc23c", "ref_doc_id": "196ab068-bce1-4c40-bd5c-a25420c412d4"}, "75ba5197-8f29-49ae-ae1c-c8b9079e39d0": {"doc_hash": "f2003b759cf341a2dad487008f23eab33ee4cfb6ef371eb0edcff930a5352945", "ref_doc_id": "13a1e471-4782-488c-88ae-ae8a07d45805"}, "76ed2928-83f4-4f51-8822-ebb7791cc63e": {"doc_hash": "05b54a68a12723ea724372cf648a0fd306db31deeed1c0e946bcb3a4c65f65c9", "ref_doc_id": "4174e529-5bac-40a6-a42c-71adf86f2e2b"}, "3ae4615e-e690-4b91-ae76-72549a4a0a46": {"doc_hash": "12810f57e7f4a91da2a9e7d230de56e643baec699fd4d0e621207b6e03592b7a", "ref_doc_id": "498e5d68-001a-486e-945e-7a4144989417"}, "1b2fa6b5-85bb-48b4-8cb9-71cea62e572b": {"doc_hash": "f173ffabb8b2744ed1ab07e947a8e52a1fc3be565eb020bb839fc8698e0f6ef4", "ref_doc_id": "30c6e9df-7d12-4968-b529-45020c041df0"}, "3542ce5a-27a2-4979-91f6-7f0c3ff069bd": {"doc_hash": "73789d45d0392b16b52eb3f71062181844585e407a675619e4eac759cbb7b971", "ref_doc_id": "2b3a19c5-1ef0-4e37-b166-9dc1db7d6a37"}, "8c762345-63b9-45d7-82bb-da58d3d1df34": {"doc_hash": "b7dda31a595263a0c3b6879e5ea87cc0b1386541ac8e13814c9db6f63928e325", "ref_doc_id": "3332c923-efb8-4443-a217-d8d349fe2a6d"}, "9078716e-2a23-47d1-8f06-1504f8685bfe": {"doc_hash": "edf0ce3863d15ebad2ed5e8f2f6ee1b0792c8c600dd35244bd2a17b60351e4fe", "ref_doc_id": "64a28575-6889-4150-a276-f2b9ca6e08d7"}, "b712d5dc-fc0f-426a-a3a6-2199d6354fb3": {"doc_hash": "7e17359c68d99ca1a7815139a6c1e12da8bc7ef4a40d7dff28874eda5bcdda83", "ref_doc_id": "bc608379-63df-4bb9-a540-7ade84f76ec3"}, "7335fe9c-efe6-4626-8a31-4d8027db5439": {"doc_hash": "0628fb319aba18e39f7d1a9122b02965adc6f44d7ae09a0647a3b555487bcf25", "ref_doc_id": "e3a94c32-7875-476e-af9a-fd1f1a3ae230"}, "36c08017-b2c6-436d-98c2-dac376ca0b05": {"doc_hash": "42e611d12f6135d9ab8906a9b2f9b9348f91a6ef1ac1979b79dc48ecf7a31e54", "ref_doc_id": "5c165f2d-39ac-4820-9d2f-05b833b94bc5"}, "10019a92-e280-4e2c-bc85-5c1a67dea947": {"doc_hash": "06d8fdca8eebe594cdd4d5211af311c727051915eec02279e82b35c3caec254c", "ref_doc_id": "39b200c9-1f24-4ed5-8ab3-1d71e1cd8ccd"}, "8c8a5d68-d477-4c0a-ad3c-7b3663d069d5": {"doc_hash": "6f7c6d16d6ea46732499bdac28b83b61cc1a6ee7c0ee167715429c8d03ddc00e", "ref_doc_id": "72638843-dc45-402d-9618-a3a03b43bf9e"}, "0c66afff-e8e1-48fd-86b6-ee6e91d74bc0": {"doc_hash": "9b1e2037437f93aaa145e15afdc2c095916cb76d45300bcc933a79140694b779", "ref_doc_id": "e8aa1a36-af32-4dd5-a440-42a0ffc1fa99"}, "b14badcd-adb7-44e1-aa5e-3282ffb91522": {"doc_hash": "e856332602ea6b4f7604e0b9d679098bca04996ebb13e7721fba8907601b0325", "ref_doc_id": "bf5a425c-53a0-43a8-812a-14d1e8963cbb"}, "7d90f68a-6203-4908-9625-587075dbff69": {"doc_hash": "eaffc42cf818cba4b6980f07c8cfc23f4ebdb58ad81be351ac9239993dbe8c15", "ref_doc_id": "10fe7c4e-0bda-4546-abe0-4b71ad91472f"}, "775a2ebc-0d43-4d2c-af8b-2af5666f7edd": {"doc_hash": "166e5ef1bdbf3d7190d3b787b30ac37df692d22ffa1e8e05ac6a87d6963a0979", "ref_doc_id": "6b885a6a-9e08-44ed-a3aa-5aaca44778f1"}, "dbd5f333-e39e-4083-8f37-acdf35019a1d": {"doc_hash": "ac6ac869e428a344690d308fc5b3997bcd1d43b6b25d551f0f5c0735a8029c0c", "ref_doc_id": "4e62801d-7a35-4ff1-aa17-642319b35c71"}, "3c38e9b5-212c-4c9b-a68c-7bd0c93dd8e5": {"doc_hash": "a11083e0c6615960c858515ea85b4c1e5e24ada834683870274f544d2608aba9", "ref_doc_id": "2ff38da6-8d1f-4878-bc32-af2d9b203592"}, "2dd7cb9f-5eab-4f78-b56b-5f0fe115a0db": {"doc_hash": "d67ecf4587b8965bf9a54a90cfcff90fd9d4463f418a0c055bd3491289270625", "ref_doc_id": "a6527f2a-9a0c-425e-8591-58e749edeb0c"}, "66c29b72-8350-431d-aece-5355ab7d69ec": {"doc_hash": "a006dcbc7a384fedcba98e2a1661614255218f386cc1d2b18181aa94c453dc9e", "ref_doc_id": "67a82af4-295d-4baa-88b8-dc93e18b293b"}, "42f679b8-42d0-4ab7-9a90-4aeadfb05f06": {"doc_hash": "d40ca01b96253c216ef466c07339abacbe18770fc00d904f1ef4f31063032451", "ref_doc_id": "47957cf9-b8c1-4fce-9d12-1767f143269e"}, "5fbf53b6-7261-46f2-a5ae-a7f1aca25ebf": {"doc_hash": "d4cd170c3b69e53653f6bac4840f083bac725013c91e9a76418e699142857e31", "ref_doc_id": "85c5bd4e-946d-4cd8-a653-51fec6567eb5"}, "cdbb1031-2d5f-4f47-97b9-bfb06d6cdb5b": {"doc_hash": "2cba28dba68eddd7439b4998b4bb7a5923161372481120d3be2ea4256bc7bc2e", "ref_doc_id": "d22e775e-a19e-4e7f-b00e-06df2a4a163a"}, "52dc0ab1-6f61-4e06-9298-0cca6ab08fa5": {"doc_hash": "9cdefd22bbb6217efd45d78280c2895632dc154d4df87113abe14158bd5f49b0", "ref_doc_id": "0f92a0d6-1409-4419-a9c9-9ebed828fea4"}, "251ada01-d425-451f-a112-79422e782dad": {"doc_hash": "699d007af751855dc4f448fbc1b3b51b547b84d44e6ca14a92b2ad28cf430adf", "ref_doc_id": "58a41555-a20a-42a9-9b7b-9f4ab164f94f"}, "2e8b47db-184c-4cf3-b0e4-57b0110ff489": {"doc_hash": "32b891046da4f5fe00786df5dcd96fd7bae91d65dd0e05565346453cae6cc9f8", "ref_doc_id": "1fb6742f-57e9-41d5-8c22-274b9d3c669d"}, "2bdf1dc6-99d4-45f5-83b4-ebc18ecda7c9": {"doc_hash": "8cf0710cdfb02aa614546a8be3f8ee38c99e8da59c14a55229a1617424bfceb9", "ref_doc_id": "bd6c2965-d1ba-482e-aee1-51aaa8fca60b"}, "059042a4-b304-419e-8331-d94647c3e944": {"doc_hash": "026b148de0cb63edbb203d7d8a8f9365272d973f0bb120a0dc859540ad32a58d", "ref_doc_id": "75fe0343-1e02-4bcc-81bb-85c584e638dd"}, "c24b5add-3b39-4c58-94af-e7a178e4cff1": {"doc_hash": "f464230b197f72e6cecdc5efe310b5ad738572930daf6b3cb645256cda9db142", "ref_doc_id": "59494823-b19d-403b-ad05-1360189fd0e7"}, "1bf032ee-2faf-4376-8694-8fa7854745cd": {"doc_hash": "f7e5a6f018b8a0a8af4dd9f76100338a9466e2327766192342913fbdb7d2b61b", "ref_doc_id": "2d0fecaa-1342-4773-9153-ce1f0673bb71"}, "515847bb-a4cb-49e0-9d2b-3d71bb4a486d": {"doc_hash": "d7a488bd64e3e091aac867686e24bc8bbc541a978040010d04b990683940c664", "ref_doc_id": "a0202601-d35a-4778-97e8-1416cb1d40b0"}, "87e16443-3e79-4601-bb84-50c6cbfc0139": {"doc_hash": "e7c83f8bc18de176f79a9a8b46c89cedced17e8e653ffd0a1d8c535fc9f98b0d", "ref_doc_id": "47496805-f1bd-4ac5-9b88-61e65eab326f"}, "e2e3068a-e3cf-4456-8edd-365a3f0f9296": {"doc_hash": "f063cd5c6938d690bd7d3573920e7a02d96fb9aa17294881c4ac6f032c22c581", "ref_doc_id": "8016b10d-8686-4b02-aabf-7264c1e5dd51"}, "d666d74e-6203-4dfa-a846-13a88a489434": {"doc_hash": "1ac1b4ca2f598741574e3b0da12465aac911f9efe9e7a51c63adad9d63a94f2e", "ref_doc_id": "23d8dc60-6c7d-444b-bd39-f2695060e598"}, "6cde527f-9cab-454b-bf45-3e622a7976e8": {"doc_hash": "c101dda5a7e20fc8bec05908bd8b8db40ab2ca0a7c6c1cc1f9be37f579cafc92", "ref_doc_id": "c15c24fc-48d4-4048-8a31-5b13804b3eac"}, "57d88b82-34ef-4dca-bf49-47cf0f71e03e": {"doc_hash": "774e0f2b3d2a9287bd7ac1c50d13989d0f1b14d1ae4a81e2689d58b69f112a76", "ref_doc_id": "2d95fa0b-5348-4e94-9d2b-ec09375e4c0a"}, "c9d594cc-8ba0-4445-850f-49779355b3df": {"doc_hash": "a0c9382c218461f07f9658333a00a800e5b0a3ac54a23461b413ec25644b9e67", "ref_doc_id": "29ce8a71-ebdf-4fe4-90a7-df77cadc0597"}, "a4dcdf71-a18a-4029-a1cc-b5a45c22fcaa": {"doc_hash": "560b5549e0681a30cb84bf062d2de0494565279bd820e1228ec47dd3c05983bb", "ref_doc_id": "491ea2be-1772-4dce-a89d-bca86f9f0a1b"}, "11f1b5b1-a4df-432f-bad0-08991e0e0f9d": {"doc_hash": "d3ea0d94e4690867cf6fbe61e97ca0cf53321ae8999608e641bed6b045ae9a10", "ref_doc_id": "73f18ff8-6c82-4fec-9751-419c75de6f77"}, "9a3961d5-07b0-4472-b2e0-c27b83ed1d05": {"doc_hash": "e8752648b982b356ae6b95c1bd6b754da2498f5a49ebc67be519b4d25218992a", "ref_doc_id": "874ecc9e-2088-40a3-8c9b-a0149c34740b"}, "cb42dbde-a4c4-4548-b94d-be9f51b6f48b": {"doc_hash": "3d0d604b443d5f98d24c93f2f412a832bfc09613943236ecc86590be5c9d3f5a", "ref_doc_id": "537bc11f-d29d-40de-973f-98eaf9b3e3bf"}, "62fbab34-2776-4dc8-a1d5-edca9b67141c": {"doc_hash": "309481dabf346423748e6980116a12a99bb140dcc2a5294f30a5978c222268b7", "ref_doc_id": "6476d238-12ee-4cf3-9af7-69a88bfcd53f"}, "e9037245-9f85-4d43-802b-92141bc9dc7b": {"doc_hash": "839eb1cd73f6ea7180a62d244ae54c5833937e456a0dde6a888d144197a65468", "ref_doc_id": "52691c9d-5a8d-49b2-88b6-bf5d28fff13f"}, "8e0ef7ca-e028-4fec-b94e-82efdaa58695": {"doc_hash": "d603ff210dd1127363c1270f6663fa6ec11a8bc39ac3367326f599e7c739bb58", "ref_doc_id": "362504aa-2f4a-460d-b953-e469544af73b"}, "16ed9bfb-cb76-4bd1-b6c2-6013827f9e0a": {"doc_hash": "00d1faf2a508de882fe35db63759ad6c4146e297bc7d886c6217fda3660a2ef9", "ref_doc_id": "d08ab820-f6d7-4188-9b8a-3d4b18ad6fc2"}, "38faacd5-3f7a-46ad-b06c-3068129eb40b": {"doc_hash": "3b99bf85b373ca5eeaa0793da58070252b6a0d3dd5d8acc09f1cb13491de6998", "ref_doc_id": "33be2da0-97eb-4239-9ff7-d7d9e5a8de60"}, "a460fc7c-baf3-41a4-b636-73848f105665": {"doc_hash": "95bec6ee66abc6ef396dfdcb22983f51ea771f088bfcace57360505e3333d4de", "ref_doc_id": "749d3916-1057-456f-b57e-a87090801b78"}, "c0815e79-e88e-4800-9399-feb5f7747389": {"doc_hash": "2ce5ac104f92f4cacc51855ed880aba9e9ffbc707a39fdb475cf699223a3f451", "ref_doc_id": "879c0ea2-9517-4705-ad8e-e422d4f0c535"}, "b10d7e57-0256-411f-80ef-5cdf0f6bbb37": {"doc_hash": "31a097488ed9cc37f4ab8865d4ebb2ac7f8c19bedc21d427a80b168f0a51d84d", "ref_doc_id": "5f18ff42-25b2-4874-b9c1-711b6730375d"}, "e001bf3c-f75c-4cb8-b2db-ecdc44e18bc0": {"doc_hash": "f8b4ff0406dc76b364096db19b5422f61ffdcb72fbac489cda5369ca99a7da3b", "ref_doc_id": "21d53f44-d41a-43a1-943c-f709044405f4"}, "74a863ca-a564-46a5-a900-9cb8332a30cb": {"doc_hash": "d2973d7fa3d2d444c2fd3da2c7eafead5e7f39d3cf6e2851925ef6695a84378f", "ref_doc_id": "7c087c30-a793-40a8-98b7-f2b2709f5e77"}, "1ccee254-1286-41ac-8fa1-1448015dd57d": {"doc_hash": "a7520aacbffd1763dadd60d3ec03d10f715c0851e761d95d8c59b1134a67507e", "ref_doc_id": "77cf12e8-62e4-4986-af61-c9c8c1c412a0"}, "bc55e92f-4ed7-4ad8-b5a2-dc697a698b37": {"doc_hash": "1bb709663d75151550a771c5dd36dd746ad29147851b6761360efce85dc4ac0b", "ref_doc_id": "d92d8bdb-8825-4a32-801b-f4742dcb6c39"}, "3bac6414-d57f-419d-bb1c-04f016018f4d": {"doc_hash": "266663911420a60b03be238d3bd852e0078aa3649a09a6114ac4a03fc9af36cd", "ref_doc_id": "83d23e43-aee3-46b0-8eab-920371d96849"}, "a4e76447-c943-4995-986f-4fbe37003ffc": {"doc_hash": "db8b95a793d76fef78e56a7bfb969c6251932dd1a8334774a8eadb44a122f2de", "ref_doc_id": "d6624e4f-7dbd-4716-b413-cd66f0055a69"}, "0b12f8a0-7c7a-484d-8de8-4c7a1e66ec4f": {"doc_hash": "1c30d476df91da0bb5dc2b91bb08e2017dd8081f1dc5e85693791a0ff3a0e0ed", "ref_doc_id": "951c4c49-2d38-4604-a115-0f8dff56ed80"}, "0250f6cd-9ffc-4d0a-8061-7d4677fda721": {"doc_hash": "27f60a967b5d0155186956a39ee617b9cebfd1827037b3ae95814bfa9b12ba65", "ref_doc_id": "73b29697-d1cb-4732-a6d9-815a63da0d05"}, "121ee863-9238-4561-99cd-46b3adbcd66e": {"doc_hash": "11e4186dae63bd4c4c42fa14097636e1b4fbc91dc0fc3397b144dd658e82a2d9", "ref_doc_id": "25ec912a-b6f7-45d4-b7d3-b03cefd03e07"}, "9dd69bfe-0179-4c4b-984a-59f9361b9a22": {"doc_hash": "76b0bee7d6ca4454aa56f2914372fea4243bd4f02a24239752fb778b3b69635a", "ref_doc_id": "eb1b3245-8f85-498b-8c99-acfb26d5c969"}, "ce78b252-4ee3-4359-8dc7-e588c899ce7c": {"doc_hash": "28dbbbb6864ef45286c2de28fd02bd4f6b5615ead54dbfbeac4be9716bdc89d0", "ref_doc_id": "dfd9bfb1-094d-4f66-be09-a912dcd9a4dc"}, "315d6c44-089f-4c97-a8df-c067e81bdbfe": {"doc_hash": "6acd5b4206281aad5b363750c9d6d6a8cde5e9035a1fd7775f6182c84a3577ed", "ref_doc_id": "59e17e04-9380-4f53-9ac7-e9f2a0385485"}, "94475ae2-be3f-4584-933d-a4ecd97b0bbe": {"doc_hash": "b26d16a66bf57cd4cfb1705022af25f42a4162942767df4eb93d6d6c54fe1c93", "ref_doc_id": "4c8ef361-db9c-4853-a030-522ec6535aee"}, "479f0d68-dbf1-47a8-bf24-fe666cdbd224": {"doc_hash": "5dd2eb7cf98a6a83f5f739ff7c2af3857bb6b2cefe1b31bf2b71e7553257a20a", "ref_doc_id": "d001133d-911d-4af4-9a0a-7ba952670862"}, "28385301-280e-422e-b172-34d08728c15a": {"doc_hash": "a9faf3f6613f3079841704a3e4166b4a33d6335c0714d9ad36634d083ce57fa4", "ref_doc_id": "4f1b0618-4642-4184-9aeb-70e4323fa0fd"}, "8d0639f9-d01e-4ed8-97bf-b4ed07d0bddf": {"doc_hash": "c56a5afdee98e50d075afc7c0ca687d70082bc25e880f69fcc12cd2d25a4e31b", "ref_doc_id": "dc798519-767a-430c-a81a-dc7c36638aa6"}, "ffdb2205-9058-41a4-b42b-663cdeae02d5": {"doc_hash": "6e460c28455289c3f43cc45848a8e3f7ec4e9e7f065ca3bdd3e3641306a35bdb", "ref_doc_id": "124b4c50-fdd5-4ebe-856d-839d9f1538b1"}, "3eb283ba-a6d0-4c5a-ad84-d7b83a5371bc": {"doc_hash": "0db45cac8f04f4e0f715b9623d007ad225284ceac9730a589bb36d379620f961", "ref_doc_id": "7fb080f3-6d65-484c-9181-880033258288"}, "2ff95447-7a05-4d05-b932-2af443623148": {"doc_hash": "0341d05fc700520887761f566bead4a8c22dd73232bf2a005f6c0a05c9080332", "ref_doc_id": "3302a6dd-22f8-4137-8488-8a627e163b02"}, "da43aa1a-7381-4b58-9b35-6520c780183f": {"doc_hash": "99e41a2f335e843eb383b6a42ca14121c177ac205db92110e8cd3cd0b611ebff", "ref_doc_id": "f8a9a860-2380-4bd7-842d-71fe3fdb8f6c"}, "803a94af-7bfd-438a-932e-914b7fdaa30d": {"doc_hash": "83a783388028ab2cbb3f3f08041667cd8b9d682e56fee4a08ade3d60330ecf9c", "ref_doc_id": "12290d7f-3841-4666-a602-db5ddba1c7c2"}, "61d4e394-6479-469b-9d78-479679e71c70": {"doc_hash": "1adae241150f68e75bdfb2da9e29f971cd4a65fe10f062ae2d688f1cc6c3411d", "ref_doc_id": "8de76ca1-1478-4cf3-b08c-af18a5ae2ff6"}, "f251fcfa-904b-4e5d-846e-0e3fd9e9c99b": {"doc_hash": "89a443360ac228446c0a01682b6033ae5c783633e9cfafcb224fd9c59d8ae339", "ref_doc_id": "63fa029d-cf64-41f9-bd0d-d4bfc46210db"}, "d261c2a7-41d1-4a96-83ed-20c73daf661f": {"doc_hash": "b02106444dba2e56d6f64325c69b36b7debe2e314a332f471857cc43c0c6f1c9", "ref_doc_id": "35dfe2e9-f655-49f3-9b1c-fc17d09e9a01"}, "e2955a4c-b889-4a6c-b478-078006eb11c5": {"doc_hash": "137f5ecce2660f35f47e9cd41bdadc747af1d92df2ee8224bb1fdc6d8e5cf07b", "ref_doc_id": "eda0303e-0beb-49be-8d08-3ee5d9c8aea6"}, "48b3a700-4052-4141-a777-b7423a9f8998": {"doc_hash": "9677d1f00faec904b6a83d9e2a1e08ac84b7a31139f65802b149bc4d30821fcd", "ref_doc_id": "b73c1a03-565b-4223-baa0-5d9d0417dcc7"}, "b8261a33-323c-45fe-a8dd-40033faaf376": {"doc_hash": "205ebe6ffb62371474fc09cdb63a2073f8cf53b9c1038b59930d0d9d10c11016", "ref_doc_id": "345a8a3d-4e15-4d26-82b8-ec307d36e7ba"}, "575fec38-86d0-4072-8f68-bf6c5817834e": {"doc_hash": "347093a9aa1be75a7977efe2a76d196b2a6eac84bd3e91892cec7baef27b368a", "ref_doc_id": "eed8740c-838b-4ea7-8d9e-24766f5de35a"}, "22500a9d-7f9d-4f63-bc64-e47857a76df4": {"doc_hash": "11d68dd97d72e065dbc6a376868ef73eb01c516fa22f5b87ba328b4fb1bb0580", "ref_doc_id": "e186f9ad-7aeb-4bb8-839d-39009f37fbd4"}, "cc4ee4f3-4aa7-4647-a69c-47593e9970cf": {"doc_hash": "ea2961ef7bb1aa7d55fa8f1126eb2c569e8b1bc99c6fbf7cd455624243785143", "ref_doc_id": "4a598210-0d54-44a9-9502-ec10dfed78e1"}, "4794e613-bb4b-48c4-b6a9-21d09533b7a5": {"doc_hash": "576142438bafdbc3a2d3c072bcb907eefae539b70a458ba37ee09d0dd2aecf67", "ref_doc_id": "0936adf8-4efe-4e42-a51a-30e6254e307d"}, "e111f854-1f76-4cc5-b8d4-f25ddd2c98e2": {"doc_hash": "97b80f4445d488a299667b334b18aecc6538df3f1dd67347a5a6bbad0ffc34cf", "ref_doc_id": "1c9b9f51-7f61-466d-b092-3e61f64ca6e4"}, "549c8289-fcb9-48f2-ae17-8a1aa78c18f5": {"doc_hash": "ad01ee2dd049748eed8ecb55a4dbe49ddea0c70de230cfba58c622a47640aaf9", "ref_doc_id": "f63d7ccc-a7dd-4fea-9c51-4d82b8b42c59"}, "7792c5ed-984f-42d6-b4b3-05d9d506e0af": {"doc_hash": "c4c090b279b302cf58caa5bfbcf1f6ffe02825d6b41783111be5ac0763aca597", "ref_doc_id": "c2ae9afc-ed09-4c1e-a1bf-ee09e7130048"}, "afdcc41b-4d19-47b7-9cec-407925fa2ec6": {"doc_hash": "dc874f25dfa4207c813e2a16c1b788d5e453cff778728dcb54529a2e2229405d", "ref_doc_id": "3ea71c8f-e875-4f17-9a99-d87468767bd7"}, "77bd090e-0514-4d8e-980b-de696eb48a71": {"doc_hash": "9f3c38a54d61143688d88201fc0b6b9814d3e1fcf9e63667f0791846d37eb433", "ref_doc_id": "88dbeb51-81c5-46a7-8629-780d2e47b287"}, "bb14270a-374d-41aa-994b-f95444f807e2": {"doc_hash": "491cf6fe05c33c0ea986b33bbf8a5e1a839f250c9939f0c5254ed2ecd7e98cc4", "ref_doc_id": "f6e43daa-7ddc-4891-a997-03a0736954bc"}, "a13a39c5-021e-47ce-86c7-6669dece0b96": {"doc_hash": "c7609a5f03743ac61c98cdbafddc5f22aa3d4f1610700338e4f16fd630d33f72", "ref_doc_id": "60f0ca57-3352-4ff5-86df-948bd5358ff7"}, "745238bb-3789-4c80-afd6-4360f2a4edaf": {"doc_hash": "2f2c80719c7325fc4d6e0cc4789dcfd578068e5f668c3afb8fda57f471ac1a73", "ref_doc_id": "95b20aef-433d-4a25-874a-ebe075d8b4b5"}, "02db40c4-fbf8-4be8-b877-95f6f6677d42": {"doc_hash": "5db3c4d87339d94681dfa497c4fb70fd0d9a750f55e2ade814da2dc2f39803bc", "ref_doc_id": "c4c09667-167e-4ce0-8055-26b96db88d24"}, "c8e13435-2962-42f5-8193-0b3979e8dfd0": {"doc_hash": "090021b84b2ddf1b9c5077f17b7354facc09cdedcba4869589ffd2e405894f4a", "ref_doc_id": "c0037b18-1040-4e5d-ad92-d82bc18c1858"}, "6b395a0c-ae9c-4b6c-ae07-641d93c5681a": {"doc_hash": "62739275208601ed823e869c17d867d245ac5203df7e70d3d026c0cd8d297c45", "ref_doc_id": "0b90b9ca-ce91-44cc-a62a-2d3c5b43aa71"}, "1555d286-0a8e-42b2-8551-a3c3443bc9fc": {"doc_hash": "369999622d32c892a99fa7303ca1ccfd97ee5dc5269c7d8f6bd84a406d75992b", "ref_doc_id": "d66b00b1-e2da-4ec0-8683-65c1b2ea9b5b"}, "93bd6082-5127-4f7c-86ac-ca615f90047f": {"doc_hash": "0ebc90d4b715d4296e7b98f8de4e11f6552687d0a32b5997469150c90c9fcbde", "ref_doc_id": "8c559614-084b-43ff-9cfc-7ead488ee6cc"}, "24ec6315-6c4c-4ca0-97e4-8d51191731b8": {"doc_hash": "325df156fc6e52ad16604bda66747573f75a1c9c2f08f488b557853bc30e5075", "ref_doc_id": "4ce965e6-4a71-4430-a8ef-acb6044f9a42"}, "67d26cdb-8f65-42aa-8921-1204673e96c0": {"doc_hash": "e672fd317797e7efabac4a49c5d0545b19245823cca28c6c63f0fe575abcff84", "ref_doc_id": "67076165-aa9b-411b-9022-003bf9b5318a"}, "6e930f6d-603c-4e78-abd2-61b3fc0385e4": {"doc_hash": "d7036c9e621074fd5e46768b1131a6822644f4d9354d8de8a5b723b231319354", "ref_doc_id": "af130a67-c49e-43bf-9b58-50a950c1ff08"}, "66329508-b3e0-4025-8e62-5fa7fd9cd628": {"doc_hash": "d5994dd6ba9102f734e4eca91a64a9e6f55deac74937fbefac94090ec73b05c0", "ref_doc_id": "eb13fee7-e50f-4d0d-9f2a-4ba29c7bc046"}, "54fac147-2b7d-4043-9fce-fdfb8d1128f9": {"doc_hash": "cfa0a2d1d3ecd78b879b5b465c14556f65c4910ea95fd4ec58658887da52cefb", "ref_doc_id": "e73266c0-a56a-46f5-aca6-5ae3e27fa699"}, "fcff0f9c-6df2-447e-aef0-7b572c15d24c": {"doc_hash": "275e1d50368c9b4cf396a9a217fb0918f14b88c6801c8c114ac218d3aa31b0fa", "ref_doc_id": "0e8b2615-740c-4739-9cad-9d4bc88a465b"}, "07ecd657-02aa-4eaa-9692-67144c0ad1e1": {"doc_hash": "f1d011bb20b0de4b5f254aa785430fec0dd8fc61d0402812b90c0d31fc86f604", "ref_doc_id": "714d2d41-4756-48a3-86a5-64b8f6f54d97"}, "fb60e919-c254-4cd3-8f0c-21441eee671c": {"doc_hash": "b61c8d8c228b699a9799f5ed88eb49916497e5f9a252ed55f4f5442075ea37cf", "ref_doc_id": "db0cebe4-09ec-4fcf-9523-bb6f5fe36294"}, "a89947bb-55f3-4541-96b1-0302003c95ef": {"doc_hash": "e020f7d410090af1819b5d496a42bd83751ec12deaf0dc44f322b33b9cbc1387", "ref_doc_id": "d35650d0-7ec1-403f-b4fc-8b39878e25f7"}, "53373e60-a5fe-4ab1-b552-6cbfa9f8ea4a": {"doc_hash": "08cee8aba0f25370da6d6611d15ef118fcc2325db9d32e3728ffdd99b63796ef", "ref_doc_id": "75584461-ea9d-4cc5-92f7-49cc25cdd0e9"}, "46bac0e0-f70c-4c81-86a2-1dbf66d1127d": {"doc_hash": "1fb85f233ed9b3814929431888e0c5c74fdd21faafef848afdcb61c30900991c", "ref_doc_id": "85b9f8fe-8a24-432d-b7db-a45a78223a81"}, "13df25be-addf-4d90-adf8-c00769ded84d": {"doc_hash": "dbffda7ecc5ca815753c3e6f4688bbdf714004ff67c81157c664952eb395479b", "ref_doc_id": "46b57060-f7ac-4571-a868-c6e91749782b"}, "089f6d1c-aa88-442a-a81a-38c9a3f9b675": {"doc_hash": "3420eee6b90a0005d98794e06a67d44a5ed0ad60e17029569965d7b5c30d3559", "ref_doc_id": "eb09f4e1-d0af-4f9d-9124-6ef7f7280147"}, "f94e1014-f7be-4e97-b2e5-8029c6d82de0": {"doc_hash": "2c84a2f7abe6ffe2a3d1809107261ae86a00ffe6c12fcd1935c17ab574c0853e", "ref_doc_id": "919841c8-0688-4df9-acc7-e86a7f2ae6d8"}, "f2cde020-300d-4fbd-aab6-9e7ca7c1c4de": {"doc_hash": "f3557c1e75c3cd81230dfce282ce27ee6d4c0f68536acaa5e8de2f17826f03bc", "ref_doc_id": "03f2be03-5506-49cd-b146-24fcbe01f957"}, "f265f3e8-dd1c-4ba1-8040-a76fd1de876c": {"doc_hash": "43a77c4a92d56575f3d58553b364583f676d758aff8ee96b7e010156d455ba6e", "ref_doc_id": "b40ad265-d181-4cef-a479-eaec0cd332c6"}, "d11501cf-dc33-4b0b-a783-87d9d2fff5da": {"doc_hash": "67223bbf4b6a8faab61cfd58082708e2a9ebb43d37051ddeb76dbdcbe01b915e", "ref_doc_id": "593a8ed3-5fa6-493c-b5ad-513122c2e444"}, "b590a616-c38e-48ee-ad45-fe1849e1500f": {"doc_hash": "ed88a286e31e4942472a97b1a8c26e46837df34c6f76ccbfc0bea3f12f4b4438", "ref_doc_id": "95768b69-1d3e-4936-b120-59e38eabf579"}, "dd639d1d-f3a2-4074-aada-647a6da30957": {"doc_hash": "d01d22f5687ea08d9122c48cfd9af966c7bad4082167fc4198615d88931fcd79", "ref_doc_id": "8b6f7b7b-52dc-477e-ae8b-97277996c19d"}, "689277d6-465d-42ba-9d9f-c32d41e7bf3f": {"doc_hash": "0ea3d9d2a04c3b9aa36e9795993ec0efd808098bdf84dfeee41e6e5241aa7a5f", "ref_doc_id": "195164b3-d902-4e34-ab9c-488282e01f97"}, "d844c2cd-126c-4f79-9fa0-70802e22894c": {"doc_hash": "a8769b63890f442c7ead7c20dbd44901de25de07012e58adee53701f9a39ff54", "ref_doc_id": "c921a356-bdc5-4096-942b-d82378df18b4"}, "8949d0cb-c011-4dbb-9327-1e40d32eb000": {"doc_hash": "3d36e4089b7dd594b7fc888fa467f17f3e301a537c5be5eb8c54c86b121f0936", "ref_doc_id": "8e8bb51e-8cbe-4634-ac6c-98bd7211de12"}, "b1c16112-5316-45ce-9559-a73cf5a73dc2": {"doc_hash": "fdd15fd95fe468f0dcac4745558d9dcddbcc512639d0141252ca4d4e0af556e0", "ref_doc_id": "b38ed499-9796-446d-ad50-333c5414f666"}, "3af368f9-a3e5-4145-97d9-90efff86a7ff": {"doc_hash": "9061cccc14014046fcc644faedb6d7533d2d9ccfe9f5728f468f323c311d1dbd", "ref_doc_id": "1d2bf0bf-8cb5-4476-aae5-3b1d76c63966"}, "538ebd86-7ef1-4052-8b60-0156284a8acc": {"doc_hash": "7d1b2dacb77b5d5860bcd3431f26ff000355c2244f44cfba869e73b00123270c", "ref_doc_id": "9131b24f-6d97-4bd8-a5a0-355b25b4db76"}, "42f75abf-b44e-4618-8e63-ccc94cad1d8a": {"doc_hash": "d3ab474bbf6ef9b2a4fad129b42d83a2e3d434d9cd8ce8ef8fcac8a1d66e6022", "ref_doc_id": "bec3700a-1655-4ff5-9f03-c325b45911aa"}, "f9ed0a3e-b517-4224-8883-e2cea1a0296a": {"doc_hash": "f05c3af9b1738f4b4a83b0066b386339c44657a17c65a6c54e2ad93645b55c35", "ref_doc_id": "b31b14b2-30c3-4d47-ae4d-c5fa2eeef456"}, "a6dca04c-7115-4346-af1b-ca27ec7d12af": {"doc_hash": "ae46669442bf065bbedae73936ab10628fa5f342a26860d49ad0ecbbab9ec7a3", "ref_doc_id": "a7a48cd7-8ab2-4550-9ebf-8f7baa43058a"}, "06591038-061d-4627-92a0-45a02758b81b": {"doc_hash": "8d70bfa3705955f95aa7da4b468329804019488386df9a665fef40cb480c2ac4", "ref_doc_id": "b2c8bbd8-bcad-479e-816e-199bff35e224"}, "cc09b4e5-fd95-44fb-8eda-33365f120b39": {"doc_hash": "181420678445b5e9812af0417facdaba49ded59d8883f28954f975993706a3b3", "ref_doc_id": "5605c88d-26f9-4dd9-ae25-29adf6c839c9"}, "1be1323b-77fb-4bc0-ba12-80dfce49d216": {"doc_hash": "c48460fbf29d891b852039913e00d5190d186f935d10ed1cafa1a1291a049f09", "ref_doc_id": "de4c517d-cab1-4e88-9e98-32bd65f36774"}, "d03c1fc6-30d7-4b3b-a4b6-9c979fd2ce56": {"doc_hash": "7172cf832785505c3fbcb220187fde11c7218746c66edffcf8d8d72f453c768f", "ref_doc_id": "d5b9410c-6060-461f-80c9-1a079d431dff"}, "5995a41e-4da1-4b08-b385-db4e083057ac": {"doc_hash": "80efbdcd98af409e507ddd0c2c6fffb3f0ba297fddeafb27b1e3e0b2c422554c", "ref_doc_id": "9ab61b82-4266-45c7-a720-45f93c985cf6"}, "a7d43dc1-4b4c-4e91-9985-48825f58d304": {"doc_hash": "b2f5c0a3b90d9722104474a3070f7c0b1feb8dc0c2f675c4eaea51ec86a2405a", "ref_doc_id": "0f778f31-41aa-4d34-a48e-9e3b4cf2a339"}, "2e95e850-091a-4987-a3cb-f0c51a045e15": {"doc_hash": "8759d8ae2461332e504b38516f7c7712bc3794e037169465c52b8e71646c1da8", "ref_doc_id": "b6911630-7e84-41ec-8d57-941199bb45ea"}, "8e7f94ab-4aa8-4f02-8675-1c62edda9592": {"doc_hash": "610bf6c596255916e50c110504940145e6002f7a2ddeb69cb1ed3b3f8b2f6549", "ref_doc_id": "843e01eb-cd13-46f1-9318-12dab5e46341"}, "9a3e0d76-d20c-42ab-8265-d6d8e1a86b88": {"doc_hash": "d9c55b360f1b9bf3d27c4680139c1c8477ff97207b3f6455967e64c5ed80fe57", "ref_doc_id": "f9f956fb-1413-4767-b5c3-3c2a1ea68001"}, "63d9fd76-b0f9-41f3-9bbc-8b495ea19308": {"doc_hash": "f63c2276f02cf6686de094c23648442fffdb7441a8480dce70370ab503ff797f", "ref_doc_id": "11e11b6a-99b7-4449-9e82-3691af2e3ae6"}, "8633b959-2abd-4958-9d23-3321ddd50ec6": {"doc_hash": "dc79aa4840e1e4045b522dcd30954e309473ead2852a168a9c12c4d2c3cdf082", "ref_doc_id": "1b5388e7-9b69-478c-9d74-5deece8363bd"}, "369a1355-c2b2-4637-a58b-52ff004c673e": {"doc_hash": "fe819c5f65bfa7c3543deda4356bc32e035a30bad312dc683245a92520e74f42", "ref_doc_id": "59c620fa-c73e-44a5-8f52-bcc51834e567"}, "51c4f586-792b-4d4a-a513-d2d145c65833": {"doc_hash": "26cf645280181d518178c497c5e299906216c0c2f2624fc3c5aa6c6ae6b82c2b", "ref_doc_id": "384aa737-d647-432e-a3e6-687a03a38726"}, "2e53834e-91c3-4f62-8285-2d0b345de351": {"doc_hash": "861e191e8e9e39dc0fa8000c1f0e5a3914397949f47d3812238ba6e12b6dd955", "ref_doc_id": "7505b684-5e5d-4c1c-a0dd-c21bb425195d"}, "008e0dfd-2f90-48f7-a1c1-0357f872896f": {"doc_hash": "ccf69057c79c88a16dc0d1190a6545d980aaf15bddfae7b2d3c7e25f70a754da", "ref_doc_id": "95399a02-52c2-46bb-8915-1ae0772afbb1"}, "040ea64c-368e-4513-baf7-2ed2063a6898": {"doc_hash": "55deba51c514341017231d936f3ca998827c813e0dbc17cbb032ad0f5343fa35", "ref_doc_id": "8a0593a2-5395-4885-b6a2-735b2177eb55"}, "8ce47405-cd92-4f1c-946d-e335e1b80d5f": {"doc_hash": "204c92be7193ff03af943880f109b6678327ab5e95d7939276b58296d402a6e3", "ref_doc_id": "d6d5b278-e94f-4115-8c0f-1bdf21f37202"}, "bef59ae7-e996-4b0e-b713-20c54b3cc47d": {"doc_hash": "3325333366d07a4bb03c8659a1c9f6f8d75c2831c02ab34e45c11a9098389ae0", "ref_doc_id": "483ae7a9-035f-4a75-a3e1-fbe28e3109a8"}, "61444208-b3a9-4ac7-b4bc-8970843cb03b": {"doc_hash": "aad99b47677b5fd52d773cc3b2999e9657094d87a630675c904686b8600f8ae7", "ref_doc_id": "504a606a-c84d-49ae-8b52-47873bd0cb30"}, "4068028f-2ed8-4790-b6ca-a65a277cb6e8": {"doc_hash": "a97067e8662025210d25935c1ea4ab68bccf7ac51c0358c67be73ac3d13b6d39", "ref_doc_id": "d45a1def-af70-4f6a-8155-9f98e7e5ca47"}, "eec6fcbe-c5bb-4104-838c-f384f5a7f3c9": {"doc_hash": "1275e2650d66b4040be9df26aca71a0fb2145ffd7ce9c18b730193ec49331482", "ref_doc_id": "2a9c34b7-7bfb-4d69-82cd-92ea5ce978ad"}, "6dcb78a9-b7e0-4b39-bd02-ca962a5f6b47": {"doc_hash": "0e0f5f783b1046aa45491e7dfaae4d14c921e3b7036fa8599c64cda0a1526b2f", "ref_doc_id": "b7389cd1-68cc-4697-b77c-eb76d0ddcba1"}, "824e6936-163e-4fe3-9f48-0a4219069a0e": {"doc_hash": "5a255446fa672126be174da979110269699c6df5a011db92480d1132500a495c", "ref_doc_id": "d52ea854-7804-4c8a-81d1-d3545322ed7a"}, "b2309a15-2930-4f20-869a-c695476365c0": {"doc_hash": "72e310ecc5e9c4ecd659af730940f8910997eead97efa3cdf4e7641f740fbc35", "ref_doc_id": "00f86e02-8911-4ead-8cc7-13ffb0337b37"}, "88cb5cb9-9069-444d-9a07-3fb26d80a2f7": {"doc_hash": "ffd7735a3a3049a469bc83b37376070f9882b9878255ec720fead8b07a306ae2", "ref_doc_id": "05ea65a2-5eeb-4a89-b854-7eafe2180cf9"}, "b3d89ad7-ef1b-4122-be2c-abd4aced6d83": {"doc_hash": "83873df20caae892bcdd1f4e8854774781552aae1276d6d643e2732e7ad7d382", "ref_doc_id": "034d0216-d8c6-4196-9f1e-82d36a15ad8c"}, "115be742-f81f-41c1-9074-bb633d086ace": {"doc_hash": "ca496867cbd53fe3eed193fb805d43ed94fc0df8b7bca3d40d9c4f41d48b2978", "ref_doc_id": "8c4fab6e-1390-46bd-a4d6-2fd115973002"}, "36e511cf-918e-4171-af39-9dbe13e45955": {"doc_hash": "e336e9e9982fed4fc880b39679e29cdc7f108b9d26bf3b50b5e3c51efd063267", "ref_doc_id": "0dbcf08e-010c-4586-abb4-bc3f8e1b7972"}, "df2aaf5c-c017-4f00-809a-350b95f2122e": {"doc_hash": "d749ac466361d9cf15016e3c0f19f90d90207be1ab845eaf0c063c1e8ee9c530", "ref_doc_id": "26bcc9c5-b2c5-439a-bcd5-59f0d7b6314e"}, "0cdd17b2-cdf0-4c0f-b8af-8d185635be8f": {"doc_hash": "ade22ed55de303fb73c7b289a32ec9d2d63aa17569afb93474235e7c3f62253b", "ref_doc_id": "dadd8d88-008b-4a35-b3ce-9c1d6a089e5c"}, "8d6de103-3582-4e1a-9d46-94ab401ddaf4": {"doc_hash": "57cd5036cef5002a001e85cd1ff72aa1bb14ad0de91efdf23f029ecc5618bd1b", "ref_doc_id": "e23216a8-c2b3-421d-a992-d9008946d8e6"}, "3ffee36e-827f-4d1d-af4b-9cfeb18d25ff": {"doc_hash": "6c9e46e2486db5d624b6969db39fd2bc893988703e13c482337d7b4f87058dba", "ref_doc_id": "8f549920-ed69-436a-9b02-bef620480b9d"}, "764c594e-5898-4204-8d91-d03a863677c9": {"doc_hash": "d3a686bc765a16f5e0720a66c6fcd145b0dba2dcf7ddaaf26bc7d40d0a8907c4", "ref_doc_id": "388cb89f-2a82-46df-a496-441d6df6e64c"}, "147f5959-c780-4186-94c9-afa4d07738cb": {"doc_hash": "dbfe6dbcdc0d0936fd732c8f553cdefe5fc42578c4104ec471eaa225d252979d", "ref_doc_id": "2c683c1c-e3e0-4220-932a-51301c1b07e4"}, "c912a3d4-3a4e-4b91-b3d5-701f6386ebed": {"doc_hash": "f44f46181b5cde8e47a03c3534e57c2ed26fa8b28b706c01c2bb9219d88bd1e3", "ref_doc_id": "a5561d24-1aca-4b2b-b322-dd5ea7226001"}, "0e32ae60-6dbc-4500-a4cb-838898df0327": {"doc_hash": "e18216ae696486acf116378ce89af60278c44cf3f22b05c0632e9164e689e9f8", "ref_doc_id": "79809051-c57e-4978-bfee-b8fc6ca852b1"}, "5392ead7-bccc-443c-8969-14c5599cda9d": {"doc_hash": "56e8b4a978893d1b552628a84d123d8a092201cfd2c508eebd3708fb17a1962a", "ref_doc_id": "8d237f0d-f526-4cdd-a73f-52644e70bd0b"}, "f95ce866-380c-4f98-be2f-abb011bcf3e7": {"doc_hash": "0196524913789d402b32e23bf90852fff7f9d23ae61ae71e2b6437ae342e1e9d", "ref_doc_id": "b2bd46fd-96a9-4435-bd28-c07e274925c1"}, "88d8b002-84a7-4dc6-82ab-73c1837b6d86": {"doc_hash": "7934cfa03fdcd32b48de6951fc6e7ffd1cc4586fa27a1f653024573a2ccfe448", "ref_doc_id": "017d6d9c-ed45-4169-95a5-59ccd6a35de0"}, "7dcc5b5f-efa4-4281-b101-f3f4b0f9c4d3": {"doc_hash": "4cb124eefa1a879707d1c5ae7aadc406133a7aa986e81a10070ab81b6e04db50", "ref_doc_id": "1aff4c98-de25-4051-800d-7b57d2ae66ab"}, "006f11ba-5d3e-4943-8360-5998e1e44070": {"doc_hash": "e25831c4cc2be3d3ab6de89f4e8ec35cf162eb1d231383e187bc8d2799b37fcf", "ref_doc_id": "e514d2da-7c09-41be-b0a7-46492578fed4"}, "4696ebd6-50ec-4f02-9aa4-bfac0f275a0e": {"doc_hash": "f18bbe8e0ee1d5c5e6cc4c8839c0b5701eed4db1f0d69c7174d8300af29f5efa", "ref_doc_id": "c6aad94f-5348-48cd-b55f-99dca572b39e"}, "55d98d56-baf8-4f8a-a8e7-fa5399ad62fd": {"doc_hash": "1d796e8332eafe0031eecad96c36bf66686c31c0060c932dfce789e1eb488de6", "ref_doc_id": "e3ec671b-22ae-47e2-bb4b-fda3fc2fcfe0"}, "3edc449f-3b74-4248-9c91-942de2c19985": {"doc_hash": "182f1ca9e9f315afc855a83b4d6d1c6743279e06439efaaa48dfe270f41df66b", "ref_doc_id": "2d55a1f2-1d5b-467e-92ec-332f0adc5247"}, "65cf5177-e10b-4eef-b04a-e8b698530545": {"doc_hash": "c24d59e8e24cf28b7e645f9fde3b05aa4a1e21d95e891e0bed5403b9fcaab7a8", "ref_doc_id": "f372b5ec-85ea-449c-9e78-4f9d9ed6f908"}, "2113153b-ccac-4a32-a2b6-1ab608e64121": {"doc_hash": "ff237d9d856fd9cedd03122fc3eda78f77a5faecc959b357295e62372298db3c", "ref_doc_id": "9bce9237-fefe-40d4-a719-c3f2e335cbf8"}, "83e9b5ba-d9ee-416e-9033-823f6e7f1772": {"doc_hash": "0dd8a8a142dfa7e9133c97e32fa97e1c5830ef30955a0278db96f9ca383c4815", "ref_doc_id": "1818347e-aa20-4baa-a8b6-c64d667cc55e"}, "3dc37a75-3b4a-4252-ae20-bafaae4d5922": {"doc_hash": "92be7c32ce3e72b21ae7b14a1941072122bc55b6cc4badc8d352672e5ccab956", "ref_doc_id": "d3cf5a39-ad8a-4105-abd2-261a9d97db68"}, "5186f191-47f2-472d-94ef-6795f19b7e3c": {"doc_hash": "64b2003b624984053e672bb9372fb7ad6a62c5b44951b85257d47bbe5d99847e", "ref_doc_id": "3c384dbd-602b-40bb-b715-e2917af954e6"}, "15bd0a88-87eb-4559-8c40-ef0c0f6b540d": {"doc_hash": "cf8e4f4a67d17292b6a6589ab2aa663d5975561a66688ad16af5b4470190bd62", "ref_doc_id": "d641463c-1bef-4129-a1d2-fcca7062f840"}, "9105c188-1b32-4f20-ad2c-c9c006c8bc54": {"doc_hash": "09bb9b29b548b898cd41060e6990e405ef1f48dc3513f1b9eb22f135a2e84bf1", "ref_doc_id": "b3d6b3e9-d57e-47e8-b6ca-d763851c7509"}, "affee9d9-3086-4424-a1f8-29050ac1aec7": {"doc_hash": "370de31e8049bac6016b1e504cf412485509b38fecd6e0f075b6a91dbd099030", "ref_doc_id": "0ae2a488-63eb-424d-a1bb-1bb5fc66abb8"}, "22070ba0-eb69-4e64-897e-a5d04a38f9df": {"doc_hash": "a107f0b38cd1cea0d5918b756e3e70fafd85cce618c5d37f15b41313794acd94", "ref_doc_id": "ae70fef7-8300-4626-9199-205f444fe720"}, "faae5c7d-8997-4203-9de1-90d49e1e96f4": {"doc_hash": "765ee188d93cbb3d2405dd029d5aff21735f728f3ebc87b084dacf7a52044f98", "ref_doc_id": "a63d4b5a-cd72-4508-bd4e-3bab18a77731"}, "b3f78a50-c1e4-4c71-bdb5-fa0b9317d810": {"doc_hash": "a6f852fceb00c933c095b938c9cc5d44d813f888a1c7994cfc98c2417dd7bff6", "ref_doc_id": "5f282b10-aaf4-4c9e-a7ea-ba2fbe971e76"}, "f2531531-5bb9-43ad-8619-b2f88c53f3ee": {"doc_hash": "83bf8cc37e25473b7473929f896b4cb4505285f7403ee64c2b38746e610cfa72", "ref_doc_id": "b73212b3-5dbc-4053-8fd5-6ec06b7ea30f"}, "ecd41e61-2c04-40db-942a-8dd73f3e5da7": {"doc_hash": "ac2d415ca51482b2108b65507d6084fef9350829a76e2b64839285ecaa12d10f", "ref_doc_id": "b5fe18cb-8e22-4ebc-b789-315eb7098b4a"}, "c737e3c2-b6a6-4b8c-9a28-98c814539daa": {"doc_hash": "cdf4566bd6b70c20244bb0c2c634c4505ff8d0a21085f1c1fcc8ea0b05f5961c", "ref_doc_id": "4f8184cb-8aaa-4941-9835-ca2235fc1473"}, "9fd34988-116e-47c8-a946-ec2634a1b4a4": {"doc_hash": "270426f48474d8f4dfa2d0497aec35933615c02a5cb4a7a78bcd42197abad232", "ref_doc_id": "55e64ba1-3626-4432-9553-4a725df12809"}, "9487f11c-0654-459d-81fe-c5d8df12c1d7": {"doc_hash": "feaf0bd99e7ab6fdf5cba07fd1407ba0b9b488bff9980af8ed2a8e20c81ae8d9", "ref_doc_id": "d4d55bfe-7b55-41c1-b30b-0a02e339b3e9"}, "f88ee966-7ef3-43f0-bd85-05f3ad109188": {"doc_hash": "70d116b33a1dfe4034ba71bd4c731ee8093516cb6d8be8dbab8357ddcbc6b34a", "ref_doc_id": "ddea5aad-c97b-4e50-823c-791b6faea3a6"}, "f6d648ac-d93b-42dc-9a73-e0ee9a0db712": {"doc_hash": "291f8043bd2e5f5745b1915e86fa9d85e639747875b20989f43c680c274de767", "ref_doc_id": "807f5503-36e6-4f89-a513-2bbfbeaba8c5"}, "a49adf62-2366-47cf-a697-10914acbedb3": {"doc_hash": "0ec7dc46f2a5bea84a06e1b3809956b7675f7b70ba226d5259386f10c579142a", "ref_doc_id": "12d93309-5e41-4d01-9f3f-025abf8714aa"}, "18613643-c614-40ff-a3a8-83e90f90f38f": {"doc_hash": "a95e9c5a027d7368fc8c408df8009ff4ef0b1e74abdbdeea431ba0e837925f4b", "ref_doc_id": "7e02434b-e831-4ec2-807a-1f1eee14ba9f"}, "56904f13-9976-4cc6-a96e-af0458a62978": {"doc_hash": "8466f76363a16544eade02e01cf4620b23a569b1ae94338e9d714b58396b4873", "ref_doc_id": "4b60672d-8eb8-43c9-b9de-d82902f8df0f"}, "86b31c19-7569-4742-a08a-7a1c8d3ab429": {"doc_hash": "5deba91a12f57b26a6ffc8d9ce7f18ba07e8865e45693dcd3c2b8bcb174bcd13", "ref_doc_id": "dfebe986-0279-4307-aa33-6308ee7cd67f"}, "fb4c5da9-5bc6-4962-ac8f-39ed2346b3e7": {"doc_hash": "e2f423efd76e62d2aa76b837a82dfe1dfbe3a2f65f2c99cbe1c97626e10899bd", "ref_doc_id": "86ffb901-20ea-4f43-a051-a25a91325180"}, "a9255cc5-dffa-4b58-91c7-cf72f0e70c6e": {"doc_hash": "e5811c046e5bb08a49adc4be2bff72a3b2c34a89a5eb7aeaa3a3456349b53cc4", "ref_doc_id": "91cc06b7-7f6d-4bc6-92e5-e9ec0d649a14"}, "bb120704-295b-4610-84b0-b95237f733b0": {"doc_hash": "d36a535c22b8924b7a770660682d9298646ade18cc1f23d1078492a431e9a889", "ref_doc_id": "5d3fd03e-4fd8-44d8-b7ef-393253e96071"}, "4249d453-86c0-4995-a9f4-e129492ca7a3": {"doc_hash": "a02c9193a12201634e64c33e7addfacf32bd46586d40def2460bd78f10aabfcd", "ref_doc_id": "9438b325-5078-4032-ab75-23aee677f786"}, "c56b5adb-907c-4ff7-8828-346b3d673581": {"doc_hash": "08355099459aaafaade5cc3364eacd642c7ca54f6c5350ccf4c4ce5d940e519f", "ref_doc_id": "a9b1a814-2bd1-467e-a2f8-90eea72933a2"}, "05c1489e-26a6-4244-8dc5-b4ca413a5eee": {"doc_hash": "836492f5bbc09df5b3e6d7cde865360aa9e1b70093a0c5a596f3ce7a8fae2135", "ref_doc_id": "db1a1697-d138-4b24-9e51-4abbeb69f6e5"}, "1ee830aa-719a-4435-a416-0aefc36799de": {"doc_hash": "41f65244bdd0bf57811f473d69f866d24e35e33002be62216c2d48d4af266f49", "ref_doc_id": "078ecd0c-4fe1-44ee-ad86-5bb08b4acd9e"}, "1aa05958-52f8-4180-84ad-d857fb4dd308": {"doc_hash": "a307eb0d59e7a4dc4b2bc0413d71f6114057b41f80c971827150589c339fc687", "ref_doc_id": "3a7c549f-524d-4575-9fcc-ef049f75fbe7"}, "d276dec9-25b1-410f-8b3c-4ebadbd2bfcd": {"doc_hash": "f4f2eaa770ebe20affd8e1b402612e9e2aac381bbfaa8121857691fb36560935", "ref_doc_id": "dc153b65-8fe2-4645-88a1-dcd4f44b9695"}, "e1acd896-ee09-476d-b84c-cf7a59921b16": {"doc_hash": "305087af8485959d049b7dc684cc80c0b7460682e5caab1c2403a5bb72341b51", "ref_doc_id": "439bd428-700d-4a3c-b961-ea2246b1e6b3"}, "ae9dbe8f-7a9e-426c-aa60-042e77642367": {"doc_hash": "a4cd5b706469c684a737b27d9eec57d21dde95f9dd6574a4ab97bc014d6ae53b", "ref_doc_id": "609dc07e-9863-456e-8249-5e6d274ad36c"}, "dbc44fc3-af1b-41f3-bf69-f7a4f75e8d04": {"doc_hash": "4c10c8179bc26a63fd3d42305e335d21ab2d2c48c4614442c0a553369dcf1a2a", "ref_doc_id": "9bb75d5e-fa94-47d1-b1fc-6778db6f4dcd"}, "4a47d7bf-0fba-4daa-beab-5f5ebfe5e4b1": {"doc_hash": "4a9ea05f77cc99bc95f0be015a013f6e3eb5fb2456253d7384f18cb0dd20acef", "ref_doc_id": "dc8b4ab0-8511-4f24-a921-7cbf29a03d9f"}, "e098c751-b2f4-4cf2-a488-789e07a1c972": {"doc_hash": "6bba2eae245e943aa0038be9431662617a5dad402e76e599ec5af45395edfc2d", "ref_doc_id": "a4ab655f-0313-44ca-ad7c-a45e7c2cf1e0"}, "95858d7e-43f8-4e2d-954c-18862952ba72": {"doc_hash": "7d84308f3902a426b515b27f3ab40bdb4e42bb93ae70e41323a1250d5bb97100", "ref_doc_id": "5e8d4c61-96a6-4108-b38e-c57469a5e0d2"}, "f2b0bbdc-8a96-4162-8786-a4591581bac0": {"doc_hash": "61387d85101297ff25fc2f0bcb7941be7118fcd47f17bab3df5b18269943beb6", "ref_doc_id": "344fb342-dcd1-41fc-8635-c0b301550fa2"}, "8d022fd9-104a-440c-ad86-b091a596086e": {"doc_hash": "bec700cc3add064bb9f94c2e29ca1b6a455cd78371b939acccf1d0de2d26a392", "ref_doc_id": "faff5807-f23f-4e13-ab66-53e25c1bb2df"}, "b76295c3-ef69-4aa7-a5d8-a2c2e30941f6": {"doc_hash": "30dc75944d6854d3ee6411450206e4dc889d653b56771e9be65c14b20f1522de", "ref_doc_id": "faa21e24-2335-4321-8210-7f4d58b666b1"}, "00452fac-97c6-4f8e-8310-76792f535037": {"doc_hash": "af462585d990a7977a10a198fd133e0fe9f3d21d391d6074eb8e1189fdc2f0dc", "ref_doc_id": "6c0350b0-7685-43e6-bcd0-765f0a95c059"}, "d94fde1a-35ef-423a-88a9-6ca42a583763": {"doc_hash": "f374178a88d25859a28ceb9ab53682727b8d41223e68a8bf9f8d101667a4721b", "ref_doc_id": "54c98d2f-258a-4d13-a0cd-a7f9fb29d5e7"}, "fe2931b8-7128-456f-91d3-13dac568976c": {"doc_hash": "117f03925f7b272c37252f3aeb506d41b5154087880fba5823c94ccf6659707e", "ref_doc_id": "17c8aff2-c905-4674-98cd-f21304fe72a4"}, "4829a8b8-3d2a-4292-a4bb-c5724289c1a3": {"doc_hash": "b440504ff586a4e891ee3f7d8cf59d2d63fed957bf74aed97ac4a318a73a4ffd", "ref_doc_id": "37142b41-f8f0-4d73-beff-2c98ebb4e9b2"}, "21a90c2d-d3e8-4a24-adf1-4b8469c01d68": {"doc_hash": "64c37813c5343cc2cb249d1604dac781d94c08f6e07b125c75009a35ce89d046", "ref_doc_id": "f64c40cb-e6a8-4648-b6ec-2747dffc2df9"}, "3f68c1d7-2890-4f89-a558-cc651cf5ffc4": {"doc_hash": "544e603379b46fe0bcc9ed08b4a100a3d0c32c92275ef50128184a680af0e9fa", "ref_doc_id": "9e97586c-d232-4cec-831c-c5e0892d54a4"}, "0bee69d8-a2ff-44d7-9cae-e951fa0522c3": {"doc_hash": "f3492cb1172b4f172776bef3c07c5d149f6338ddef3a38dde53320c0771f6b7a", "ref_doc_id": "0868aa4e-7f15-4586-bbca-f7524768c0bd"}, "f59a7c48-6005-4c00-947c-c86209f8c0e6": {"doc_hash": "ff5977824547607c87ad9b561086bb3df77e4511214e6e8c11a32d8480192608", "ref_doc_id": "5425a85e-b5ad-4dd6-953e-464a22a789ad"}, "54c3160d-3a0d-4b48-af46-83cddb434a3c": {"doc_hash": "37e9009061984c3d167347aec215740c9582c85de41f7cc4e8cd3a879102f98c", "ref_doc_id": "25d505de-fcd3-4b28-b6c5-c61c3059ccf7"}, "df0c5bbe-7b64-42a7-a671-efcd977d61af": {"doc_hash": "5fe2c7f847a676d5df428b0e874593ed10fc1ef4d69aeab61e37641e5bece64b", "ref_doc_id": "0f4c1cd9-a48c-49d5-a4cd-9da444550806"}, "35627666-0443-4c6a-8fdb-924f311fc955": {"doc_hash": "5a6bee126cad55b77bdab715d6b5ad7438613e068798174901406b7eabe2df1b", "ref_doc_id": "692e4d1a-9030-4f99-97c5-d881fdd6131f"}, "938fcc53-d73f-46a3-96b1-4853fd35995f": {"doc_hash": "fbab4c11a066381f3523682fab63e3f9fdaf04671804589651be9b851fea23a5", "ref_doc_id": "efd61919-c942-4d87-9e00-f944444079ef"}, "e467cecd-0de3-44b5-8349-db9ded548315": {"doc_hash": "00abce71238254bb116f96cd64b916e09429f1f508cb8871bf0b299d5aa8cf08", "ref_doc_id": "26e77fce-5744-440a-839e-633e524ad6b9"}, "1f11fa67-2ca5-457d-bea1-fc701c92c0bc": {"doc_hash": "5b660a0261ab68ddebbc99f5c8fa87ef339f10396a995d93ef52f6569c1f637a", "ref_doc_id": "86643081-82a7-408c-ba32-a5e5badd9400"}, "c5165da8-806d-46f6-bdca-65328e624bf5": {"doc_hash": "e26860ab9c1e4129911c0cef5a5a69b9d6838673cafb3221405f97ca749e559b", "ref_doc_id": "aafe8ec4-cf51-4792-b20c-6d0042ba4a13"}, "ba431e41-e6cf-4b36-a38c-50b0c043c7fc": {"doc_hash": "2dd9d9974020bc3a73ab06127406287466104c39914de3af79295439df31fd41", "ref_doc_id": "ffd9175d-2101-4323-a1cf-1487f936a32a"}, "82f2bae5-5b11-47b2-bbb4-41c14a938318": {"doc_hash": "161da7d49980abf70efb9300346d5aca341ba2f34e2df1424f677ebfb3691fff", "ref_doc_id": "43d79507-4bfd-4089-b9da-cfca60c33aa1"}, "71adcf2a-c930-43cf-8fbc-d9a834f0f6fb": {"doc_hash": "47eeff2656bf8b317996084a0d6401f628d1d93872ee64fcab8012664189b30e", "ref_doc_id": "72f75733-9d8e-4758-95ad-ef2bc7cfcab0"}, "82acb866-e5f8-4f75-988b-3b880b9630ff": {"doc_hash": "63631db33371f4ed50bcd7fa8bdadaaa6e8e23fb409416e8c7396d301aeeb8f9", "ref_doc_id": "297a91aa-162b-478d-bd1b-0e14265aa459"}, "1d90d14d-3730-42a5-86d6-e3b8702dab5c": {"doc_hash": "c40b012296b9d2ff7f66602d4a8fb9a01ee00226e918fb3ed6d871fb35562c6e", "ref_doc_id": "a458f7be-3ba2-4ac7-a214-36d7e556600f"}, "72510c96-12a9-430c-87c4-9e59db09b427": {"doc_hash": "f215a92da35615b04441bba9395c9f0812fbca1f407f2e6de0fd5303b941ee7f", "ref_doc_id": "0cc3edc6-2603-4fdf-934a-f264119a3e9b"}, "6ed4b873-65a2-48dc-b542-f3045d6830a4": {"doc_hash": "753cdae0f03bc18447620cc440391cb107feab60110c013a12ecb3375b2010ac", "ref_doc_id": "90c905e1-5ea6-446c-b280-4061f7830663"}, "b9a8f1fe-216d-44b2-a28d-3d14c6337ace": {"doc_hash": "09712d4930b59e2a6d706771e8a78a86107267a574e5de245a029a5ba65b6234", "ref_doc_id": "6794b52d-e16b-472e-ac32-00d8f30f0870"}, "ee5a8d66-02f1-41fa-941d-cf0c7ed87e44": {"doc_hash": "43950f9ece94434ab69f2d9c3be41e2e0182e5232f2ae9580fd1bffe44d65836", "ref_doc_id": "b65208a1-da2e-44d9-b943-449420968989"}, "c2579d4d-3b93-4e6b-9e49-30f33ba36e68": {"doc_hash": "39b79b0fdab6198164baa417b61a22e47d67251845e9dc00f7bbcfbc2210ca33", "ref_doc_id": "17433cef-5ad8-4e43-b827-07d57be06b58"}, "f76d9530-6870-449a-8783-cfa87f7b6128": {"doc_hash": "747b5f878a7af727ad109c6e13bd2c5548ea54bf094e6063bca37afca5d0157c", "ref_doc_id": "f084b401-eb4c-4664-968c-ecff59af92d2"}, "41c1494d-0a71-453a-8e2b-abea48e25e7e": {"doc_hash": "e3eb92a33abed1574a8d07f4fec91cc1f7fed5cd03b4c0b821b61b0700f84df7", "ref_doc_id": "a1984c0f-7d75-46a0-b135-b7d25cebc7cc"}, "dbcb5c31-13fd-47e3-bd4a-9012d36ae7ef": {"doc_hash": "2923ddb35a8ac6c32bf6e1bbe543ba45dfb9dce0e665f1f596137f8c2738c878", "ref_doc_id": "24f49a3e-a823-43a1-8e11-4eb53efcd002"}, "3353eae1-aad4-426c-ad6b-e6d47a175c73": {"doc_hash": "e346aabc4144153008df487b42aa40c9f9afe40a4a0dbe0a2bf83958f7d39013", "ref_doc_id": "9b0bd982-d606-4186-9c30-2700f52e4f78"}, "27678ab8-265a-4ec6-bf1f-447e3fbf91d8": {"doc_hash": "3d3f9434eeb9d126d4f8ef6147a616da734506f0a538195ca429c2828245de17", "ref_doc_id": "5062fc32-8fcc-4657-8dd9-32aa692b11a0"}, "808f8330-1680-4df8-9a21-d6b448342624": {"doc_hash": "a3984d84c768db678eed961b3ce2bde2295acb7afb301f1ecb30c479f1a2dd76", "ref_doc_id": "1d50e18f-ab9b-496e-b029-a462597b901e"}, "1a276039-1ee1-4a89-96a1-a62f2d63774a": {"doc_hash": "f1bd81b24198fd226f2fd57a83447bfa670ce4074c3eb74d6712318c670f7bbc", "ref_doc_id": "6f0c8856-047f-44ad-b6a1-c854209e5789"}, "be61191d-5935-42e9-9ee6-d26361fef5b8": {"doc_hash": "3b1fec117b3279c12e047dd1f537c9e21c316f71b5e35d3263c842a8c2edab25", "ref_doc_id": "a901a4b1-b8a5-4ae0-8c4c-2a48d5d7b51d"}, "5dbfb414-0379-46dd-9df6-7dc965d13c1d": {"doc_hash": "8170f82807b74a947e33641f12e47d8ae38a3a557a045930653586697fa4b609", "ref_doc_id": "3cf8bd25-df63-4537-bd80-9fafe18103fa"}, "f06cc0b9-76ca-4eb4-bb50-2cb06877f298": {"doc_hash": "b7db05ff8ca046a43f524adc4b9a0fc17d0b8edaa346f4eac3cc5f2531ed3cc1", "ref_doc_id": "d7029599-2a8b-4350-aa5a-429aa86669be"}, "f9fde751-018b-4a76-bfdf-00a515b5869b": {"doc_hash": "bfc00615e1b89268530172c1136af352c23cf377221e349debe85d13bbf1a7af", "ref_doc_id": "afebad18-6729-4eb7-aa94-e1a408169132"}, "87974162-00de-46bf-a9f8-68c1899f7827": {"doc_hash": "5a85239f35bcc370d2c8fb9551e712520e20db33c4847a8c482317b00816a517", "ref_doc_id": "1922822c-7f61-41e8-882c-d455ea7dd004"}, "590ec620-cc35-48f4-8cfb-f4bcfc8ccd50": {"doc_hash": "da85535e03ecc6b6527ca1f08c5fff9ecd82536d373e9c9ea36d5bb7f4d39d66", "ref_doc_id": "06f08744-8672-4555-93ff-8b27740a7105"}, "91b6c310-4a2e-4115-88e2-238b54ae9168": {"doc_hash": "2c768a4c412eee846766cb57c0b7839a2143ff11c834410089859be87c5fc77d", "ref_doc_id": "e8369408-a7a0-46cb-9641-fe7b484dfb39"}, "5f9f1759-3369-4930-a4c4-7cfcd1be5491": {"doc_hash": "bf2e4bd0cfc5b494eee8ec330358d61abd473a551470e318a41f784bf98cc65c", "ref_doc_id": "6463ed32-d90e-4ae3-abde-fa5cfcfc1305"}, "193abb13-9ee6-4158-bd4c-fc39ad25fef6": {"doc_hash": "e95aa29128a43009ab40e6a8bd754c60f7dd809fe45aa33eded5b66165a6984c", "ref_doc_id": "08274d27-5b3c-4259-85d1-00e40ab9e6db"}, "0bca4f73-72cd-4e1b-ad78-0a9a2006c8ee": {"doc_hash": "72d380f1eb4e0f4adb8c416deb022c0c0b8703a9d14485423da0eab371be41bd", "ref_doc_id": "c16c5a69-4a10-455c-8808-8cfdb8891d86"}, "5fbfd3be-e56a-4cad-9d14-6aa3f12caed7": {"doc_hash": "e56a54c5100ebfea381859056d6eee3b50ef3668e1281bdb751437e88c661d0a", "ref_doc_id": "6bc6e1ea-2743-49a3-8414-fff1a177faeb"}, "12a8b523-f14d-44a7-8a7a-53b31a4ce63f": {"doc_hash": "c1c310e37370186e278a0f684edd5bfd5c4dfb6f8d6e49d01de30919d9950645", "ref_doc_id": "76901f59-946a-4a22-ad06-71d826f3bdb7"}, "03c3b789-8e17-4bcb-a94a-85322c763587": {"doc_hash": "aca0b2918705d44db4b04ca75b1eee6e0c44dace59af30990d04c55c41afbc82", "ref_doc_id": "280c5167-53c5-4c84-ae47-536a354972ab"}, "a448399b-1559-461e-b941-b19d6b026eae": {"doc_hash": "5ab29c2ec892cffab2346cc6b9f275a9d3b660f122bb0f0922ba1653149a37cf", "ref_doc_id": "b6689f3b-3150-45b0-852c-663332516c6b"}, "05b54e45-8cfe-4aff-b6e0-02a481436b76": {"doc_hash": "1548f304438678a0489636c675f43e98c65f8c0f1b6dc62588aa948e64cb7126", "ref_doc_id": "6fe95aaa-bd99-4cad-bd5f-b5171fdf5793"}, "ce5d2775-f66e-4983-a866-f41d5dc1538e": {"doc_hash": "ee0e8af2fa3b51a8236ccf967a42c42ee3877b9622be9f95dff8d49424737b13", "ref_doc_id": "285e2dbb-5bba-47eb-826a-aa493ce6035e"}, "d64137af-b224-46f8-8af9-6acdf6ab2acd": {"doc_hash": "1fc0f0c8da212eb3e6b281fa70db49ffbf62c6456de88f9da667914c6417848b", "ref_doc_id": "fba866dc-60b3-4fad-b3de-64ec560e7a0e"}, "72b76ca3-008b-4646-a14b-ec8c164ca5ce": {"doc_hash": "7495ef5674d0d45c5b5475be26a6e249389e8d283ce117c3f9180c834a125bda", "ref_doc_id": "3d919f57-9e0c-4cb2-ab1a-a404211b3290"}, "5b3675f7-9549-4639-81e6-7dd966756621": {"doc_hash": "2dd729cbdf2ded15737352110347340c0e939c0538bdda22d5207e2d62f4d8c0", "ref_doc_id": "a1a825b3-44fe-429f-848a-655dc6e4c2df"}, "3bdfa28f-bc01-46f4-9d17-1c6765b0f4bd": {"doc_hash": "52d81130f30b280bf3dd76dbd59b525532a95b439fa42a2b265ffe6344568d8a", "ref_doc_id": "870e626b-c339-4276-beb3-5c1352e52297"}, "efe245cc-3a9b-4cef-8a8f-1921c3b8b737": {"doc_hash": "82598ac871d16279542ba5ec05a80c35dc6747311393f44d3dcf57bd3b602566", "ref_doc_id": "0ed1ed0e-1db8-4616-b55b-d5e6628152f4"}, "dc44001f-1e98-4112-bc79-b1168a49692f": {"doc_hash": "32ef205eb78fb44c22eb454eac59e2afe7dd1f9ab6ac51f7ce34dd467e61a52a", "ref_doc_id": "4964c129-c94a-456e-916a-33e51414535b"}, "61a19777-6ed6-4461-8c50-d307c1671ccb": {"doc_hash": "ac63800be2f04912f63acbf5bd5830f38e4b75efb56cbbb95fa8a845cf22ea43", "ref_doc_id": "8c17705d-01dd-4013-b142-da1c13c79ed6"}, "b915999c-4e19-4802-952f-e65440abe990": {"doc_hash": "40b9570592cb9e440ca31a735cd57897f8b39127b2a3b00046076e1bd72e46ca", "ref_doc_id": "64fffc0d-b0ac-4198-a2b3-fc555b5d8eee"}, "baaaaa1c-c866-4041-96f1-1a997847bfff": {"doc_hash": "85cd6804a50f082c6b4bf52e1c2fa633cb4c131e96aabb64c69747ffdaaf598f", "ref_doc_id": "7ec1b229-ea8d-49e2-810f-43774a11dadc"}, "fab27331-15dc-44f1-af30-9157c91fdd2e": {"doc_hash": "765d39d6e27cbd06b234bd8eaf619e77e3bc86e4da4ffc382f569e04ee773af3", "ref_doc_id": "8326b6d6-b9da-4823-bb9a-6206d306e680"}, "736b478b-e213-46db-8762-8c59c6ca6f03": {"doc_hash": "d5ee8fae058fa39a8e2918c6a02ae34d1e40f494e7859c92b1585ede395f986f", "ref_doc_id": "d3dd3086-f639-4b29-b85f-44b42d7311b8"}, "ba643ffe-56d7-482b-a16b-4b2b5a64d3c1": {"doc_hash": "a12c9001be1c201dd6711b39c0fac14a702cebc8f6a51b2710546b3c0234d516", "ref_doc_id": "53b432d7-598d-4280-bb3e-bd446d7cc82e"}, "767f35b3-1c06-4823-97d6-44da033cfdd0": {"doc_hash": "958f7e4b38d5e4451dba3af4bda7d7e04d9d5db6ff2dbf109ea0d7bc3440eda3", "ref_doc_id": "3337c29f-6428-4c88-bba8-26230533f988"}, "1706a084-50d7-4251-8447-5dbd68d7f39f": {"doc_hash": "8ce86bfa7502f207a3a1cbffd0338c162daa304a60c6f66d9f2de928046b2911", "ref_doc_id": "da4f8257-8431-4e24-8e8d-8189011b9d3e"}, "cb03cd18-f380-4d13-bce7-64ee7c5004f9": {"doc_hash": "a24b35c5d48a728d588f30e66a7165046f56d450052956f91c9c5a156f06806f", "ref_doc_id": "805aea54-bc7d-433e-90be-4d8e421a1b25"}, "1e4eccff-29f1-4199-ac74-481319a7caab": {"doc_hash": "0d8ce7f9f177892d610087d3fd2d31b1f053b2deaa638d35c5e80bb539396470", "ref_doc_id": "dd11957a-d0e9-46f1-a2de-544a1101cb70"}, "8f71136d-ff0e-4237-b864-c311d4bb8d9d": {"doc_hash": "875e23800cf36d97325bc4f9839aa9a3b77ab94f2d0867ec004f9369da784bf3", "ref_doc_id": "e011e8d8-0f71-4918-b56e-0cf1f9df4e58"}, "91bc95a3-761e-4105-813e-9e8b1c726b8a": {"doc_hash": "41ee8e0b64b8f4e82b3bd32578552a0571737f3bd146a6050e39b58958278ea2", "ref_doc_id": "cc5b80f8-5814-414b-8e4d-4a3056d6b9f3"}, "d134199f-e7f6-4340-abef-50cb140fa5e8": {"doc_hash": "3e61369ae91504fb08bad3343d7716987c1d3823bbd6d7431f4ccf76b1f70050", "ref_doc_id": "b7185b27-654a-45aa-80e2-62b8cd9e817e"}, "0f152fba-89e1-4850-b131-ff401ccedb7e": {"doc_hash": "cf950730599c509cd9382963dfc1ca56227cdeeb3498cc63a3f3c2860c212dfa", "ref_doc_id": "7da5b45b-7598-45d9-998d-2531afcfb62b"}, "2c0a0407-70ed-4ed3-9599-749d72a747e6": {"doc_hash": "b9bd4923dc5976f6c3c99827b81004c25bec1f0239fe868cf9ee598d2ebbed87", "ref_doc_id": "2ee31911-576c-44eb-a412-8552d0ffb9c9"}, "30a61c3b-d1ad-4c13-98f5-091a416ede63": {"doc_hash": "2cfaa1719137ba6ac8a0a71d28b08b6ace8250cfde95216c21ec0200e38dca97", "ref_doc_id": "3f512eb2-6e49-49e9-b014-22933af6692b"}, "6c603a64-d67d-4d8e-a0d2-d7f0f3e269d0": {"doc_hash": "649996a46c191bd6eeb6e23d4ed8572c00455a7d58a81bfd526301722f6b599c", "ref_doc_id": "802d2266-c773-464d-b233-cec2ba0e113f"}, "c6d59a26-7c14-4944-add8-2246b00b3e88": {"doc_hash": "a3f476d016d6b032a96e1c634f9004c6cb9350e385651eb3ef37a2210558d48a", "ref_doc_id": "aa37e8b2-13a0-4eb4-98b4-c84698883a2c"}, "a2716131-b820-4f98-a85e-1927b0ed2e9f": {"doc_hash": "245a1fc426020f86f16fdc8f4236cd806a990067493df2fab4b4d6f29dc575e3", "ref_doc_id": "3f9f5dc8-7542-499b-a32f-e9f3fc19cd02"}, "5acfb37e-18a6-4b68-83c1-e3171489a851": {"doc_hash": "1baa9ba93572874efcbb58e1ce159fb2b90d094eaaa95559883422450b6cc47d", "ref_doc_id": "e301a2a6-da2f-4aa7-b180-c9e47279cad3"}, "5d8c94c5-b90e-495e-b1d4-bd2e353e331b": {"doc_hash": "a66493a7926a8d108e22d1f2f70e3e101a16f546a88e8c9ba6a6218bba950068", "ref_doc_id": "90da39fc-2439-4d31-958f-025eb82fee62"}, "1330881a-3351-46b7-b477-b51928cd296b": {"doc_hash": "09a732227d23a4497e3bc25fa2e7401f5d6e1284dcb2a3297b104b82ed941540", "ref_doc_id": "4294a845-4635-4233-9e19-aa7d389dce76"}, "efa279e0-8fda-4507-8c15-cdc87bb6b7c5": {"doc_hash": "a7e23aba1390a90efc84a1c49a71f03c35cf3dc1c0f0e4509246f2c00129e38d", "ref_doc_id": "2c45232a-fe8b-4edd-a0b7-20fadcd4f31f"}, "b21eba9d-611c-4381-9dee-81a4d4ff5698": {"doc_hash": "df0c5f339f2410eb24eb02d5740b2e3a544d262dd08457bb8a74dac78fe55a87", "ref_doc_id": "77802f9b-ccc2-44ef-9254-8700cc5a8c4b"}, "e7149c2a-4357-40fe-8c8b-c1da6ec5d906": {"doc_hash": "466f92845141500b7589472842190eb683dc4546298c7f070833775706267c41", "ref_doc_id": "4abe58bf-6574-4b43-8a84-bd5af396f967"}, "fdae96b2-0a05-4c06-a0fa-b9a658a8d8af": {"doc_hash": "e40c478e3b2d3a9941f3b612da408d13a29523b721c5ad396b68b0e9a9e45812", "ref_doc_id": "065d5d0c-83ba-429e-bb5f-98493f53da54"}, "6765d998-408f-49ad-be32-71a543e161de": {"doc_hash": "5c2431eddd276d2e7e69f9394d49c15f9bc5cd1a87215dbafbd83731896bae25", "ref_doc_id": "696d5cd1-317e-41a2-b635-958f4ac7a36c"}, "8c06d128-0917-4be4-9a5d-6669b3b77e9a": {"doc_hash": "372932b2dbe0b54c7c45966386d6ab2844afc57d18c68729793481d5866cf57c", "ref_doc_id": "a0a95923-7449-46b7-8ecc-0066c504ec5e"}, "b3068b87-d308-45dd-b31d-ee3cd7ca0df1": {"doc_hash": "6b3a712a96df18c95f37306e2c4158c19d35fcd640c561cb2b3257700999de08", "ref_doc_id": "9ec43339-e699-40ca-ba21-a5b5428a097b"}, "b1230c82-b0f9-4148-a2ee-966d9fed23d6": {"doc_hash": "248ce63259e4dfe256e66ee43093647e08af3e37148d36544fdce8f931682a44", "ref_doc_id": "f1a57b63-993f-4ce6-b6ec-86385572464f"}, "cb4d2a41-4910-47ba-b5ba-19da953b6faa": {"doc_hash": "7003623d63d54b7fc384d7f9fd154513db5de636b4c5812e87c96016be0b3635", "ref_doc_id": "29d45a3f-475e-4287-93cd-c7a0cf01754b"}, "5f7379fd-fe10-4967-bcee-62283fb9710a": {"doc_hash": "4486024628fb91fac497b95d206eb6de181d887c98a22351bedb0ff39a910da4", "ref_doc_id": "d9f7158e-35f8-4d8e-bb54-e30a6ea0b366"}, "0b023ad1-0939-46ea-8ae5-a86560934aa6": {"doc_hash": "bf3b22211f6a6292752576e25897ce0f5da787d65378d65713a785c837325dee", "ref_doc_id": "c8de2432-1598-47fa-a541-d114fdaa0991"}, "2c289531-325b-4a8c-a56c-e87d089d6b48": {"doc_hash": "595167a614a95fc0c6ac89c23eb3c24039b31756039fa58b631e1fee62c7d19f", "ref_doc_id": "cdb01ea5-7da9-47a0-998f-79c69f9f42c1"}, "6d193a44-3bd4-4d2b-b3f2-fa01f864d0e6": {"doc_hash": "3073d3ac1b249684032d8a56e23faa0d4b5a53690015985f41b2f7245129286a", "ref_doc_id": "ca285005-75a7-4c7e-b2b5-46e62f3530de"}, "35ce4b15-151b-4e46-abf5-389ffea5e772": {"doc_hash": "a27b8a7c5f529228517669cd274d2252bfc1f612aaa0188741a2c3bbe2557fc1", "ref_doc_id": "6be3961a-8656-4d71-86da-09c94d726dfc"}, "b62a8b44-85e1-4e30-8642-2b024d45c432": {"doc_hash": "7eb54d21ef629b4b0dec2acba45dbee4ddb55fbd36b07ccc561e36857a36916c", "ref_doc_id": "99e4b054-2d92-4585-b778-a7ca930409c5"}, "29cd7e4f-9cc2-4f00-9ad3-3a29340e8820": {"doc_hash": "f5dcd9fa08a27871e1b5eda0b4492d88eb781e032edf5dbf97abb87fdfddb992", "ref_doc_id": "3d8a5ef6-a6bc-4498-a913-a6ae3d1c049a"}, "6ee6a0b2-e5ca-486a-821c-8190075c4704": {"doc_hash": "244d86d6a929567e22e213732c036a7edaa623b6f09a1231e413692bfdc14b47", "ref_doc_id": "7683e053-388f-42b1-98b3-b6d1eb9d9636"}, "c5584b9d-f38f-4fad-9da1-5c77e9e93ff7": {"doc_hash": "a913460291cf3a5cfdf2527b693ede67d224c870ab8e17125719d115a335f285", "ref_doc_id": "e4048736-6fb3-4e67-8272-69948092b646"}, "a7920269-c0f8-4305-8d4a-369714e6dd2a": {"doc_hash": "e3949a6537f80c1e2bbb5e62a1fe66bbeb87e13d4ef1c9d832e1018926ac28cf", "ref_doc_id": "7c02a25a-494f-43c6-a565-446fb458179d"}, "a8606b33-d7dd-4791-b8b6-f45bd5d87a1a": {"doc_hash": "ca8bb8966bc8bc16e0a3490c8bc044cacab4fbe2910945f1a82bafcda24f8823", "ref_doc_id": "6bf757dc-44bc-414b-ba9b-2eb96f452623"}, "77662751-1b92-4b0d-8337-091d61726928": {"doc_hash": "88fb014db57584631d057d2ea73efd356061054295d95cf49ae357c3d15815da", "ref_doc_id": "6aa317e1-ae0b-4749-a6db-c673a5e7b5a4"}, "cb46fb0c-ac37-48aa-a599-7c648d41fcd0": {"doc_hash": "5046c0efd7bb02d0ebdab14217d45625f210008eb88e7e8402c3e2c6e0c9f286", "ref_doc_id": "61c9d363-6a29-41fd-b772-95058130ac42"}, "94bcc9be-4361-420c-b54a-aa6e38ac0cf7": {"doc_hash": "b784630ed9caf212bebb1733bcf9076a8dc02fb4d4dcf7d123bb2862debafa87", "ref_doc_id": "3129500b-7b60-4971-9871-95973d3d53e0"}, "b75bab2f-74b5-4bc7-93f6-625322a8abca": {"doc_hash": "32cc26fac68001cb5f51a8ebe5fbf7614241cf4b55c0189e581e0334fd8aecef", "ref_doc_id": "5544f9d2-9915-4226-abf0-c3ee6454199e"}, "3e610af4-060d-4aa2-b17a-7d7c2645c1ea": {"doc_hash": "b5f38b9e5d2f6c0a63e5257634aee4c2fb461aa59dde93737e15b607c7f89fab", "ref_doc_id": "287fdc18-ac7c-488d-9a6b-63a934d73724"}, "2bdee4a0-2a66-402f-bfe4-692ad4db56b7": {"doc_hash": "d8fe39b3feaf284858ab919956eb7881e439ef7dd1c346277c10d5464735196d", "ref_doc_id": "03d2d7df-6831-485d-bdcd-76b4ec0bfb08"}, "58e4d1a4-7c18-4c48-81c5-643a9f102d6b": {"doc_hash": "41825233feb2bbcbbbe8028942f5880b3efdbb1c787d8c71ff9c4764a4894ad5", "ref_doc_id": "2af1954a-ab08-44b4-8593-3b714352b5dd"}, "008ecae1-2464-43ff-8fc7-a122a427c509": {"doc_hash": "cbba5c75e304f8c19f86a6fe6660e4742711b4359ab2b02b9160e6a9904a0cfc", "ref_doc_id": "cf0d539a-a3bf-4b8f-b151-88cba71d43b6"}, "ef29320a-0e48-4d9c-adb9-2d1af822929a": {"doc_hash": "51adfba488bdb9112d3b48680a74dc4579804f43f03ffa01ba8c3e0c77075409", "ref_doc_id": "01073416-15d2-4ed3-9569-c3a7758c2fbf"}, "80d4f7d8-cc79-40ec-8432-a39c92f93e90": {"doc_hash": "3cc2b1a971d646dc3950d0a4ec552998bdfda44c2a397a13ced3eade36e20921", "ref_doc_id": "054e3658-ddc7-49ec-810d-a6de0f47add0"}, "1724835e-dc0b-417e-95f7-6ada304ee40f": {"doc_hash": "3a4eeb8af9e5d51e248cd7fc736f6cc9c5deaabb4355d1d888b32ed8836dad43", "ref_doc_id": "bd6e70eb-c1a9-4d6e-8a91-e9a0845d9ee7"}, "f941bcff-6544-4e72-b05f-35614512cf60": {"doc_hash": "7692729955c687d0b857f0831d10d5400d90e0d00947c3cf6c935d6b2cc032c1", "ref_doc_id": "95dc23d6-cde0-4dba-8678-b0dec0c87ff1"}, "f5636584-a70f-4a40-b689-d1e51459e6fa": {"doc_hash": "980bf8f2f034c7b9f8bf6f6cb32b4ea1340764faba57a181d586923462f6cc95", "ref_doc_id": "e25f08ce-c3aa-4165-a5f5-bc0a06378411"}, "3417aa77-c8dd-4474-abdf-469a528017a0": {"doc_hash": "b7ee06ccbf1d0cd73fe342998fe0df5a0303cfdc2b722ec6d8d255805da5f175", "ref_doc_id": "4a5e6544-5550-4555-85be-40e06f5acc5a"}, "1605ec7e-f09e-4ca4-a5b2-f20e9cbe68d6": {"doc_hash": "5bcb00d7d86563322a52a452d8034dbbab1b302f17f679c8ebe401c8c202e83a", "ref_doc_id": "bcad1dc8-f51f-4443-9685-1add2b409bf5"}, "f10aaa10-f44c-4783-81e2-c5785357f2d6": {"doc_hash": "058546d47d3fa148eecd355bf22505651415ec2bd0c04a3147d48bc39520aeff", "ref_doc_id": "75efab75-6103-4a86-a65a-95b3f5f6803c"}, "2bcbf27c-3aa3-47f1-88a5-60118d64f2d2": {"doc_hash": "7c0960662f9710306a451be60c5dec39eca4c9ea9a08f7e7952c8ffc72a3ae2b", "ref_doc_id": "f3bca2b0-a931-4def-a018-aaa835af6265"}, "d79deba2-1ac3-4483-878e-50dd552007db": {"doc_hash": "80329d0afe93fe79b529398bc234bfeb157f7743addade91d1b6ece892af28d5", "ref_doc_id": "4dea3af1-89f0-4cff-9fe0-e62c4e20be25"}, "c10d0bf3-73f0-482b-9af2-f77b19642dfe": {"doc_hash": "1ae7c30764d919831d4eaa80d87ba7105396b11e9d1d217243101602a55805d5", "ref_doc_id": "84ad2d4d-3861-4c5b-ab04-05459126052d"}, "d05cf104-5ec3-4ae0-a7ba-4ac2e6c77bfb": {"doc_hash": "486335e0ebeabe7a7c50fd83f56c04caf38c2a07811573f74fb0489f41d7e201", "ref_doc_id": "c5ad3909-7bc2-4fbf-b861-0ed12ef9cf7a"}, "2329e98c-13e8-401c-bcab-451ba37cc0fa": {"doc_hash": "87086ad2af097e12681f66f2c34f66b1b9b5d45825e611a5587dac95355ea17f", "ref_doc_id": "a7bd547d-9bdc-41b4-bd95-b27fba8ebf35"}, "646b4931-b8e6-41af-9fa4-a865e5db6bab": {"doc_hash": "d8764fda76cd0688fc6b2be3314a7d45e5ffaa8408083bbffee5fd18cc2632df", "ref_doc_id": "2900c8c7-cf10-455b-9d3a-3370dad43dac"}, "3ff5d943-cefc-4ee9-947e-b1b9299382a9": {"doc_hash": "52adf5c9d29ca2f2a16a9006981c0dad59380f2f92034cd3b5b75bb3ec6d7be2", "ref_doc_id": "6a96b9b3-1198-446e-b8d9-6255eebfb3be"}, "741b1bd2-52d7-468b-980a-f9423a4e8741": {"doc_hash": "a6dcfdc62cc41cd41fd2ef00eadb21a66bbb5b96ca36b7d5a9571197a6fc837e", "ref_doc_id": "97eb87f5-8826-4de8-93ba-962fba5e0936"}, "7f280e60-6e62-4f83-9bac-8166db5aea09": {"doc_hash": "d87ceb48150c46c33082711f1d86d7ca0599b7195af1c9fdf747186727c3323c", "ref_doc_id": "af67df2a-d9d9-4735-8d02-691dbb378710"}, "f8cbc429-ae87-408c-8c1f-447823b54de9": {"doc_hash": "3176a0580fe0c5b01be140ed969dfe717a6c4e31160de40b7efd79e0b96d1a5e", "ref_doc_id": "0921d14d-082c-4af2-b132-e553b872873e"}, "7d08237d-c0a9-41ee-97c3-24aff7f0e0ba": {"doc_hash": "f19626f428c8b48eec9b9ef18e104be282ea20b606e3cf180eb313939e58d11b", "ref_doc_id": "ae0da04c-ccf6-4d95-9801-a037af371b77"}, "9bdd4460-fcef-4e78-8b98-6c06985ba9d5": {"doc_hash": "b96813a3a8b4abca1afbad46ca27b6db7a8c754356591ad04be24d76517c6cd3", "ref_doc_id": "df11fac4-a4f4-4a88-80c5-b88544cd60b5"}, "a8006abb-fa9a-4244-bb3a-02f8f54f1a8d": {"doc_hash": "eac67e13bc36093432748f1efae383f8c1e175ce35f24ca4d1a3aae50bddd444", "ref_doc_id": "addf1f2d-18a5-486d-b004-995e5ed621ce"}, "5237fc7f-0bcd-4349-82da-3c48080a3796": {"doc_hash": "a67de40effadca923a2051493a32409e37aea79cb0502420310504e048e1b3a6", "ref_doc_id": "0c6532e6-85bb-49fb-9e38-c7e180fae61a"}, "b29f464c-64fe-4b0d-8776-724b1858c4ef": {"doc_hash": "3a5a813454cf5e8ab3b121263942b865ad78c415560416dde6c6602cee1f9e23", "ref_doc_id": "45f61d53-fdd2-45e0-b68b-43316ab82c64"}, "7381cfad-2e9d-4f55-a90d-73941c0dcd75": {"doc_hash": "f36758cf806c0f41c9045d71aebe85fe8c40400d5f23cac69888a95297f3efd5", "ref_doc_id": "bea78591-7b96-4433-9d19-b7fe32c97193"}, "d94f311a-1ee0-4696-b08a-86c6f63fab4b": {"doc_hash": "24b025f9b6bda0b900f5880c071c0434376e57c37b451f54263fccb218c38399", "ref_doc_id": "9c855298-579f-4f17-b6be-3cf70cd81aa5"}, "ab66a745-7c35-493d-afa7-e4ba7e39a67c": {"doc_hash": "6724042ed2110a6188dfe83690416a10d077a61b43873bc53ff942f0decacc94", "ref_doc_id": "54f93970-accf-4166-ae70-b619f6fab915"}, "fd94e5c3-da99-43fd-bcbe-8c84bcf690fd": {"doc_hash": "cf7344bb9cd9f288ebc4d17ce26744cc5bad87b9bbae24221b5f2316fd7f8bb6", "ref_doc_id": "adf2fa83-68e0-4d59-a937-052769668a2b"}, "6c2671cf-f32b-42ad-8070-c7682a496ffa": {"doc_hash": "284614782da680609a926573c818ee59f4166b8afdfab122d910d709c0348511", "ref_doc_id": "7dd88692-0b1e-45b3-88c4-0ccb4fbaeeae"}, "40efb552-d122-4550-ad61-3f6fbeac35ae": {"doc_hash": "245575ccde951c5a13e6548672a8dd899fb462bd444b30874e0bc652b421ee48", "ref_doc_id": "6e80d002-31b9-401d-aabe-ab84c20d52ab"}, "40770dcc-da68-4882-b16d-8cd4915f1f37": {"doc_hash": "2b307b70da449aa19513b8826ad358686ba6136091e6d058a595e19531fbe467", "ref_doc_id": "a22269d0-f9ea-4172-a34e-41b479aacce3"}, "1008ba1b-3c00-4d20-814b-9ff89e276f63": {"doc_hash": "cd1ead2818aef1002bcda498f80ee9efb566fb2cb50fd2c06fb7543f3aa04510", "ref_doc_id": "e4621f15-7b4d-4bd3-8711-6dbad76f7731"}, "ed553707-91e6-439a-8b34-1704a58f40b9": {"doc_hash": "55431f69436979cdab3b55164d66cb8029555f3ca29ba15bb315387cac3214be", "ref_doc_id": "f8d59f6e-509c-48ef-8805-84c3cfa94d23"}, "1b19bf71-d287-4428-bc19-debdbe537176": {"doc_hash": "1dfe3824a0411d47aa255691734aa0c5c8235224bcfef4e551a17154f8199d8d", "ref_doc_id": "80e080f4-3428-4c42-ba30-5a7c9d710361"}, "354f0f70-2e2f-4d67-86fe-f76b33ee59fc": {"doc_hash": "198018a87c4c847f36d8ec82e6ec24309966d1495582d6f59356a6b625d314e7", "ref_doc_id": "c1b36253-3fdb-4b05-9ed9-ac2f7a5fb212"}, "1027363b-a0bf-4b1f-8574-5c9ca5167056": {"doc_hash": "ea1e4798acdba8e3deed8956fb9748902ca9c9e6720a732c64de45fac6625e43", "ref_doc_id": "32076889-1c93-4889-b785-b1203477eab4"}, "19221bb1-c45d-40b5-b096-257cbc71b5cc": {"doc_hash": "9b20e885af3b50ae048bf87ddcec3691eb17a1f5beb3e05dd8096ef323da301d", "ref_doc_id": "3311af09-25a9-43a2-8f45-c4edf1d0a72e"}, "9d339f5e-b6c8-4665-8625-6fa639d47f9f": {"doc_hash": "386eed77050dd443102a23d0a10e714ded56343fef302f8d243a28b448bbffe2", "ref_doc_id": "1a06ebf6-0823-4253-bc6c-7edc3aa13df6"}, "99fab3bf-7a6e-4401-8a15-d83176c447a6": {"doc_hash": "4957256d0f0c682b31b01d753fef433b89433765317b2eacdf05423107d71a1c", "ref_doc_id": "8c440d27-4be8-4565-b4bb-f853c17399df"}, "75fdd03e-9023-4afc-b987-f30603e6fb4f": {"doc_hash": "d84f2f313236b3b9cc129d8cea63149d918f45b3763d35016d560f4100bcca8e", "ref_doc_id": "f719e8f8-2109-4fc1-a46c-145ac5021b50"}, "d9e27290-8b7b-408e-98f3-b3a22ed3067d": {"doc_hash": "1eb8da8ee9d2613e4e6a0da3b9efecc5db9d4316184fb3494a38e1073409dd7b", "ref_doc_id": "7055f343-6d9f-4afc-a566-1fe19af54aa3"}, "e02410c2-077a-4f3f-88d6-cfebbcf3209d": {"doc_hash": "7b65cc6fa2a78d98a9937a88d135e5cad588f7a57e426dff263099d18b37bc38", "ref_doc_id": "28772a37-b87f-4a6f-ad17-d1081cd2900b"}, "57b84501-5d6a-455e-84e6-3832a8625a32": {"doc_hash": "1434a06001ccf452d790907d6602db6ba46c4c8a3052f782247002fac0ff3981", "ref_doc_id": "3e1a3ecb-9dae-4847-965d-598c86a43865"}, "8a6c33ac-27fb-4402-acde-0644490055e1": {"doc_hash": "34570c8ba0c647a2336f2234d2b3f2e919e99c28f4e1cd07cf292b270df9e782", "ref_doc_id": "0842b10d-a6fa-4dc2-b017-5b14af8da28c"}, "734e1815-3c0d-41f2-91f4-28b6d6926b03": {"doc_hash": "73732404869968fb5ef2bddf73e468896c428e4488da5d174e3d043070b88c2a", "ref_doc_id": "3dc193bd-bba1-4922-8a69-fdcd6642501b"}, "65365ce8-d0bc-48b3-bf6d-a11936fc4520": {"doc_hash": "ee838b1bcf7c45c82d21765f8d81dda8cf84f724c2441436eb9c16c08472d30b", "ref_doc_id": "4389bd7a-45a2-497a-8acb-3175ba8ef500"}, "aac8f3ed-b1fa-4cad-af85-41e34aaa5e28": {"doc_hash": "27af9691c08e50664828f38cd7de383cd7a7f363e0677a4ba3c75db0b7cbba46", "ref_doc_id": "cf2a28be-b1bc-4be7-a797-b894e1a9e90c"}, "cdbac4ef-2d1b-4d49-b450-70c5a84972f1": {"doc_hash": "b2393f54d17313a85087bb8cf966a71d1c5e263fdb1121e0e7d904e155d32ba3", "ref_doc_id": "684cb30f-884f-4c91-a90e-11279924a187"}, "0af58246-6f30-40b0-b928-d8f576b481d7": {"doc_hash": "160df4e39c3ca807c76c97056184c9eaeeaecd76ef1f42ef99081d595fa3d123", "ref_doc_id": "1bbe6e75-21e8-4dcc-95be-cba9027560b7"}, "01d2fab5-48d8-4ab4-b338-4e3f86d2f0c0": {"doc_hash": "3eb6c8c645df1b5671bd3d894915a09aee791a1df10f8f0f575ac21a450f989e", "ref_doc_id": "2e8afd3e-c770-415a-838c-60b15b8e1b78"}, "cc466f1c-52b3-45c1-982d-dd6e97acf071": {"doc_hash": "ed43b82b1b50a63a7161aea4ce6d7fb1a009ed53c875199be9b2ef7c1a4dd8ba", "ref_doc_id": "9566c3f6-07c9-40a0-9738-9c200d9de840"}, "7b064d87-5acd-4449-badd-4dc0f9d0dc98": {"doc_hash": "449194ecd54f539867b78fd30b4d76b48abb9b4272d20ee392553bde494b3f49", "ref_doc_id": "9a76d14f-c044-4894-a9ae-b358f3e8b501"}, "b101ca7d-68b7-4e28-91ee-5a53ddb45242": {"doc_hash": "a5cfbf9f4fbbfa28b3483269a43fe9fa2409905f8262b820f60160dcb96aa3b9", "ref_doc_id": "5672d9cb-a86b-4556-ac6c-ea32f804b33e"}, "b9d3953e-843e-4c75-a6ea-3aac1268744f": {"doc_hash": "1c3b8e3b25b348dcb667fd0d93a4ca6f2f67d27bb39c6193fd0f54fe092ef389", "ref_doc_id": "f11afa31-403d-4507-907c-10cd9b848534"}, "df58522f-9c7b-481d-9d6b-1d281d2f164c": {"doc_hash": "9cfdd9ae67d8716f5278f7fdf0d1192bb8551829b599c754d69146fc175ed31f", "ref_doc_id": "7d53a1cb-7313-430c-b35f-76e44bfe5a0c"}, "53d2dab7-4d80-4ffd-a1e8-ea6811484742": {"doc_hash": "e5345ab107d403b5eed6790b9f880715b647526f853c02d34f288c4aa56c2c22", "ref_doc_id": "98137909-efe0-49e2-95a9-82cab4f68d19"}, "c61f8765-247b-495b-a57d-cf6a1d4526e8": {"doc_hash": "e2d614a0aeb3af53e6112f9f044069ca60fce6a9a33bf1d2bdb6b29403206612", "ref_doc_id": "a0a8ae2d-72c8-4247-8b63-60632d8febcc"}, "ce3ceb0a-2bac-4c67-bd77-fa348e11c68c": {"doc_hash": "4da1898f3993cd5d3ba562eb0ae5a10787f4487730381d4d590bbefc53ba391e", "ref_doc_id": "311873fc-c662-42de-a871-fde4739a3499"}, "13052a63-5af9-45aa-bb9f-961019e1cd87": {"doc_hash": "a9b2d65b16929a533024356a026d21e90e252e2b4221fcd556cdbe2186b0ee68", "ref_doc_id": "b4b40766-a5ea-486a-941c-f53d4e5c928f"}, "97d643d7-8421-4827-b893-0a1f77d074a2": {"doc_hash": "c1f4b40aa0c2b2eab115f06d7beab0335e9102c2b12fc0cc0636f342338e6a09", "ref_doc_id": "dbd1239c-7778-443c-b15d-589a0124e1da"}, "a7cb41ea-a9c9-4b4f-8631-aa80efe3783c": {"doc_hash": "022e5aa7f4fbbc43cf2569b730872c9e02b0f39173ff3358ff0999dbdf63ea87", "ref_doc_id": "4ccfaec2-a9b1-4d76-87f8-dae6ba181373"}, "556eeec3-fe84-4ccf-862a-8d1762daa777": {"doc_hash": "814282a541464f8a20dab2cef83f77e98fefff6ffe13352889641c505d4cb8cd", "ref_doc_id": "6fc83394-5da9-4f8a-84eb-0148832b3c55"}, "a0d440cb-b701-4c96-8515-1982e0b47d61": {"doc_hash": "a17a1160af55f8ba9df4249df6b355adb3752354230ea9ea16ebb275a894d9e6", "ref_doc_id": "b2de1ac1-0d16-453e-841a-20d054c85f8b"}, "6e58aeaa-8a6a-406a-bd5b-adaa0e9207a8": {"doc_hash": "35875e23e4ca5be67f1ddde217bb47ff2600e11f2fae98a57dbd8970289d9767", "ref_doc_id": "5a800053-18d3-4b3f-9ae6-2b083f34a392"}, "3b458f0e-76d5-49c2-a651-dadd129a24e4": {"doc_hash": "44781c4c1158d22ddedb72ef5ea630cc08b6d28955830147b47993f3ca7351bf", "ref_doc_id": "a713f84e-7055-42f0-a623-2ff9653d3ed4"}, "ac576e8f-b787-4ff3-bd54-ca75ee8d8d0f": {"doc_hash": "18412685ed1f25f9d67192f717f27ef5464d445acddb4d606fee1d8cb5206376", "ref_doc_id": "9b1cc965-6768-4867-9a27-cfc85f41de16"}, "7934df17-32bf-4175-83d0-d97a66c8d31a": {"doc_hash": "69bf86d17e8edad24616f393528f83906e209175fb608c05ca26c63d94ed1148", "ref_doc_id": "0bd41b34-a1d2-49a6-b1b8-83ea4323ba5d"}, "7036e587-75ea-4984-b7d2-8ca4613c5a27": {"doc_hash": "d080aa0f7eabec8051f43bfba692cfcf93853ac3935b6cc52a229b9188f087ac", "ref_doc_id": "c7a5009b-c4a1-420b-8a9c-e3ca17a7bba4"}, "cd87e09a-5461-405e-a0a4-c6dc70bbd0c6": {"doc_hash": "5f92f0649b1f07b9bb9cd01f4d16ddaaef9dd482ce3ad7ffa51dc226a3c71cb1", "ref_doc_id": "ddec3343-bef6-4269-b65e-e767294dc820"}, "ddfd46c7-a179-427c-b09d-2f8914c12272": {"doc_hash": "7b329c5b6c3b7deb36b3272c538fa509557a66238d7c47400e599cfa52fa9e31", "ref_doc_id": "1b142e80-6c3c-4b3f-88f7-2b38b6d9f797"}, "140882aa-ffda-4d6d-a5ac-529bccd29869": {"doc_hash": "999d1527e62fd7cdb8a4d5c4c4da05f6890f5d5afafdc8c7036920000451ce79", "ref_doc_id": "162430e6-5ea3-4f9a-85f4-5d89a5385c8e"}, "7bf33fbd-e388-440b-99a6-6ff3f3bf58a9": {"doc_hash": "492f619d84eb525a325e6ad356f88b716187713298040f324726b0be22dc5d4b", "ref_doc_id": "141c6cc9-ab51-4ded-8fe2-cac73bc41f75"}, "e87f6f78-accf-457c-ae9e-131b41f98e0c": {"doc_hash": "4d0ae570273b151b308370b0ae19e92a7ee8e29330740a792800e13f9cf86ee3", "ref_doc_id": "a513c3fe-3f32-42db-b7bd-f3d23a9fc402"}, "39f3e16d-771f-42f1-9d2a-af6eacd98d2b": {"doc_hash": "bb5f7bf68b842f2f5c486ef59c5351ed8659d2fd5dec7a528e3a2879aa7c21cb", "ref_doc_id": "a1a3367d-07f0-4546-84da-b64d32f5628d"}, "e0fda21f-51fb-4cd6-b2f0-7f18c703df39": {"doc_hash": "4da6ac8c0a9c1636561d63a72ec25fa0a6dee18e9a9b9cb164d1e9fa48405a71", "ref_doc_id": "f04e1ae7-0130-4947-927f-b0daf049078d"}, "ecbfd787-41da-465f-b804-15509f7fa6e7": {"doc_hash": "c47fbb4d7371c931cd870c8c07aa86b7941fefafb0c5ebe949e1bf699ee8bd6c", "ref_doc_id": "aa41a984-149b-411e-a492-2ff95084de3f"}, "029844a3-5ba5-448e-b565-a09d87e3ebbc": {"doc_hash": "20d9d85c181a2f9fab5a7a75432552cda3c87cab14fcae4e4e47f70a970bccf8", "ref_doc_id": "f9de3e58-a408-4d1d-b213-c6f2e02648af"}, "988a997a-689f-4dfe-9c55-e47e7ddd8ba7": {"doc_hash": "6e0939fdfc19dc07c2ff859be98ce99c76b46af954ae03c44b53ffcd9eee6749", "ref_doc_id": "3813571e-45be-49ca-90b3-54b9b4ab303d"}, "55b3d3da-97b5-40b8-9231-ffcead09b3ac": {"doc_hash": "913fe597030c09aa33dc5072a8ba5965022814f6e0dfeebd6bacdc3717a87bc6", "ref_doc_id": "b1aa33c2-fce2-425d-82a6-352a0eed3b42"}, "3c451913-c2ef-4c58-b637-ea8b046115d1": {"doc_hash": "9ed57819c3cda2044df7247dacec986bfabd6e0e7a86e9ff06553050de6f1e98", "ref_doc_id": "e7457fbf-ec07-4c45-959b-e13f8e0968f9"}, "0efc5463-0d2c-449a-83e1-83d1d90a0a50": {"doc_hash": "86a09cc3a1e2bae4a85666d7d7157adc816f14fe0d7d162c0c743fa4a3ff3a2a", "ref_doc_id": "ec2ad01f-991e-4a1b-acac-7109e76e8137"}, "d8d69b19-ca7f-4b14-a616-2074968b4ef2": {"doc_hash": "a447f6a4eefa2ed020874a7528c84ae177658b33775be177806d092e03b6ee32", "ref_doc_id": "46f07b8b-004b-4221-8799-c9224afb7e58"}, "82d17465-b07b-4f14-8f21-17d721755217": {"doc_hash": "723e8f3df17617952f37d3426d3d2c6647b1e38f9f10d9e34ab679192e7a24a7", "ref_doc_id": "d90a2264-16d4-468b-9ae4-806739cca1b2"}, "db83a839-6773-40e5-a08d-80fac0b13b12": {"doc_hash": "772a056d2a32d5afe88f70ad34c5c4b99479c4e42e7045b31d0accde8bf71f76", "ref_doc_id": "5827da53-13ed-415b-b577-ca6514d5b27f"}, "2bfd7494-d6b3-4934-84b7-e113c354d575": {"doc_hash": "5065844d2f51b735dc1fedc91fba5610dc6f89c0eacfb3fb435fd85e190c785e", "ref_doc_id": "365bcf18-7006-498b-9573-a2c1c1405f7e"}, "b4c1e291-9e9b-4726-924a-831a589db950": {"doc_hash": "e3d08f056b3ca7a317b0338d3402f14d088e2113dca2ff4bc667d8893b2e93fc", "ref_doc_id": "8210b0c6-b22d-4537-bbd3-72854118a090"}, "60b89021-aaec-4a2b-8367-03cf30ddcd0d": {"doc_hash": "d0cd83fceef62a9579f04baeca1fe4b7b1b05dfcaa1d064406f3c9213048a6be", "ref_doc_id": "934eb9d2-92c3-4f32-b94b-7d6e7428cf41"}, "ed136d59-d9c5-42f3-abe0-60b7078ac7b7": {"doc_hash": "72097155694ee53196778d0a94fd196b3336b490e787e81a3673b1440b529810", "ref_doc_id": "14773323-390a-4edd-9ff0-adc79735bac4"}, "87f141bc-a64b-4028-be2b-47877aabfd6a": {"doc_hash": "3fb06ce7c8795e2b76df8a0384a13838cc97be2a72710e7f6e0c558f0e03b8ef", "ref_doc_id": "a99d9ab7-1192-4469-a10c-332562365e28"}, "f69ba5ef-80d2-4ca1-ad81-72281e925876": {"doc_hash": "992472f6b4351237b5c392c6109b9fe41637d9e8554cd5e428c8a47706102f2d", "ref_doc_id": "84d75a17-6aba-4591-86fb-bc5d19de2eb0"}, "dcc86546-203e-42da-9cff-182bba8ec427": {"doc_hash": "5c6b1a975c00c6288645e26d1189ef74db482b383c2ab9480e69b842a583724c", "ref_doc_id": "fa5099dd-36e0-4dc9-ab6c-546dd398816e"}, "6ee6e46c-6950-42e9-851f-643cf8674306": {"doc_hash": "272b89290172d53f747961dc6538391347ac1ae154d674a18351f6a6272d075d", "ref_doc_id": "7ee217a0-d49c-4749-8e89-5697487841db"}, "f435471e-2824-4ac1-8c03-5a47da58fddf": {"doc_hash": "908285bf5426574f9d628ddd3fd5b2d24012fa8fb29f1ae15d2605e5c006ce64", "ref_doc_id": "2e1f0b58-7bb1-414c-b934-c5850fe5bb4a"}, "cf28b57e-ce26-4f2e-b50c-f748dda3373e": {"doc_hash": "14d547082c7a3ae000faa1b7c75c8aa533977ed10cbdea820a24f93b2ee9acdc", "ref_doc_id": "416d6001-a1eb-44bd-b44c-853d0dbd64b0"}, "5de6c55c-4b3b-4038-a391-6c0a21ace325": {"doc_hash": "8f203179705e93c3a505b15f5bf4d249d51d29f63e90d8f855ff1ec2461099a6", "ref_doc_id": "b9eab09e-4d9f-45b7-bb51-ea4432e9b921"}, "8fa2451c-b3e3-4970-9d44-7dceb51f15a3": {"doc_hash": "0665550a21102d0e57e78244f15f63baa02d41deea7ec0aa3423545b8ba44c7c", "ref_doc_id": "5b2022a6-0497-44a4-a48d-d2e6146eaa10"}, "b1616702-c8c5-4580-9b05-79363c1e6f05": {"doc_hash": "14e9c52ee51b2d71d85f6d168c4a9e03933249d6110ddfbf5ced4b034291f795", "ref_doc_id": "9d534fcc-46ec-440c-8a4f-19cc53ae45a5"}, "ab385e4c-eba5-4a35-bd7b-6fea18681725": {"doc_hash": "9331176dca36c50ceb74d092c85cf8016c9d7790beff7d66d0280a06b8b3e703", "ref_doc_id": "84a15434-eb42-4ff2-bfa6-9e1039197932"}, "c745b5e9-7026-4801-816b-e929e2df29fe": {"doc_hash": "e97549a4f43dd57e53d0b87056f93726cf3077b3f5db3d9378dadd97fcc6815b", "ref_doc_id": "bf89a88f-b93b-4995-8968-125b7cb57ead"}, "c30524f2-5f2d-4a87-9d26-b3c8765c588c": {"doc_hash": "132a32178e457341a604b9c4501cc259ef7db2bbe1b8d33fd791605e3b302a6d", "ref_doc_id": "8e7cd6b6-9acf-47b8-8558-24cbde3b5e21"}, "eb8740b2-e499-4281-88d5-dd19f2741edf": {"doc_hash": "49f484b15a5dac0a24b23bb0ae3219ced4f77f9bb3ed3c6c188190eb1fb9100a", "ref_doc_id": "5ede781f-94cc-4ec1-a0ac-79734c148224"}, "53ca8140-9c62-42dc-8964-d43bd3bf4920": {"doc_hash": "a4eefb16db0f3e894a49b881328918d27cd54a1d05fc0dc2fe57379ff7e6fa12", "ref_doc_id": "b06ace18-964e-4f5b-a098-c0a394666dc9"}, "ed63a022-efee-4971-a6f0-4b87082346d2": {"doc_hash": "6df43eae8a0af25372b56178aecd38ddef732400326e8e8cecc9cab9d7113e9a", "ref_doc_id": "1b346ba6-c764-432b-9006-7a833ff633d0"}, "6a78d55d-52b7-4e7d-9655-2fd9b8116163": {"doc_hash": "952d759663eb4ea6440e4c0ea37815b4fee003c87c1c955c909ec5cf7368151c", "ref_doc_id": "0a68d8ca-712f-400f-b87f-270898937ad7"}, "89240719-6ec7-4128-8c1a-6355ff0a4feb": {"doc_hash": "0a07449abf64fb78763551465a682d1ad07360259c642891a43c838ae879c952", "ref_doc_id": "6ba51c12-507c-4e81-9f86-55b53a2f7f69"}, "6e8a07ce-5d0f-43d9-9043-757eab459f51": {"doc_hash": "b75c815a6248e62ebbe20b6e0393a7ff18ec1428a92f12875005feb643190362", "ref_doc_id": "cd2be06c-6fe0-4823-8285-44c5689b8258"}, "5239067f-3e5d-4801-815f-fe47d16250dc": {"doc_hash": "565f9a3cef96aeb39abcfec773668db2f22cbed4f6562fe102e0bb50c8fe0be8", "ref_doc_id": "968c7f06-2151-472f-8649-79563383d892"}, "8f7b017b-c969-4cbc-9554-79ba8e21acfd": {"doc_hash": "15068ed51ce1194130c669319500f52b197e407fc33d59c24022f88671d8df7f", "ref_doc_id": "a8695a0c-5403-4b4a-b00c-8b131d041388"}, "b5d83bff-764e-411f-ab0a-64414049d7a7": {"doc_hash": "b6ca49c84d7cf155cde7c68d3c950fd68326eff1842b45e2e6bd6e7638407c84", "ref_doc_id": "77511991-81d6-4abd-b506-97de6f31b92b"}, "bec6f598-c082-49a6-b5d0-0423c575fd00": {"doc_hash": "b5ab85dac2c25fd5aa07f8742be95ee8a9a00a9a578fe28e1e84844553ff1281", "ref_doc_id": "1c595d90-86f7-4117-9d03-12cebbd672fa"}, "b73a98d1-c0fe-4ddb-80b7-855974b95bf0": {"doc_hash": "71f9eb4daf90b1a2a83c302f9ec49f784ac7a88f737849864becea6e894371a3", "ref_doc_id": "be45592f-7e7f-4683-9a84-d84d94fbfa9a"}, "102e125c-f6fa-4863-a495-82a084834b7f": {"doc_hash": "a9078942868480eea8f78857ab568c6cfe47f951bea675c5e7ec1a5c537a8b47", "ref_doc_id": "7e14ee7e-a8dc-4a4f-914e-f5418c518135"}, "8f4d2df9-ea45-4708-ae79-3290a72a4090": {"doc_hash": "ac0a65e0845f7e158859941b71440044120bce06b4ff76b73e93017d4f6ca537", "ref_doc_id": "b137b1d1-76eb-4c28-8474-f9d9cb9fb11f"}, "7ba336cd-3f58-4c84-b1e4-3eb60aa766f3": {"doc_hash": "986ddab2a52e40715ec232173d6cf34b79acd2204b7c1c457db35b900bffcf2a", "ref_doc_id": "4738e651-60f3-49d9-9f0e-c6e80fc363a0"}, "d183b6af-427e-46d4-a96e-03a5fadc083b": {"doc_hash": "2a44de7abe6d1e56ecb947d830524589e886f486c950675b851986feea0c9562", "ref_doc_id": "f07ef0e3-bac2-4112-a550-4aa731fd8236"}, "7917cce0-f165-406e-885e-7760e9931f5b": {"doc_hash": "1f81b56c62b224a1fc5c596ba6c994e23e0473c182df675a2e02a5a264a7fd91", "ref_doc_id": "d4b6e2d4-c5e9-412c-a263-4dd7ac67138e"}, "fdf849e5-e62f-4641-8a6a-bb69fcc12901": {"doc_hash": "f26e4cd4de0c09e9b9e7c04c2760ac2ba8bd4300973db35e140af3c15642d49c", "ref_doc_id": "268e3d23-d175-460f-9b0c-3c63224b0b97"}, "b4421a52-79e9-4458-aeec-6b001af320fc": {"doc_hash": "f2c19d3ced9de0a81775126d7022d575ec50075aa8692eaa37a88da904e11762", "ref_doc_id": "fe45250b-25e0-46f3-be4b-8b26d7ee8be7"}, "3afa05b5-d65a-4d39-99bd-e883b621ef19": {"doc_hash": "4e3f22bad9ba9cb277d7853bced16eded24e0452b25137e9c345053765bb5ac9", "ref_doc_id": "a33f9f31-d173-4f26-ab64-b78e454e6e7b"}, "a4242c3d-88b9-418d-acfe-7d17143e2b90": {"doc_hash": "75730e5d6c6ac828fc940a9294f97cd54f0ce09d799a2a520fee4ffcda590428", "ref_doc_id": "759b35bf-9700-4feb-a710-198a75996616"}, "a01450af-e5ee-48b7-ada1-b13bda805a30": {"doc_hash": "44f538638d6844f7bb32c8f71589e92f9500dd7d0c4b5a694acbb89b286f1a3b", "ref_doc_id": "0cda95b9-3300-4296-bd13-f0ce7b223d37"}, "79322ace-b4a6-4be2-9497-cf78ef7d56b3": {"doc_hash": "b78d3bcae4c8560ef73156556caedbe4cb1cbc34bebdccb228ed85eeb3aa2d34", "ref_doc_id": "e0325c47-3f7e-46e9-aaca-b5618ebf055c"}, "4f6b9974-7da9-4a32-baaf-db0f144776d8": {"doc_hash": "f6fd953a455725ab8ebf5ca975f5dd9111884a89faf267c344b30387bb14fb20", "ref_doc_id": "556f65ab-88fb-4aba-8e8c-7829a9de24a6"}, "ef0c9b42-428d-40ac-8f9c-4802e8f3fcb2": {"doc_hash": "af99a944bc3d8f2b00539bc49d71ac47ef2a4e6ecfa66f091c92fa51dfd835d0", "ref_doc_id": "dea88df1-71db-48ff-861f-cfd6d384bd06"}, "9c775082-e439-4cc5-a189-e99bb59ce3cd": {"doc_hash": "2ce19be8464e624506b902b714b73835f181814f35e28576e7e60263aaab5086", "ref_doc_id": "89a5411c-aa01-4c2b-9d00-04d430ac929b"}, "443f7b96-1b65-475f-92b1-e44745626f13": {"doc_hash": "8764872e63c6217c09384743fdfa9db118609a86325b3537d51c78593a3d0f8d", "ref_doc_id": "4fae5da1-37ad-4a3c-8804-fdd107bbcb39"}, "c539421d-cfc5-43ca-bf63-bf4c9d11ca5e": {"doc_hash": "ba2401651150ffdb75752f9cb334b8c54c1d7ff7e7b94d9d670fdc5466258a73", "ref_doc_id": "cb91518e-ccca-4a97-8afa-b65ddedc40c8"}, "f086129b-5cd9-4e3d-a8b4-c81808701ddb": {"doc_hash": "514e6172ea6ee85111cfc918dfb2314510d8c345762780ad8945ec028410d147", "ref_doc_id": "3e74ee4f-2c37-4c3a-9094-67291e7aed0c"}, "6ccebcd0-69bd-4d59-a113-7fa96f57b959": {"doc_hash": "faf6875c46a1505da7c388aacc5c508885e910ab940397f837267352fe7d3019", "ref_doc_id": "e6ed040d-4599-4f63-9713-21643c08d14b"}, "21ac0cbf-9094-4a7e-8934-d0791f6423fd": {"doc_hash": "290259928b2863ea3af5b0e2eacebc028fcd1202e49396e6151134bf18362f05", "ref_doc_id": "2507874c-147f-4ba2-b52d-0bce0275324f"}, "f2537d46-2f96-46f8-b062-a2039c1d5119": {"doc_hash": "c908f1cfae3cd13cec9dd698fe1d2da0fbadb93ce4c2ee5205e03f90b45770d6", "ref_doc_id": "aae6d196-35aa-4723-a43d-12ee2e12dad4"}, "7454ecad-a503-4d2b-aee3-3ce634036818": {"doc_hash": "e526dd700ec301335879d4773c3383c62b88de01d8fdd63dcfecdd0638039586", "ref_doc_id": "885ccbb4-34ac-456b-87b4-03cf1b17db25"}, "ba0d06b2-5e73-4c31-b4f0-1a885f68c384": {"doc_hash": "94501e8427e93e27b75fc835e8febc59f6c173c621b1b824504c23161d45cb40", "ref_doc_id": "ff564d44-d7e6-474f-abba-fd234ef7df00"}, "4de2793c-4712-4165-b783-bd094e6b0811": {"doc_hash": "a4bd5020338411c15f418cc48e67254681e2225049aa0d310ac81c1879278a23", "ref_doc_id": "5e22e822-31c6-4813-82e3-3b0dd629067a"}, "b0f19198-d87a-430f-a53f-a4059a0884c3": {"doc_hash": "c412b8b3fdd656f7eaed8792bb85ff03ac9c26096f6e45b1236b03784d35ed3e", "ref_doc_id": "13238669-c93a-448f-8db4-b4ad15071394"}, "dc295ccb-67b5-456f-b56d-bf9f2c0bbdfa": {"doc_hash": "eec143cc6d3cf2f0e100ed30d65a0afe55402da5c136c0f7e7328c31d9416711", "ref_doc_id": "9fc8790d-3459-4432-9a9a-2dc4513c3c3f"}, "3613c13b-e0a7-4228-ab85-a7b88f3c4a83": {"doc_hash": "ace8abec02de72d4245473c0d012c6ffc8a64426ca247dcfc717f85521ca2842", "ref_doc_id": "b3ae67c8-10a6-492d-98fd-caf2df38d93a"}, "b86e8f6a-376a-465d-a70a-b301ced72c89": {"doc_hash": "e616e7ff05f62724e7f89191c017c585e856ccf34630133edfc3723682945d84", "ref_doc_id": "2c973714-1327-4f17-a8ad-3e6bbbe6c8c7"}, "ad71549c-8290-491a-b8fa-de15fab0163b": {"doc_hash": "ea13f6d813d7c9fcfc69bf6e997cc0d3fec1dffe6063a5dd32bd2a9aa8430e90", "ref_doc_id": "2463e0f9-f295-4325-9a22-a2489ed55899"}, "498cc788-2eb3-4341-9088-ddd75b485c07": {"doc_hash": "70af10103d1712393ec9d3be5f00c799d60bdc6e87bc85889b66bf213fe93df8", "ref_doc_id": "8aa29d93-878f-441e-a49d-089ece12d7b0"}, "099df565-1fea-455b-b414-774a18f0c18c": {"doc_hash": "85e8ff21b17f199b1fdf634ddc6c5eb2d023e2cc636c448c01fb384ea6fddeb0", "ref_doc_id": "8d290785-3a12-4b06-a821-4e7adf4d3912"}, "28070354-b27b-4996-a730-3c5cf87ac7d1": {"doc_hash": "cfa9ed9ae89e8e07e7ae05e0557236c54e9b790399d4a73b071c5524a8e5d6ba", "ref_doc_id": "4aed5dff-ca5c-43dd-bde8-1c4102935cbf"}, "6149301d-90d4-4cba-8598-d905bf9a2832": {"doc_hash": "c9ef0e510138c257d50bc5f3e6e726c262caa09c0ef206bdf68dd3a67cb89b86", "ref_doc_id": "6c80f8f5-2c34-444f-9cd2-a89f9e0e7de9"}, "16960e18-b9fd-43ad-953b-cbaf0367dc4f": {"doc_hash": "09db0a4c5eabb211e96316cb39e67b7860d349a473b3e790700e636477e732dc", "ref_doc_id": "22110560-cf2e-47d5-9618-c6eb6b2be016"}, "a7771cfb-4128-4267-a213-5f1727df684c": {"doc_hash": "6825e74b73177493a867d4d3542923d64c579701b11adf12d4b8c4680defe9bc", "ref_doc_id": "9d027828-b439-4704-b10d-9ef1051c3c31"}, "eb026f77-7839-4e0e-9942-4429f1794509": {"doc_hash": "797a978a774437c27f79b326673e52bdbf0e2b469fbcccec6201cb92ff51fcd3", "ref_doc_id": "7bb47ef6-b3b7-45ff-9da9-479566d83fd7"}, "7d869ef0-6dfa-4a09-8809-bdb98780f251": {"doc_hash": "a6a1d5eca5c716f80b42be8fcf9581907eb1cfbd11f4f33aa1773da1554286ca", "ref_doc_id": "b3e6e739-1c89-4f43-8ce7-77ea9514c8cd"}, "e900c715-67be-4e10-9a4d-5ce5da0ac1c3": {"doc_hash": "bfbb7d42fb483433c71e11c7ef57e5c91b83e596483030d9ee4f0857bdd7bbbd", "ref_doc_id": "c08874de-13ed-4097-ba87-dff6e43fb537"}, "25b1798f-e663-45c0-bfb4-e86489c1c23f": {"doc_hash": "1faa813bedadec82800305a53ce7badecc8501a61775d6ba74f5667106a66c8d", "ref_doc_id": "8705eb3a-1f5f-4bad-9654-edfbb8f88068"}, "8daf03cd-cf7b-44ca-9550-0ae9615f1b3a": {"doc_hash": "784f67f0ee7b0d04a9e102af3bb5b704dd0f2bdbe4e180825038c73afb63b8f2", "ref_doc_id": "ae7cf2de-36d9-4a7a-9387-66cb81cb2068"}, "32493781-67e8-4b03-a60f-ce207c00c59d": {"doc_hash": "20f6f461d421556a1db9f164049dba5b13662249359dd9bbc75299a127a8f17a", "ref_doc_id": "2bcde958-2fa2-4bd8-88e1-f60f9966fa94"}, "b70d3a3b-8ff7-44a2-a798-a7dc9336a65b": {"doc_hash": "7e892476ce95cd03de65902ee151a82825d7d84dcdf2eef87546672ba662054c", "ref_doc_id": "2daa6184-1982-45da-bfee-483064f41798"}, "41ec6ef9-0334-4155-ad11-49b5d38229a9": {"doc_hash": "5c998d715a8dff25218c3aca11a8f83cc6d93a94a86a8b6b7f0a0ff036833e4c", "ref_doc_id": "66cdc326-000e-4d3d-acfe-9d46440b8b04"}, "cedde4ec-54e2-4f3f-8a31-eb8a74ce9679": {"doc_hash": "fa098d4bc841ae73873e5ff415c6e2c7853898db513e44aaf2d6c802aa5327a3", "ref_doc_id": "ed03184b-4e03-462b-9ec0-193a951c8487"}, "d3a0c1ed-8ee5-4625-8db9-35e595ad7fb0": {"doc_hash": "561f2bb48ce779f3eeccf2e7c65fd156b0c0d32216695118d15c089a88976d0e", "ref_doc_id": "65421512-2153-418f-a356-5f80f4d63fdc"}, "faad3477-2005-4101-bcdf-585338b2e818": {"doc_hash": "9f8a3b89cf30c2bc6852cd4ec319c63af81504475cfb4c739cc43a2db7ab2f38", "ref_doc_id": "bf6206f9-d55a-4a4f-9245-71b7ee4a85a6"}, "7f820a50-6787-4f35-82fe-a091f80c4be4": {"doc_hash": "c03c4ed5a3c37c5661ddc3379f6fbf59d0b1130d81b228a025dff6f5f733aa4e", "ref_doc_id": "fe9ef192-dd43-40d4-9660-73382dbb3fa4"}, "5ce76664-0e88-49f7-983e-dc41346532a4": {"doc_hash": "ba62ddca8d56d01c87ef132fc3ac7c7021eaf63f9e6875b3e77c2673ba7b3027", "ref_doc_id": "3f655cb7-3c82-4f42-af76-6975240df378"}, "c3b2b703-04ff-4c73-95ba-ac95076fc688": {"doc_hash": "4f25e5e389905af18221aa78a418942780216f920ee5b706afba6c2cdb42015e", "ref_doc_id": "0984b0e1-8825-4bab-a777-5b25903afb80"}, "09258b97-4b8b-4c6f-82d5-1ee07e793cdb": {"doc_hash": "1267dca9153f9f8bdbe34fb8ab2b39a379bf64c33ff2c2427562e13601bc18b1", "ref_doc_id": "16dabdeb-269c-4d97-b25d-1602538fe2ae"}, "e611c3de-0710-4a63-a007-e0da60b5d1c0": {"doc_hash": "537c9be45fb8f5e040397cb610226d0389d940f6866732c0fdcdd9427d5f61f4", "ref_doc_id": "1e73ea35-227f-4865-afba-b5325315169b"}, "5180069a-c02c-4941-baa4-2494b5c06d85": {"doc_hash": "25004d743a42e8476d596f2a6d31be545f34ff60406d13caa51fa869ba0b776c", "ref_doc_id": "54cee247-9a64-4b4a-9526-013be196d873"}, "ba971737-4ff0-409c-a5a8-f748ef1739ec": {"doc_hash": "b0a7204627bfe5017fff0cdc027110891629628ec06a2f0000e4a7726aaf13d9", "ref_doc_id": "6abe6038-5b67-4bf4-bb6e-47c4a3e0f197"}, "7c3793f3-be04-484d-b65e-3383693709dd": {"doc_hash": "8ab7eb026555dd135c90fa3960da3ffa388f6be7a4526887283117cdbcec906d", "ref_doc_id": "f53dcc64-eb02-4465-b9ed-0625a409b519"}, "524ee179-6372-4b35-aa27-a5e3eff6a904": {"doc_hash": "c06801a643b01e7a53764dae3797645026a7b99ee049c83d668117300fff2bf2", "ref_doc_id": "681c7631-815a-4a12-affe-c1cdfff0ab8f"}, "63012630-b566-458e-9929-8448e34b2e16": {"doc_hash": "6d4197743c96f7bfcd37f5864b1180f38993a7856374ab0e964a5006a11e23e6", "ref_doc_id": "b50947cc-b197-4b41-a716-a767606c578f"}, "cf3f82fc-54fc-4c25-917e-c595a64c6f7a": {"doc_hash": "84a6fa2a093773f4ff10b6414c670d585f5686af5eaeef81f922bdfa90f5476d", "ref_doc_id": "1240fab6-21e1-4f4c-9dbe-a28e69663082"}, "c750e3b0-1d8a-4cec-ad1a-4a4469dae57d": {"doc_hash": "07fbc9d3b7c7f9cf2c1afcdd5e88ba31767fe90c3cfe11c419069b6161436771", "ref_doc_id": "60e0c99c-3ccb-4d86-9315-ce2b8bece4af"}, "96aac05d-08a0-487d-98f1-333a5f568531": {"doc_hash": "aff86bce625b5d4f5e6f0926c4293bc1eb940eb16d94208fbe4a9167e78b3b52", "ref_doc_id": "e8c598b6-7706-4683-9167-3704832bf3c4"}, "e492b05b-1a3f-41aa-af6b-efe4cfd30074": {"doc_hash": "8e9a3b74ad236b5f8e30dab00dd77761655839932e3a70ce03a361af5018780a", "ref_doc_id": "9fa80710-f940-4a97-b983-d00dfb02adb5"}, "99f7e23e-947a-4cd3-807a-953a2c398f84": {"doc_hash": "7d94cd51a99fda298a84a526349018ee135d32fbea22350d5f07bba7b1a25be7", "ref_doc_id": "11c9715d-dc9d-40a0-86f4-4aa11d06ac66"}, "6074dd95-2f2e-4524-9356-6ddb60a80f6e": {"doc_hash": "ec02a8857e1c80d68b6089e3a705125eb0fd4bd1db7281750772e3a2f2c842af", "ref_doc_id": "ac15fe04-145d-4f53-ac4c-7f00a54a4cf3"}, "e0467500-8fc9-4bf0-8068-df0e643ab802": {"doc_hash": "0ab58358cd46e15e1d44cbaeb7fcffa48c5c4f97ee52c5709a67bbe2de72ba65", "ref_doc_id": "f2e4f884-77fd-49e8-a4c5-c53f6fb4bdb5"}, "23113c86-7fd9-48ca-9a1a-f9497804b1e8": {"doc_hash": "7d110d1cdde09d8fa93e6575f6bcead05f32c6b25a5760b7ae7cc19164ff393e", "ref_doc_id": "eb48d5c0-da60-4aa4-8734-06294983fb1c"}, "120dfe82-aa7f-4acc-b3b7-57b7c1565bee": {"doc_hash": "8c7baeca583f0dbd2053e16d98d577bcac6a73bac03e9b24f21d219fe5015ce6", "ref_doc_id": "c2201a65-c24f-4b78-9c57-543a46df7785"}, "22d1546d-3d7d-417b-8930-82e7d47fa038": {"doc_hash": "639a81e9bd84f74efb0cab298ba7f20efd0582619b34f4e80d4a1f61250adc64", "ref_doc_id": "70070691-a812-46e1-820a-72b8d72a191c"}, "495b45b5-7118-4a83-a7a1-bc424043c912": {"doc_hash": "f392d54f6dd54114957f2a40519e3d0a35c5faf596a95ec90bd9d08cdd4f79d5", "ref_doc_id": "6bfd0d76-acb5-4a41-8f60-295f26f0ba1a"}, "c5109382-b8b9-4fc7-a197-78d2cd5b9034": {"doc_hash": "d1c9fb9771b472bd29230fe8bad64c42f798ff54c2bc552d94c9e6ae0307c646", "ref_doc_id": "1e65ec52-ec77-4f0b-b62d-36ec7a0b7aa3"}, "5521481e-9acf-40ee-b452-96a6dcdf80ca": {"doc_hash": "091d2135dfdba684db9ff151b9fab1808e647e65af44130d9f8d87ab48b6f8a4", "ref_doc_id": "f27bd5ff-f5f6-4e01-9e67-5ec2795a3dd4"}, "8d8c0d28-0a09-467c-bbaf-6134ee16c6a0": {"doc_hash": "fcd1ecfb2937725365efc36c3a148ed32a9069f6272ffe335a0eeeedaf09748c", "ref_doc_id": "82b6e5e3-0b77-4480-9b8f-ecf6fac3e528"}, "8009b1c1-c8bb-47cc-b01a-9f0b6b7701f6": {"doc_hash": "fd63edee65352488317859a701755e7aef2692a87f73fad531e4c62b8aa49bd8", "ref_doc_id": "6fc1f10d-3311-47cb-b4e6-d101dec4e87e"}, "9a9fce72-0464-4609-895b-c98cf4727c4a": {"doc_hash": "92ba3b1f74d35435ccf48086cf2c68211a17dd3c2b3a8510730a2298c86f7924", "ref_doc_id": "d7738e21-a5b4-416b-8377-e673f3f8f5f6"}, "19151ddf-e2ce-4d90-937c-fd7c96b4cc83": {"doc_hash": "bf351974c03390d56ef101cc0fa4b5ab31157df82a2abf949f00c9c921b6b484", "ref_doc_id": "80f0582b-5836-4bdc-81a1-01afee9c253a"}, "65fbcd57-1afc-4fb5-82af-dca8c8e21b47": {"doc_hash": "5ee7ea37e98124fa798fe0de80fbae0c4b0f95f4ccd3bf9606b8f2a9d563fd63", "ref_doc_id": "c6ce5391-1394-43ec-87de-97316d6008ea"}, "e6a3b9c3-cd81-4cb1-8c36-06a89b633131": {"doc_hash": "bfcca8209929b279fbcf760277a17bbd3e90dfea2b5f35d8833fc1f1b39296c0", "ref_doc_id": "435f96e8-f705-466b-8e3c-a20eba1a0ab6"}, "a0489b91-cdd9-47d3-b3dc-b41210749bbe": {"doc_hash": "691e858863a9c214a73b605ee161489fba42d6c521ccddce50682ba6a4f0e6b9", "ref_doc_id": "a270f694-a4c1-44c4-9b76-b362fdd7be7a"}, "69e4c1eb-c844-4b9f-8309-7ffcc57469c5": {"doc_hash": "7150c1a874d21a668b5a35bb7da1dcae0fabcd0babc0f6c8aad1886194d3037e", "ref_doc_id": "0e697497-e117-450e-96db-567cc1fd941f"}, "85481bec-fe3a-4fe6-9272-b86ab47179d4": {"doc_hash": "805fc4d27b08c890b12dccbf7964421941bceb57da266fca3d6c4c0df592ea0a", "ref_doc_id": "632b6f14-3965-45d9-a77d-e92181f5bb8c"}, "fd25fc63-4d0b-4d29-9670-a6119124598a": {"doc_hash": "3573dd54e9da6b6596413de241a3aea430bd9d300dfbddb86e8a3ff2f978d295", "ref_doc_id": "8262359e-29ed-4003-bae3-37dc012c80a6"}, "5685792d-17fb-4614-88c2-68520751abc8": {"doc_hash": "eb86930b5713bae46e2e6e3f92e0058ad28b9240d8ddabbcdf6b09bcb3b71cc9", "ref_doc_id": "18624e86-96dd-4ca1-80fd-cbf4788d4b7d"}, "eda916be-b392-4bc8-a356-0c80f1e044d6": {"doc_hash": "11797d11e04ec76ce83746cd89d80fdd45ba1bcbb60db2537f2995592983a0c7", "ref_doc_id": "b3808f54-f124-4d0c-ab1f-618eda95593a"}, "3f10ee03-a0b6-4b02-af48-1848d629639a": {"doc_hash": "bc14bf23edbe0f2449df3abb3842bc4144a0016bc298a06400ed0a72a06c7a75", "ref_doc_id": "1f1c5137-1bbc-4a40-bd25-db7996926143"}, "059b6400-033b-4b11-a0d0-8dd0c02ca619": {"doc_hash": "f7fc1fe4330f2f507dfc0d4a913756eddd2a58ea702567851d818148b06cfb38", "ref_doc_id": "3e90d93d-5469-493c-ac99-d2cd7e80f725"}, "339907ab-f39a-4eed-a7d6-e86106dcdd6f": {"doc_hash": "769a8b1e655e62f3793e969ec7d9965b94c50abb6d00c97c63bcd8cf71f171c9", "ref_doc_id": "bda2501a-784e-484b-a1df-bfdea29358a2"}, "c1442b64-4520-4316-be19-b90d40b2b9a8": {"doc_hash": "6de53049b2079a04fc5c28a08f5aedc7f695cfea5035477fbc2db0ceda4e8e8a", "ref_doc_id": "7cc10ee2-2872-43b0-a6dc-059019cbfc7c"}, "317777d7-9742-47af-be50-825be566268e": {"doc_hash": "0d168065148f30bb1a40460e7605285f703758373a201143116b04fd90ce41dd", "ref_doc_id": "985383d5-8688-4d9c-92c5-0e27ee0b0112"}, "183d2e7a-c302-4afd-aaff-eeae2ff3f3dd": {"doc_hash": "ac781df9be1aa3b38bd61fa9ce901086270c1b091f3b93f0d77e3f99dd2f06d1", "ref_doc_id": "3ec9c0c5-b5f6-4e0a-96e3-4cdff960da1f"}, "6d19626d-b338-4847-b1ca-120e1e30f76e": {"doc_hash": "af07f98cd3c776ab6c9d0505921722f295b588dda1e5a7f570f6c05adc0f86a9", "ref_doc_id": "de17f2e6-650c-4858-9fa3-216f1eb249e7"}, "29cc148e-8e91-435d-be63-cabb63a006f0": {"doc_hash": "df3b1e58877b7c76bba187c3839d9d9b5113fe22a761c58beacd8e5f83fe8563", "ref_doc_id": "d98de347-796a-46d5-9155-f3aaa2f3a921"}, "5b4da239-7fd8-4aba-b908-dec4cb4d6342": {"doc_hash": "c670fee34956fe49510878790983a0acd7f517051a39cff9691169580b768f2f", "ref_doc_id": "6eeeb315-2816-4bf0-92b5-f5d92eed548b"}, "344780e3-f4d3-42d0-aa2b-974f955db383": {"doc_hash": "3e3fcacfd972479ab5324d79c26b4d579ee55481ef9b9b13dd0e72c3f683a52d", "ref_doc_id": "a4d3d7bd-e13d-4f3a-a5cc-c53e97a1b090"}, "26ff6cb2-ae2a-4d1b-b08d-c68aa1fe6638": {"doc_hash": "5ec4be3eeac0485f81b518f21551b7684954ec76fc07f97796dc89457b0bafbd", "ref_doc_id": "570bc122-9b98-430e-9c45-81f50eca26e4"}, "b08a5680-2817-4cd5-a233-dfb3b9b478f7": {"doc_hash": "7bac0e969b803ebbbbde1fd69ab301f094b85b43084e9b7046f8db7e994d1e1a", "ref_doc_id": "85a17d89-f7f9-4f69-a1e4-4a7f196609a6"}, "f35fdf52-7d2f-4503-8483-1845fdf8fae3": {"doc_hash": "9a0ab346fd08de1f3b46805251ee52c3c168405e3efea1b86b730eb60990d76f", "ref_doc_id": "9836168d-450a-4873-be3c-64d13d4707c9"}, "be7a4b96-5689-46c4-a7a4-75d3bc7a8256": {"doc_hash": "5ebda937ffde10ba6a67d844b1a10ffe7660206ba417f35447c4a77ade2e97d4", "ref_doc_id": "8861b292-6a4e-462f-bf12-ebeaf3cb795b"}, "30b28b21-5eb9-4624-9dbe-d444ad80fd0e": {"doc_hash": "fa2c0c9d43fda9795e7f54539c8f6b573914cca26f8cb2393c51ec1ac6439b3f", "ref_doc_id": "19053491-90c1-4900-b3df-cec4d042a334"}, "7026a587-e6d6-4d8d-a05a-8504c68e6ccf": {"doc_hash": "5d8d90a7fcd8cc3a8acb7a13a4f91a1a17270460770270144a591fb02fe9649a", "ref_doc_id": "85fc3415-8fbd-4e4d-9f77-7ed86812eba3"}, "29b07c41-2f88-478b-8082-927a1713397b": {"doc_hash": "14e749cc798676e3b7bfdf670764eb22bf5113430a17bcd664aac8abda469813", "ref_doc_id": "7be73b89-e40e-4d0a-bcdf-76a5aa7b79cb"}, "79c924ef-ba3d-4498-b7e1-9423edddcf29": {"doc_hash": "849f88315e4c359a6c157f290338df88aa8f80396a4fc7fd4011c7b36cdaf128", "ref_doc_id": "02648872-446a-41e0-a655-eca95044163c"}, "6b0821c1-ad26-4d07-b0a2-0b75dd5800ab": {"doc_hash": "3145d6511e8afaf145482e00a3fac96921bad4eb84e2fe7a8f54403e91ef050e", "ref_doc_id": "87aa9734-c741-46ea-844c-5abe89d376b7"}, "20552f36-f7b5-4058-8e0a-ba0045d371c7": {"doc_hash": "5ad5f91543c696dccf3934e0559beddad576e2617abbee0e9da771fdf5e51a51", "ref_doc_id": "703cc9b9-e2b0-48a5-95cc-1ea42d282b60"}, "d6ac57a4-6f8f-4044-b86f-bd34617c1ba1": {"doc_hash": "c476282b3e426117c4351f0c3b76904d252ef0ab8501f9cfbf0343db244191b2", "ref_doc_id": "6935f477-88a3-44dd-98aa-f92408087518"}, "b7a9e2b4-7d94-438f-962f-69543f8ab415": {"doc_hash": "30e7e9a1098f88eea0f542a3ffecf1495fea202d9d54cb7b33e0435e3cad4966", "ref_doc_id": "3509afe1-f76a-4f0b-adfa-80cffbcd902c"}, "7a81e309-8882-4fa3-8116-4e30fcccda52": {"doc_hash": "de5f367f03788d1c30c323e6e988ffac8e00b7330aeadc72023e6fdda7839649", "ref_doc_id": "9718f708-f716-43c9-aef5-3d79f9a41216"}, "0c426982-35e8-4e5a-b4fc-a57f032d2b34": {"doc_hash": "ab476df02f2eebc64665a52c1d467aea5d704822eb675e4d17ecbac661fdfdb7", "ref_doc_id": "6d8940cf-4a56-40c8-a14d-65b8e7a452c6"}, "e11c3d95-5be7-4553-89e4-df6aae1257c5": {"doc_hash": "2a8e0bc5c0d3d7127a911a25c8ab99f416ba5c0110b5841b9e37f404ab298fbe", "ref_doc_id": "18692ed0-051a-40f8-9c9d-5b4f5776e51d"}, "63361d68-0270-4fc7-a375-1b33379c6b44": {"doc_hash": "7dab5bff7b9bb5b50db368f2c2e4cf8c604927cb2ed18da80c8feefabb5ab493", "ref_doc_id": "c93bc452-1746-464d-a1d6-1aca61539cf8"}, "447de163-9c35-42be-93c3-c6ca5ef9f0f4": {"doc_hash": "3d9b8b38694db1ac55916d769d3e8374e81465ade0e555e27b2747fbe834e071", "ref_doc_id": "891e1a39-3797-4686-aac2-2324a61fac11"}, "caf2b763-2722-4f06-a202-dc42883dbb8b": {"doc_hash": "dc2b033e203951459b1f5c8b16b05948f0e0e91df29b0bca9590fa4c08a40d79", "ref_doc_id": "a3e1383c-3df4-4f5a-851c-fc3f75c8497e"}, "5417d6fd-3a96-4069-b649-74028b7231ca": {"doc_hash": "bca63d84365174384891643f71f704a9235d5df843700e4fceb76e52321efccb", "ref_doc_id": "506e2a77-43ad-4b8c-aeaa-5b97f719292e"}, "5bbb5831-61f8-4342-80c3-08839d713697": {"doc_hash": "97b6730b87ddf7bf1b4d4b0d1d286873608bbdfc4b6e250889b552930b424582", "ref_doc_id": "d33cdea7-d2d2-4530-95b1-a6c2bddeb32b"}, "49d5413a-8404-4664-b614-78e0837cb644": {"doc_hash": "1154653071ad32f9c8e139bf813834627544cceb40d9b2e8fe5d107276e68ff5", "ref_doc_id": "e16615fa-db1b-4c60-9dfd-398cb857304f"}, "8ce3a23c-aa96-46ec-bf3e-ced02a058149": {"doc_hash": "29f10a3252d654651ceaba66cab10d5edffab4ce39ade4de813567d6b3fde939", "ref_doc_id": "e74c6fc9-111f-44b9-a5c1-040c52410f6c"}, "90f6b2b6-a419-414c-8145-6f27d67a3aef": {"doc_hash": "1354c89dc6b8639b14863ff7ad1d47f9b8984e510198b2f36ba7df29fb84ed19", "ref_doc_id": "d93faed3-ba2e-4915-8026-8365d2d14423"}, "a9091752-0abf-46a7-865a-43ed73a2a757": {"doc_hash": "5d2d4e0b525086f82f9578dd76648c55e0db11489f45c42d49fc0c41ea4ddd41", "ref_doc_id": "48477fcc-691f-463f-b45b-2849a53d6aa5"}, "aa55c503-693b-471c-b732-65d2c1616ae1": {"doc_hash": "ebcdce516f503612964d06c5d7866c4f13e648ee8cb637bafe067b8c2623c124", "ref_doc_id": "80287f50-da5d-4e30-bcc2-86fa7f642a27"}, "c6c9df40-ffc1-49dd-ad02-c73fd103b973": {"doc_hash": "2f00555ccac4e308b331dd726c566496e33190bf3f36cc271cd0b7d1037d45bc", "ref_doc_id": "3e1922ff-baa8-48e7-b731-94c1daca510b"}, "f61bf414-8a45-4bd1-b582-744bb2e980c6": {"doc_hash": "862f227a5a9ab83663a95f5c8a6b86ce5d5a97b144dcd0c0c64ed2f43ca85674", "ref_doc_id": "36a7f4b5-af65-4b7e-a0ea-10d884b2f7b7"}, "2c3011dd-0e32-4c59-be7c-eeace2eb27eb": {"doc_hash": "7addba73df9a23f2784726fe50e3f15a34ff46ab2a8376eae43b2828e1c51271", "ref_doc_id": "63ee8a48-1f9a-4aa6-9ecf-14c016d732d2"}, "c7d44800-1881-4576-9697-6103d0371f4e": {"doc_hash": "415e9c1b843b7eeecfc91c0da554d2a078159bd493c0cb4a2ef8694f3f443069", "ref_doc_id": "749e268c-6685-49d1-8911-06d614772d2c"}, "1fb3d4c8-0a73-463b-9ae8-223de1733891": {"doc_hash": "c6a9b67d920e1939a68ff9694a59348e7e25c8758a8b230b59fb29d8fd4e7412", "ref_doc_id": "0273c2ed-70a8-46c6-a209-dec0015be041"}, "168012dd-7ed7-4a11-a10d-aaf5e606b579": {"doc_hash": "991af325d6e37bf2064431f4d0df15ea46d66adb094f2afdc5d8f5a6a06bad41", "ref_doc_id": "5c9636ae-360b-4680-a22c-b48120ff1284"}, "f32e30a1-40a6-4c2e-96b3-ed3533296b1f": {"doc_hash": "3cbb1cb1b979aaffbeefcd07e298f8050d28063a8f9aa5ba48356c630145a0d5", "ref_doc_id": "4c2a7d84-7bb9-4173-aba2-b8fd1e180b45"}, "e0437c4c-8716-4354-a64c-cac858c3935a": {"doc_hash": "26c687014dd7fb49ac288d13a2a0da9d18eb4a353a7b7879e36d7cff513f181c", "ref_doc_id": "be85ea15-9607-435f-a738-b0994566d1c8"}, "6cb767f9-eb9b-445b-a2b3-2dc877c4da9b": {"doc_hash": "ec69373af55c319d08418d017fb303de0b0870e602229a25aee052346ec742c2", "ref_doc_id": "3b92e74d-d69a-4e0d-9bc2-0cd9b49943bc"}, "7761c669-78f6-43a5-9bca-c6eb51c5c498": {"doc_hash": "711ba434574f6933b9901026a46d34bd691c1669f261618834a61e1322bdfc17", "ref_doc_id": "2b276a36-4177-4614-90f4-ced07f7eb874"}, "000bc57d-2008-4cd9-a075-e6202829f8f8": {"doc_hash": "890ab7dca9143852181a0245c4dac88af20d20c7be26856ee07c4f3443eea048", "ref_doc_id": "ec61542d-8b98-4f03-8581-edc47443e7d7"}, "f712bb45-e9bb-4204-9e98-8700b7ad2a8a": {"doc_hash": "b9cd9a219388be42d576d200b25ed9c50ffbada03e677bac589ea9a134bd05c4", "ref_doc_id": "adeb9a86-3040-4f7e-aeff-6c399f3e1d05"}, "b38d4ef2-4cab-4935-b9ee-9050a6ef9f60": {"doc_hash": "f6a72756cd94705b8043aa529aa18db9ea86ab08da45498747d6d10d3ac23b60", "ref_doc_id": "e81d1485-4097-498e-ae45-4dcca4c7a01f"}, "ebf7af3c-7272-4fb4-8ef1-a59e65eddde0": {"doc_hash": "71c7b9b3042fa994ca56b250e14ec4d4815869e6e1a4e40061ef22b5302435cc", "ref_doc_id": "3631ed6d-7b27-46ce-beb3-4fc6b4b8447c"}, "6729502a-e921-4090-9281-21453d5fcef5": {"doc_hash": "10859fc574e8f4533097a07ed26a2263a980a341cd0da48d816c7a063fc0df17", "ref_doc_id": "65680103-60a1-43a6-8038-837d7875d63d"}, "d6d75f0a-9ead-4ef3-99c5-b37e1bbf3184": {"doc_hash": "e04b7303f71ddb4f7f356dee9851e66a4e101f6c0d0883c703b70341c9d39e78", "ref_doc_id": "59074bad-f21b-4fb0-8e24-f280fb071a9a"}, "6715eaaf-5a68-4fb9-bca7-b6adebeeb0e9": {"doc_hash": "3071d28337ebd00a6a3e2135112d2ff63507302541aa385222964ae5d96de742", "ref_doc_id": "788fe1cc-44ad-46ab-b7e0-8d390ceb0fea"}, "eedc25c1-8879-4ee7-b69f-bfc15b49c2bc": {"doc_hash": "a8d1a66061dfd0538c4a7ca8a923ca25f069a2f16e220744f531a9711d97abbc", "ref_doc_id": "521a8690-5463-439b-8c35-4cb6b8345bc9"}, "b986c154-fd8a-4979-9aad-6e31a29f0336": {"doc_hash": "7bf9a649a36613b1a14c9fa773eda380d1d6de8ed63469cfa9ac70b53d60c22c", "ref_doc_id": "29c94398-ca63-42fa-bdc2-da01c5a7d075"}, "362643ab-8026-4a88-ad53-9c1803807039": {"doc_hash": "23dc66c33d205b9ccb683910a0df1c820114947ed81eb725d22d981db82d60d4", "ref_doc_id": "d299fd8f-72ea-4fc8-a8a8-82f0fff1328c"}, "13747bef-1046-4d49-90d0-0af8047029f5": {"doc_hash": "c4a292ff4122083dd1c3aac526dfa1e5fec9af733fa8ce03c74b28a6acb78e3d", "ref_doc_id": "5ed38320-35a0-4b26-9be7-8d5a4eef7519"}, "6f41b7af-e4e1-45bc-8d39-ee2ecf409200": {"doc_hash": "a2211692e9505de95c3eff9de86ce6ee2667c93c8b86df85dc6b642cb4f0d0d8", "ref_doc_id": "a3197b74-fa14-40df-8acc-33bd968f10b6"}}, "docstore/data": {"da934f90-7e2a-4913-94f7-f4f693618e6a": {"__data__": {"id_": "da934f90-7e2a-4913-94f7-f4f693618e6a", "embedding": null, "metadata": {"page_label": "Cover", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "158b4de7-8a7e-4aeb-b7bf-c5d216ed1a23", "node_type": "4", "metadata": {"page_label": "Cover", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2e4811c04895a1bae117316c0163a7da940b6682744556d0a8435eee773cceb6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Martin Kleppmann\nDesigning \nData-Intensive \nApplications\nTHE BIG IDEAS BEHIND RELIABLE, SCALABLE,  \nAND MAINTAINABLE SYSTEMS", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64c4f981-23df-4f51-8e58-9e7b062924b2": {"__data__": {"id_": "64c4f981-23df-4f51-8e58-9e7b062924b2", "embedding": null, "metadata": {"page_label": "BackCover", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f17a44af-b268-42bc-9cc7-b4d76930c2ab", "node_type": "4", "metadata": {"page_label": "BackCover", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b27adc0-5f42-4f41-beee-07f2029f8e40": {"__data__": {"id_": "5b27adc0-5f42-4f41-beee-07f2029f8e40", "embedding": null, "metadata": {"page_label": "i", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "803f0f88-0565-4a83-9050-ce9030447b37", "node_type": "4", "metadata": {"page_label": "i", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d0ee4426e44ffa1d1be9b5d164d9f6b48a2c79c5c40261272cd1ebd79a99322c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Martin Kleppmann\nDesigning Data-Intensive\nApplications\nThe Big Ideas Behind Reliable, Scalable,\nand Maintainable Systems\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2814700-a0d0-4423-b231-7fc15bd83b17": {"__data__": {"id_": "e2814700-a0d0-4423-b231-7fc15bd83b17", "embedding": null, "metadata": {"page_label": "ii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d962c8b5-54cc-4680-be29-9586eb3e7519", "node_type": "4", "metadata": {"page_label": "ii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3dc6510aac91b0220e1d2c406ed540fcc21df97f350df597578402e7e52b0f1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "978-1-449-37332-0\n[LSI]\nDesigning Data-Intensive Applications\nby Martin Kleppmann\nCopyright \u00a9 2017 Martin Kleppmann. All rights reserved.\nPrinted in the United States of America.\nPublished by O\u2019Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO\u2019Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti\u2010\ntutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditors: Ann Spencer and Marie Beaugureau Indexer: Ellen Troutman-Zaig\nProduction Editor: Kristen Brown Interior Designer: David Futato\nCopyeditor: Rachel Head Cover Designer: Karen Montgomery\nProofreader: Amanda Kersey Illustrator: Rebecca Demarest\nMarch 2017:  First Edition\nRevision History for the First Edition\n2017-03-01: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449373320 for release details.\nThe O\u2019Reilly logo is a registered trademark of O\u2019Reilly Media, Inc. Designing Data-Intensive Applications,\nthe cover image, and related trade dress are trademarks of O\u2019Reilly Media, Inc.\nWhile the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "561613d5-88da-4b28-843d-e6694db14db9": {"__data__": {"id_": "561613d5-88da-4b28-843d-e6694db14db9", "embedding": null, "metadata": {"page_label": "iii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8be8a3ec-7583-40d3-ac97-2a239839d10b", "node_type": "4", "metadata": {"page_label": "iii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c18e2e0007de7d6001e052ea6840ad3cb2a5d58774accddf1b2158473db191b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Technology is a powerful force in our society. Data, software, and communication can\nbe used for bad: to entrench unfair power structures, to undermine human rights, and\nto protect vested interests. But they can also be used for good: to make underrepresented\npeople\u2019s voices heard, to create opportunities for everyone, and to avert disasters. This\nbook is dedicated to everyone working toward the good.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6be9233-0bee-466e-b872-30cd57e71566": {"__data__": {"id_": "f6be9233-0bee-466e-b872-30cd57e71566", "embedding": null, "metadata": {"page_label": "iv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7278b6a0-f9c7-49bd-a0e3-deaffbf982e7", "node_type": "4", "metadata": {"page_label": "iv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da7b41a6-830f-4c6a-895c-1d1cfaf0c24a": {"__data__": {"id_": "da7b41a6-830f-4c6a-895c-1d1cfaf0c24a", "embedding": null, "metadata": {"page_label": "v", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59e1a370-fd70-40e6-8186-1014e376068f", "node_type": "4", "metadata": {"page_label": "v", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "990b94352dc6f87f40d6cb2af6ea2bd49422be46e839b7ae18128d3c24e8b20d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Computing is pop culture. [\u2026] Pop culture holds a disdain for history. Pop culture is all\nabout identity and feeling like you\u2019re participating. It has nothing to do with cooperation,\nthe past or the future\u2014it\u2019s living in the present. I think the same is true of most people who\nwrite code for money. They have no idea where [their culture came from].\n\u2014Alan Kay, in interview with Dr Dobb\u2019s Journal (2012)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f11c00a-0d78-4788-a7ca-2f2ed2ee24e8": {"__data__": {"id_": "6f11c00a-0d78-4788-a7ca-2f2ed2ee24e8", "embedding": null, "metadata": {"page_label": "vi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45466ecd-1ae1-48a2-90fd-2398a596695a", "node_type": "4", "metadata": {"page_label": "vi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5d9c598-3a7e-4c64-889b-701c8c1d0a2c": {"__data__": {"id_": "c5d9c598-3a7e-4c64-889b-701c8c1d0a2c", "embedding": null, "metadata": {"page_label": "vii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5661d06-2aae-4f13-8056-4e2db3b68406", "node_type": "4", "metadata": {"page_label": "vii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6864bc21fe255caf84b986871bad6b4aca0af87941f540b154ff1d416c897121", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\nPart I. Foundations of Data Systems\n1. Reliable, Scalable, and Maintainable Applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nThinking About Data Systems                                                                                       4\nReliability                                                                                                                          6\nHardware Faults                                                                                                           7\nSoftware Errors                                                                                                             8\nHuman Errors                                                                                                               9\nHow Important Is Reliability?                                                                                  10\nScalability                                                                                                                        10\nDescribing Load                                                                                                         11\nDescribing Performance                                                                                           13\nApproaches for Coping with Load                                                                          17\nMaintainability                                                                                                               18\nOperability: Making Life Easy for Operations                                                       19\nSimplicity: Managing Complexity                                                                           20\nEvolvability: Making Change Easy                                                                          21\nSummary                                                                                                                         22\n2. Data Models and Query Languages. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  27\nRelational Model Versus Document Model                                                              28\nThe Birth of NoSQL                                                                                                   29\nThe Object-Relational Mismatch                                                                             29\nMany-to-One and Many-to-Many Relationships                                                 33\nAre Document Databases Repeating History?                                                       36\nvii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32a1f12f-099a-4716-bce1-d9e47a3e63d1": {"__data__": {"id_": "32a1f12f-099a-4716-bce1-d9e47a3e63d1", "embedding": null, "metadata": {"page_label": "viii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38b20bf6-887c-4e02-bb3b-ece2d2f397ce", "node_type": "4", "metadata": {"page_label": "viii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e77d79d7a9c441a382b8769e99c4a0dac83a076c89b712c6eecc888877c2c279", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Relational Versus Document Databases Today                                                     38\nQuery Languages for Data                                                                                            42\nDeclarative Queries on the Web                                                                              44\nMapReduce Querying                                                                                                46\nGraph-Like Data Models                                                                                              49\nProperty Graphs                                                                                                         50\nThe Cypher Query Language                                                                                   52\nGraph Queries in SQL                                                                                               53\nTriple-Stores and SPARQL                                                                                       55\nThe Foundation: Datalog                                                                                          60\nSummary                                                                                                                         63\n3. Storage and Retrieval. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  69\nData Structures That Power Your Database                                                              70\nHash Indexes                                                                                                               72\nSSTables and LSM-Trees                                                                                           76\nB-Trees                                                                                                                         79\nComparing B-Trees and LSM-Trees                                                                       83\nOther Indexing Structures                                                                                        85\nTransaction Processing or Analytics?                                                                         90\nData Warehousing                                                                                                     91\nStars and Snowflakes: Schemas for Analytics                                                        93\nColumn-Oriented Storage                                                                                            95\nColumn Compression                                                                                               97\nSort Order in Column Storage                                                                                 99\nWriting to Column-Oriented Storage                                                                  101\nAggregation: Data Cubes and Materialized Views                                             101\nSummary                                                                                                                       103\n4. Encoding and Evolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111\nFormats for Encoding Data                                                                                        112\nLanguage-Specific Formats                                                                                    113\nJSON, XML, and Binary Variants                                                                          114\nThrift and Protocol Buffers                                                                                    117\nAvro                                                                                                                           122\nThe Merits of Schemas                                                                                            127\nModes of Dataflow                                                                                                      128\nDataflow Through Databases                                                                                 129\nDataflow Through Services: REST and RPC                                                        131\nMessage-Passing Dataflow                                                                                      136\nSummary                                                                                                                       139\nviii | Table of Contents", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2aff21b3-b096-470d-beaf-771591da1674": {"__data__": {"id_": "2aff21b3-b096-470d-beaf-771591da1674", "embedding": null, "metadata": {"page_label": "ix", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53b4b53a-1edc-428e-a3d5-7e6fc768608e", "node_type": "4", "metadata": {"page_label": "ix", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "143a54d50d779208c3ca2ca17894c4aff4ca2079920ad2be64e488e14656c5fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part II. Distributed Data\n5. Replication. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  151\nLeaders and Followers                                                                                                 152\nSynchronous Versus Asynchronous Replication                                                153\nSetting Up New Followers                                                                                      155\nHandling Node Outages                                                                                         156\nImplementation of Replication Logs                                                                     158\nProblems with Replication Lag                                                                                  161\nReading Your Own Writes                                                                                     162\nMonotonic Reads                                                                                                     164\nConsistent Prefix Reads                                                                                          165\nSolutions for Replication Lag                                                                                 167\nMulti-Leader Replication                                                                                           168\nUse Cases for Multi-Leader Replication                                                               168\nHandling Write Conflicts                                                                                       171\nMulti-Leader Replication Topologies                                                                   175\nLeaderless Replication                                                                                                 177\nWriting to the Database When a Node Is Down                                                177\nLimitations of Quorum Consistency                                                                    181\nSloppy Quorums and Hinted Handoff                                                                 183\nDetecting Concurrent Writes                                                                                 184\nSummary                                                                                                                       192\n6. Partitioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  199\nPartitioning and Replication                                                                                      200\nPartitioning of Key-Value Data                                                                                 201\nPartitioning by Key Range                                                                                      202\nPartitioning by Hash of Key                                                                                   203\nSkewed Workloads and Relieving Hot Spots                                                       205\nPartitioning and Secondary Indexes                                                                         206\nPartitioning Secondary Indexes by Document                                                    206\nPartitioning Secondary Indexes by Term                                                             208\nRebalancing Partitions                                                                                                209\nStrategies for Rebalancing                                                                                       210\nOperations: Automatic or Manual Rebalancing                                                 213\nRequest Routing                                                                                                           214\nParallel Query Execution                                                                                        216\nSummary                                                                                                                       216\n7. Transactions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221\nThe Slippery Concept of a Transaction                                                                    222\nTable of Contents | ix", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9d4215b-0830-4e7e-9fe9-f865f50c2640": {"__data__": {"id_": "b9d4215b-0830-4e7e-9fe9-f865f50c2640", "embedding": null, "metadata": {"page_label": "x", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee656fb-8bfe-4b51-822d-4eb09a6d2e06", "node_type": "4", "metadata": {"page_label": "x", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f13b68256424e4f1aebd4a003eee8f4c686bda3420a912a9b01ab3302b556813", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Meaning of ACID                                                                                            223\nSingle-Object and Multi-Object Operations                                                        228\nWeak Isolation Levels                                                                                                 233\nRead Committed                                                                                                      234\nSnapshot Isolation and Repeatable Read                                                              237\nPreventing Lost Updates                                                                                         242\nWrite Skew and Phantoms                                                                                     246\nSerializability                                                                                                                251\nActual Serial Execution                                                                                           252\nTwo-Phase Locking (2PL)                                                                                      257\nSerializable Snapshot Isolation (SSI)                                                                     261\nSummary                                                                                                                       266\n8. The Trouble with Distributed Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  273\nFaults and Partial Failures                                                                                          274\nCloud Computing and Supercomputing                                                              275\nUnreliable Networks                                                                                                   277\nNetwork Faults in Practice                                                                                     279\nDetecting Faults                                                                                                        280\nTimeouts and Unbounded Delays                                                                         281\nSynchronous Versus Asynchronous Networks                                                   284\nUnreliable Clocks                                                                                                        287\nMonotonic Versus Time-of-Day Clocks                                                              288\nClock Synchronization and Accuracy                                                                   289\nRelying on Synchronized Clocks                                                                           291\nProcess Pauses                                                                                                          295\nKnowledge, Truth, and Lies                                                                                       300\nThe Truth Is Defined by the Majority                                                                   300\nByzantine Faults                                                                                                       304\nSystem Model and Reality                                                                                       306\nSummary                                                                                                                       310\n9. Consistency and Consensus. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  321\nConsistency Guarantees                                                                                             322\nLinearizability                                                                                                               324\nWhat Makes a System Linearizable?                                                                     325\nRelying on Linearizability                                                                                       330\nImplementing Linearizable Systems                                                                     332\nThe Cost of Linearizability                                                                                     335\nOrdering Guarantees                                                                                                   339\nOrdering and Causality                                                                                           339\nSequence Number Ordering                                                                                  343\nx | Table of Contents", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33292fe8-84ee-4ad3-8b7e-3e2edba505ed": {"__data__": {"id_": "33292fe8-84ee-4ad3-8b7e-3e2edba505ed", "embedding": null, "metadata": {"page_label": "xi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96780b3c-3e8a-49ae-85b9-10e089161a90", "node_type": "4", "metadata": {"page_label": "xi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "117b1c32beb902c8c52f9214b8e149501385395770dc148619052cb0ab9ce525", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Total Order Broadcast                                                                                             348\nDistributed Transactions and Consensus                                                                352\nAtomic Commit and Two-Phase Commit (2PC)                                               354\nDistributed Transactions in Practice                                                                    360\nFault-Tolerant Consensus                                                                                      364\nMembership and Coordination Services                                                              370\nSummary                                                                                                                       373\nPart III. Derived Data\n10. Batch Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  389\nBatch Processing with Unix Tools                                                                            391\nSimple Log Analysis                                                                                                 391\nThe Unix Philosophy                                                                                               394\nMapReduce and Distributed Filesystems                                                                 397\nMapReduce Job Execution                                                                                     399\nReduce-Side Joins and Grouping                                                                          403\nMap-Side Joins                                                                                                         408\nThe Output of Batch Workflows                                                                           411\nComparing Hadoop to Distributed Databases                                                    414\nBeyond MapReduce                                                                                                    419\nMaterialization of Intermediate State                                                                   419\nGraphs and Iterative Processing                                                                            424\nHigh-Level APIs and Languages                                                                            426\nSummary                                                                                                                       429\n11. Stream Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  439\nTransmitting Event Streams                                                                                      440\nMessaging Systems                                                                                                   441\nPartitioned Logs                                                                                                       446\nDatabases and Streams                                                                                                451\nKeeping Systems in Sync                                                                                         452\nChange Data Capture                                                                                              454\nEvent Sourcing                                                                                                         457\nState, Streams, and Immutability                                                                           459\nProcessing Streams                                                                                                      464\nUses of Stream Processing                                                                                      465\nReasoning About Time                                                                                           468\nStream Joins                                                                                                              472\nFault Tolerance                                                                                                         476\nSummary                                                                                                                       479\nTable of Contents | xi", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "feb08288-990c-4c69-880e-27b35286a12e": {"__data__": {"id_": "feb08288-990c-4c69-880e-27b35286a12e", "embedding": null, "metadata": {"page_label": "xii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f70a001-7676-4bd5-9b3d-aeaa32f2cf4e", "node_type": "4", "metadata": {"page_label": "xii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "806926220877a1040e0aff53686de46bcc21cf862471bc9212d9c09ebeb55000", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12. The Future of Data Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  489\nData Integration                                                                                                           490\nCombining Specialized Tools by Deriving Data                                                 490\nBatch and Stream Processing                                                                                 494\nUnbundling Databases                                                                                                499\nComposing Data Storage Technologies                                                                499\nDesigning Applications Around Dataflow                                                           504\nObserving Derived State                                                                                         509\nAiming for Correctness                                                                                              515\nThe End-to-End Argument for Databases                                                           516\nEnforcing Constraints                                                                                             521\nTimeliness and Integrity                                                                                         524\nTrust, but Verify                                                                                                       528\nDoing the Right Thing                                                                                                533\nPredictive Analytics                                                                                                 533\nPrivacy and Tracking                                                                                               536\nSummary                                                                                                                       543\nGlossary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  553\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  559\nxii | Table of Contents", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c70be966-a9dc-4884-b846-69f450cc34c3": {"__data__": {"id_": "c70be966-a9dc-4884-b846-69f450cc34c3", "embedding": null, "metadata": {"page_label": "xiii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9ccaf6f-00ac-4d62-aad7-7f976149b58f", "node_type": "4", "metadata": {"page_label": "xiii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e6414793f73a9eec88ac93fcce7188c2f0d12a55ac807fb14ffc6a5951018ec6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Preface\nIf you have worked in software engineering in recent years, especially in server-side\nand backend systems, you have probably been bombarded with a plethora of buzz\u2010\nwords relating to storage and processing of data. NoSQL! Big Data! Web-scale!\nSharding! Eventual consistency! ACID! CAP theorem! Cloud services! MapReduce!\nReal-time!\nIn the last decade we have seen many interesting developments in databases, in dis\u2010\ntributed systems, and in the ways we build applications on top of them. There are\nvarious driving forces for these developments:\n\u2022 Internet companies such as Google, Yahoo!, Amazon, Facebook, LinkedIn,\nMicrosoft, and Twitter are handling huge volumes of data and traffic, forcing\nthem to create new tools that enable them to efficiently handle such scale.\n\u2022 Businesses need to be agile, test hypotheses cheaply, and respond quickly to new\nmarket insights by keeping development cycles short and data models flexible.\n\u2022 Free and open source software has become very successful and is now preferred\nto commercial or bespoke in-house software in many environments.\n\u2022 CPU clock speeds are barely increasing, but multi-core processors are standard,\nand networks are getting faster. This means parallelism is only going to increase.\n\u2022 Even if you work on a small team, you can now build systems that are distributed\nacross many machines and even multiple geographic regions, thanks to infra\u2010\nstructure as a service (IaaS) such as Amazon Web Services.\n\u2022 Many services are now expected to be highly available; extended downtime due\nto outages or maintenance is becoming increasingly unacceptable.\nData-intensive applications are pushing the boundaries of what is possible by making\nuse of these technological developments. We call an application data-intensive if data\nis its primary challenge\u2014the quantity of data, the complexity of data, or the speed at\nPreface | xiii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "771adf29-419d-466e-b73c-400a09313b67": {"__data__": {"id_": "771adf29-419d-466e-b73c-400a09313b67", "embedding": null, "metadata": {"page_label": "xiv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57b2c53d-1303-4874-a654-481e669c432c", "node_type": "4", "metadata": {"page_label": "xiv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "98d84111582c36e74ebb42b5b96a5b945c57e3d6f48b256d39303f491bbb202e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which it is changing\u2014as opposed to compute-intensive, where CPU cycles are the\nbottleneck.\nThe tools and technologies that help data-intensive applications store and process\ndata have been rapidly adapting to these changes. New types of database systems\n(\u201cNoSQL\u201d) have been getting lots of attention, but message queues, caches, search\nindexes, frameworks for batch and stream processing, and related technologies are\nvery important too. Many applications use some combination of these.\nThe buzzwords that fill this space are a sign of enthusiasm for the new possibilities,\nwhich is a great thing. However, as software engineers and architects, we also need to\nhave a technically accurate and precise understanding of the various technologies and\ntheir trade-offs if we want to build good applications. For that understanding, we\nhave to dig deeper than buzzwords.\nFortunately, behind the rapid changes in technology, there are enduring principles\nthat remain true, no matter which version of a particular tool you are using. If you\nunderstand those principles, you\u2019re in a position to see where each tool fits in, how to\nmake good use of it, and how to avoid its pitfalls. That\u2019s where this book comes in.\nThe goal of this book is to help you navigate the diverse and fast-changing landscape\nof technologies for processing and storing data. This book is not a tutorial for one\nparticular tool, nor is it a textbook full of dry theory. Instead, we will look at examples\nof successful data systems: technologies that form the foundation of many popular\napplications and that have to meet scalability, performance, and reliability require\u2010\nments in production every day.\nWe will dig into the internals of those systems, tease apart their key algorithms, dis\u2010\ncuss their principles and the trade-offs they have to make. On this journey, we will try\nto find useful ways of thinking about data systems\u2014not just how they work, but also\nwhy they work that way, and what questions we need to ask.\nAfter reading this book, you will be in a great position to decide which kind of tech\u2010\nnology is appropriate for which purpose, and understand how tools can be combined\nto form the foundation of a good application architecture. You won\u2019t be ready to\nbuild your own database storage engine from scratch, but fortunately that is rarely\nnecessary. You will, however, develop a good intuition for what your systems are\ndoing under the hood so that you can reason about their behavior, make good design\ndecisions, and track down any problems that may arise.\nWho Should Read This Book?\nIf you develop applications that have some kind of server/backend for storing or pro\u2010\ncessing data, and your applications use the internet (e.g., web applications, mobile\napps, or internet-connected sensors), then this book is for you.\nxiv | Preface", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec261c0b-7e21-4e43-ae4a-e8c393e6a089": {"__data__": {"id_": "ec261c0b-7e21-4e43-ae4a-e8c393e6a089", "embedding": null, "metadata": {"page_label": "xv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fd3a217-c3bc-47a7-a2cc-3660c00ad8dd", "node_type": "4", "metadata": {"page_label": "xv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3c53f3be36b5c779d02d642c150cb7c48bb92162fd383ea752fbc945efc4aa7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This book is for software engineers, software architects, and technical managers who\nlove to code. It is especially relevant if you need to make decisions about the architec\u2010\nture of the systems you work on\u2014for example, if you need to choose tools for solving\na given problem and figure out how best to apply them. But even if you have no\nchoice over your tools, this book will help you better understand their strengths and\nweaknesses.\nYou should have some experience building web-based applications or network serv\u2010\nices, and you should be familiar with relational databases and SQL. Any non-\nrelational databases and other data-related tools you know are a bonus, but not\nrequired. A general understanding of common network protocols like TCP and\nHTTP is helpful. Your choice of programming language or framework makes no dif\u2010\nference for this book.\nIf any of the following are true for you, you\u2019ll find this book valuable:\n\u2022 You want to learn how to make data systems scalable, for example, to support\nweb or mobile apps with millions of users.\n\u2022 You need to make applications highly available (minimizing downtime) and\noperationally robust.\n\u2022 You are looking for ways of making systems easier to maintain in the long run,\neven as they grow and as requirements and technologies change.\n\u2022 You have a natural curiosity for the way things work and want to know what\ngoes on inside major websites and online services. This book breaks down the\ninternals of various databases and data processing systems, and it\u2019s great fun to\nexplore the bright thinking that went into their design.\nSometimes, when discussing scalable data systems, people make comments along the\nlines of, \u201cYou\u2019re not Google or Amazon. Stop worrying about scale and just use a\nrelational database.\u201d There is truth in that statement: building for scale that you don\u2019t\nneed is wasted effort and may lock you into an inflexible design. In effect, it is a form\nof premature optimization. However, it\u2019s also important to choose the right tool for\nthe job, and different technologies each have their own strengths and weaknesses. As\nwe shall see, relational databases are important but not the final word on dealing with\ndata.\nScope of This Book\nThis book does not attempt to give detailed instructions on how to install or use spe\u2010\ncific software packages or APIs, since there is already plenty of documentation for\nthose things. Instead we discuss the various principles and trade-offs that are funda\u2010\nmental to data systems, and we explore the different design decisions taken by differ\u2010\nent products.\nPreface | xv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afb1c572-c830-49e6-953c-91b5838bb6ce": {"__data__": {"id_": "afb1c572-c830-49e6-953c-91b5838bb6ce", "embedding": null, "metadata": {"page_label": "xvi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e36b77e3-bf09-4bdd-9aed-9e9095f7e345", "node_type": "4", "metadata": {"page_label": "xvi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "78cb05b730e9f30d4736514243e486a4d498f124f7d92e5a53c98c03dd597a8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the ebook editions we have included links to the full text of online resources. All\nlinks were verified at the time of publication, but unfortunately links tend to break\nfrequently due to the nature of the web. If you come across a broken link, or if you\nare reading a print copy of this book, you can look up references using a search\nengine. For academic papers, you can search for the title in Google Scholar to find\nopen-access PDF files. Alternatively, you can find all of the references at https://\ngithub.com/ept/ddia-references, where we maintain up-to-date links.\nWe look primarily at the architecture of data systems and the ways they are integrated\ninto data-intensive applications. This book doesn\u2019t have space to cover deployment,\noperations, security, management, and other areas\u2014those are complex and impor\u2010\ntant topics, and we wouldn\u2019t do them justice by making them superficial side notes in\nthis book. They deserve books of their own.\nMany of the technologies described in this book fall within the realm of the Big Data\nbuzzword. However, the term \u201cBig Data\u201d is so overused and underdefined that it is\nnot useful in a serious engineering discussion. This book uses less ambiguous terms,\nsuch as single-node versus distributed systems, or online/interactive versus offline/\nbatch processing systems.\nThis book has a bias toward free and open source software (FOSS), because reading,\nmodifying, and executing source code is a great way to understand how something\nworks in detail. Open platforms also reduce the risk of vendor lock-in. However,\nwhere appropriate, we also discuss proprietary software (closed-source software, soft\u2010\nware as a service, or companies\u2019 in-house software that is only described in literature\nbut not released publicly).\nOutline of This Book\nThis book is arranged into three parts:\n1. In Part I , we discuss the fundamental ideas that underpin the design of data-\nintensive applications. We start in Chapter 1  by discussing what we\u2019re actually\ntrying to achieve: reliability, scalability, and maintainability; how we need to\nthink about them; and how we can achieve them. In Chapter 2 we compare sev\u2010\neral different data models and query languages, and see how they are appropriate\nto different situations. In Chapter 3 we talk about storage engines: how databases\narrange data on disk so that we can find it again efficiently. Chapter 4 turns to\nformats for data encoding (serialization) and evolution of schemas over time.\n2. In Part II, we move from data stored on one machine to data that is distributed\nacross multiple machines. This is often necessary for scalability, but brings with\nit a variety of unique challenges. We first discuss replication ( Chapter 5), parti\u2010\ntioning/sharding ( Chapter 6 ), and transactions ( Chapter 7 ). We then go into\nxvi | Preface", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da232409-ed36-4e1b-9e7a-ee174dab1226": {"__data__": {"id_": "da232409-ed36-4e1b-9e7a-ee174dab1226", "embedding": null, "metadata": {"page_label": "xvii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "949be309-81f7-4b2d-b8fb-b6e1c8243b8a", "node_type": "4", "metadata": {"page_label": "xvii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "11d0cbb69e34199832748273380a98cc3f2a68d361414f9fcdcca097c07c8718", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "more detail on the problems with distributed systems ( Chapter 8 ) and what it\nmeans to achieve consistency and consensus in a distributed system (Chapter 9).\n3. In Part III , we discuss systems that derive some datasets from other datasets.\nDerived data often occurs in heterogeneous systems: when there is no one data\u2010\nbase that can do everything well, applications need to integrate several different\ndatabases, caches, indexes, and so on. In Chapter 10 we start with a batch pro\u2010\ncessing approach to derived data, and we build upon it with stream processing in\nChapter 11 . Finally, in Chapter 12  we put everything together and discuss\napproaches for building reliable, scalable, and maintainable applications in the\nfuture.\nReferences and Further Reading\nMost of what we discuss in this book has already been said elsewhere in some form or\nanother\u2014in conference presentations, research papers, blog posts, code, bug trackers,\nmailing lists, and engineering folklore. This book summarizes the most important\nideas from many different sources, and it includes pointers to the original literature\nthroughout the text. The references at the end of each chapter are a great resource if\nyou want to explore an area in more depth, and most of them are freely available\nonline.\nO\u2019Reilly Safari\nSafari (formerly Safari Books Online) is a membership-based\ntraining and reference platform for enterprise, government,\neducators, and individuals.\nMembers have access to thousands of books, training videos, Learning Paths, interac\u2010\ntive tutorials, and curated playlists from over 250 publishers, including O\u2019Reilly\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Pro\u2010\nfessional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco\nPress, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt,\nAdobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett,\nand Course Technology, among others.\nFor more information, please visit http://oreilly.com/safari.\nPreface | xvii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecd6489a-f60b-4f7f-91c3-93c2c2fb22f4": {"__data__": {"id_": "ecd6489a-f60b-4f7f-91c3-93c2c2fb22f4", "embedding": null, "metadata": {"page_label": "xviii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "333b3eb1-c28f-40bd-92b5-e0e2fa35310d", "node_type": "4", "metadata": {"page_label": "xviii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f48314a45f3286f88444cf834a2cbcb96f0fa4afff420c331213de28a53a1a1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO\u2019Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at http://bit.ly/designing-data-intensive-apps.\nTo comment or ask technical questions about this book, send email to bookques\u2010\ntions@oreilly.com.\nFor more information about our books, courses, conferences, and news, see our web\u2010\nsite at http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nThis book is an amalgamation and systematization of a large number of other peo\u2010\nple\u2019s ideas and knowledge, combining experience from both academic research and\nindustrial practice. In computing we tend to be attracted to things that are new and\nshiny, but I think we have a huge amount to learn from things that have been done\nbefore. This book has over 800 references to articles, blog posts, talks, documenta\u2010\ntion, and more, and they have been an invaluable learning resource for me. I am very\ngrateful to the authors of this material for sharing their knowledge.\nI have also learned a lot from personal conversations, thanks to a large number of\npeople who have taken the time to discuss ideas or patiently explain things to me. In\nparticular, I would like to thank Joe Adler, Ross Anderson, Peter Bailis, M\u00e1rton\nBalassi, Alastair Beresford, Mark Callaghan, Mat Clayton, Patrick Collison, Sean\nCribbs, Shirshanka Das, Niklas Ekstr\u00f6m, Stephan Ewen, Alan Fekete, Gyula F\u00f3ra,\nCamille Fournier, Andres Freund, John Garbutt, Seth Gilbert, Tom Haggett, Pat Hel\u2010\nland, Joe Hellerstein, Jakob Homan, Heidi Howard, John Hugg, Julian Hyde, Conrad\nIrwin, Evan Jones, Flavio Junqueira, Jessica Kerr, Kyle Kingsbury, Jay Kreps, Carl\nLerche, Nicolas Liochon, Steve Loughran, Lee Mallabone, Nathan Marz, Caitie\nxviii | Preface", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f710380-b0c9-44d0-b96e-77bd42e5b0af": {"__data__": {"id_": "8f710380-b0c9-44d0-b96e-77bd42e5b0af", "embedding": null, "metadata": {"page_label": "xix", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dede16a6-53e1-44ac-8b35-395e6e4fbf07", "node_type": "4", "metadata": {"page_label": "xix", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ce0e8e9a7872379ca2a1f3e46e82019310a4706c53acb3d271124510eae7283f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "McCaffrey, Josie McLellan, Christopher Meiklejohn, Ian Meyers, Neha Narkhede,\nNeha Narula, Cathy O\u2019Neil, Onora O\u2019Neill, Ludovic Orban, Zoran Perkov, Julia\nPowles, Chris Riccomini, Henry Robinson, David Rosenthal, Jennifer Rullmann,\nMatthew Sackman, Martin Scholl, Amit Sela, Gwen Shapira, Greg Spurrier, Sam\nStokes, Ben Stopford, Tom Stuart, Diana Vasile, Rahul Vohra, Pete Warden, and\nBrett Wooldridge.\nSeveral more people have been invaluable to the writing of this book by reviewing\ndrafts and providing feedback. For these contributions I am particularly indebted to\nRaul Agepati, Tyler Akidau, Mattias Andersson, Sasha Baranov, Veena Basavaraj,\nDavid Beyer, Jim Brikman, Paul Carey, Raul Castro Fernandez, Joseph Chow, Derek\nElkins, Sam Elliott, Alexander Gallego, Mark Grover, Stu Halloway, Heidi Howard,\nNicola Kleppmann, Stefan Kruppa, Bjorn Madsen, Sander Mak, Stefan Podkowinski,\nPhil Potter, Hamid Ramazani, Sam Stokes, and Ben Summers. Of course, I take all\nresponsibility for any remaining errors or unpalatable opinions in this book.\nFor helping this book become real, and for their patience with my slow writing and\nunusual requests, I am grateful to my editors Marie Beaugureau, Mike Loukides, Ann\nSpencer, and all the team at O\u2019Reilly. For helping find the right words, I thank Rachel\nHead. For giving me the time and freedom to write in spite of other work commit\u2010\nments, I thank Alastair Beresford, Susan Goodhue, Neha Narkhede, and Kevin Scott.\nVery special thanks are due to Shabbir Diwan and Edie Freedman, who illustrated\nwith great care the maps that accompany the chapters. It\u2019s wonderful that they took\non the unconventional idea of creating maps, and made them so beautiful and com\u2010\npelling.\nFinally, my love goes to my family and friends, without whom I would not have been\nable to get through this writing process that has taken almost four years. You\u2019re the\nbest.\nPreface | xix", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1907, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "276e76e1-f1dd-45a0-bea9-caf7cc81111a": {"__data__": {"id_": "276e76e1-f1dd-45a0-bea9-caf7cc81111a", "embedding": null, "metadata": {"page_label": "xx", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c728a22-0319-46d7-a8ba-9fd84fe074df", "node_type": "4", "metadata": {"page_label": "xx", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecd76de6-5648-489f-a243-15c87e1cced5": {"__data__": {"id_": "ecd76de6-5648-489f-a243-15c87e1cced5", "embedding": null, "metadata": {"page_label": "1", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "175f7a36-82be-4a56-81c6-4679d8ad2aa4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ad98cd2273f467eb3bfa97e720bf8d07ca356025ff252aad5a3f4d2de9235d68", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PART I\nFoundations of Data Systems\nThe first four chapters go through the fundamental ideas that apply to all data sys\u2010\ntems, whether running on a single machine or distributed across a cluster of\nmachines:\n1. Chapter 1  introduces the terminology and approach that we\u2019re going to use\nthroughout this book. It examines what we actually mean by words like reliabil\u2010\nity, scalability, and maintainability, and how we can try to achieve these goals.\n2. Chapter 2  compares several different data models and query languages\u2014the\nmost visible distinguishing factor between databases from a developer\u2019s point of\nview. We will see how different models are appropriate to different situations.\n3. Chapter 3 turns to the internals of storage engines and looks at how databases lay\nout data on disk. Different storage engines are optimized for different workloads,\nand choosing the right one can have a huge effect on performance.\n4. Chapter 4 compares various formats for data encoding (serialization) and espe\u2010\ncially examines how they fare in an environment where application requirements\nchange and schemas need to adapt over time.\nLater, Part II will turn to the particular issues of distributed data systems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3574d821-e7e4-4d0e-9f7b-352aabc0a303": {"__data__": {"id_": "3574d821-e7e4-4d0e-9f7b-352aabc0a303", "embedding": null, "metadata": {"page_label": "2", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cefe1f2c-0a0c-4644-a493-f1785b91d7fa", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a005d3f4-4167-41ab-b374-ed60eda103cb": {"__data__": {"id_": "a005d3f4-4167-41ab-b374-ed60eda103cb", "embedding": null, "metadata": {"page_label": "3", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3cdb196-2131-48e5-a27a-a6244c693ba5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4abe886d1eb1794c6434ea5dbf91d79fa5530bfb4d80c9bd218f4b2588024ace", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 1\nReliable, Scalable, and\nMaintainable Applications\nThe Internet was done so well that most people think of it as a natural resource like the\nPacific Ocean, rather than something that was man-made. When was the last time a tech\u2010\nnology with a scale like that was so error-free?\n\u2014Alan Kay, in interview with Dr Dobb\u2019s Journal (2012)\nMany applications today are data-intensive, as opposed to compute-intensive. Raw\nCPU power is rarely a limiting factor for these applications\u2014bigger problems are\nusually the amount of data, the complexity of data, and the speed at which it is\nchanging.\nA data-intensive application is typically built from standard building blocks that pro\u2010\nvide commonly needed functionality. For example, many applications need to:\n\u2022 Store data so that they, or another application, can find it again later (databases)\n\u2022 Remember the result of an expensive operation, to speed up reads (caches)\n\u2022 Allow users to search data by keyword or filter it in various ways (search indexes)\n\u2022 Send a message to another process, to be handled asynchronously ( stream pro\u2010\ncessing)\n\u2022 Periodically crunch a large amount of accumulated data (batch processing)\nIf that sounds painfully obvious, that\u2019s just because these data systems are such a suc\u2010\ncessful abstraction: we use them all the time without thinking too much. When build\u2010\ning an application, most engineers wouldn\u2019t dream of writing a new data storage\nengine from scratch, because databases are a perfectly good tool for the job.\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94f1f223-a88a-4ad1-94bb-6700d9f4b46f": {"__data__": {"id_": "94f1f223-a88a-4ad1-94bb-6700d9f4b46f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "800c85e3-444a-4195-a9de-d5aabbe2cc68", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bd2f2b4222e44ec68dde2eb6fd90e53dc44fab1f95df0dc1587f6887f8f9b0c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But reality is not that simple. There are many database systems with different charac\u2010\nteristics, because different applications have different requirements. There are vari\u2010\nous approaches to caching, several ways of building search indexes, and so on. When\nbuilding an application, we still need to figure out which tools and which approaches\nare the most appropriate for the task at hand. And it can be hard to combine tools\nwhen you need to do something that a single tool cannot do alone.\nThis book is a journey through both the principles and the practicalities of data sys\u2010\ntems, and how you can use them to build data-intensive applications. We will explore\nwhat different tools have in common, what distinguishes them, and how they achieve\ntheir characteristics.\nIn this chapter, we will start by exploring the fundamentals of what we are trying to\nachieve: reliable, scalable, and maintainable data systems. We\u2019ll clarify what those\nthings mean, outline some ways of thinking about them, and go over the basics that\nwe will need for later chapters. In the following chapters we will continue layer by\nlayer, looking at different design decisions that need to be considered when working\non a data-intensive application.\nThinking About Data Systems\nWe typically think of databases, queues, caches, etc. as being very different categories\nof tools. Although a database and a message queue have some superficial similarity\u2014\nboth store data for some time\u2014they have very different access patterns, which means\ndifferent performance characteristics, and thus very different implementations.\nSo why should we lump them all together under an umbrella term like data systems?\nMany new tools for data storage and processing have emerged in recent years. They\nare optimized for a variety of different use cases, and they no longer neatly fit into\ntraditional categories [1]. For example, there are datastores that are also used as mes\u2010\nsage queues (Redis), and there are message queues with database-like durability guar\u2010\nantees (Apache Kafka). The boundaries between the categories are becoming blurred.\nSecondly, increasingly many applications now have such demanding or wide-ranging\nrequirements that a single tool can no longer meet all of its data processing and stor\u2010\nage needs. Instead, the work is broken down into tasks that can be performed effi\u2010\nciently on a single tool, and those different tools are stitched together using\napplication code.\nFor example, if you have an application-managed caching layer (using Memcached\nor similar), or a full-text search server (such as Elasticsearch or Solr) separate from\nyour main database, it is normally the application code\u2019s responsibility to keep those\ncaches and indexes in sync with the main database. Figure 1-1 gives a glimpse of what\nthis may look like (we will go into detail in later chapters).\n4 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20fee458-a49a-4c7e-ba21-0e5ea8c8609e": {"__data__": {"id_": "20fee458-a49a-4c7e-ba21-0e5ea8c8609e", "embedding": null, "metadata": {"page_label": "5", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c065824-2202-4ddc-b76e-8b087036ad99", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "35ffc3f0bb339f2737f39746bf30144dbc9db89f2cddb421bce5a006b43a91c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1-1. One possible architecture for a data system that combines several\ncomponents.\nWhen you combine several tools in order to provide a service, the service\u2019s interface\nor application programming interface (API) usually hides those implementation\ndetails from clients. Now you have essentially created a new, special-purpose data\nsystem from smaller, general-purpose components. Your composite data system may\nprovide certain guarantees: e.g., that the cache will be correctly invalidated or upda\u2010\nted on writes so that outside clients see consistent results. You are now not only an\napplication developer, but also a data system designer.\nIf you are designing a data system or service, a lot of tricky questions arise. How do\nyou ensure that the data remains correct and complete, even when things go wrong\ninternally? How do you provide consistently good performance to clients, even when\nparts of your system are degraded? How do you scale to handle an increase in load?\nWhat does a good API for the service look like?\nThere are many factors that may influence the design of a data system, including the\nskills and experience of the people involved, legacy system dependencies, the time\u2010\nscale for delivery, your organization\u2019s tolerance of different kinds of risk, regulatory\nconstraints, etc. Those factors depend very much on the situation.\nThinking About Data Systems | 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1afc7f8-e083-414b-9da6-a028894cf4fc": {"__data__": {"id_": "a1afc7f8-e083-414b-9da6-a028894cf4fc", "embedding": null, "metadata": {"page_label": "6", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e90b83f0-de99-49a7-b817-3e17ed0ea68f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8be4f7739d2cde6ffb3baeb3de144cf27b9bd47822da9622cdf0f7ee9e086977", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this book, we focus on three concerns that are important in most software systems:\nReliability\nThe system should continue to work correctly (performing the correct function at\nthe desired level of performance) even in the face of adversity (hardware or soft\u2010\nware faults, and even human error). See \u201cReliability\u201d on page 6.\nScalability\nAs the system grows (in data volume, traffic volume, or complexity), there should\nbe reasonable ways of dealing with that growth. See \u201cScalability\u201d on page 10.\nMaintainability\nOver time, many different people will work on the system (engineering and oper\u2010\nations, both maintaining current behavior and adapting the system to new use\ncases), and they should all be able to work on it productively. See \u201cMaintainabil\u2010\nity\u201d on page 18.\nThese words are often cast around without a clear understanding of what they mean.\nIn the interest of thoughtful engineering, we will spend the rest of this chapter\nexploring ways of thinking about reliability, scalability, and maintainability. Then, in\nthe following chapters, we will look at various techniques, architectures, and algo\u2010\nrithms that are used in order to achieve those goals.\nReliability\nEverybody has an intuitive idea of what it means for something to be reliable or unre\u2010\nliable. For software, typical expectations include:\n\u2022 The application performs the function that the user expected.\n\u2022 It can tolerate the user making mistakes or using the software in unexpected\nways.\n\u2022 Its performance is good enough for the required use case, under the expected\nload and data volume.\n\u2022 The system prevents any unauthorized access and abuse.\nIf all those things together mean \u201cworking correctly,\u201d then we can understand relia\u2010\nbility as meaning, roughly, \u201ccontinuing to work correctly, even when things go\nwrong.\u201d\nThe things that can go wrong are called faults, and systems that anticipate faults and\ncan cope with them are called fault-tolerant or resilient. The former term is slightly\nmisleading: it suggests that we could make a system tolerant of every possible kind of\nfault, which in reality is not feasible. If the entire planet Earth (and all servers on it)\nwere swallowed by a black hole, tolerance of that fault would require web hosting in\n6 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d6071a6-9c88-40da-89ed-b72ca452bc97": {"__data__": {"id_": "8d6071a6-9c88-40da-89ed-b72ca452bc97", "embedding": null, "metadata": {"page_label": "7", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d5e453c-0641-47c2-aaf0-7d96236a2634", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cb69c993fb13a11a4d49deaa817c66a49a7113125d85bcd15f79c17f3dde8a89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "space\u2014good luck getting that budget item approved. So it only makes sense to talk\nabout tolerating certain types of faults.\nNote that a fault is not the same as a failure [ 2]. A fault is usually defined as one com\u2010\nponent of the system deviating from its spec, whereas a failure is when the system as a\nwhole stops providing the required service to the user. It is impossible to reduce the\nprobability of a fault to zero; therefore it is usually best to design fault-tolerance\nmechanisms that prevent faults from causing failures. In this book we cover several\ntechniques for building reliable systems from unreliable parts.\nCounterintuitively, in such fault-tolerant systems, it can make sense to increase the\nrate of faults by triggering them deliberately\u2014for example, by randomly killing indi\u2010\nvidual processes without warning. Many critical bugs are actually due to poor error\nhandling [ 3]; by deliberately inducing faults, you ensure that the fault-tolerance\nmachinery is continually exercised and tested, which can increase your confidence\nthat faults will be handled correctly when they occur naturally. The Netflix Chaos\nMonkey [4] is an example of this approach.\nAlthough we generally prefer tolerating faults over preventing faults, there are cases\nwhere prevention is better than cure (e.g., because no cure exists). This is the case\nwith security matters, for example: if an attacker has compromised a system and\ngained access to sensitive data, that event cannot be undone. However, this book\nmostly deals with the kinds of faults that can be cured, as described in the following\nsections.\nHardware Faults\nWhen we think of causes of system failure, hardware faults quickly come to mind.\nHard disks crash, RAM becomes faulty, the power grid has a blackout, someone\nunplugs the wrong network cable. Anyone who has worked with large datacenters\ncan tell you that these things happen all the time when you have a lot of machines.\nHard disks are reported as having a mean time to failure (MTTF) of about 10 to 50\nyears [5, 6]. Thus, on a storage cluster with 10,000 disks, we should expect on average\none disk to die per day.\nOur first response is usually to add redundancy to the individual hardware compo\u2010\nnents in order to reduce the failure rate of the system. Disks may be set up in a RAID\nconfiguration, servers may have dual power supplies and hot-swappable CPUs, and\ndatacenters may have batteries and diesel generators for backup power. When one\ncomponent dies, the redundant component can take its place while the broken com\u2010\nponent is replaced. This approach cannot completely prevent hardware problems\nfrom causing failures, but it is well understood and can often keep a machine running\nuninterrupted for years.\nReliability | 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2750, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5c9bc4b4-f952-457c-9fc0-74541858758d": {"__data__": {"id_": "5c9bc4b4-f952-457c-9fc0-74541858758d", "embedding": null, "metadata": {"page_label": "8", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9975b226-070a-4ce9-8252-29c09b570218", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e90ad1698417692e8509a2d3b63151de09afe487e53c19d2e6b8cd4b5c88af95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. Defined in \u201cApproaches for Coping with Load\u201d on page 17.\nUntil recently, redundancy of hardware components was sufficient for most applica\u2010\ntions, since it makes total failure of a single machine fairly rare. As long as you can\nrestore a backup onto a new machine fairly quickly, the downtime in case of failure is\nnot catastrophic in most applications. Thus, multi-machine redundancy was only\nrequired by a small number of applications for which high availability was absolutely\nessential.\nHowever, as data volumes and applications\u2019 computing demands have increased,\nmore applications have begun using larger numbers of machines, which proportion\u2010\nally increases the rate of hardware faults. Moreover, in some cloud platforms such as\nAmazon Web Services (AWS) it is fairly common for virtual machine instances to\nbecome unavailable without warning [ 7], as the platforms are designed to prioritize\nflexibility and elasticityi over single-machine reliability.\nHence there is a move toward systems that can tolerate the loss of entire machines, by\nusing software fault-tolerance techniques in preference or in addition to hardware\nredundancy. Such systems also have operational advantages: a single-server system\nrequires planned downtime if you need to reboot the machine (to apply operating\nsystem security patches, for example), whereas a system that can tolerate machine\nfailure can be patched one node at a time, without downtime of the entire system (a\nrolling upgrade; see Chapter 4).\nSoftware Errors\nWe usually think of hardware faults as being random and independent from each\nother: one machine\u2019s disk failing does not imply that another machine\u2019s disk is going\nto fail. There may be weak correlations (for example due to a common cause, such as\nthe temperature in the server rack), but otherwise it is unlikely that a large number of\nhardware components will fail at the same time.\nAnother class of fault is a systematic error within the system [ 8]. Such faults are\nharder to anticipate, and because they are correlated across nodes, they tend to cause\nmany more system failures than uncorrelated hardware faults [5]. Examples include:\n\u2022 A software bug that causes every instance of an application server to crash when\ngiven a particular bad input. For example, consider the leap second on June 30,\n2012, that caused many applications to hang simultaneously due to a bug in the\nLinux kernel [9].\n\u2022 A runaway process that uses up some shared resource\u2014CPU time, memory, disk\nspace, or network bandwidth.\n8 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2580, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12e1958a-06c6-4067-8be1-9a189801485e": {"__data__": {"id_": "12e1958a-06c6-4067-8be1-9a189801485e", "embedding": null, "metadata": {"page_label": "9", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ae163d3-8fb6-45ff-9723-3aead34f0341", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "594fa6fca9f26b0108d846866e605c3b223823f15cf966f35270fd0c0ee0bae5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 A service that the system depends on that slows down, becomes unresponsive, or\nstarts returning corrupted responses.\n\u2022 Cascading failures, where a small fault in one component triggers a fault in\nanother component, which in turn triggers further faults [10].\nThe bugs that cause these kinds of software faults often lie dormant for a long time\nuntil they are triggered by an unusual set of circumstances. In those circumstances, it\nis revealed that the software is making some kind of assumption about its environ\u2010\nment\u2014and while that assumption is usually true, it eventually stops being true for\nsome reason [11].\nThere is no quick solution to the problem of systematic faults in software. Lots of\nsmall things can help: carefully thinking about assumptions and interactions in the\nsystem; thorough testing; process isolation; allowing processes to crash and restart;\nmeasuring, monitoring, and analyzing system behavior in production. If a system is\nexpected to provide some guarantee (for example, in a message queue, that the num\u2010\nber of incoming messages equals the number of outgoing messages), it can constantly\ncheck itself while it is running and raise an alert if a discrepancy is found [12].\nHuman Errors\nHumans design and build software systems, and the operators who keep the systems\nrunning are also human. Even when they have the best intentions, humans are\nknown to be unreliable. For example, one study of large internet services found that\nconfiguration errors by operators were the leading cause of outages, whereas hard\u2010\nware faults (servers or network) played a role in only 10\u201325% of outages [13].\nHow do we make our systems reliable, in spite of unreliable humans? The best sys\u2010\ntems combine several approaches:\n\u2022 Design systems in a way that minimizes opportunities for error. For example,\nwell-designed abstractions, APIs, and admin interfaces make it easy to do \u201cthe\nright thing\u201d and discourage \u201cthe wrong thing.\u201d However, if the interfaces are too\nrestrictive people will work around them, negating their benefit, so this is a tricky\nbalance to get right.\n\u2022 Decouple the places where people make the most mistakes from the places where\nthey can cause failures. In particular, provide fully featured non-production\nsandbox environments where people can explore and experiment safely, using\nreal data, without affecting real users.\n\u2022 Test thoroughly at all levels, from unit tests to whole-system integration tests and\nmanual tests [ 3]. Automated testing is widely used, well understood, and espe\u2010\ncially valuable for covering corner cases that rarely arise in normal operation.\nReliability | 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c40765cc-eb46-4419-b9d0-9d55e71571d2": {"__data__": {"id_": "c40765cc-eb46-4419-b9d0-9d55e71571d2", "embedding": null, "metadata": {"page_label": "10", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f618ba9-af3d-40ac-9cf7-a150ad794244", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0a1f83b80bfc73ad4e0ac9660ddd820e4b02de602f807a2ef3cf8433ce1b08e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Allow quick and easy recovery from human errors, to minimize the impact in the\ncase of a failure. For example, make it fast to roll back configuration changes, roll\nout new code gradually (so that any unexpected bugs affect only a small subset of\nusers), and provide tools to recompute data (in case it turns out that the old com\u2010\nputation was incorrect).\n\u2022 Set up detailed and clear monitoring, such as performance metrics and error\nrates. In other engineering disciplines this is referred to as telemetry. (Once a\nrocket has left the ground, telemetry is essential for tracking what is happening,\nand for understanding failures [ 14].) Monitoring can show us early warning sig\u2010\nnals and allow us to check whether any assumptions or constraints are being vio\u2010\nlated. When a problem occurs, metrics can be invaluable in diagnosing the issue.\n\u2022 Implement good management practices and training\u2014a complex and important\naspect, and beyond the scope of this book.\nHow Important Is Reliability?\nReliability is not just for nuclear power stations and air traffic control software\u2014\nmore mundane applications are also expected to work reliably. Bugs in business\napplications cause lost productivity (and legal risks if figures are reported incor\u2010\nrectly), and outages of ecommerce sites can have huge costs in terms of lost revenue\nand damage to reputation.\nEven in \u201cnoncritical\u201d applications we have a responsibility to our users. Consider a\nparent who stores all their pictures and videos of their children in your photo appli\u2010\ncation [ 15]. How would they feel if that database was suddenly corrupted? Would\nthey know how to restore it from a backup?\nThere are situations in which we may choose to sacrifice reliability in order to reduce\ndevelopment cost (e.g., when developing a prototype product for an unproven mar\u2010\nket) or operational cost (e.g., for a service with a very narrow profit margin)\u2014but we\nshould be very conscious of when we are cutting corners. \nScalability\nEven if a system is working reliably today, that doesn\u2019t mean it will necessarily work\nreliably in the future. One common reason for degradation is increased load: perhaps\nthe system has grown from 10,000 concurrent users to 100,000 concurrent users, or\nfrom 1 million to 10 million. Perhaps it is processing much larger volumes of data\nthan it did before.\nScalability is the term we use to describe a system\u2019s ability to cope with increased\nload. Note, however, that it is not a one-dimensional label that we can attach to a sys\u2010\ntem: it is meaningless to say \u201cX is scalable\u201d or \u201cY doesn\u2019t scale.\u201d Rather, discussing\n10 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2657, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a04a40f0-fca5-43da-9033-4ee60387574f": {"__data__": {"id_": "a04a40f0-fca5-43da-9033-4ee60387574f", "embedding": null, "metadata": {"page_label": "11", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84d26b-4153-49da-a775-5697c59a786e", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "457bafdcd01859dcda04556965a65a58c1a07bf6a0abec91032b876dcdb82a60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. A term borrowed from electronic engineering, where it describes the number of logic gate inputs that are\nattached to another gate\u2019s output. The output needs to supply enough current to drive all the attached inputs.\nIn transaction processing systems, we use it to describe the number of requests to other services that we need\nto make in order to serve one incoming request.\nscalability means considering questions like \u201cIf the system grows in a particular way,\nwhat are our options for coping with the growth?\u201d and \u201cHow can we add computing\nresources to handle the additional load?\u201d\nDescribing Load\nFirst, we need to succinctly describe the current load on the system; only then can we\ndiscuss growth questions (what happens if our load doubles?). Load can be described\nwith a few numbers which we call load parameters . The best choice of parameters\ndepends on the architecture of your system: it may be requests per second to a web\nserver, the ratio of reads to writes in a database, the number of simultaneously active\nusers in a chat room, the hit rate on a cache, or something else. Perhaps the average\ncase is what matters for you, or perhaps your bottleneck is dominated by a small\nnumber of extreme cases.\nTo make this idea more concrete, let\u2019s consider Twitter as an example, using data\npublished in November 2012 [16]. Two of Twitter\u2019s main operations are:\nPost tweet\nA user can publish a new message to their followers (4.6k requests/sec on aver\u2010\nage, over 12k requests/sec at peak).\nHome timeline\nA user can view tweets posted by the people they follow (300k requests/sec).\nSimply handling 12,000 writes per second (the peak rate for posting tweets) would be\nfairly easy. However, Twitter\u2019s scaling challenge is not primarily due to tweet volume,\nbut due to fan-outii\u2014each user follows many people, and each user is followed by\nmany people. There are broadly two ways of implementing these two operations:\n1. Posting a tweet simply inserts the new tweet into a global collection of tweets.\nWhen a user requests their home timeline, look up all the people they follow,\nfind all the tweets for each of those users, and merge them (sorted by time). In a\nrelational database like in Figure 1-2, you could write a query such as:\nSELECT tweets.*, users.* FROM tweets\n  JOIN users   ON tweets.sender_id    = users.id\n  JOIN follows ON follows.followee_id = users.id\n  WHERE follows.follower_id = current_user\nScalability | 11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb32bdbd-f078-418e-a3b2-aa4e86a9b38d": {"__data__": {"id_": "cb32bdbd-f078-418e-a3b2-aa4e86a9b38d", "embedding": null, "metadata": {"page_label": "12", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cde927c-fa2c-4f44-bbb3-117e7192373d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ab0105b8c978b930c6f48f181ee619414ac0462b9ca9168520c938549922bd5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. Maintain a cache for each user\u2019s home timeline\u2014like a mailbox of tweets for\neach recipient user (see Figure 1-3). When a user posts a tweet , look up all the\npeople who follow that user, and insert the new tweet into each of their home\ntimeline caches. The request to read the home timeline is then cheap, because its\nresult has been computed ahead of time.\nFigure 1-2. Simple relational schema for implementing a Twitter home timeline.\nFigure 1-3. Twitter\u2019s data pipeline for delivering tweets to followers, with load parame\u2010\nters as of November 2012 [16].\nThe first version of Twitter used approach 1, but the systems struggled to keep up\nwith the load of home timeline queries, so the company switched to approach 2. This\nworks better because the average rate of published tweets is almost two orders of\nmagnitude lower than the rate of home timeline reads, and so in this case it\u2019s prefera\u2010\nble to do more work at write time and less at read time.\nHowever, the downside of approach 2 is that posting a tweet now requires a lot of\nextra work. On average, a tweet is delivered to about 75 followers, so 4.6k tweets per\nsecond become 345k writes per second to the home timeline caches. But this average\nhides the fact that the number of followers per user varies wildly, and some users\n12 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1355, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49c4f477-90d5-4d38-a73e-1c7874cc47f3": {"__data__": {"id_": "49c4f477-90d5-4d38-a73e-1c7874cc47f3", "embedding": null, "metadata": {"page_label": "13", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ddd9787-8809-4b44-83d8-73f4e4c605bb", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6c3cde551b5ad8cb822f2514b3e571277431e5d05664db08ec5c2dfd9f938d1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. In an ideal world, the running time of a batch job is the size of the dataset divided by the throughput. In\npractice, the running time is often longer, due to skew (data not being spread evenly across worker processes)\nand needing to wait for the slowest task to complete.\nhave over 30 million followers. This means that a single tweet may result in over 30\nmillion writes to home timelines! Doing this in a timely manner\u2014Twitter tries to\ndeliver tweets to followers within five seconds\u2014is a significant challenge.\nIn the example of Twitter, the distribution of followers per user (maybe weighted by\nhow often those users tweet) is a key load parameter for discussing scalability, since it\ndetermines the fan-out load. Your application may have very different characteristics,\nbut you can apply similar principles to reasoning about its load.\nThe final twist of the Twitter anecdote: now that approach 2 is robustly implemented,\nTwitter is moving to a hybrid of both approaches. Most users\u2019 tweets continue to be\nfanned out to home timelines at the time when they are posted, but a small number\nof users with a very large number of followers (i.e., celebrities) are excepted from this\nfan-out. Tweets from any celebrities that a user may follow are fetched separately and\nmerged with that user\u2019s home timeline when it is read, like in approach 1. This hybrid\napproach is able to deliver consistently good performance. We will revisit this exam\u2010\nple in Chapter 12 after we have covered some more technical ground.\nDescribing Performance\nOnce you have described the load on your system, you can investigate what happens\nwhen the load increases. You can look at it in two ways:\n\u2022 When you increase a load parameter and keep the system resources (CPU, mem\u2010\nory, network bandwidth, etc.) unchanged, how is the performance of your system\naffected?\n\u2022 When you increase a load parameter, how much do you need to increase the\nresources if you want to keep performance unchanged?\nBoth questions require performance numbers, so let\u2019s look briefly at describing the\nperformance of a system.\nIn a batch processing system such as Hadoop, we usually care about throughput\u2014the\nnumber of records we can process per second, or the total time it takes to run a job\non a dataset of a certain size.iii In online systems, what\u2019s usually more important is the\nservice\u2019s response time \u2014that is, the time between a client sending a request and\nreceiving a response.\nScalability | 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d13c574-23b7-4096-a223-9732f4846259": {"__data__": {"id_": "6d13c574-23b7-4096-a223-9732f4846259", "embedding": null, "metadata": {"page_label": "14", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82681f19-cfd0-4373-8c02-5a140c7d4416", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "67c67f9cd6df7169821f641f3b3b0a5e260fc3955ee4d27ed16506095c5ef197", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Latency and response time\nLatency and response time are often used synonymously, but they\nare not the same. The response time is what the client sees: besides\nthe actual time to process the request (the service time), it includes\nnetwork delays and queueing delays. Latency is the duration that a\nrequest is waiting to be handled\u2014during which it is latent, await\u2010\ning service [17].\nEven if you only make the same request over and over again, you\u2019ll get a slightly dif\u2010\nferent response time on every try. In practice, in a system handling a variety of\nrequests, the response time can vary a lot. We therefore need to think of response\ntime not as a single number, but as a distribution of values that you can measure.\nIn Figure 1-4 , each gray bar represents a request to a service, and its height shows\nhow long that request took. Most requests are reasonably fast, but there are occa\u2010\nsional outliers that take much longer. Perhaps the slow requests are intrinsically more\nexpensive, e.g., because they process more data. But even in a scenario where you\u2019d\nthink all requests should take the same time, you get variation: random additional\nlatency could be introduced by a context switch to a background process, the loss of a\nnetwork packet and TCP retransmission, a garbage collection pause, a page fault\nforcing a read from disk, mechanical vibrations in the server rack [18], or many other\ncauses.\nFigure 1-4. Illustrating mean and percentiles: response times for a sample of 100\nrequests to a service.\nIt\u2019s common to see the average response time of a service reported. (Strictly speaking,\nthe term \u201caverage\u201d doesn\u2019t refer to any particular formula, but in practice it is usually\nunderstood as the arithmetic mean: given n values, add up all the values, and divide\nby n.) However, the mean is not a very good metric if you want to know your \u201ctypi\u2010\ncal\u201d response time, because it doesn\u2019t tell you how many users actually experienced\nthat delay.\nUsually it is better to use percentiles. If you take your list of response times and sort it\nfrom fastest to slowest, then the median is the halfway point: for example, if your\n14 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df49c6f6-dd50-4d70-9128-2a075a8de127": {"__data__": {"id_": "df49c6f6-dd50-4d70-9128-2a075a8de127", "embedding": null, "metadata": {"page_label": "15", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33c010c0-cde3-4d06-8dae-aa630c182a01", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e6c6d4a0fc55f8594aed6a5e038892743d59516efec240c2b345ed2705a6cbc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "median response time is 200 ms, that means half your requests return in less than\n200 ms, and half your requests take longer than that.\nThis makes the median a good metric if you want to know how long users typically\nhave to wait: half of user requests are served in less than the median response time,\nand the other half take longer than the median. The median is also known as the 50th\npercentile, and sometimes abbreviated as p50. Note that the median refers to a single\nrequest; if the user makes several requests (over the course of a session, or because\nseveral resources are included in a single page), the probability that at least one of\nthem is slower than the median is much greater than 50%.\nIn order to figure out how bad your outliers are, you can look at higher percentiles:\nthe 95th, 99th, and 99.9th percentiles are common (abbreviated p95, p99, and p999).\nThey are the response time thresholds at which 95%, 99%, or 99.9% of requests are\nfaster than that particular threshold. For example, if the 95th percentile response time\nis 1.5 seconds, that means 95 out of 100 requests take less than 1.5 seconds, and 5 out\nof 100 requests take 1.5 seconds or more. This is illustrated in Figure 1-4.\nHigh percentiles of response times, also known as tail latencies , are important\nbecause they directly affect users\u2019 experience of the service. For example, Amazon\ndescribes response time requirements for internal services in terms of the 99.9th per\u2010\ncentile, even though it only affects 1 in 1,000 requests. This is because the customers\nwith the slowest requests are often those who have the most data on their accounts\nbecause they have made many purchases\u2014that is, they\u2019re the most valuable custom\u2010\ners [19]. It\u2019s important to keep those customers happy by ensuring the website is fast\nfor them: Amazon has also observed that a 100 ms increase in response time reduces\nsales by 1% [20], and others report that a 1-second slowdown reduces a customer sat\u2010\nisfaction metric by 16% [21, 22].\nOn the other hand, optimizing the 99.99th percentile (the slowest 1 in 10,000\nrequests) was deemed too expensive and to not yield enough benefit for Amazon\u2019s\npurposes. Reducing response times at very high percentiles is difficult because they\nare easily affected by random events outside of your control, and the benefits are\ndiminishing.\nFor example, percentiles are often used in service level objectives  (SLOs) and service\nlevel agreements (SLAs), contracts that define the expected performance and availa\u2010\nbility of a service. An SLA may state that the service is considered to be up if it has a\nmedian response time of less than 200 ms and a 99th percentile under 1 s (if the\nresponse time is longer, it might as well be down), and the service may be required to\nbe up at least 99.9% of the time. These metrics set expectations for clients of the ser\u2010\nvice and allow customers to demand a refund if the SLA is not met.\nQueueing delays often account for a large part of the response time at high percen\u2010\ntiles. As a server can only process a small number of things in parallel (limited, for\nScalability | 15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9ff7186-5177-4874-a324-5b1949726998": {"__data__": {"id_": "c9ff7186-5177-4874-a324-5b1949726998", "embedding": null, "metadata": {"page_label": "16", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebca9fb4-f907-467b-a919-92a54c84d9ae", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6c9c9c5c46b6c8de0472ed0ec70d5bf4e48a352de10d346ba57c0f88b50aff75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "example, by its number of CPU cores), it only takes a small number of slow requests\nto hold up the processing of subsequent requests\u2014an effect sometimes known as\nhead-of-line blocking . Even if those subsequent requests are fast to process on the\nserver, the client will see a slow overall response time due to the time waiting for the\nprior request to complete. Due to this effect, it is important to measure response\ntimes on the client side.\nWhen generating load artificially in order to test the scalability of a system, the load-\ngenerating client needs to keep sending requests independently of the response time.\nIf the client waits for the previous request to complete before sending the next one,\nthat behavior has the effect of artificially keeping the queues shorter in the test than\nthey would be in reality, which skews the measurements [23].\nPercentiles in Practice\nHigh percentiles become especially important in backend services that are called mul\u2010\ntiple times as part of serving a single end-user request. Even if you make the calls in\nparallel, the end-user request still needs to wait for the slowest of the parallel calls to\ncomplete. It takes just one slow call to make the entire end-user request slow, as illus\u2010\ntrated in Figure 1-5 . Even if only a small percentage of backend calls are slow, the\nchance of getting a slow call increases if an end-user request requires multiple back\u2010\nend calls, and so a higher proportion of end-user requests end up being slow (an\neffect known as tail latency amplification [24]).\nIf you want to add response time percentiles to the monitoring dashboards for your\nservices, you need to efficiently calculate them on an ongoing basis. For example, you\nmay want to keep a rolling window of response times of requests in the last 10\nminutes. Every minute, you calculate the median and various percentiles over the val\u2010\nues in that window and plot those metrics on a graph.\nThe na\u00efve implementation is to keep a list of response times for all requests within the\ntime window and to sort that list every minute. If that is too inefficient for you, there\nare algorithms that can calculate a good approximation of percentiles at minimal\nCPU and memory cost, such as forward decay [ 25], t-digest [ 26], or HdrHistogram\n[27]. Beware that averaging percentiles, e.g., to reduce the time resolution or to com\u2010\nbine data from several machines, is mathematically meaningless\u2014the right way of\naggregating response time data is to add the histograms [28].\n16 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbcd03c3-2428-48a8-a09e-2d45e3f8c6d1": {"__data__": {"id_": "fbcd03c3-2428-48a8-a09e-2d45e3f8c6d1", "embedding": null, "metadata": {"page_label": "17", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "026a0aff-325d-4816-a4c7-ac3210891bba", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0a508a09bab60e964bcbab4235ae5c12b3f24c0b399b29c07f09c32e102913e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1-5. When several backend calls are needed to serve a request, it takes just a sin\u2010\ngle slow backend request to slow down the entire end-user request.\nApproaches for Coping with Load\nNow that we have discussed the parameters for describing load and metrics for meas\u2010\nuring performance, we can start discussing scalability in earnest: how do we maintain\ngood performance even when our load parameters increase by some amount?\nAn architecture that is appropriate for one level of load is unlikely to cope with 10\ntimes that load. If you are working on a fast-growing service, it is therefore likely that\nyou will need to rethink your architecture on every order of magnitude load increase\n\u2014or perhaps even more often than that.\nPeople often talk of a dichotomy between scaling up  (vertical scaling , moving to a\nmore powerful machine) and scaling out  (horizontal scaling , distributing the load\nacross multiple smaller machines). Distributing load across multiple machines is also\nknown as a shared-nothing architecture. A system that can run on a single machine is\noften simpler, but high-end machines can become very expensive, so very intensive\nworkloads often can\u2019t avoid scaling out. In reality, good architectures usually involve\na pragmatic mixture of approaches: for example, using several fairly powerful\nmachines can still be simpler and cheaper than a large number of small virtual\nmachines.\nSome systems are elastic, meaning that they can automatically add computing resour\u2010\nces when they detect a load increase, whereas other systems are scaled manually (a\nhuman analyzes the capacity and decides to add more machines to the system). An\nelastic system can be useful if load is highly unpredictable, but manually scaled sys\u2010\ntems are simpler and may have fewer operational surprises (see \u201cRebalancing Parti\u2010\ntions\u201d on page 209).\nScalability | 17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71416b74-ecd1-4437-b035-685abbf85a6f": {"__data__": {"id_": "71416b74-ecd1-4437-b035-685abbf85a6f", "embedding": null, "metadata": {"page_label": "18", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65c8d38a-117b-4a97-b221-12ec138c1fe0", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "be8500ddca9df77069120374a61ed0ecc9f255efd1d00adc31afcfb826830b19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While distributing stateless services across multiple machines is fairly straightfor\u2010\nward, taking stateful data systems from a single node to a distributed setup can intro\u2010\nduce a lot of additional complexity. For this reason, common wisdom until recently\nwas to keep your database on a single node (scale up) until scaling cost or high-\navailability requirements forced you to make it distributed.\nAs the tools and abstractions for distributed systems get better, this common wisdom\nmay change, at least for some kinds of applications. It is conceivable that distributed\ndata systems will become the default in the future, even for use cases that don\u2019t han\u2010\ndle large volumes of data or traffic. Over the course of the rest of this book we will\ncover many kinds of distributed data systems, and discuss how they fare not just in\nterms of scalability, but also ease of use and maintainability.\nThe architecture of systems that operate at large scale is usually highly specific to the\napplication\u2014there is no such thing as a generic, one-size-fits-all scalable architecture\n(informally known as magic scaling sauce). The problem may be the volume of reads,\nthe volume of writes, the volume of data to store, the complexity of the data, the\nresponse time requirements, the access patterns, or (usually) some mixture of all of\nthese plus many more issues.\nFor example, a system that is designed to handle 100,000 requests per second, each\n1 kB in size, looks very different from a system that is designed for 3 requests per\nminute, each 2 GB in size\u2014even though the two systems have the same data through\u2010\nput.\nAn architecture that scales well for a particular application is built around assump\u2010\ntions of which operations will be common and which will be rare\u2014the load parame\u2010\nters. If those assumptions turn out to be wrong, the engineering effort for scaling is at\nbest wasted, and at worst counterproductive. In an early-stage startup or an unpro\u2010\nven product it\u2019s usually more important to be able to iterate quickly on product fea\u2010\ntures than it is to scale to some hypothetical future load.\nEven though they are specific to a particular application, scalable architectures are\nnevertheless usually built from general-purpose building blocks, arranged in familiar\npatterns. In this book we discuss those building blocks and patterns. \nMaintainability\nIt is well known that the majority of the cost of software is not in its initial develop\u2010\nment, but in its ongoing maintenance\u2014fixing bugs, keeping its systems operational,\ninvestigating failures, adapting it to new platforms, modifying it for new use cases,\nrepaying technical debt, and adding new features.\nYet, unfortunately, many people working on software systems dislike maintenance of\nso-called legacy systems\u2014perhaps it involves fixing other people\u2019s mistakes, or work\u2010\n18 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3098fd5-4a3f-44f0-92b6-4f521b827017": {"__data__": {"id_": "e3098fd5-4a3f-44f0-92b6-4f521b827017", "embedding": null, "metadata": {"page_label": "19", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0521757f-09a7-4b3f-a7d5-4ed986a399fc", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "67f5cdcf16d9812766b22df1f9e83304c0f360c2f38b72e9ceff23aedf3c666b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ing with platforms that are now outdated, or systems that were forced to do things\nthey were never intended for. Every legacy system is unpleasant in its own way, and\nso it is difficult to give general recommendations for dealing with them.\nHowever, we can and should design software in such a way that it will hopefully min\u2010\nimize pain during maintenance, and thus avoid creating legacy software ourselves. To\nthis end, we will pay particular attention to three design principles for software\nsystems:\nOperability\nMake it easy for operations teams to keep the system running smoothly.\nSimplicity\nMake it easy for new engineers to understand the system, by removing as much\ncomplexity as possible from the system. (Note this is not the same as simplicity\nof the user interface.)\nEvolvability\nMake it easy for engineers to make changes to the system in the future, adapting\nit for unanticipated use cases as requirements change. Also known as extensibil\u2010\nity, modifiability, or plasticity.\nAs previously with reliability and scalability, there are no easy solutions for achieving\nthese goals. Rather, we will try to think about systems with operability, simplicity,\nand evolvability in mind.\nOperability: Making Life Easy for Operations\nIt has been suggested that \u201cgood operations can often work around the limitations of\nbad (or incomplete) software, but good software cannot run reliably with bad opera\u2010\ntions\u201d [12]. While some aspects of operations can and should be automated, it is still\nup to humans to set up that automation in the first place and to make sure it\u2019s work\u2010\ning correctly.\nOperations teams are vital to keeping a software system running smoothly. A good\noperations team typically is responsible for the following, and more [29]:\n\u2022 Monitoring the health of the system and quickly restoring service if it goes into a\nbad state\n\u2022 Tracking down the cause of problems, such as system failures or degraded per\u2010\nformance\n\u2022 Keeping software and platforms up to date, including security patches\n\u2022 Keeping tabs on how different systems affect each other, so that a problematic\nchange can be avoided before it causes damage\nMaintainability | 19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed6d9857-bf66-4f99-b70c-dd2e83e927c9": {"__data__": {"id_": "ed6d9857-bf66-4f99-b70c-dd2e83e927c9", "embedding": null, "metadata": {"page_label": "20", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78dc8c45-d28c-44dd-8bbe-b1ac778d145e", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "07caab51d34891b4de236a66bc0db0e83544732db36c6d0df96ccbc29730e94c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Anticipating future problems and solving them before they occur (e.g., capacity\nplanning)\n\u2022 Establishing good practices and tools for deployment, configuration manage\u2010\nment, and more\n\u2022 Performing complex maintenance tasks, such as moving an application from one\nplatform to another\n\u2022 Maintaining the security of the system as configuration changes are made\n\u2022 Defining processes that make operations predictable and help keep the produc\u2010\ntion environment stable\n\u2022 Preserving the organization\u2019s knowledge about the system, even as individual\npeople come and go\nGood operability means making routine tasks easy, allowing the operations team to\nfocus their efforts on high-value activities. Data systems can do various things to\nmake routine tasks easy, including:\n\u2022 Providing visibility into the runtime behavior and internals of the system, with\ngood monitoring\n\u2022 Providing good support for automation and integration with standard tools\n\u2022 Avoiding dependency on individual machines (allowing machines to be taken\ndown for maintenance while the system as a whole continues running uninter\u2010\nrupted)\n\u2022 Providing good documentation and an easy-to-understand operational model\n(\u201cIf I do X, Y will happen\u201d)\n\u2022 Providing good default behavior, but also giving administrators the freedom to\noverride defaults when needed\n\u2022 Self-healing where appropriate, but also giving administrators manual control\nover the system state when needed\n\u2022 Exhibiting predictable behavior, minimizing surprises\nSimplicity: Managing Complexity\nSmall software projects can have delightfully simple and expressive code, but as\nprojects get larger, they often become very complex and difficult to understand. This\ncomplexity slows down everyone who needs to work on the system, further increas\u2010\ning the cost of maintenance. A software project mired in complexity is sometimes\ndescribed as a big ball of mud [30].\n20 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1946, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2a5b2f4-0b4d-48fd-87fe-68fc06298680": {"__data__": {"id_": "e2a5b2f4-0b4d-48fd-87fe-68fc06298680", "embedding": null, "metadata": {"page_label": "21", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b4da5b9-4280-4507-ae6b-0236588afcbb", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "494c5bdf1e7a00d3ca2736134735f1093c0050f34188f0cb8fcfb0f75a1b55bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are various possible symptoms of complexity: explosion of the state space, tight\ncoupling of modules, tangled dependencies, inconsistent naming and terminology,\nhacks aimed at solving performance problems, special-casing to work around issues\nelsewhere, and many more. Much has been said on this topic already [31, 32, 33].\nWhen complexity makes maintenance hard, budgets and schedules are often over\u2010\nrun. In complex software, there is also a greater risk of introducing bugs when mak\u2010\ning a change: when the system is harder for developers to understand and reason\nabout, hidden assumptions, unintended consequences, and unexpected interactions\nare more easily overlooked. Conversely, reducing complexity greatly improves the\nmaintainability of software, and thus simplicity should be a key goal for the systems\nwe build.\nMaking a system simpler does not necessarily mean reducing its functionality; it can\nalso mean removing accidental complexity. Moseley and Marks [ 32] define complex\u2010\nity as accidental if it is not inherent in the problem that the software solves (as seen\nby the users) but arises only from the implementation.\nOne of the best tools we have for removing accidental complexity is abstraction. A\ngood abstraction can hide a great deal of implementation detail behind a clean,\nsimple-to-understand fa\u00e7ade. A good abstraction can also be used for a wide range of\ndifferent applications. Not only is this reuse more efficient than reimplementing a\nsimilar thing multiple times, but it also leads to higher-quality software, as quality\nimprovements in the abstracted component benefit all applications that use it.\nFor example, high-level programming languages are abstractions that hide machine\ncode, CPU registers, and syscalls. SQL is an abstraction that hides complex on-disk\nand in-memory data structures, concurrent requests from other clients, and inconsis\u2010\ntencies after crashes. Of course, when programming in a high-level language, we are\nstill using machine code; we are just not using it directly, because the programming\nlanguage abstraction saves us from having to think about it.\nHowever, finding good abstractions is very hard. In the field of distributed systems,\nalthough there are many good algorithms, it is much less clear how we should be\npackaging them into abstractions that help us keep the complexity of the system at a\nmanageable level.\nThroughout this book, we will keep our eyes open for good abstractions that allow us\nto extract parts of a large system into well-defined, reusable components.\nEvolvability: Making Change Easy\nIt\u2019s extremely unlikely that your system\u2019s requirements will remain unchanged for\u2010\never. They are much more likely to be in constant flux: you learn new facts, previ\u2010\nously unanticipated use cases emerge, business priorities change, users request new\nMaintainability | 21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "505b222f-a61e-4bf8-828b-92566105bba7": {"__data__": {"id_": "505b222f-a61e-4bf8-828b-92566105bba7", "embedding": null, "metadata": {"page_label": "22", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86f26097-22cc-419d-90db-a1d6c342bcf3", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fc83fa8c4ba35a258ca2c0066f599911fd0db3fb08ac0339b3254085c101382d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "features, new platforms replace old platforms, legal or regulatory requirements\nchange, growth of the system forces architectural changes, etc.\nIn terms of organizational processes, Agile working patterns provide a framework for\nadapting to change. The Agile community has also developed technical tools and pat\u2010\nterns that are helpful when developing software in a frequently changing environ\u2010\nment, such as test-driven development (TDD) and refactoring.\nMost discussions of these Agile techniques focus on a fairly small, local scale (a cou\u2010\nple of source code files within the same application). In this book, we search for ways\nof increasing agility on the level of a larger data system, perhaps consisting of several\ndifferent applications or services with different characteristics. For example, how\nwould you \u201crefactor\u201d Twitter\u2019s architecture for assembling home timelines (\u201cDescrib\u2010\ning Load\u201d on page 11) from approach 1 to approach 2?\nThe ease with which you can modify a data system, and adapt it to changing require\u2010\nments, is closely linked to its simplicity and its abstractions: simple and easy-to-\nunderstand systems are usually easier to modify than complex ones. But since this is\nsuch an important idea, we will use a different word to refer to agility on a data sys\u2010\ntem level: evolvability [34]. \nSummary\nIn this chapter, we have explored some fundamental ways of thinking about data-\nintensive applications. These principles will guide us through the rest of the book,\nwhere we dive into deep technical detail.\nAn application has to meet various requirements in order to be useful. There are\nfunctional requirements  (what it should do, such as allowing data to be stored,\nretrieved, searched, and processed in various ways), and some nonfunctional require\u2010\nments (general properties like security, reliability, compliance, scalability, compatibil\u2010\nity, and maintainability). In this chapter we discussed reliability, scalability, and\nmaintainability in detail.\nReliability means making systems work correctly, even when faults occur. Faults can\nbe in hardware (typically random and uncorrelated), software (bugs are typically sys\u2010\ntematic and hard to deal with), and humans (who inevitably make mistakes from\ntime to time). Fault-tolerance techniques can hide certain types of faults from the end\nuser.\nScalability means having strategies for keeping performance good, even when load\nincreases. In order to discuss scalability, we first need ways of describing load and\nperformance quantitatively. We briefly looked at Twitter\u2019s home timelines as an\nexample of describing load, and response time percentiles as a way of measuring per\u2010\n22 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2722, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2ad2570-f543-4a8e-807b-6d145ccac168": {"__data__": {"id_": "e2ad2570-f543-4a8e-807b-6d145ccac168", "embedding": null, "metadata": {"page_label": "23", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3907149-1b10-4755-9b11-62c174602753", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "207aa3c1319ebade7d081d154f37e04073dabb3e5d4cd8885ebccb6fe6ac5c33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "formance. In a scalable system, you can add processing capacity in order to remain\nreliable under high load.\nMaintainability has many facets, but in essence it\u2019s about making life better for the\nengineering and operations teams who need to work with the system. Good abstrac\u2010\ntions can help reduce complexity and make the system easier to modify and adapt for\nnew use cases. Good operability means having good visibility into the system\u2019s health,\nand having effective ways of managing it.\nThere is unfortunately no easy fix for making applications reliable, scalable, or main\u2010\ntainable. However, there are certain patterns and techniques that keep reappearing in\ndifferent kinds of applications. In the next few chapters we will take a look at some\nexamples of data systems and analyze how they work toward those goals.\nLater in the book, in Part III, we will look at patterns for systems that consist of sev\u2010\neral components working together, such as the one in Figure 1-1.\nReferences\n[1] Michael Stonebraker and U\u011fur \u00c7etintemel: \u201c \u2018One Size Fits All\u2019: An Idea Whose\nTime Has Come and Gone ,\u201d at 21st International Conference on Data Engineering\n(ICDE), April 2005.\n[2] Walter L. Heimerdinger and Charles B. Weinstock: \u201c A Conceptual Framework\nfor System Fault Tolerance ,\u201d Technical Report CMU/SEI-92-TR-033, Software Engi\u2010\nneering Institute, Carnegie Mellon University, October 1992.\n[3] Ding Yuan, Yu Luo, Xin Zhuang, et al.: \u201c Simple Testing Can Prevent Most Criti\u2010\ncal Failures: An Analysis of Production Failures in Distributed Data-Intensive Sys\u2010\ntems,\u201d at 11th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI), October 2014.\n[4] Yury Izrailevsky and Ariel Tseitlin: \u201c The Netflix Simian Army ,\u201d techblog.net\u2010\nflix.com, July 19, 2011.\n[5] Daniel Ford, Fran\u00e7ois Labelle, Florentina I. Popovici, et al.: \u201c Availability in Glob\u2010\nally Distributed Storage Systems ,\u201d at 9th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI), October 2010.\n[6] Brian Beach: \u201cHard Drive Reliability Update \u2013 Sep 2014,\u201d backblaze.com, Septem\u2010\nber 23, 2014.\n[7] Laurie Voss: \u201cAWS: The Good, the Bad and the Ugly,\u201d blog.awe.sm, December 18,\n2012.\nSummary | 23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "251474d9-8435-431b-89fc-9e2d6cfac3b7": {"__data__": {"id_": "251474d9-8435-431b-89fc-9e2d6cfac3b7", "embedding": null, "metadata": {"page_label": "24", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53f880cd-cd8b-4f03-b685-8e26a3ca7dae", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4ec0b7809b39323bd753c04101184dfc28f685035fc38949e621960aec6f4a72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[8] Haryadi S. Gunawi, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: \u201c What\nBugs Live in the Cloud? ,\u201d at 5th ACM Symposium on Cloud Computing  (SoCC),\nNovember 2014. doi:10.1145/2670979.2670986\n[9] Nelson Minar: \u201c Leap Second Crashes Half the Internet ,\u201d somebits.com, July 3,\n2012.\n[10] Amazon Web Services: \u201c Summary of the Amazon EC2 and Amazon RDS Ser\u2010\nvice Disruption in the US East Region,\u201d aws.amazon.com, April 29, 2011.\n[11] Richard I. Cook: \u201c How Complex Systems Fail ,\u201d Cognitive Technologies Labora\u2010\ntory, April 2000.\n[12] Jay Kreps: \u201c Getting Real About Distributed System Reliability ,\u201d blog.empathy\u2010\nbox.com, March 19, 2012.\n[13] David Oppenheimer, Archana Ganapathi, and David A. Patterson: \u201c Why Do\nInternet Services Fail, and What Can Be Done About It? ,\u201d at 4th USENIX Symposium\non Internet Technologies and Systems (USITS), March 2003.\n[14] Nathan Marz: \u201c Principles of Software Engineering, Part 1 ,\u201d nathanmarz.com,\nApril 2, 2013.\n[15] Michael Jurewitz: \u201cThe Human Impact of Bugs,\u201d jury.me, March 15, 2013.\n[16] Raffi Krikorian: \u201cTimelines at Scale,\u201d at QCon San Francisco, November 2012.\n[17] Martin Fowler: Patterns of Enterprise Application Architecture . Addison Wesley,\n2002. ISBN: 978-0-321-12742-6\n[18] Kelly Sommers: \u201cAfter all that run around, what caused 500ms disk latency even\nwhen we replaced physical server?\u201d twitter.com, November 13, 2014.\n[19] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: \u201c Dynamo: Ama\u2010\nzon\u2019s Highly Available Key-Value Store ,\u201d at 21st ACM Symposium on Operating Sys\u2010\ntems Principles (SOSP), October 2007.\n[20] Greg Linden: \u201c Make Data Useful ,\u201d slides from presentation at Stanford Univer\u2010\nsity Data Mining class (CS345), December 2006.\n[21] Tammy Everts: \u201c The Real Cost of Slow Time vs Downtime ,\u201d webperformanceto\u2010\nday.com, November 12, 2014.\n[22] Jake Brutlag: \u201c Speed Matters for Google Web Search ,\u201d googleresearch.blog\u2010\nspot.co.uk, June 22, 2009.\n[23] Tyler Treat: \u201c Everything You Know About Latency Is Wrong ,\u201d bravenew\u2010\ngeek.com, December 12, 2015.\n24 | Chapter 1: Reliable, Scalable, and Maintainable Applications", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2099, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2499712-9e36-4c91-a963-0eb58f9750a1": {"__data__": {"id_": "e2499712-9e36-4c91-a963-0eb58f9750a1", "embedding": null, "metadata": {"page_label": "25", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b47728d4-e10c-490a-9da9-712f188da937", "node_type": "4", "metadata": {"page_label": "25", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9842c49e7ac53fac3b6776bacd74117bc594a73e6c3815d1c834db1f3af3168e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[24] Jeffrey Dean and Luiz Andr\u00e9 Barroso: \u201cThe Tail at Scale,\u201d Communications of the\nACM, volume 56, number 2, pages 74\u201380, February 2013. doi:\n10.1145/2408776.2408794\n[25] Graham Cormode, Vladislav Shkapenyuk, Divesh Srivastava, and Bojian Xu:\n\u201cForward Decay: A Practical Time Decay Model for Streaming Systems ,\u201d at 25th\nIEEE International Conference on Data Engineering (ICDE), March 2009.\n[26] Ted Dunning and Otmar Ertl: \u201c Computing Extremely Accurate Quantiles Using\nt-Digests,\u201d github.com, March 2014.\n[27] Gil Tene: \u201cHdrHistogram,\u201d hdrhistogram.org.\n[28] Baron Schwartz: \u201c Why Percentiles Don\u2019t Work the Way You Think ,\u201d vividcor\u2010\ntex.com, December 7, 2015.\n[29] James Hamilton: \u201c On Designing and Deploying Internet-Scale Services ,\u201d at 21st\nLarge Installation System Administration Conference (LISA), November 2007.\n[30] Brian Foote and Joseph Yoder: \u201c Big Ball of Mud ,\u201d at 4th Conference on Pattern\nLanguages of Programs (PLoP), September 1997.\n[31] Frederick P Brooks: \u201cNo Silver Bullet \u2013 Essence and Accident in Software Engi\u2010\nneering,\u201d in The Mythical Man-Month , Anniversary edition, Addison-Wesley, 1995.\nISBN: 978-0-201-83595-3\n[32] Ben Moseley and Peter Marks: \u201c Out of the Tar Pit ,\u201d at BCS Software Practice\nAdvancement (SPA), 2006.\n[33] Rich Hickey: \u201cSimple Made Easy,\u201d at Strange Loop, September 2011.\n[34] Hongyu Pei Breivold, Ivica Crnkovic, and Peter J. Eriksson: \u201c Analyzing Software\nEvolvability,\u201d at 32nd Annual IEEE International Computer Software and Applica\u2010\ntions Conference (COMPSAC), July 2008. doi:10.1109/COMPSAC.2008.50\nSummary | 25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4adafff2-cc87-42de-bd97-cb62071b6080": {"__data__": {"id_": "4adafff2-cc87-42de-bd97-cb62071b6080", "embedding": null, "metadata": {"page_label": "26", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a343cc4-40f9-437b-8868-bccb9e00f012", "node_type": "4", "metadata": {"page_label": "26", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a7f0a37-cef9-4724-8a49-6a5490c29d32": {"__data__": {"id_": "0a7f0a37-cef9-4724-8a49-6a5490c29d32", "embedding": null, "metadata": {"page_label": "27", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "accfafda-224c-4f91-a224-3ded2f116879", "node_type": "4", "metadata": {"page_label": "27", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c763277575c3aa455078f142cca624ad367948d7d444da0132d31b64b2275038", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 2\nData Models and Query Languages\nThe limits of my language mean the limits of my world.\n\u2014Ludwig Wittgenstein, Tractatus Logico-Philosophicus (1922)\nData models are perhaps the most important part of developing software, because\nthey have such a profound effect: not only on how the software is written, but also on\nhow we think about the problem that we are solving.\nMost applications are built by layering one data model on top of another. For each\nlayer, the key question is: how is it represented in terms of the next-lower layer? For\nexample:\n1. As an application developer, you look at the real world (in which there are peo\u2010\nple, organizations, goods, actions, money flows, sensors, etc.) and model it in\nterms of objects or data structures, and APIs that manipulate those data struc\u2010\ntures. Those structures are often specific to your application.\n2. When you want to store those data structures, you express them in terms of a\ngeneral-purpose data model, such as JSON or XML documents, tables in a rela\u2010\ntional database, or a graph model.\n3. The engineers who built your database software decided on a way of representing\nthat JSON/XML/relational/graph data in terms of bytes in memory, on disk, or\non a network. The representation may allow the data to be queried, searched,\nmanipulated, and processed in various ways.\n4. On yet lower levels, hardware engineers have figured out how to represent bytes\nin terms of electrical currents, pulses of light, magnetic fields, and more.\nIn a complex application there may be more intermediary levels, such as APIs built\nupon APIs, but the basic idea is still the same: each layer hides the complexity of the\nlayers below it by providing a clean data model. These abstractions allow different\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8456caa-1e90-468b-b26e-ddaeda24a612": {"__data__": {"id_": "c8456caa-1e90-468b-b26e-ddaeda24a612", "embedding": null, "metadata": {"page_label": "28", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1cad1385-7571-4f49-a8d3-aa0846d34b47", "node_type": "4", "metadata": {"page_label": "28", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a5c8200ec81b223a534c1cdb957fabd834e9cd0c20e294448d0e68ea19d1aa9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "groups of people\u2014for example, the engineers at the database vendor and the applica\u2010\ntion developers using their database\u2014to work together effectively.\nThere are many different kinds of data models, and every data model embodies\nassumptions about how it is going to be used. Some kinds of usage are easy and some\nare not supported; some operations are fast and some perform badly; some data\ntransformations feel natural and some are awkward.\nIt can take a lot of effort to master just one data model (think how many books there\nare on relational data modeling). Building software is hard enough, even when work\u2010\ning with just one data model and without worrying about its inner workings. But\nsince the data model has such a profound effect on what the software above it can\nand can\u2019t do, it\u2019s important to choose one that is appropriate to the application.\nIn this chapter we will look at a range of general-purpose data models for data stor\u2010\nage and querying (point 2 in the preceding list). In particular, we will compare the\nrelational model, the document model, and a few graph-based data models. We will\nalso look at various query languages and compare their use cases. In Chapter 3  we\nwill discuss how storage engines work; that is, how these data models are actually\nimplemented (point 3 in the list).\nRelational Model Versus Document Model\nThe best-known data model today is probably that of SQL, based on the relational\nmodel proposed by Edgar Codd in 1970 [ 1]: data is organized into relations (called\ntables in SQL), where each relation is an unordered collection of tuples (rows in SQL).\nThe relational model was a theoretical proposal, and many people at the time\ndoubted whether it could be implemented efficiently. However, by the mid-1980s,\nrelational database management systems (RDBMSes) and SQL had become the tools\nof choice for most people who needed to store and query data with some kind of reg\u2010\nular structure. The dominance of relational databases has lasted around 25\u201230 years\n\u2014an eternity in computing history.\nThe roots of relational databases lie in business data processing, which was performed\non mainframe computers in the 1960s and \u201970s. The use cases appear mundane from\ntoday\u2019s perspective: typically transaction processing (entering sales or banking trans\u2010\nactions, airline reservations, stock-keeping in warehouses) and batch processing (cus\u2010\ntomer invoicing, payroll, reporting).\nOther databases at that time forced application developers to think a lot about the\ninternal representation of the data in the database. The goal of the relational model\nwas to hide that implementation detail behind a cleaner interface.\nOver the years, there have been many competing approaches to data storage and\nquerying. In the 1970s and early 1980s, the network model and the hierarchical model\n28 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54819b5a-2f66-4d5a-bb5d-cba105077fa3": {"__data__": {"id_": "54819b5a-2f66-4d5a-bb5d-cba105077fa3", "embedding": null, "metadata": {"page_label": "29", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce0ab60f-d163-449b-b741-2eb1c7578f18", "node_type": "4", "metadata": {"page_label": "29", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "887aefe8280da5daab0b97e8eddb81be33f199145dcade218f273e9057b80781", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "were the main alternatives, but the relational model came to dominate them. Object\ndatabases came and went again in the late 1980s and early 1990s. XML databases\nappeared in the early 2000s, but have only seen niche adoption. Each competitor to\nthe relational model generated a lot of hype in its time, but it never lasted [2].\nAs computers became vastly more powerful and networked, they started being used\nfor increasingly diverse purposes. And remarkably, relational databases turned out to\ngeneralize very well, beyond their original scope of business data processing, to a\nbroad variety of use cases. Much of what you see on the web today is still powered by\nrelational databases, be it online publishing, discussion, social networking, ecom\u2010\nmerce, games, software-as-a-service productivity applications, or much more.\nThe Birth of NoSQL\nNow, in the 2010s, NoSQL is the latest attempt to overthrow the relational model\u2019s\ndominance. The name \u201cNoSQL\u201d is unfortunate, since it doesn\u2019t actually refer to any\nparticular technology\u2014it was originally intended simply as a catchy Twitter hashtag\nfor a meetup on open source, distributed, nonrelational databases in 2009 [ 3]. Never\u2010\ntheless, the term struck a nerve and quickly spread through the web startup commu\u2010\nnity and beyond. A number of interesting database systems are now associated with\nthe #NoSQL hashtag, and it has been retroactively reinterpreted as Not Only SQL [4].\nThere are several driving forces behind the adoption of NoSQL databases, including:\n\u2022 A need for greater scalability than relational databases can easily achieve, includ\u2010\ning very large datasets or very high write throughput\n\u2022 A widespread preference for free and open source software over commercial\ndatabase products\n\u2022 Specialized query operations that are not well supported by the relational model\n\u2022 Frustration with the restrictiveness of relational schemas, and a desire for a more\ndynamic and expressive data model [5]\nDifferent applications have different requirements, and the best choice of technology\nfor one use case may well be different from the best choice for another use case. It\ntherefore seems likely that in the foreseeable future, relational databases will continue\nto be used alongside a broad variety of nonrelational datastores\u2014an idea that is\nsometimes called polyglot persistence [3].\nThe Object-Relational Mismatch\nMost application development today is done in object-oriented programming lan\u2010\nguages, which leads to a common criticism of the SQL data model: if data is stored in\nrelational tables, an awkward translation layer is required between the objects in the\nRelational Model Versus Document Model | 29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7a641ac-aecf-42c1-bc58-751b2b3cbde9": {"__data__": {"id_": "b7a641ac-aecf-42c1-bc58-751b2b3cbde9", "embedding": null, "metadata": {"page_label": "30", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebf38025-9a55-4f3b-8b77-d934a8affa83", "node_type": "4", "metadata": {"page_label": "30", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a2a9d8364ce7c7f743dddce6240c0be7d8fa8cdb3a66ae49356e20e382bfd7e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. A term borrowed from electronics. Every electric circuit has a certain impedance (resistance to alternating\ncurrent) on its inputs and outputs. When you connect one circuit\u2019s output to another one\u2019s input, the power\ntransfer across the connection is maximized if the output and input impedances of the two circuits match. An\nimpedance mismatch can lead to signal reflections and other troubles.\napplication code and the database model of tables, rows, and columns. The discon\u2010\nnect between the models is sometimes called an impedance mismatch.i\nObject-relational mapping (ORM) frameworks like ActiveRecord and Hibernate\nreduce the amount of boilerplate code required for this translation layer, but they\ncan\u2019t completely hide the differences between the two models.\nFor example, Figure 2-1  illustrates how a r\u00e9sum\u00e9 (a LinkedIn profile) could be\nexpressed in a relational schema. The profile as a whole can be identified by a unique\nidentifier, user_id. Fields like first_name and last_name appear exactly once per\nuser, so they can be modeled as columns on the users table. However, most people\nhave had more than one job in their career (positions), and people may have varying\nnumbers of periods of education and any number of pieces of contact information.\nThere is a one-to-many relationship from the user to these items, which can be repre\u2010\nsented in various ways:\n\u2022 In the traditional SQL model (prior to SQL:1999), the most common normalized\nrepresentation is to put positions, education, and contact information in separate\ntables, with a foreign key reference to the users table, as in Figure 2-1.\n\u2022 Later versions of the SQL standard added support for structured datatypes and\nXML data; this allowed multi-valued data to be stored within a single row, with\nsupport for querying and indexing inside those documents. These features are\nsupported to varying degrees by Oracle, IBM DB2, MS SQL Server, and Post\u2010\ngreSQL [6, 7]. A JSON datatype is also supported by several databases, including\nIBM DB2, MySQL, and PostgreSQL [8].\n\u2022 A third option is to encode jobs, education, and contact info as a JSON or XML\ndocument, store it on a text column in the database, and let the application inter\u2010\npret its structure and content. In this setup, you typically cannot use the database\nto query for values inside that encoded column.\n30 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef1fc31f-7cdc-4696-9560-d4d585421c35": {"__data__": {"id_": "ef1fc31f-7cdc-4696-9560-d4d585421c35", "embedding": null, "metadata": {"page_label": "31", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b0b820e-2840-455b-88fe-8136912889f5", "node_type": "4", "metadata": {"page_label": "31", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4406390e156ca00e522ee0c1cf5a7c3949dd8a5198af1531adac61c1dd90aa2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 2-1. Representing a LinkedIn profile using a relational schema. Photo of Bill\nGates courtesy of Wikimedia Commons, Ricardo Stuckert, Ag\u00eancia Brasil.\nFor a data structure like a r\u00e9sum\u00e9, which is mostly a self-contained document, a JSON\nrepresentation can be quite appropriate: see Example 2-1 . JSON has the appeal of\nbeing much simpler than XML. Document-oriented databases like MongoDB [ 9],\nRethinkDB [10], CouchDB [11], and Espresso [12] support this data model.\nExample 2-1. Representing a LinkedIn profile as a JSON document\n{\n  \"user_id\":     251,\n  \"first_name\":  \"Bill\",\n  \"last_name\":   \"Gates\",\n  \"summary\":     \"Co-chair of the Bill & Melinda Gates... Active blogger.\",\n  \"region_id\":   \"us:91\",\n  \"industry_id\": 131,\n  \"photo_url\":   \"/p/7/000/253/05b/308dd6e.jpg\",\nRelational Model Versus Document Model | 31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fd81cef-b9fc-4bab-926f-63ea33a0c8cc": {"__data__": {"id_": "8fd81cef-b9fc-4bab-926f-63ea33a0c8cc", "embedding": null, "metadata": {"page_label": "32", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eea40c6e-3053-4fa0-9ae7-4ce87614052a", "node_type": "4", "metadata": {"page_label": "32", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0d5e46d336ee3c2d6419ee50c06e0de5913c70330cd45d31f1173b6e4850f413", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"positions\": [\n    {\"job_title\": \"Co-chair\", \"organization\": \"Bill & Melinda Gates Foundation\"},\n    {\"job_title\": \"Co-founder, Chairman\", \"organization\": \"Microsoft\"}\n  ],\n  \"education\": [\n    {\"school_name\": \"Harvard University\",       \"start\": 1973, \"end\": 1975},\n    {\"school_name\": \"Lakeside School, Seattle\", \"start\": null, \"end\": null}\n  ],\n  \"contact_info\": {\n    \"blog\":    \"http://thegatesnotes.com\",\n    \"twitter\": \"http://twitter.com/BillGates\"\n  }\n}\nSome developers feel that the JSON model reduces the impedance mismatch between\nthe application code and the storage layer. However, as we shall see in Chapter 4 ,\nthere are also problems with JSON as a data encoding format. The lack of a schema is\noften cited as an advantage; we will discuss this in \u201cSchema flexibility in the docu\u2010\nment model\u201d on page 39.\nThe JSON representation has better locality than the multi-table schema in\nFigure 2-1. If you want to fetch a profile in the relational example, you need to either\nperform multiple queries (query each table by user_id) or perform a messy multi-\nway join between the users table and its subordinate tables. In the JSON representa\u2010\ntion, all the relevant information is in one place, and one query is sufficient.\nThe one-to-many relationships from the user profile to the user\u2019s positions, educa\u2010\ntional history, and contact information imply a tree structure in the data, and the\nJSON representation makes this tree structure explicit (see Figure 2-2).\nFigure 2-2. One-to-many relationships forming a tree structure.\n32 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11f2614e-7e73-49e8-85e2-498ed3156c00": {"__data__": {"id_": "11f2614e-7e73-49e8-85e2-498ed3156c00", "embedding": null, "metadata": {"page_label": "33", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62a15cf9-434d-4e3b-aa87-461f55460f67", "node_type": "4", "metadata": {"page_label": "33", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f0a19a783897646dcfbb6dcce2e6a83907c9d80eead201ffdf1b3aa35fbbe702", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. Literature on the relational model distinguishes several different normal forms, but the distinctions are of\nlittle practical interest. As a rule of thumb, if you\u2019re duplicating values that could be stored in just one place,\nthe schema is not normalized.\nMany-to-One and Many-to-Many Relationships\nIn Example 2-1  in the preceding section, region_id and industry_id are given as\nIDs, not as plain-text strings \"Greater Seattle Area\" and \"Philanthropy\". Why?\nIf the user interface has free-text fields for entering the region and the industry, it\nmakes sense to store them as plain-text strings. But there are advantages to having\nstandardized lists of geographic regions and industries, and letting users choose from\na drop-down list or autocompleter:\n\u2022 Consistent style and spelling across profiles\n\u2022 Avoiding ambiguity (e.g., if there are several cities with the same name)\n\u2022 Ease of updating\u2014the name is stored in only one place, so it is easy to update\nacross the board if it ever needs to be changed (e.g., change of a city name due to\npolitical events)\n\u2022 Localization support\u2014when the site is translated into other languages, the stand\u2010\nardized lists can be localized, so the region and industry can be displayed in the\nviewer\u2019s language\n\u2022 Better search\u2014e.g., a search for philanthropists in the state of Washington can\nmatch this profile, because the list of regions can encode the fact that Seattle is in\nWashington (which is not apparent from the string \"Greater Seattle Area\")\nWhether you store an ID or a text string is a question of duplication. When you use\nan ID, the information that is meaningful to humans (such as the word Philanthropy)\nis stored in only one place, and everything that refers to it uses an ID (which only has\nmeaning within the database). When you store the text directly, you are duplicating\nthe human-meaningful information in every record that uses it.\nThe advantage of using an ID is that because it has no meaning to humans, it never\nneeds to change: the ID can remain the same, even if the information it identifies\nchanges. Anything that is meaningful to humans may need to change sometime in\nthe future\u2014and if that information is duplicated, all the redundant copies need to be\nupdated. That incurs write overheads, and risks inconsistencies (where some copies\nof the information are updated but others aren\u2019t). Removing such duplication is the\nkey idea behind normalization in databases.ii\nRelational Model Versus Document Model | 33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4ca28c6-d6eb-4d68-80e9-a5920132d524": {"__data__": {"id_": "a4ca28c6-d6eb-4d68-80e9-a5920132d524", "embedding": null, "metadata": {"page_label": "34", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07b91301-91b9-4d5d-bee0-114c22b370c6", "node_type": "4", "metadata": {"page_label": "34", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d8bbd56d97c284d48834a5b5e174e9fe0c49128e24e000df3ea395d4d21409e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. At the time of writing, joins are supported in RethinkDB, not supported in MongoDB, and only sup\u2010\nported in predeclared views in CouchDB.\nDatabase administrators and developers love to argue about nor\u2010\nmalization and denormalization, but we will suspend judgment for\nnow. In Part III of this book we will return to this topic and explore\nsystematic ways of dealing with caching, denormalization, and\nderived data.\nUnfortunately, normalizing this data requires many-to-one relationships (many peo\u2010\nple live in one particular region, many people work in one particular industry), which\ndon\u2019t fit nicely into the document model. In relational databases, it\u2019s normal to refer\nto rows in other tables by ID, because joins are easy. In document databases, joins are\nnot needed for one-to-many tree structures, and support for joins is often weak.iii\nIf the database itself does not support joins, you have to emulate a join in application\ncode by making multiple queries to the database. (In this case, the lists of regions and\nindustries are probably small and slow-changing enough that the application can\nsimply keep them in memory. But nevertheless, the work of making the join is shifted\nfrom the database to the application code.)\nMoreover, even if the initial version of an application fits well in a join-free docu\u2010\nment model, data has a tendency of becoming more interconnected as features are\nadded to applications. For example, consider some changes we could make to the\nr\u00e9sum\u00e9 example:\nOrganizations and schools as entities\nIn the previous description, organization (the company where the user worked)\nand school_name (where they studied) are just strings. Perhaps they should be\nreferences to entities instead? Then each organization, school, or university could\nhave its own web page (with logo, news feed, etc.); each r\u00e9sum\u00e9 could link to the\norganizations and schools that it mentions, and include their logos and other\ninformation (see Figure 2-3 for an example from LinkedIn).\nRecommendations\nSay you want to add a new feature: one user can write a recommendation for\nanother user. The recommendation is shown on the r\u00e9sum\u00e9 of the user who was\nrecommended, together with the name and photo of the user making the recom\u2010\nmendation. If the recommender updates their photo, any recommendations they\nhave written need to reflect the new photo. Therefore, the recommendation\nshould have a reference to the author\u2019s profile.\n34 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfddffbb-e575-42ad-b1ac-4ad7a436df4a": {"__data__": {"id_": "dfddffbb-e575-42ad-b1ac-4ad7a436df4a", "embedding": null, "metadata": {"page_label": "35", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "196ab068-bce1-4c40-bd5c-a25420c412d4", "node_type": "4", "metadata": {"page_label": "35", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7411ac7208ca63517848ba4314d1662678024974c005932831101621290647f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 2-3. The company name is not just a string, but a link to a company entity.\nScreenshot of linkedin.com.\nFigure 2-4  illustrates how these new features require many-to-many relationships.\nThe data within each dotted rectangle can be grouped into one document, but the\nreferences to organizations, schools, and other users need to be represented as refer\u2010\nences, and require joins when queried.\nFigure 2-4. Extending r\u00e9sum\u00e9s with many-to-many relationships.\nRelational Model Versus Document Model | 35", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75ba5197-8f29-49ae-ae1c-c8b9079e39d0": {"__data__": {"id_": "75ba5197-8f29-49ae-ae1c-c8b9079e39d0", "embedding": null, "metadata": {"page_label": "36", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13a1e471-4782-488c-88ae-ae8a07d45805", "node_type": "4", "metadata": {"page_label": "36", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ddb464a87965cd05220af5bfef785cd83e1bae5a8b6a80c449e43c1265865526", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Are Document Databases Repeating History?\nWhile many-to-many relationships and joins are routinely used in relational data\u2010\nbases, document databases and NoSQL reopened the debate on how best to represent\nsuch relationships in a database. This debate is much older than NoSQL\u2014in fact, it\ngoes back to the very earliest computerized database systems.\nThe most popular database for business data processing in the 1970s was IBM\u2019s Infor\u2010\nmation Management System  (IMS), originally developed for stock-keeping in the\nApollo space program and first commercially released in 1968 [ 13]. It is still in use\nand maintained today, running on OS/390 on IBM mainframes [14].\nThe design of IMS used a fairly simple data model called the hierarchical model ,\nwhich has some remarkable similarities to the JSON model used by document data\u2010\nbases [2]. It represented all data as a tree of records nested within records, much like\nthe JSON structure of Figure 2-2.\nLike document databases, IMS worked well for one-to-many relationships, but it\nmade many-to-many relationships difficult, and it didn\u2019t support joins. Developers\nhad to decide whether to duplicate (denormalize) data or to manually resolve refer\u2010\nences from one record to another. These problems of the 1960s and \u201970s were very\nmuch like the problems that developers are running into with document databases\ntoday [15].\nVarious solutions were proposed to solve the limitations of the hierarchical model.\nThe two most prominent were the relational model  (which became SQL, and took\nover the world) and the network model  (which initially had a large following but\neventually faded into obscurity). The \u201cgreat debate\u201d between these two camps lasted\nfor much of the 1970s [2].\nSince the problem that the two models were solving is still so relevant today, it\u2019s\nworth briefly revisiting this debate in today\u2019s light.\nThe network model\nThe network model was standardized by a committee called the Conference on Data\nSystems Languages (CODASYL) and implemented by several different database ven\u2010\ndors; it is also known as the CODASYL model [16].\nThe CODASYL model was a generalization of the hierarchical model. In the tree\nstructure of the hierarchical model, every record has exactly one parent; in the net\u2010\nwork model, a record could have multiple parents. For example, there could be one\nrecord for the \"Greater Seattle Area\"  region, and every user who lived in that\nregion could be linked to it. This allowed many-to-one and many-to-many relation\u2010\nships to be modeled.\n36 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "76ed2928-83f4-4f51-8822-ebb7791cc63e": {"__data__": {"id_": "76ed2928-83f4-4f51-8822-ebb7791cc63e", "embedding": null, "metadata": {"page_label": "37", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4174e529-5bac-40a6-a42c-71adf86f2e2b", "node_type": "4", "metadata": {"page_label": "37", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fe55e3b92c2d49ae6f7827dbeb7f3864a3f135f0bf89fd8bb2943d6e2c08e2de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. Foreign key constraints allow you to restrict modifications, but such constraints are not required by the\nrelational model. Even with constraints, joins on foreign keys are performed at query time, whereas in\nCODASYL, the join was effectively done at insert time.\nThe links between records in the network model were not foreign keys, but more like\npointers in a programming language (while still being stored on disk). The only way\nof accessing a record was to follow a path from a root record along these chains of\nlinks. This was called an access path.\nIn the simplest case, an access path could be like the traversal of a linked list: start at\nthe head of the list, and look at one record at a time until you find the one you want.\nBut in a world of many-to-many relationships, several different paths can lead to the\nsame record, and a programmer working with the network model had to keep track\nof these different access paths in their head.\nA query in CODASYL was performed by moving a cursor through the database by\niterating over lists of records and following access paths. If a record had multiple\nparents (i.e., multiple incoming pointers from other records), the application code\nhad to keep track of all the various relationships. Even CODASYL committee mem\u2010\nbers admitted that this was like navigating around an n-dimensional data space [17].\nAlthough manual access path selection was able to make the most efficient use of the\nvery limited hardware capabilities in the 1970s (such as tape drives, whose seeks are\nextremely slow), the problem was that they made the code for querying and updating\nthe database complicated and inflexible. With both the hierarchical and the network\nmodel, if you didn\u2019t have a path to the data you wanted, you were in a difficult situa\u2010\ntion. You could change the access paths, but then you had to go through a lot of\nhandwritten database query code and rewrite it to handle the new access paths. It was\ndifficult to make changes to an application\u2019s data model.\nThe relational model\nWhat the relational model did, by contrast, was to lay out all the data in the open: a\nrelation (table) is simply a collection of tuples (rows), and that\u2019s it. There are no laby\u2010\nrinthine nested structures, no complicated access paths to follow if you want to look\nat the data. You can read any or all of the rows in a table, selecting those that match\nan arbitrary condition. You can read a particular row by designating some columns\nas a key and matching on those. You can insert a new row into any table without\nworrying about foreign key relationships to and from other tables.iv\nIn a relational database, the query optimizer automatically decides which parts of the\nquery to execute in which order, and which indexes to use. Those choices are effec\u2010\ntively the \u201caccess path,\u201d but the big difference is that they are made automatically by\nRelational Model Versus Document Model | 37", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ae4615e-e690-4b91-ae76-72549a4a0a46": {"__data__": {"id_": "3ae4615e-e690-4b91-ae76-72549a4a0a46", "embedding": null, "metadata": {"page_label": "38", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "498e5d68-001a-486e-945e-7a4144989417", "node_type": "4", "metadata": {"page_label": "38", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5076dca470d1900fc270053f3a49f13847a4d36b84632cda4d96b911e70b7376", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the query optimizer, not by the application developer, so we rarely need to think\nabout them.\nIf you want to query your data in new ways, you can just declare a new index, and\nqueries will automatically use whichever indexes are most appropriate. You don\u2019t\nneed to change your queries to take advantage of a new index. (See also \u201cQuery Lan\u2010\nguages for Data\u201d on page 42.) The relational model thus made it much easier to add\nnew features to applications.\nQuery optimizers for relational databases are complicated beasts, and they have con\u2010\nsumed many years of research and development effort [ 18]. But a key insight of the\nrelational model was this: you only need to build a query optimizer once, and then all\napplications that use the database can benefit from it. If you don\u2019t have a query opti\u2010\nmizer, it\u2019s easier to handcode the access paths for a particular query than to write a\ngeneral-purpose optimizer\u2014but the general-purpose solution wins in the long run.\nComparison to document databases\nDocument databases reverted back to the hierarchical model in one aspect: storing\nnested records (one-to-many relationships, like positions, education, and\ncontact_info in Figure 2-1 ) within their parent record rather than in a separate\ntable.\nHowever, when it comes to representing many-to-one and many-to-many relation\u2010\nships, relational and document databases are not fundamentally different: in both\ncases, the related item is referenced by a unique identifier, which is called a foreign\nkey in the relational model and a document reference  in the document model [ 9].\nThat identifier is resolved at read time by using a join or follow-up queries. To date,\ndocument databases have not followed the path of CODASYL.\nRelational Versus Document Databases Today\nThere are many differences to consider when comparing relational databases to\ndocument databases, including their fault-tolerance properties (see Chapter 5 ) and\nhandling of concurrency (see Chapter 7). In this chapter, we will concentrate only on\nthe differences in the data model.\nThe main arguments in favor of the document data model are schema flexibility, bet\u2010\nter performance due to locality, and that for some applications it is closer to the data\nstructures used by the application. The relational model counters by providing better\nsupport for joins, and many-to-one and many-to-many relationships.\nWhich data model leads to simpler application code?\nIf the data in your application has a document-like structure (i.e., a tree of one-to-\nmany relationships, where typically the entire tree is loaded at once), then it\u2019s proba\u2010\n38 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b2fa6b5-85bb-48b4-8cb9-71cea62e572b": {"__data__": {"id_": "1b2fa6b5-85bb-48b4-8cb9-71cea62e572b", "embedding": null, "metadata": {"page_label": "39", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30c6e9df-7d12-4968-b529-45020c041df0", "node_type": "4", "metadata": {"page_label": "39", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "feca1f786338c66e658b18badb46fbba4ad40a8de6d12cdb85127bb88e53c533", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "bly a good idea to use a document model. The relational technique of shredding\u2014\nsplitting a document-like structure into multiple tables (like positions, education,\nand contact_info in Figure 2-1)\u2014can lead to cumbersome schemas and unnecessa\u2010\nrily complicated application code.\nThe document model has limitations: for example, you cannot refer directly to a nes\u2010\nted item within a document, but instead you need to say something like \u201cthe second\nitem in the list of positions for user 251\u201d (much like an access path in the hierarchical\nmodel). However, as long as documents are not too deeply nested, that is not usually\na problem.\nThe poor support for joins in document databases may or may not be a problem,\ndepending on the application. For example, many-to-many relationships may never\nbe needed in an analytics application that uses a document database to record which\nevents occurred at which time [19].\nHowever, if your application does use many-to-many relationships, the document\nmodel becomes less appealing. It\u2019s possible to reduce the need for joins by denormal\u2010\nizing, but then the application code needs to do additional work to keep the denor\u2010\nmalized data consistent. Joins can be emulated in application code by making\nmultiple requests to the database, but that also moves complexity into the application\nand is usually slower than a join performed by specialized code inside the database.\nIn such cases, using a document model can lead to significantly more complex appli\u2010\ncation code and worse performance [15].\nIt\u2019s not possible to say in general which data model leads to simpler application code;\nit depends on the kinds of relationships that exist between data items. For highly\ninterconnected data, the document model is awkward, the relational model is accept\u2010\nable, and graph models (see \u201cGraph-Like Data Models\u201d on page 49) are the most\nnatural.\nSchema flexibility in the document model\nMost document databases, and the JSON support in relational databases, do not\nenforce any schema on the data in documents. XML support in relational databases\nusually comes with optional schema validation. No schema means that arbitrary keys\nand values can be added to a document, and when reading, clients have no guaran\u2010\ntees as to what fields the documents may contain.\nDocument databases are sometimes called schemaless, but that\u2019s misleading, as the\ncode that reads the data usually assumes some kind of structure\u2014i.e., there is an\nimplicit schema, but it is not enforced by the database [ 20]. A more accurate term is\nschema-on-read (the structure of the data is implicit, and only interpreted when the\ndata is read), in contrast with schema-on-write (the traditional approach of relational\nRelational Model Versus Document Model | 39", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3542ce5a-27a2-4979-91f6-7f0c3ff069bd": {"__data__": {"id_": "3542ce5a-27a2-4979-91f6-7f0c3ff069bd", "embedding": null, "metadata": {"page_label": "40", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b3a19c5-1ef0-4e37-b166-9dc1db7d6a37", "node_type": "4", "metadata": {"page_label": "40", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "66902bc1024d8567e1c3e814fc242d5c5f5aeb1f2cec7ac8b9913437e6f2085c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "databases, where the schema is explicit and the database ensures all written data con\u2010\nforms to it) [21].\nSchema-on-read is similar to dynamic (runtime) type checking in programming lan\u2010\nguages, whereas schema-on-write is similar to static (compile-time) type checking.\nJust as the advocates of static and dynamic type checking have big debates about their\nrelative merits [22], enforcement of schemas in database is a contentious topic, and in\ngeneral there\u2019s no right or wrong answer.\nThe difference between the approaches is particularly noticeable in situations where\nan application wants to change the format of its data. For example, say you are cur\u2010\nrently storing each user\u2019s full name in one field, and you instead want to store the\nfirst name and last name separately [ 23]. In a document database, you would just\nstart writing new documents with the new fields and have code in the application that\nhandles the case when old documents are read. For example:\nif (user && user.name && !user.first_name) {\n    // Documents written before Dec 8, 2013 don't have first_name\n    user.first_name = user.name.split(\" \")[0];\n}\nOn the other hand, in a \u201cstatically typed\u201d database schema, you would typically per\u2010\nform a migration along the lines of:\nALTER TABLE users ADD COLUMN first_name text;\nUPDATE users SET first_name = split_part(name, ' ', 1);      -- PostgreSQL\nUPDATE users SET first_name = substring_index(name, ' ', 1);      -- MySQL\nSchema changes have a bad reputation of being slow and requiring downtime. This\nreputation is not entirely deserved: most relational database systems execute the\nALTER TABLE statement in a few milliseconds. MySQL is a notable exception\u2014it\ncopies the entire table on ALTER TABLE, which can mean minutes or even hours of\ndowntime when altering a large table\u2014although various tools exist to work around\nthis limitation [24, 25, 26].\nRunning the UPDATE statement on a large table is likely to be slow on any database,\nsince every row needs to be rewritten. If that is not acceptable, the application can\nleave first_name set to its default of NULL and fill it in at read time, like it would with\na document database.\nThe schema-on-read approach is advantageous if the items in the collection don\u2019t all\nhave the same structure for some reason (i.e., the data is heterogeneous)\u2014for exam\u2010\nple, because:\n\u2022 There are many different types of objects, and it is not practical to put each type\nof object in its own table.\n40 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c762345-63b9-45d7-82bb-da58d3d1df34": {"__data__": {"id_": "8c762345-63b9-45d7-82bb-da58d3d1df34", "embedding": null, "metadata": {"page_label": "41", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3332c923-efb8-4443-a217-d8d349fe2a6d", "node_type": "4", "metadata": {"page_label": "41", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "25e9e6b3a36fa3b3406192679c153f2a9fa65f1dce0107340e0271a9346c0c0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The structure of the data is determined by external systems over which you have\nno control and which may change at any time.\nIn situations like these, a schema may hurt more than it helps, and schemaless docu\u2010\nments can be a much more natural data model. But in cases where all records are\nexpected to have the same structure, schemas are a useful mechanism for document\u2010\ning and enforcing that structure. We will discuss schemas and schema evolution in\nmore detail in Chapter 4.\nData locality for queries\nA document is usually stored as a single continuous string, encoded as JSON, XML,\nor a binary variant thereof (such as MongoDB\u2019s BSON). If your application often\nneeds to access the entire document (for example, to render it on a web page), there is\na performance advantage to this storage locality. If data is split across multiple tables,\nlike in Figure 2-1, multiple index lookups are required to retrieve it all, which may\nrequire more disk seeks and take more time.\nThe locality advantage only applies if you need large parts of the document at the\nsame time. The database typically needs to load the entire document, even if you\naccess only a small portion of it, which can be wasteful on large documents. On\nupdates to a document, the entire document usually needs to be rewritten\u2014only\nmodifications that don\u2019t change the encoded size of a document can easily be per\u2010\nformed in place [ 19]. For these reasons, it is generally recommended that you keep\ndocuments fairly small and avoid writes that increase the size of a document [ 9].\nThese performance limitations significantly reduce the set of situations in which\ndocument databases are useful.\nIt\u2019s worth pointing out that the idea of grouping related data together for locality is\nnot limited to the document model. For example, Google\u2019s Spanner database offers\nthe same locality properties in a relational data model, by allowing the schema to\ndeclare that a table\u2019s rows should be interleaved (nested) within a parent table [ 27].\nOracle allows the same, using a feature called multi-table index cluster tables  [28].\nThe column-family concept in the Bigtable data model (used in Cassandra and\nHBase) has a similar purpose of managing locality [29].\nWe will also see more on locality in Chapter 3.\nConvergence of document and relational databases\nMost relational database systems (other than MySQL) have supported XML since the\nmid-2000s. This includes functions to make local modifications to XML documents\nand the ability to index and query inside XML documents, which allows applications\nto use data models very similar to what they would do when using a document data\u2010\nbase.\nRelational Model Versus Document Model | 41", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9078716e-2a23-47d1-8f06-1504f8685bfe": {"__data__": {"id_": "9078716e-2a23-47d1-8f06-1504f8685bfe", "embedding": null, "metadata": {"page_label": "42", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64a28575-6889-4150-a276-f2b9ca6e08d7", "node_type": "4", "metadata": {"page_label": "42", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9641628789f8872b05cd875d68fc124bd582421411147a6198418ff3d2942bfd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "v. Codd\u2019s original description of the relational model [ 1] actually allowed something quite similar to JSON\ndocuments within a relational schema. He called it nonsimple domains. The idea was that a value in a row\ndoesn\u2019t have to just be a primitive datatype like a number or a string, but could also be a nested relation\n(table)\u2014so you can have an arbitrarily nested tree structure as a value, much like the JSON or XML support\nthat was added to SQL over 30 years later.\nPostgreSQL since version 9.3 [ 8], MySQL since version 5.7, and IBM DB2 since ver\u2010\nsion 10.5 [ 30] also have a similar level of support for JSON documents. Given the\npopularity of JSON for web APIs, it is likely that other relational databases will follow\nin their footsteps and add JSON support.\nOn the document database side, RethinkDB supports relational-like joins in its query\nlanguage, and some MongoDB drivers automatically resolve database references\n(effectively performing a client-side join, although this is likely to be slower than a\njoin performed in the database since it requires additional network round-trips and is\nless optimized).\nIt seems that relational and document databases are becoming more similar over\ntime, and that is a good thing: the data models complement each other. v If a database\nis able to handle document-like data and also perform relational queries on it, appli\u2010\ncations can use the combination of features that best fits their needs.\nA hybrid of the relational and document models is a good route for databases to take\nin the future. \nQuery Languages for Data\nWhen the relational model was introduced, it included a new way of querying data:\nSQL is a declarative query language, whereas IMS and CODASYL queried the data\u2010\nbase using imperative code. What does that mean?\nMany commonly used programming languages are imperative. For example, if you\nhave a list of animal species, you might write something like this to return only the\nsharks in the list:\nfunction getSharks() {\n    var sharks = [];\n    for (var i = 0; i < animals.length; i++) {\n        if (animals[i].family === \"Sharks\") {\n            sharks.push(animals[i]);\n        }\n    }\n    return sharks;\n}\nIn the relational algebra, you would instead write:\nsharks  =  \u03c3family = \u201cSharks\u201d (animals)\n42 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b712d5dc-fc0f-426a-a3a6-2199d6354fb3": {"__data__": {"id_": "b712d5dc-fc0f-426a-a3a6-2199d6354fb3", "embedding": null, "metadata": {"page_label": "43", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc608379-63df-4bb9-a540-7ade84f76ec3", "node_type": "4", "metadata": {"page_label": "43", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2414f747c5214a010cbd5fb435ec3fc2ee65992a204e1cbd0b9cb63ad6efa0b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "where \u03c3 (the Greek letter sigma) is the selection operator, returning only those ani\u2010\nmals that match the condition family = \u201cSharks\u201d.\nWhen SQL was defined, it followed the structure of the relational algebra fairly\nclosely:\nSELECT * FROM animals WHERE family = 'Sharks';\nAn imperative language tells the computer to perform certain operations in a certain\norder. You can imagine stepping through the code line by line, evaluating conditions,\nupdating variables, and deciding whether to go around the loop one more time.\nIn a declarative query language, like SQL or relational algebra, you just specify the\npattern of the data you want\u2014what conditions the results must meet, and how you\nwant the data to be transformed (e.g., sorted, grouped, and aggregated)\u2014but not how\nto achieve that goal. It is up to the database system\u2019s query optimizer to decide which\nindexes and which join methods to use, and in which order to execute various parts\nof the query.\nA declarative query language is attractive because it is typically more concise and eas\u2010\nier to work with than an imperative API. But more importantly, it also hides imple\u2010\nmentation details of the database engine, which makes it possible for the database\nsystem to introduce performance improvements without requiring any changes to\nqueries.\nFor example, in the imperative code shown at the beginning of this section, the list of\nanimals appears in a particular order. If the database wants to reclaim unused disk\nspace behind the scenes, it might need to move records around, changing the order in\nwhich the animals appear. Can the database do that safely, without breaking queries?\nThe SQL example doesn\u2019t guarantee any particular ordering, and so it doesn\u2019t mind if\nthe order changes. But if the query is written as imperative code, the database can\nnever be sure whether the code is relying on the ordering or not. The fact that SQL is\nmore limited in functionality gives the database much more room for automatic opti\u2010\nmizations.\nFinally, declarative languages often lend themselves to parallel execution. Today,\nCPUs are getting faster by adding more cores, not by running at significantly higher\nclock speeds than before [ 31]. Imperative code is very hard to parallelize across mul\u2010\ntiple cores and multiple machines, because it specifies instructions that must be per\u2010\nformed in a particular order. Declarative languages have a better chance of getting\nfaster in parallel execution because they specify only the pattern of the results, not the\nalgorithm that is used to determine the results. The database is free to use a parallel\nimplementation of the query language, if appropriate [32].\nQuery Languages for Data | 43", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7335fe9c-efe6-4626-8a31-4d8027db5439": {"__data__": {"id_": "7335fe9c-efe6-4626-8a31-4d8027db5439", "embedding": null, "metadata": {"page_label": "44", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3a94c32-7875-476e-af9a-fd1f1a3ae230", "node_type": "4", "metadata": {"page_label": "44", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c760c147f41105530dcbc6aa2d980284d7f88e3651c1d3224a5298d0107f9566", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Declarative Queries on the Web\nThe advantages of declarative query languages are not limited to just databases. To\nillustrate the point, let\u2019s compare declarative and imperative approaches in a com\u2010\npletely different environment: a web browser.\nSay you have a website about animals in the ocean. The user is currently viewing the\npage on sharks, so you mark the navigation item \u201cSharks\u201d as currently selected, like\nthis:\n<ul>\n    <li class=\"selected\"> \n        <p>Sharks</p> \n        <ul>\n            <li>Great White Shark</li>\n            <li>Tiger Shark</li>\n            <li>Hammerhead Shark</li>\n        </ul>\n    </li>\n    <li>\n        <p>Whales</p>\n        <ul>\n            <li>Blue Whale</li>\n            <li>Humpback Whale</li>\n            <li>Fin Whale</li>\n        </ul>\n    </li>\n</ul>\nThe selected item is marked with the CSS class \"selected\".\n<p>Sharks</p> is the title of the currently selected page.\nNow say you want the title of the currently selected page to have a blue background,\nso that it is visually highlighted. This is easy, using CSS:\nli.selected > p {\n    background-color: blue;\n}\nHere the CSS selector li.selected > p declares the pattern of elements to which we\nwant to apply the blue style: namely, all <p> elements whose direct parent is an <li>\nelement with a CSS class of selected. The element <p>Sharks</p> in the example\nmatches this pattern, but <p>Whales</p> does not match because its <li> parent\nlacks class=\"selected\".\n44 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36c08017-b2c6-436d-98c2-dac376ca0b05": {"__data__": {"id_": "36c08017-b2c6-436d-98c2-dac376ca0b05", "embedding": null, "metadata": {"page_label": "45", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c165f2d-39ac-4820-9d2f-05b833b94bc5", "node_type": "4", "metadata": {"page_label": "45", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f37b256d6c915c632014d85e5117fb9f6057a81b944bcac44b2ef676f943b523", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you were using XSL instead of CSS, you could do something similar:\n<xsl:template match=\"li[@class='selected']/p\">\n    <fo:block background-color=\"blue\">\n        <xsl:apply-templates/>\n    </fo:block>\n</xsl:template>\nHere, the XPath expression li[@class='selected']/p is equivalent to the CSS selec\u2010\ntor li.selected > p in the previous example. What CSS and XSL have in common\nis that they are both declarative languages for specifying the styling of a document.\nImagine what life would be like if you had to use an imperative approach. In Java\u2010\nScript, using the core Document Object Model (DOM) API, the result might look\nsomething like this:\nvar liElements = document.getElementsByTagName(\"li\");\nfor (var i = 0; i < liElements.length; i++) {\n    if (liElements[i].className === \"selected\") {\n        var children = liElements[i].childNodes;\n        for (var j = 0; j < children.length; j++) {\n            var child = children[j];\n            if (child.nodeType === Node.ELEMENT_NODE && child.tagName === \"P\") {\n                child.setAttribute(\"style\", \"background-color: blue\");\n            }\n        }\n    }\n}\nThis JavaScript imperatively sets the element <p>Sharks</p> to have a blue back\u2010\nground, but the code is awful. Not only is it much longer and harder to understand\nthan the CSS and XSL equivalents, but it also has some serious problems:\n\u2022 If the selected class is removed (e.g., because the user clicks a different page),\nthe blue color won\u2019t be removed, even if the code is rerun\u2014and so the item will\nremain highlighted until the entire page is reloaded. With CSS, the browser auto\u2010\nmatically detects when the li.selected > p rule no longer applies and removes\nthe blue background as soon as the selected class is removed.\n\u2022 If you want to take advantage of a new API, such as document.getElementsBy\nClassName(\"selected\") or even document.evaluate()\u2014which may improve\nperformance\u2014you have to rewrite the code. On the other hand, browser vendors\ncan improve the performance of CSS and XPath without breaking compatibility.\nQuery Languages for Data | 45", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10019a92-e280-4e2c-bc85-5c1a67dea947": {"__data__": {"id_": "10019a92-e280-4e2c-bc85-5c1a67dea947", "embedding": null, "metadata": {"page_label": "46", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39b200c9-1f24-4ed5-8ab3-1d71e1cd8ccd", "node_type": "4", "metadata": {"page_label": "46", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "77abb73070bae8db55b13b61237337b900d08c166b3b8a24fafe08ec98aa47b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vi. IMS and CODASYL both used imperative query APIs. Applications typically used COBOL code to iterate\nover records in the database, one record at a time [2, 16].\nIn a web browser, using declarative CSS styling is much better than manipulating\nstyles imperatively in JavaScript. Similarly, in databases, declarative query languages\nlike SQL turned out to be much better than imperative query APIs.vi\nMapReduce Querying\nMapReduce is a programming model for processing large amounts of data in bulk\nacross many machines, popularized by Google [ 33]. A limited form of MapReduce is\nsupported by some NoSQL datastores, including MongoDB and CouchDB, as a\nmechanism for performing read-only queries across many documents.\nMapReduce in general is described in more detail in Chapter 10. For now, we\u2019ll just\nbriefly discuss MongoDB\u2019s use of the model.\nMapReduce is neither a declarative query language nor a fully imperative query API,\nbut somewhere in between: the logic of the query is expressed with snippets of code,\nwhich are called repeatedly by the processing framework. It is based on the map (also\nknown as collect) and reduce (also known as fold or inject) functions that exist\nin many functional programming languages.\nTo give an example, imagine you are a marine biologist, and you add an observation\nrecord to your database every time you see animals in the ocean. Now you want to\ngenerate a report saying how many sharks you have sighted per month.\nIn PostgreSQL you might express that query like this:\nSELECT date_trunc('month', observation_timestamp) AS observation_month, \n       sum(num_animals) AS total_animals\nFROM observations\nWHERE family = 'Sharks'\nGROUP BY observation_month;\nThe date_trunc('month', timestamp) function determines the calendar month\ncontaining timestamp, and returns another timestamp representing the begin\u2010\nning of that month. In other words, it rounds a timestamp down to the nearest\nmonth.\nThis query first filters the observations to only show species in the Sharks family,\nthen groups the observations by the calendar month in which they occurred, and\nfinally adds up the number of animals seen in all observations in that month.\nThe same can be expressed with MongoDB\u2019s MapReduce feature as follows:\n46 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c8a5d68-d477-4c0a-ad3c-7b3663d069d5": {"__data__": {"id_": "8c8a5d68-d477-4c0a-ad3c-7b3663d069d5", "embedding": null, "metadata": {"page_label": "47", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72638843-dc45-402d-9618-a3a03b43bf9e", "node_type": "4", "metadata": {"page_label": "47", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "caab8b29d6734faf639b4e5b6f19d1b73868d4079d46988227424e67b2bb9e91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "db.observations.mapReduce(\n    function map() { \n        var year  = this.observationTimestamp.getFullYear();\n        var month = this.observationTimestamp.getMonth() + 1;\n        emit(year + \"-\" + month, this.numAnimals); \n    },\n    function reduce(key, values) { \n        return Array.sum(values); \n    },\n    {\n        query: { family: \"Sharks\" }, \n        out: \"monthlySharkReport\" \n    }\n);\nThe filter to consider only shark species can be specified declaratively (this is a\nMongoDB-specific extension to MapReduce).\nThe JavaScript function map is called once for every document that matches\nquery, with this set to the document object.\nThe map function emits a key (a string consisting of year and month, such as\n\"2013-12\" or \"2014-1\") and a value (the number of animals in that observation).\nThe key-value pairs emitted by map are grouped by key. For all key-value pairs\nwith the same key (i.e., the same month and year), the reduce function is called\nonce.\nThe reduce function adds up the number of animals from all observations in a\nparticular month.\nThe final output is written to the collection monthlySharkReport.\nFor example, say the observations collection contains these two documents:\n{\n    observationTimestamp: Date.parse(\"Mon, 25 Dec 1995 12:34:56 GMT\"),\n    family:     \"Sharks\",\n    species:    \"Carcharodon carcharias\",\n    numAnimals: 3\n}\n{\n    observationTimestamp: Date.parse(\"Tue, 12 Dec 1995 16:17:18 GMT\"),\n    family:     \"Sharks\",\n    species:    \"Carcharias taurus\",\n    numAnimals: 4\n}\nQuery Languages for Data | 47", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c66afff-e8e1-48fd-86b6-ee6e91d74bc0": {"__data__": {"id_": "0c66afff-e8e1-48fd-86b6-ee6e91d74bc0", "embedding": null, "metadata": {"page_label": "48", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8aa1a36-af32-4dd5-a440-42a0ffc1fa99", "node_type": "4", "metadata": {"page_label": "48", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1ce5abf43bb3f1f04e956fe222af19343d012540e1c4837b32f925a18377cacb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The map function would be called once for each document, resulting in\nemit(\"1995-12\", 3) and emit(\"1995-12\", 4). Subsequently, the reduce function\nwould be called with reduce(\"1995-12\", [3, 4]), returning 7.\nThe map and reduce functions are somewhat restricted in what they are allowed to\ndo. They must be pure functions, which means they only use the data that is passed to\nthem as input, they cannot perform additional database queries, and they must not\nhave any side effects. These restrictions allow the database to run the functions any\u2010\nwhere, in any order, and rerun them on failure. However, they are nevertheless pow\u2010\nerful: they can parse strings, call library functions, perform calculations, and more.\nMapReduce is a fairly low-level programming model for distributed execution on a\ncluster of machines. Higher-level query languages like SQL can be implemented as a\npipeline of MapReduce operations (see Chapter 10 ), but there are also many dis\u2010\ntributed implementations of SQL that don\u2019t use MapReduce. Note there is nothing in\nSQL that constrains it to running on a single machine, and MapReduce doesn\u2019t have\na monopoly on distributed query execution.\nBeing able to use JavaScript code in the middle of a query is a great feature for\nadvanced queries, but it\u2019s not limited to MapReduce\u2014some SQL databases can be\nextended with JavaScript functions too [34].\nA usability problem with MapReduce is that you have to write two carefully coordi\u2010\nnated JavaScript functions, which is often harder than writing a single query. More\u2010\nover, a declarative query language offers more opportunities for a query optimizer to\nimprove the performance of a query. For these reasons, MongoDB 2.2 added support\nfor a declarative query language called the aggregation pipeline [9]. In this language,\nthe same shark-counting query looks like this:\ndb.observations.aggregate([\n    { $match: { family: \"Sharks\" } },\n    { $group: {\n        _id: {\n            year:  { $year:  \"$observationTimestamp\" },\n            month: { $month: \"$observationTimestamp\" }\n        },\n        totalAnimals: { $sum: \"$numAnimals\" }\n    } }\n]);\nThe aggregation pipeline language is similar in expressiveness to a subset of SQL, but\nit uses a JSON-based syntax rather than SQL\u2019s English-sentence-style syntax; the dif\u2010\nference is perhaps a matter of taste. The moral of the story is that a NoSQL system\nmay find itself accidentally reinventing SQL, albeit in disguise. \n48 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b14badcd-adb7-44e1-aa5e-3282ffb91522": {"__data__": {"id_": "b14badcd-adb7-44e1-aa5e-3282ffb91522", "embedding": null, "metadata": {"page_label": "49", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf5a425c-53a0-43a8-812a-14d1e8963cbb", "node_type": "4", "metadata": {"page_label": "49", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f550289525db05866d5c221630a11a39ccd14ae89b8caa3d3311c23ccbe57cc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Graph-Like Data Models\nWe saw earlier that many-to-many relationships are an important distinguishing fea\u2010\nture between different data models. If your application has mostly one-to-many rela\u2010\ntionships (tree-structured data) or no relationships between records, the document\nmodel is appropriate.\nBut what if many-to-many relationships are very common in your data? The rela\u2010\ntional model can handle simple cases of many-to-many relationships, but as the con\u2010\nnections within your data become more complex, it becomes more natural to start\nmodeling your data as a graph.\nA graph consists of two kinds of objects: vertices (also known as nodes or entities) and\nedges (also known as relationships or arcs). Many kinds of data can be modeled as a\ngraph. Typical examples include:\nSocial graphs\nVertices are people, and edges indicate which people know each other.\nThe web graph\nVertices are web pages, and edges indicate HTML links to other pages.\nRoad or rail networks\nVertices are junctions, and edges represent the roads or railway lines between\nthem.\nWell-known algorithms can operate on these graphs: for example, car navigation sys\u2010\ntems search for the shortest path between two points in a road network, and\nPageRank can be used on the web graph to determine the popularity of a web page\nand thus its ranking in search results.\nIn the examples just given, all the vertices in a graph represent the same kind of thing\n(people, web pages, or road junctions, respectively). However, graphs are not limited\nto such homogeneous data: an equally powerful use of graphs is to provide a consis\u2010\ntent way of storing completely different types of objects in a single datastore. For\nexample, Facebook maintains a single graph with many different types of vertices and\nedges: vertices represent people, locations, events, checkins, and comments made by\nusers; edges indicate which people are friends with each other, which checkin hap\u2010\npened in which location, who commented on which post, who attended which event,\nand so on [35].\nIn this section we will use the example shown in Figure 2-5. It could be taken from a\nsocial network or a genealogical database: it shows two people, Lucy from Idaho and\nAlain from Beaune, France. They are married and living in London.\nGraph-Like Data Models | 49", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d90f68a-6203-4908-9625-587075dbff69": {"__data__": {"id_": "7d90f68a-6203-4908-9625-587075dbff69", "embedding": null, "metadata": {"page_label": "50", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10fe7c4e-0bda-4546-abe0-4b71ad91472f", "node_type": "4", "metadata": {"page_label": "50", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "33a77213d13d6c543018c9fce51f21d8577773e4914165ac65f2533ccc6fc19b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 2-5. Example of graph-structured data (boxes represent vertices, arrows repre\u2010\nsent edges).\nThere are several different, but related, ways of structuring and querying data in\ngraphs. In this section we will discuss the property graph  model (implemented by\nNeo4j, Titan, and InfiniteGraph) and the triple-store model (implemented by\nDatomic, AllegroGraph, and others). We will look at three declarative query lan\u2010\nguages for graphs: Cypher, SPARQL, and Datalog. Besides these, there are also\nimperative graph query languages such as Gremlin [ 36] and graph processing frame\u2010\nworks like Pregel (see Chapter 10).\nProperty Graphs\nIn the property graph model, each vertex consists of:\n\u2022 A unique identifier\n\u2022 A set of outgoing edges\n\u2022 A set of incoming edges\n\u2022 A collection of properties (key-value pairs)\nEach edge consists of:\n\u2022 A unique identifier\n\u2022 The vertex at which the edge starts (the tail vertex)\n50 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "775a2ebc-0d43-4d2c-af8b-2af5666f7edd": {"__data__": {"id_": "775a2ebc-0d43-4d2c-af8b-2af5666f7edd", "embedding": null, "metadata": {"page_label": "51", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b885a6a-9e08-44ed-a3aa-5aaca44778f1", "node_type": "4", "metadata": {"page_label": "51", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bd97ceccf493344fb7caace22c915f69b9ab11d7587d5f796b2693017ee5f09a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The vertex at which the edge ends (the head vertex)\n\u2022 A label to describe the kind of relationship between the two vertices\n\u2022 A collection of properties (key-value pairs)\nYou can think of a graph store as consisting of two relational tables, one for vertices\nand one for edges, as shown in Example 2-2 (this schema uses the PostgreSQL json\ndatatype to store the properties of each vertex or edge). The head and tail vertex are\nstored for each edge; if you want the set of incoming or outgoing edges for a vertex,\nyou can query the edges table by head_vertex or tail_vertex, respectively.\nExample 2-2. Representing a property graph using a relational schema\nCREATE TABLE vertices (\n    vertex_id   integer PRIMARY KEY,\n    properties  json\n);\nCREATE TABLE edges (\n    edge_id     integer PRIMARY KEY,\n    tail_vertex integer REFERENCES vertices (vertex_id),\n    head_vertex integer REFERENCES vertices (vertex_id),\n    label       text,\n    properties  json\n);\nCREATE INDEX edges_tails ON edges (tail_vertex);\nCREATE INDEX edges_heads ON edges (head_vertex);\nSome important aspects of this model are:\n1. Any vertex can have an edge connecting it with any other vertex. There is no\nschema that restricts which kinds of things can or cannot be associated.\n2. Given any vertex, you can efficiently find both its incoming and its outgoing\nedges, and thus traverse the graph\u2014i.e., follow a path through a chain of vertices\n\u2014both forward and backward. (That\u2019s why Example 2-2 has indexes on both the\ntail_vertex and head_vertex columns.)\n3. By using different labels for different kinds of relationships, you can store several\ndifferent kinds of information in a single graph, while still maintaining a clean\ndata model.\nThose features give graphs a great deal of flexibility for data modeling, as illustrated\nin Figure 2-5. The figure shows a few things that would be difficult to express in a\ntraditional relational schema, such as different kinds of regional structures in differ\u2010\nent countries (France has d\u00e9partements and r\u00e9gions, whereas the US has counties and\nstates), quirks of history such as a country within a country (ignoring for now the\nGraph-Like Data Models | 51", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbd5f333-e39e-4083-8f37-acdf35019a1d": {"__data__": {"id_": "dbd5f333-e39e-4083-8f37-acdf35019a1d", "embedding": null, "metadata": {"page_label": "52", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e62801d-7a35-4ff1-aa17-642319b35c71", "node_type": "4", "metadata": {"page_label": "52", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "989f7d2461a75eb683196dd320e712b17c1bb1d25415a833a8f008ade5921782", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "intricacies of sovereign states and nations), and varying granularity of data (Lucy\u2019s\ncurrent residence is specified as a city, whereas her place of birth is specified only at\nthe level of a state).\nYou could imagine extending the graph to also include many other facts about Lucy\nand Alain, or other people. For instance, you could use it to indicate any food aller\u2010\ngies they have (by introducing a vertex for each allergen, and an edge between a per\u2010\nson and an allergen to indicate an allergy), and link the allergens with a set of vertices\nthat show which foods contain which substances. Then you could write a query to\nfind out what is safe for each person to eat. Graphs are good for evolvability: as you\nadd features to your application, a graph can easily be extended to accommodate\nchanges in your application\u2019s data structures.\nThe Cypher Query Language\nCypher is a declarative query language for property graphs, created for the Neo4j\ngraph database [37]. (It is named after a character in the movie The Matrix and is not\nrelated to ciphers in cryptography [38].)\nExample 2-3 shows the Cypher query to insert the lefthand portion of Figure 2-5 into\na graph database. The rest of the graph can be added similarly and is omitted for\nreadability. Each vertex is given a symbolic name like USA or Idaho, and other parts of\nthe query can use those names to create edges between the vertices, using an arrow\nnotation: (Idaho) -[:WITHIN]-> (USA) creates an edge labeled WITHIN, with Idaho\nas the tail node and USA as the head node.\nExample 2-3. A subset of the data in Figure 2-5, represented as a Cypher query\nCREATE\n  (NAmerica:Location {name:'North America', type:'continent'}),\n  (USA:Location      {name:'United States', type:'country'  }),\n  (Idaho:Location    {name:'Idaho',         type:'state'    }),\n  (Lucy:Person       {name:'Lucy' }),\n  (Idaho) -[:WITHIN]->  (USA)  -[:WITHIN]-> (NAmerica),\n  (Lucy)  -[:BORN_IN]-> (Idaho)\nWhen all the vertices and edges of Figure 2-5 are added to the database, we can start\nasking interesting questions: for example, find the names of all the people who emigra\u2010\nted from the United States to Europe . To be more precise, here we want to find all the\nvertices that have a BORN_IN edge to a location within the US, and also a LIVING_IN\nedge to a location within Europe, and return the name property of each of those verti\u2010\nces.\nExample 2-4 shows how to express that query in Cypher. The same arrow notation is\nused in a MATCH clause to find patterns in the graph: (person) -[:BORN_IN]-> ()\n52 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c38e9b5-212c-4c9b-a68c-7bd0c93dd8e5": {"__data__": {"id_": "3c38e9b5-212c-4c9b-a68c-7bd0c93dd8e5", "embedding": null, "metadata": {"page_label": "53", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ff38da6-8d1f-4878-bc32-af2d9b203592", "node_type": "4", "metadata": {"page_label": "53", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9452105db8064b93c0ae4e118d786b041caf404257c4fb369889ae322301d76c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "matches any two vertices that are related by an edge labeled BORN_IN. The tail vertex\nof that edge is bound to the variable person, and the head vertex is left unnamed.\nExample 2-4. Cypher query to find people who emigrated from the US to Europe\nMATCH\n  (person) -[:BORN_IN]->  () -[:WITHIN*0..]-> (us:Location {name:'United States'}),\n  (person) -[:LIVES_IN]-> () -[:WITHIN*0..]-> (eu:Location {name:'Europe'})\nRETURN person.name\nThe query can be read as follows:\nFind any vertex (call it person) that meets both of the following conditions:\n1. person has an outgoing BORN_IN edge to some vertex. From that vertex, you can\nfollow a chain of outgoing WITHIN edges until eventually you reach a vertex of\ntype Location, whose name property is equal to \"United States\".\n2. That same person vertex also has an outgoing LIVES_IN edge. Following that\nedge, and then a chain of outgoing WITHIN edges, you eventually reach a vertex of\ntype Location, whose name property is equal to \"Europe\".\nFor each such person vertex, return the name property.\nThere are several possible ways of executing the query. The description given here\nsuggests that you start by scanning all the people in the database, examine each per\u2010\nson\u2019s birthplace and residence, and return only those people who meet the criteria.\nBut equivalently, you could start with the two Location vertices and work backward.\nIf there is an index on the name property, you can probably efficiently find the two\nvertices representing the US and Europe. Then you can proceed to find all locations\n(states, regions, cities, etc.) in the US and Europe respectively by following all incom\u2010\ning WITHIN edges. Finally, you can look for people who can be found through an\nincoming BORN_IN or LIVES_IN edge at one of the location vertices.\nAs is typical for a declarative query language, you don\u2019t need to specify such execu\u2010\ntion details when writing the query: the query optimizer automatically chooses the\nstrategy that is predicted to be the most efficient, so you can get on with writing the\nrest of your application.\nGraph Queries in SQL\nExample 2-2  suggested that graph data can be represented in a relational database.\nBut if we put graph data in a relational structure, can we also query it using SQL?\nThe answer is yes, but with some difficulty. In a relational database, you usually know\nin advance which joins you need in your query. In a graph query, you may need to\nGraph-Like Data Models | 53", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2dd7cb9f-5eab-4f78-b56b-5f0fe115a0db": {"__data__": {"id_": "2dd7cb9f-5eab-4f78-b56b-5f0fe115a0db", "embedding": null, "metadata": {"page_label": "54", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6527f2a-9a0c-425e-8591-58e749edeb0c", "node_type": "4", "metadata": {"page_label": "54", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8938b0f0b9e37d657c11049f3bec0760504dd52bd9eb977d478513ca247a723c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "traverse a variable number of edges before you find the vertex you\u2019re looking for\u2014\nthat is, the number of joins is not fixed in advance.\nIn our example, that happens in the () -[:WITHIN*0..]-> ()  rule in the Cypher\nquery. A person\u2019s LIVES_IN edge may point at any kind of location: a street, a city, a\ndistrict, a region, a state, etc. A city may be WITHIN a region, a region WITHIN a state, a\nstate WITHIN a country, etc. The LIVES_IN edge may point directly at the location ver\u2010\ntex you\u2019re looking for, or it may be several levels removed in the location hierarchy.\nIn Cypher, :WITHIN*0.. expresses that fact very concisely: it means \u201cfollow a WITHIN\nedge, zero or more times.\u201d It is like the * operator in a regular expression.\nSince SQL:1999, this idea of variable-length traversal paths in a query can be\nexpressed using something called recursive common table expressions  (the WITH\nRECURSIVE syntax). Example 2-5 shows the same query\u2014finding the names of people\nwho emigrated from the US to Europe\u2014expressed in SQL using this technique (sup\u2010\nported in PostgreSQL, IBM DB2, Oracle, and SQL Server). However, the syntax is\nvery clumsy in comparison to Cypher.\nExample 2-5. The same query as Example 2-4, expressed in SQL using recursive\ncommon table expressions\nWITH RECURSIVE\n  -- in_usa is the set of vertex IDs of all locations within the United States\n  in_usa(vertex_id) AS (\n      SELECT vertex_id FROM vertices WHERE properties->>'name' = 'United States' \n    UNION\n      SELECT edges.tail_vertex FROM edges \n        JOIN in_usa ON edges.head_vertex = in_usa.vertex_id\n        WHERE edges.label = 'within'\n  ),\n  -- in_europe is the set of vertex IDs of all locations within Europe\n  in_europe(vertex_id) AS (\n      SELECT vertex_id FROM vertices WHERE properties->>'name' = 'Europe' \n    UNION\n      SELECT edges.tail_vertex FROM edges\n        JOIN in_europe ON edges.head_vertex = in_europe.vertex_id\n        WHERE edges.label = 'within'\n  ),\n  -- born_in_usa is the set of vertex IDs of all people born in the US\n  born_in_usa(vertex_id) AS ( \n    SELECT edges.tail_vertex FROM edges\n      JOIN in_usa ON edges.head_vertex = in_usa.vertex_id\n      WHERE edges.label = 'born_in'\n  ),\n54 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "66c29b72-8350-431d-aece-5355ab7d69ec": {"__data__": {"id_": "66c29b72-8350-431d-aece-5355ab7d69ec", "embedding": null, "metadata": {"page_label": "55", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67a82af4-295d-4baa-88b8-dc93e18b293b", "node_type": "4", "metadata": {"page_label": "55", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6b90180544476179a13ab48b45e471176b988ef54bedaf3d4575b71ac704734e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "-- lives_in_europe is the set of vertex IDs of all people living in Europe\n  lives_in_europe(vertex_id) AS ( \n    SELECT edges.tail_vertex FROM edges\n      JOIN in_europe ON edges.head_vertex = in_europe.vertex_id\n      WHERE edges.label = 'lives_in'\n  )\nSELECT vertices.properties->>'name'\nFROM vertices\n-- join to find those people who were both born in the US *and* live in Europe\nJOIN born_in_usa     ON vertices.vertex_id = born_in_usa.vertex_id \nJOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id;\nFirst find the vertex whose name property has the value \"United States\", and\nmake it the first element of the set of vertices in_usa.\nFollow all incoming within edges from vertices in the set in_usa, and add them\nto the same set, until all incoming within edges have been visited.\nDo the same starting with the vertex whose name property has the value\n\"Europe\", and build up the set of vertices in_europe.\nFor each of the vertices in the set in_usa, follow incoming born_in edges to find\npeople who were born in some place within the United States.\nSimilarly, for each of the vertices in the set in_europe, follow incoming lives_in\nedges to find people who live in Europe.\nFinally, intersect the set of people born in the USA with the set of people living in\nEurope, by joining them.\nIf the same query can be written in 4 lines in one query language but requires 29 lines\nin another, that just shows that different data models are designed to satisfy different\nuse cases. It\u2019s important to pick a data model that is suitable for your application.\nTriple-Stores and SPARQL\nThe triple-store model is mostly equivalent to the property graph model, using differ\u2010\nent words to describe the same ideas. It is nevertheless worth discussing, because\nthere are various tools and languages for triple-stores that can be valuable additions\nto your toolbox for building applications.\nIn a triple-store, all information is stored in the form of very simple three-part state\u2010\nments: (subject, predicate, object). For example, in the triple ( Jim, likes, bananas), Jim\nis the subject, likes is the predicate (verb), and bananas is the object.\nGraph-Like Data Models | 55", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42f679b8-42d0-4ab7-9a90-4aeadfb05f06": {"__data__": {"id_": "42f679b8-42d0-4ab7-9a90-4aeadfb05f06", "embedding": null, "metadata": {"page_label": "56", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47957cf9-b8c1-4fce-9d12-1767f143269e", "node_type": "4", "metadata": {"page_label": "56", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "86cf07c1f5a7f31ce869bad5cb8b9448fb96f9a9673a4ecaaef828770c4c0b7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The subject of a triple is equivalent to a vertex in a graph. The object is one of two\nthings:\n1. A value in a primitive datatype, such as a string or a number. In that case, the\npredicate and object of the triple are equivalent to the key and value of a property\non the subject vertex. For example, ( lucy, age, 33) is like a vertex lucy with prop\u2010\nerties {\"age\":33}.\n2. Another vertex in the graph. In that case, the predicate is an edge in the graph,\nthe subject is the tail vertex, and the object is the head vertex. For example, in\n(lucy, marriedTo, alain) the subject and object lucy and alain are both vertices,\nand the predicate marriedTo is the label of the edge that connects them.\nExample 2-6 shows the same data as in Example 2-3, written as triples in a format\ncalled Turtle, a subset of Notation3 (N3) [39].\nExample 2-6. A subset of the data in Figure 2-5, represented as Turtle triples\n@prefix : <urn:example:>.\n_:lucy     a       :Person.\n_:lucy     :name   \"Lucy\".\n_:lucy     :bornIn _:idaho.\n_:idaho    a       :Location.\n_:idaho    :name   \"Idaho\".\n_:idaho    :type   \"state\".\n_:idaho    :within _:usa.\n_:usa      a       :Location.\n_:usa      :name   \"United States\".\n_:usa      :type   \"country\".\n_:usa      :within _:namerica.\n_:namerica a       :Location.\n_:namerica :name   \"North America\".\n_:namerica :type   \"continent\".\nIn this example, vertices of the graph are written as _:someName. The name doesn\u2019t\nmean anything outside of this file; it exists only because we otherwise wouldn\u2019t know\nwhich triples refer to the same vertex. When the predicate represents an edge, the\nobject is a vertex, as in _:idaho :within _:usa. When the predicate is a property,\nthe object is a string literal, as in _:usa :name \"United States\".\nIt\u2019s quite repetitive to repeat the same subject over and over again, but fortunately\nyou can use semicolons to say multiple things about the same subject. This makes the\nTurtle format quite nice and readable: see Example 2-7.\n56 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5fbf53b6-7261-46f2-a5ae-a7f1aca25ebf": {"__data__": {"id_": "5fbf53b6-7261-46f2-a5ae-a7f1aca25ebf", "embedding": null, "metadata": {"page_label": "57", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85c5bd4e-946d-4cd8-a653-51fec6567eb5", "node_type": "4", "metadata": {"page_label": "57", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "60c9f9e6b0766d0c0b6fe7d7d0a8dc9ddac3cd417128a558201ec4193e675a6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vii. Technically, Datomic uses 5-tuples rather than triples; the two additional fields are metadata for version\u2010\ning.\nExample 2-7. A more concise way of writing the data in Example 2-6\n@prefix : <urn:example:>.\n_:lucy     a :Person;   :name \"Lucy\";          :bornIn _:idaho.\n_:idaho    a :Location; :name \"Idaho\";         :type \"state\";   :within _:usa.\n_:usa      a :Location; :name \"United States\"; :type \"country\"; :within _:namerica.\n_:namerica a :Location; :name \"North America\"; :type \"continent\".\nThe semantic web\nIf you read more about triple-stores, you may get sucked into a maelstrom of articles\nwritten about the semantic web. The triple-store data model is completely independ\u2010\nent of the semantic web\u2014for example, Datomic [ 40] is a triple-store that does not\nclaim to have anything to do with it. vii But since the two are so closely linked in many\npeople\u2019s minds, we should discuss them briefly.\nThe semantic web is fundamentally a simple and reasonable idea: websites already\npublish information as text and pictures for humans to read, so why don\u2019t they also\npublish information as machine-readable data for computers to read? The Resource\nDescription Framework (RDF) [ 41] was intended as a mechanism for different web\u2010\nsites to publish data in a consistent format, allowing data from different websites to\nbe automatically combined into a web of data \u2014a kind of internet-wide \u201cdatabase of\neverything.\u201d\nUnfortunately, the semantic web was overhyped in the early 2000s but so far hasn\u2019t\nshown any sign of being realized in practice, which has made many people cynical\nabout it. It has also suffered from a dizzying plethora of acronyms, overly complex\nstandards proposals, and hubris.\nHowever, if you look past those failings, there is also a lot of good work that has come\nout of the semantic web project. Triples can be a good internal data model for appli\u2010\ncations, even if you have no interest in publishing RDF data on the semantic web.\nThe RDF data model\nThe Turtle language we used in Example 2-7  is a human-readable format for RDF\ndata. Sometimes RDF is also written in an XML format, which does the same thing\nmuch more verbosely\u2014see Example 2-8. Turtle/N3 is preferable as it is much easier\non the eyes, and tools like Apache Jena [42] can automatically convert between differ\u2010\nent RDF formats if necessary.\nGraph-Like Data Models | 57", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2364, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cdbb1031-2d5f-4f47-97b9-bfb06d6cdb5b": {"__data__": {"id_": "cdbb1031-2d5f-4f47-97b9-bfb06d6cdb5b", "embedding": null, "metadata": {"page_label": "58", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d22e775e-a19e-4e7f-b00e-06df2a4a163a", "node_type": "4", "metadata": {"page_label": "58", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b140008536a6137cec6b5c684430d391309d775153c79d39f2ce430310bdd842", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example 2-8. The data of Example 2-7, expressed using RDF/XML syntax\n<rdf:RDF xmlns=\"urn:example:\"\n    xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n  <Location rdf:nodeID=\"idaho\">\n    <name>Idaho</name>\n    <type>state</type>\n    <within>\n      <Location rdf:nodeID=\"usa\">\n        <name>United States</name>\n        <type>country</type>\n        <within>\n          <Location rdf:nodeID=\"namerica\">\n            <name>North America</name>\n            <type>continent</type>\n          </Location>\n        </within>\n      </Location>\n    </within>\n  </Location>\n  <Person rdf:nodeID=\"lucy\">\n    <name>Lucy</name>\n    <bornIn rdf:nodeID=\"idaho\"/>\n  </Person>\n</rdf:RDF>\nRDF has a few quirks due to the fact that it is designed for internet-wide data\nexchange. The subject, predicate, and object of a triple are often URIs. For example, a\npredicate might be an URI such as <http://my-company.com/namespace#within> or\n<http://my-company.com/namespace#lives_in>, rather than just WITHIN or\nLIVES_IN. The reasoning behind this design is that you should be able to combine\nyour data with someone else\u2019s data, and if they attach a different meaning to the word\nwithin or lives_in, you won\u2019t get a conflict because their predicates are actually\n<http://other.org/foo#within> and <http://other.org/foo#lives_in>.\nThe URL <http://my-company.com/namespace> doesn\u2019t necessarily need to resolve\nto anything\u2014from RDF\u2019s point of view, it is simply a namespace. To avoid potential\nconfusion with http:// URLs, the examples in this section use non-resolvable URIs\nsuch as urn:example:within. Fortunately, you can just specify this prefix once at the\ntop of the file, and then forget about it.\n58 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52dc0ab1-6f61-4e06-9298-0cca6ab08fa5": {"__data__": {"id_": "52dc0ab1-6f61-4e06-9298-0cca6ab08fa5", "embedding": null, "metadata": {"page_label": "59", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f92a0d6-1409-4419-a9c9-9ebed828fea4", "node_type": "4", "metadata": {"page_label": "59", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d984c2a0ea56645a8f60a91a40ab62066b085c97174f84abbe48ac4bc328ea71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The SPARQL query language\nSPARQL is a query language for triple-stores using the RDF data model [ 43]. (It is an\nacronym for SPARQL Protocol and RDF Query Language , pronounced \u201csparkle.\u201d) It\npredates Cypher, and since Cypher\u2019s pattern matching is borrowed from SPARQL,\nthey look quite similar [37].\nThe same query as before\u2014finding people who have moved from the US to Europe\u2014\nis even more concise in SPARQL than it is in Cypher (see Example 2-9).\nExample 2-9. The same query as Example 2-4, expressed in SPARQL\nPREFIX : <urn:example:>\nSELECT ?personName WHERE {\n  ?person :name ?personName.\n  ?person :bornIn  / :within* / :name \"United States\".\n  ?person :livesIn / :within* / :name \"Europe\".\n}\nThe structure is very similar. The following two expressions are equivalent (variables\nstart with a question mark in SPARQL):\n(person) -[:BORN_IN]-> () -[:WITHIN*0..]-> (location)   # Cypher\n?person :bornIn / :within* ?location.                   # SPARQL\nBecause RDF doesn\u2019t distinguish between properties and edges but just uses predi\u2010\ncates for both, you can use the same syntax for matching properties. In the following\nexpression, the variable usa is bound to any vertex that has a name property whose\nvalue is the string \"United States\":\n(usa {name:'United States'})   # Cypher\n?usa :name \"United States\".    # SPARQL\nSPARQL is a nice query language\u2014even if the semantic web never happens, it can be\na powerful tool for applications to use internally. \nGraph-Like Data Models | 59", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "251ada01-d425-451f-a112-79422e782dad": {"__data__": {"id_": "251ada01-d425-451f-a112-79422e782dad", "embedding": null, "metadata": {"page_label": "60", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58a41555-a20a-42a9-9b7b-9f4ab164f94f", "node_type": "4", "metadata": {"page_label": "60", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0edd3601e8b6cdf1ab0d8f449fd46b51b85d62004eb157186c6c064896de76b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "viii. Datomic and Cascalog use a Clojure S-expression syntax for Datalog. In the following examples we use a\nProlog syntax, which is a little easier to read, but this makes no functional difference.\nGraph Databases Compared to the Network Model\nIn \u201cAre Document Databases Repeating History?\u201d on page 36 we discussed how\nCODASYL and the relational model competed to solve the problem of many-to-\nmany relationships in IMS. At first glance, CODASYL\u2019s network model looks similar\nto the graph model. Are graph databases the second coming of CODASYL in\ndisguise?\nNo. They differ in several important ways:\n\u2022 In CODASYL, a database had a schema that specified which record type could be\nnested within which other record type. In a graph database, there is no such\nrestriction: any vertex can have an edge to any other vertex. This gives much\ngreater flexibility for applications to adapt to changing requirements.\n\u2022 In CODASYL, the only way to reach a particular record was to traverse one of\nthe access paths to it. In a graph database, you can refer directly to any vertex by\nits unique ID, or you can use an index to find vertices with a particular value.\n\u2022 In CODASYL, the children of a record were an ordered set, so the database had\nto maintain that ordering (which had consequences for the storage layout) and\napplications that inserted new records into the database had to worry about the\npositions of the new records in these sets. In a graph database, vertices and edges\nare not ordered (you can only sort the results when making a query).\n\u2022 In CODASYL, all queries were imperative, difficult to write and easily broken by\nchanges in the schema. In a graph database, you can write your traversal in\nimperative code if you want to, but most graph databases also support high-level,\ndeclarative query languages such as Cypher or SPARQL.\nThe Foundation: Datalog\nDatalog is a much older language than SPARQL or Cypher, having been studied\nextensively by academics in the 1980s [ 44, 45, 46]. It is less well known among soft\u2010\nware engineers, but it is nevertheless important, because it provides the foundation\nthat later query languages build upon.\nIn practice, Datalog is used in a few data systems: for example, it is the query lan\u2010\nguage of Datomic [ 40], and Cascalog [ 47] is a Datalog implementation for querying\nlarge datasets in Hadoop.viii\n60 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e8b47db-184c-4cf3-b0e4-57b0110ff489": {"__data__": {"id_": "2e8b47db-184c-4cf3-b0e4-57b0110ff489", "embedding": null, "metadata": {"page_label": "61", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1fb6742f-57e9-41d5-8c22-274b9d3c669d", "node_type": "4", "metadata": {"page_label": "61", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2358ea03bcaa79617896563d91c512ee19abd2cfb05119888ebceee57ea7fcf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Datalog\u2019s data model is similar to the triple-store model, generalized a bit. Instead of\nwriting a triple as ( subject, predicate, object), we write it as predicate(subject, object).\nExample 2-10 shows how to write the data from our example in Datalog.\nExample 2-10. A subset of the data in Figure 2-5, represented as Datalog facts\nname(namerica, 'North America').\ntype(namerica, continent).\nname(usa, 'United States').\ntype(usa, country).\nwithin(usa, namerica).\nname(idaho, 'Idaho').\ntype(idaho, state).\nwithin(idaho, usa).\nname(lucy, 'Lucy').\nborn_in(lucy, idaho).\nNow that we have defined the data, we can write the same query as before, as shown\nin Example 2-11. It looks a bit different from the equivalent in Cypher or SPARQL,\nbut don\u2019t let that put you off. Datalog is a subset of Prolog, which you might have\nseen before if you\u2019ve studied computer science.\nExample 2-11. The same query as Example 2-4, expressed in Datalog\nwithin_recursive(Location, Name) :- name(Location, Name).     /* Rule 1 */\nwithin_recursive(Location, Name) :- within(Location, Via),    /* Rule 2 */\n                                    within_recursive(Via, Name).\nmigrated(Name, BornIn, LivingIn) :- name(Person, Name),       /* Rule 3 */\n                                    born_in(Person, BornLoc),\n                                    within_recursive(BornLoc, BornIn),\n                                    lives_in(Person, LivingLoc),\n                                    within_recursive(LivingLoc, LivingIn).\n?- migrated(Who, 'United States', 'Europe').\n/* Who = 'Lucy'. */\nCypher and SPARQL jump in right away with SELECT, but Datalog takes a small step\nat a time. We define rules that tell the database about new predicates: here, we define\ntwo new predicates, within_recursive and migrated. These predicates aren\u2019t triples\nstored in the database, but instead they are derived from data or from other rules.\nRules can refer to other rules, just like functions can call other functions or recur\u2010\nsively call themselves. Like this, complex queries can be built up a small piece at a\ntime.\nGraph-Like Data Models | 61", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bdf1dc6-99d4-45f5-83b4-ebc18ecda7c9": {"__data__": {"id_": "2bdf1dc6-99d4-45f5-83b4-ebc18ecda7c9", "embedding": null, "metadata": {"page_label": "62", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd6c2965-d1ba-482e-aee1-51aaa8fca60b", "node_type": "4", "metadata": {"page_label": "62", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "52a9d738a718bb4f92c73a266d022600c453ef184cf163eaf4c84467dddce935", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In rules, words that start with an uppercase letter are variables, and predicates are\nmatched like in Cypher and SPARQL. For example, name(Location, Name) matches\nthe triple name(namerica, 'North America')  with variable bindings Location =\nnamerica and Name = 'North America'.\nA rule applies if the system can find a match for all predicates on the righthand side\nof the :- operator. When the rule applies, it\u2019s as though the lefthand side of the :-\nwas added to the database (with variables replaced by the values they matched).\nOne possible way of applying the rules is thus:\n1. name(namerica, 'North America') exists in the database, so rule 1 applies. It\ngenerates within_recursive(namerica, 'North America').\n2. within(usa, namerica) exists in the database and the previous step generated\nwithin_recursive(namerica, 'North America'), so rule 2 applies. It generates\nwithin_recursive(usa, 'North America').\n3. within(idaho, usa) exists in the database and the previous step generated\nwithin_recursive(usa, 'North America') , so rule 2 applies. It generates\nwithin_recursive(idaho, 'North America').\nBy repeated application of rules 1 and 2, the within_recursive predicate can tell us\nall the locations in North America (or any other location name) contained in our\ndatabase. This process is illustrated in Figure 2-6.\nFigure 2-6. Determining that Idaho is in North America, using the Datalog rules from\nExample 2-11.\nNow rule 3 can find people who were born in some location BornIn and live in some\nlocation LivingIn. By querying with BornIn = 'United States'  and LivingIn =\n'Europe', and leaving the person as a variable Who, we ask the Datalog system to find\nout which values can appear for the variable Who. So, finally we get the same answer as\nin the earlier Cypher and SPARQL queries.\n62 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1844, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "059042a4-b304-419e-8331-d94647c3e944": {"__data__": {"id_": "059042a4-b304-419e-8331-d94647c3e944", "embedding": null, "metadata": {"page_label": "63", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75fe0343-1e02-4bcc-81bb-85c584e638dd", "node_type": "4", "metadata": {"page_label": "63", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0d08678131d0d0d0883504badb1aa1362f4f6d6fae42135cbc90e0e06a7e431a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Datalog approach requires a different kind of thinking to the other query lan\u2010\nguages discussed in this chapter, but it\u2019s a very powerful approach, because rules can\nbe combined and reused in different queries. It\u2019s less convenient for simple one-off\nqueries, but it can cope better if your data is complex. \nSummary\nData models are a huge subject, and in this chapter we have taken a quick look at a\nbroad variety of different models. We didn\u2019t have space to go into all the details of\neach model, but hopefully the overview has been enough to whet your appetite to\nfind out more about the model that best fits your application\u2019s requirements.\nHistorically, data started out being represented as one big tree (the hierarchical\nmodel), but that wasn\u2019t good for representing many-to-many relationships, so the\nrelational model was invented to solve that problem. More recently, developers found\nthat some applications don\u2019t fit well in the relational model either. New nonrelational\n\u201cNoSQL\u201d datastores have diverged in two main directions:\n1. Document databases target use cases where data comes in self-contained docu\u2010\nments and relationships between one document and another are rare.\n2. Graph databases go in the opposite direction, targeting use cases where anything\nis potentially related to everything.\nAll three models (document, relational, and graph) are widely used today, and each is\ngood in its respective domain. One model can be emulated in terms of another model\n\u2014for example, graph data can be represented in a relational database\u2014but the result\nis often awkward. That\u2019s why we have different systems for different purposes, not a\nsingle one-size-fits-all solution.\nOne thing that document and graph databases have in common is that they typically\ndon\u2019t enforce a schema for the data they store, which can make it easier to adapt\napplications to changing requirements. However, your application most likely still\nassumes that data has a certain structure; it\u2019s just a question of whether the schema is\nexplicit (enforced on write) or implicit (handled on read).\nEach data model comes with its own query language or framework, and we discussed\nseveral examples: SQL, MapReduce, MongoDB\u2019s aggregation pipeline, Cypher,\nSPARQL, and Datalog. We also touched on CSS and XSL/XPath, which aren\u2019t data\u2010\nbase query languages but have interesting parallels.\nAlthough we have covered a lot of ground, there are still many data models left\nunmentioned. To give just a few brief examples:\n\u2022 Researchers working with genome data often need to perform sequence-\nsimilarity searches , which means taking one very long string (representing a\nSummary | 63", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c24b5add-3b39-4c58-94af-e7a178e4cff1": {"__data__": {"id_": "c24b5add-3b39-4c58-94af-e7a178e4cff1", "embedding": null, "metadata": {"page_label": "64", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59494823-b19d-403b-ad05-1360189fd0e7", "node_type": "4", "metadata": {"page_label": "64", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bfc6df80bdbb7faa12d6019f80b167f317deea8ef6112fcc046ef14e34ce69e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DNA molecule) and matching it against a large database of strings that are simi\u2010\nlar, but not identical. None of the databases described here can handle this kind\nof usage, which is why researchers have written specialized genome database\nsoftware like GenBank [48].\n\u2022 Particle physicists have been doing Big Data\u2013style large-scale data analysis for\ndecades, and projects like the Large Hadron Collider (LHC) now work with hun\u2010\ndreds of petabytes! At such a scale custom solutions are required to stop the\nhardware cost from spiraling out of control [49].\n\u2022 Full-text search is arguably a kind of data model that is frequently used alongside\ndatabases. Information retrieval is a large specialist subject that we won\u2019t cover in\ngreat detail in this book, but we\u2019ll touch on search indexes in Chapter 3  and\nPart III.\nWe have to leave it there for now. In the next chapter we will discuss some of the\ntrade-offs that come into play when implementing the data models described in this\nchapter. \nReferences\n[1] Edgar F. Codd: \u201c A Relational Model of Data for Large Shared Data Banks ,\u201d Com\u2010\nmunications of the ACM , volume 13, number 6, pages 377\u2013387, June 1970. doi:\n10.1145/362384.362685\n[2] Michael Stonebraker and Joseph M. Hellerstein: \u201c What Goes Around Comes\nAround,\u201d in Readings in Database Systems, 4th edition, MIT Press, pages 2\u201341, 2005.\nISBN: 978-0-262-69314-1\n[3] Pramod J. Sadalage and Martin Fowler: NoSQL Distilled. Addison-Wesley, August\n2012. ISBN: 978-0-321-82662-6\n[4] Eric Evans: \u201cNoSQL: What\u2019s in a Name?,\u201d blog.sym-link.com, October 30, 2009.\n[5] James Phillips: \u201c Surprises in Our NoSQL Adoption Survey ,\u201d blog.couchbase.com,\nFebruary 8, 2012.\n[6] Michael Wagner: SQL/XML:2006 \u2013 Evaluierung der Standardkonformit\u00e4t ausge\u2010\nw\u00e4hlter Datenbanksysteme . Diplomica Verlag, Hamburg, 2010. ISBN:\n978-3-836-64609-3\n[7] \u201c XML Data in SQL Server ,\u201d SQL Server 2012 documentation, technet.micro\u2010\nsoft.com, 2013.\n[8] \u201c PostgreSQL 9.3.1 Documentation ,\u201d The PostgreSQL Global Development\nGroup, 2013.\n[9] \u201cThe MongoDB 2.4 Manual,\u201d MongoDB, Inc., 2013.\n64 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bf032ee-2faf-4376-8694-8fa7854745cd": {"__data__": {"id_": "1bf032ee-2faf-4376-8694-8fa7854745cd", "embedding": null, "metadata": {"page_label": "65", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d0fecaa-1342-4773-9153-ce1f0673bb71", "node_type": "4", "metadata": {"page_label": "65", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5f7e592d25b83d679942d5d3df83e8d831ba98bfcad25dc5d35621b9984b0894", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[10] \u201cRethinkDB 1.11 Documentation,\u201d rethinkdb.com, 2013.\n[11] \u201cApache CouchDB 1.6 Documentation,\u201d docs.couchdb.org, 2014.\n[12] Lin Qiao, Kapil Surlaker, Shirshanka Das, et al.: \u201c On Brewing Fresh Espresso:\nLinkedIn\u2019s Distributed Data Serving Platform ,\u201d at ACM International Conference on\nManagement of Data (SIGMOD), June 2013.\n[13] Rick Long, Mark Harrington, Robert Hain, and Geoff Nicholls: IMS Primer .\nIBM Redbook SG24-5352-00, IBM International Technical Support Organization,\nJanuary 2000.\n[14] Stephen D. Bartlett: \u201c IBM\u2019s IMS\u2014Myths, Realities, and Opportunities ,\u201d The\nClipper Group Navigator, TCG2013015LI, July 2013.\n[15] Sarah Mei: \u201c Why You Should Never Use MongoDB ,\u201d sarahmei.com, November\n11, 2013.\n[16] J. S. Knowles and D. M. R. Bell: \u201cThe CODASYL Model,\u201d in Databases\u2014Role\nand Structure: An Advanced Course, edited by P. M. Stocker, P. M. D. Gray, and M. P.\nAtkinson, pages 19\u201356, Cambridge University Press, 1984. ISBN: 978-0-521-25430-4\n[17] Charles W. Bachman: \u201c The Programmer as Navigator ,\u201d Communications of the\nACM, volume 16, number 11, pages 653\u2013658, November 1973. doi:\n10.1145/355611.362534\n[18] Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton: \u201c Architecture\nof a Database System ,\u201d Foundations and Trends in Databases , volume 1, number 2,\npages 141\u2013259, November 2007. doi:10.1561/1900000002\n[19] Sandeep Parikh and Kelly Stirman: \u201c Schema Design for Time Series Data in\nMongoDB,\u201d blog.mongodb.org, October 30, 2013.\n[20] Martin Fowler: \u201c Schemaless Data Structures ,\u201d martinfowler.com, January 7,\n2013.\n[21] Amr Awadallah: \u201c Schema-on-Read vs. Schema-on-Write ,\u201d at Berkeley EECS\nRAD Lab Retreat, Santa Cruz, CA, May 2009.\n[22] Martin Odersky: \u201cThe Trouble with Types,\u201d at Strange Loop, September 2013.\n[23] Conrad Irwin: \u201c MongoDB\u2014Confessions of a PostgreSQL Lover ,\u201d at\nHTML5DevConf, October 2013.\n[24] \u201c Percona Toolkit Documentation: pt-online-schema-change ,\u201d Percona Ireland\nLtd., 2013.\n[25] Rany Keddo, Tobias Bielohlawek, and Tobias Schmidt: \u201c Large Hadron Migra\u2010\ntor,\u201d SoundCloud, 2013.\nSummary | 65", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "515847bb-a4cb-49e0-9d2b-3d71bb4a486d": {"__data__": {"id_": "515847bb-a4cb-49e0-9d2b-3d71bb4a486d", "embedding": null, "metadata": {"page_label": "66", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0202601-d35a-4778-97e8-1416cb1d40b0", "node_type": "4", "metadata": {"page_label": "66", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d68860def14560b2ad8f2815c9a32177ce48ff8b28c0a7944e6a5516ce1f34c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[26] Shlomi Noach: \u201c gh-ost: GitHub\u2019s Online Schema Migration Tool for MySQL ,\u201d\ngithubengineering.com, August 1, 2016.\n[27] James C. Corbett, Jeffrey Dean, Michael Epstein, et al.: \u201c Spanner: Google\u2019s\nGlobally-Distributed Database ,\u201d at 10th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), October 2012.\n[28] Donald K. Burleson: \u201cReduce I/O with Oracle Cluster Tables,\u201d dba-oracle.com.\n[29] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, et al.: \u201c Bigtable: A Distributed Stor\u2010\nage System for Structured Data ,\u201d at 7th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), November 2006.\n[30] Bobbie J. Cochrane and Kathy A. McKnight: \u201c DB2 JSON Capabilities, Part 1:\nIntroduction to DB2 JSON,\u201d IBM developerWorks, June 20, 2013.\n[31] Herb Sutter: \u201c The Free Lunch Is Over: A Fundamental Turn Toward Concur\u2010\nrency in Software ,\u201d Dr. Dobb\u2019s Journal , volume 30, number 3, pages 202-210, March\n2005.\n[32] Joseph M. Hellerstein: \u201c The Declarative Imperative: Experiences and Conjec\u2010\ntures in Distributed Logic ,\u201d Electrical Engineering and Computer Sciences, Univer\u2010\nsity of California at Berkeley, Tech report UCB/EECS-2010-90, June 2010.\n[33] Jeffrey Dean and Sanjay Ghemawat: \u201cMapReduce: Simplified Data Processing on\nLarge Clusters,\u201d at 6th USENIX Symposium on Operating System Design and Imple\u2010\nmentation (OSDI), December 2004.\n[34] Craig Kerstiens: \u201cJavaScript in Your Postgres,\u201d blog.heroku.com, June 5, 2013.\n[35] Nathan Bronson, Zach Amsden, George Cabrera, et al.: \u201c TAO: Facebook\u2019s Dis\u2010\ntributed Data Store for the Social Graph ,\u201d at USENIX Annual Technical Conference\n(USENIX ATC), June 2013.\n[36] \u201cApache TinkerPop3.2.3 Documentation,\u201d tinkerpop.apache.org, October 2016.\n[37] \u201cThe Neo4j Manual v2.0.0,\u201d Neo Technology, 2013.\n[38] Emil Eifrem: Twitter correspondence, January 3, 2014.\n[39] David Beckett and Tim Berners-Lee: \u201c Turtle \u2013 Terse RDF Triple Language ,\u201d\nW3C Team Submission, March 28, 2011.\n[40] \u201cDatomic Development Resources,\u201d Metadata Partners, LLC, 2013.\n[41] W3C RDF Working Group: \u201c Resource Description Framework (RDF) ,\u201d w3.org,\n10 February 2004.\n[42] \u201cApache Jena,\u201d Apache Software Foundation.\n66 | Chapter 2: Data Models and Query Languages", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87e16443-3e79-4601-bb84-50c6cbfc0139": {"__data__": {"id_": "87e16443-3e79-4601-bb84-50c6cbfc0139", "embedding": null, "metadata": {"page_label": "67", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47496805-f1bd-4ac5-9b88-61e65eab326f", "node_type": "4", "metadata": {"page_label": "67", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6c8ec8384d53b42e1d680f0e7b9954cb8eb90164e1c4a5f0e6563b8cc9cd7372", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[43] Steve Harris, Andy Seaborne, and Eric Prud\u2019hommeaux: \u201c SPARQL 1.1 Query\nLanguage,\u201d W3C Recommendation, March 2013.\n[44] Todd J. Green, Shan Shan Huang, Boon Thau Loo, and Wenchao Zhou: \u201c Data\u2010\nlog and Recursive Query Processing ,\u201d Foundations and Trends in Databases , volume\n5, number 2, pages 105\u2013195, November 2013. doi:10.1561/1900000017\n[45] Stefano Ceri, Georg Gottlob, and Letizia Tanca: \u201c What You Always Wanted to\nKnow About Datalog (And Never Dared to Ask) ,\u201d IEEE Transactions on Knowledge\nand Data Engineering , volume 1, number 1, pages 146\u2013166, March 1989. doi:\n10.1109/69.43410\n[46] Serge Abiteboul, Richard Hull, and Victor Vianu: Foundations of Databases .\nAddison-Wesley, 1995. ISBN: 978-0-201-53771-0, available online at web\u2010\ndam.inria.fr/Alice\n[47] Nathan Marz: \u201cCascalog,\u201d cascalog.org.\n[48] Dennis A. Benson, Ilene Karsch-Mizrachi, David J. Lipman, et al.: \u201c GenBank,\u201d\nNucleic Acids Research, volume 36, Database issue, pages D25\u2013D30, December 2007.\ndoi:10.1093/nar/gkm929\n[49] Fons Rademakers: \u201c ROOT for Big Data Analysis ,\u201d at Workshop on the Future of\nBig Data Management, London, UK, June 2013.\nSummary | 67", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2e3068a-e3cf-4456-8edd-365a3f0f9296": {"__data__": {"id_": "e2e3068a-e3cf-4456-8edd-365a3f0f9296", "embedding": null, "metadata": {"page_label": "68", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8016b10d-8686-4b02-aabf-7264c1e5dd51", "node_type": "4", "metadata": {"page_label": "68", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d666d74e-6203-4dfa-a846-13a88a489434": {"__data__": {"id_": "d666d74e-6203-4dfa-a846-13a88a489434", "embedding": null, "metadata": {"page_label": "69", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23d8dc60-6c7d-444b-bd39-f2695060e598", "node_type": "4", "metadata": {"page_label": "69", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "96855862a2113a7e1eba881587d5960fa155d614d7a690fa4db8bd1e727b216c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 3\nStorage and Retrieval\nWer Ordnung h\u00e4lt, ist nur zu faul zum Suchen.\n(If you keep things tidily ordered, you\u2019re just too lazy to go searching.)\n\u2014German proverb\nOn the most fundamental level, a database needs to do two things: when you give it\nsome data, it should store the data, and when you ask it again later, it should give the\ndata back to you.\nIn Chapter 2  we discussed data models and query languages\u2014i.e., the format in\nwhich you (the application developer) give the database your data, and the mecha\u2010\nnism by which you can ask for it again later. In this chapter we discuss the same from\nthe database\u2019s point of view: how we can store the data that we\u2019re given, and how we\ncan find it again when we\u2019re asked for it.\nWhy should you, as an application developer, care how the database handles storage\nand retrieval internally? You\u2019re probably not going to implement your own storage\nengine from scratch, but you do need to select a storage engine that is appropriate for\nyour application, from the many that are available. In order to tune a storage engine\nto perform well on your kind of workload, you need to have a rough idea of what the\nstorage engine is doing under the hood.\nIn particular, there is a big difference between storage engines that are optimized for\ntransactional workloads and those that are optimized for analytics. We will explore\nthat distinction later in \u201cTransaction Processing or Analytics?\u201d on page 90, and in\n\u201cColumn-Oriented Storage\u201d on page 95 we\u2019ll discuss a family of storage engines that\nis optimized for analytics.\nHowever, first we\u2019ll start this chapter by talking about storage engines that are used in\nthe kinds of databases that you\u2019re probably familiar with: traditional relational data\u2010\nbases, and also most so-called NoSQL databases. We will examine two families of\n69", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6cde527f-9cab-454b-bf45-3e622a7976e8": {"__data__": {"id_": "6cde527f-9cab-454b-bf45-3e622a7976e8", "embedding": null, "metadata": {"page_label": "70", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c15c24fc-48d4-4048-8a31-5b13804b3eac", "node_type": "4", "metadata": {"page_label": "70", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ec5c69d731af74c27cba9946f7ae4553b975dbbf1911f4e67e524cf85e2ee07e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "storage engines: log-structured storage engines, and page-oriented storage engines\nsuch as B-trees.\nData Structures That Power Your Database\nConsider the world\u2019s simplest database, implemented as two Bash functions:\n#!/bin/bash\ndb_set () {\n    echo \"$1,$2\" >> database\n}\ndb_get () {\n    grep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1\n}\nThese two functions implement a key-value store. You can call db_set key value,\nwhich will store key and value in the database. The key and value can be (almost)\nanything you like\u2014for example, the value could be a JSON document. You can then\ncall db_get key, which looks up the most recent value associated with that particular\nkey and returns it.\nAnd it works:\n$ db_set 123456 '{\"name\":\"London\",\"attractions\":[\"Big Ben\",\"London Eye\"]}'\n$ db_set 42 '{\"name\":\"San Francisco\",\"attractions\":[\"Golden Gate Bridge\"]}'\n$ db_get 42\n{\"name\":\"San Francisco\",\"attractions\":[\"Golden Gate Bridge\"]}\nThe underlying storage format is very simple: a text file where each line contains a\nkey-value pair, separated by a comma (roughly like a CSV file, ignoring escaping\nissues). Every call to db_set appends to the end of the file, so if you update a key sev\u2010\neral times, the old versions of the value are not overwritten\u2014you need to look at the\nlast occurrence of a key in a file to find the latest value (hence the tail -n 1  in\ndb_get):\n$ db_set 42 '{\"name\":\"San Francisco\",\"attractions\":[\"Exploratorium\"]}'\n$ db_get 42\n{\"name\":\"San Francisco\",\"attractions\":[\"Exploratorium\"]}\n$ cat database\n123456,{\"name\":\"London\",\"attractions\":[\"Big Ben\",\"London Eye\"]}\n42,{\"name\":\"San Francisco\",\"attractions\":[\"Golden Gate Bridge\"]}\n42,{\"name\":\"San Francisco\",\"attractions\":[\"Exploratorium\"]}\n70 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57d88b82-34ef-4dca-bf49-47cf0f71e03e": {"__data__": {"id_": "57d88b82-34ef-4dca-bf49-47cf0f71e03e", "embedding": null, "metadata": {"page_label": "71", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d95fa0b-5348-4e94-9d2b-ec09375e4c0a", "node_type": "4", "metadata": {"page_label": "71", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8fd317bff629d73a8ed835470ac4a666207b300952ff7c35d92334f8567ee506", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our db_set function actually has pretty good performance for something that is so\nsimple, because appending to a file is generally very efficient. Similarly to what\ndb_set does, many databases internally use a log, which is an append-only data file.\nReal databases have more issues to deal with (such as concurrency control, reclaim\u2010\ning disk space so that the log doesn\u2019t grow forever, and handling errors and partially\nwritten records), but the basic principle is the same. Logs are incredibly useful, and\nwe will encounter them several times in the rest of this book.\nThe word log is often used to refer to application logs, where an\napplication outputs text that describes what\u2019s happening. In this\nbook, log is used in the more general sense: an append-only\nsequence of records. It doesn\u2019t have to be human-readable; it might\nbe binary and intended only for other programs to read.\nOn the other hand, our db_get function has terrible performance if you have a large\nnumber of records in your database. Every time you want to look up a key, db_get\nhas to scan the entire database file from beginning to end, looking for occurrences of\nthe key. In algorithmic terms, the cost of a lookup is O(n): if you double the number\nof records n in your database, a lookup takes twice as long. That\u2019s not good.\nIn order to efficiently find the value for a particular key in the database, we need a\ndifferent data structure: an index. In this chapter we will look at a range of indexing\nstructures and see how they compare; the general idea behind them is to keep some\nadditional metadata on the side, which acts as a signpost and helps you to locate the\ndata you want. If you want to search the same data in several different ways, you may\nneed several different indexes on different parts of the data.\nAn index is an additional structure that is derived from the primary data. Many data\u2010\nbases allow you to add and remove indexes, and this doesn\u2019t affect the contents of the\ndatabase; it only affects the performance of queries. Maintaining additional structures\nincurs overhead, especially on writes. For writes, it\u2019s hard to beat the performance of\nsimply appending to a file, because that\u2019s the simplest possible write operation. Any\nkind of index usually slows down writes, because the index also needs to be updated\nevery time data is written.\nThis is an important trade-off in storage systems: well-chosen indexes speed up read\nqueries, but every index slows down writes. For this reason, databases don\u2019t usually\nindex everything by default, but require you\u2014the application developer or database\nadministrator\u2014to choose indexes manually, using your knowledge of the applica\u2010\ntion\u2019s typical query patterns. You can then choose the indexes that give your applica\u2010\ntion the greatest benefit, without introducing more overhead than necessary.\nData Structures That Power Your Database | 71", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9d594cc-8ba0-4445-850f-49779355b3df": {"__data__": {"id_": "c9d594cc-8ba0-4445-850f-49779355b3df", "embedding": null, "metadata": {"page_label": "72", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29ce8a71-ebdf-4fe4-90a7-df77cadc0597", "node_type": "4", "metadata": {"page_label": "72", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "23378401374110293e4976a2fb49f2c2684d6932ca574f9193f945a28826aaf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hash Indexes\nLet\u2019s start with indexes for key-value data. This is not the only kind of data you can\nindex, but it\u2019s very common, and it\u2019s a useful building block for more complex\nindexes.\nKey-value stores are quite similar to the dictionary type that you can find in most\nprogramming languages, and which is usually implemented as a hash map (hash\ntable). Hash maps are described in many algorithms textbooks [ 1, 2], so we won\u2019t go\ninto detail of how they work here. Since we already have hash maps for our in-\nmemory data structures, why not use them to index our data on disk?\nLet\u2019s say our data storage consists only of appending to a file, as in the preceding\nexample. Then the simplest possible indexing strategy is this: keep an in-memory\nhash map where every key is mapped to a byte offset in the data file\u2014the location at\nwhich the value can be found, as illustrated in Figure 3-1. Whenever you append a\nnew key-value pair to the file, you also update the hash map to reflect the offset of the\ndata you just wrote (this works both for inserting new keys and for updating existing\nkeys). When you want to look up a value, use the hash map to find the offset in the\ndata file, seek to that location, and read the value.\nFigure 3-1. Storing a log of key-value pairs in a CSV-like format, indexed with an in-\nmemory hash map.\nThis may sound simplistic, but it is a viable approach. In fact, this is essentially what\nBitcask (the default storage engine in Riak) does [ 3]. Bitcask offers high-performance\nreads and writes, subject to the requirement that all the keys fit in the available RAM,\nsince the hash map is kept completely in memory. The values can use more space\nthan there is available memory, since they can be loaded from disk with just one disk\n72 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4dcdf71-a18a-4029-a1cc-b5a45c22fcaa": {"__data__": {"id_": "a4dcdf71-a18a-4029-a1cc-b5a45c22fcaa", "embedding": null, "metadata": {"page_label": "73", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "491ea2be-1772-4dce-a89d-bca86f9f0a1b", "node_type": "4", "metadata": {"page_label": "73", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "133db8d981f707e0475ec37b5e7a2447e51096b6ab6d808cf29dc7f2a53b6862", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "seek. If that part of the data file is already in the filesystem cache, a read doesn\u2019t\nrequire any disk I/O at all.\nA storage engine like Bitcask is well suited to situations where the value for each key\nis updated frequently. For example, the key might be the URL of a cat video, and the\nvalue might be the number of times it has been played (incremented every time\nsomeone hits the play button). In this kind of workload, there are a lot of writes, but\nthere are not too many distinct keys\u2014you have a large number of writes per key, but\nit\u2019s feasible to keep all keys in memory.\nAs described so far, we only ever append to a file\u2014so how do we avoid eventually\nrunning out of disk space? A good solution is to break the log into segments of a cer\u2010\ntain size by closing a segment file when it reaches a certain size, and making subse\u2010\nquent writes to a new segment file. We can then perform compaction on these\nsegments, as illustrated in Figure 3-2 . Compaction means throwing away duplicate\nkeys in the log, and keeping only the most recent update for each key.\nFigure 3-2. Compaction of a key-value update log (counting the number of times each\ncat video was played), retaining only the most recent value for each key.\nMoreover, since compaction often makes segments much smaller (assuming that a\nkey is overwritten several times on average within one segment), we can also merge\nseveral segments together at the same time as performing the compaction, as shown\nin Figure 3-3 . Segments are never modified after they have been written, so the\nmerged segment is written to a new file. The merging and compaction of frozen seg\u2010\nments can be done in a background thread, and while it is going on, we can still con\u2010\ntinue to serve read and write requests as normal, using the old segment files. After the\nmerging process is complete, we switch read requests to using the new merged seg\u2010\nment instead of the old segments\u2014and then the old segment files can simply be\ndeleted.\nData Structures That Power Your Database | 73", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11f1b5b1-a4df-432f-bad0-08991e0e0f9d": {"__data__": {"id_": "11f1b5b1-a4df-432f-bad0-08991e0e0f9d", "embedding": null, "metadata": {"page_label": "74", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73f18ff8-6c82-4fec-9751-419c75de6f77", "node_type": "4", "metadata": {"page_label": "74", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9718ae6b563e8b9470e4e4cbc11a6029bec80f515904c18e8d1ae012f4d7aaf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3-3. Performing compaction and segment merging simultaneously.\nEach segment now has its own in-memory hash table, mapping keys to file offsets. In\norder to find the value for a key, we first check the most recent segment\u2019s hash map;\nif the key is not present we check the second-most-recent segment, and so on. The\nmerging process keeps the number of segments small, so lookups don\u2019t need to check\nmany hash maps.\nLots of detail goes into making this simple idea work in practice. Briefly, some of the\nissues that are important in a real implementation are:\nFile format\nCSV is not the best format for a log. It\u2019s faster and simpler to use a binary format\nthat first encodes the length of a string in bytes, followed by the raw string\n(without need for escaping).\nDeleting records\nIf you want to delete a key and its associated value, you have to append a special\ndeletion record to the data file (sometimes called a tombstone). When log seg\u2010\nments are merged, the tombstone tells the merging process to discard any previ\u2010\nous values for the deleted key.\nCrash recovery\nIf the database is restarted, the in-memory hash maps are lost. In principle, you\ncan restore each segment\u2019s hash map by reading the entire segment file from\nbeginning to end and noting the offset of the most recent value for every key as\nyou go along. However, that might take a long time if the segment files are large,\nwhich would make server restarts painful. Bitcask speeds up recovery by storing\n74 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a3961d5-07b0-4472-b2e0-c27b83ed1d05": {"__data__": {"id_": "9a3961d5-07b0-4472-b2e0-c27b83ed1d05", "embedding": null, "metadata": {"page_label": "75", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "874ecc9e-2088-40a3-8c9b-a0149c34740b", "node_type": "4", "metadata": {"page_label": "75", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e0feadfe6be01164bde7fc6e985a2e87ed33f78a13d8272ccc531b10a9efcf81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a snapshot of each segment\u2019s hash map on disk, which can be loaded into mem\u2010\nory more quickly.\nPartially written records\nThe database may crash at any time, including halfway through appending a\nrecord to the log. Bitcask files include checksums, allowing such corrupted parts\nof the log to be detected and ignored.\nConcurrency control\nAs writes are appended to the log in a strictly sequential order, a common imple\u2010\nmentation choice is to have only one writer thread. Data file segments are\nappend-only and otherwise immutable, so they can be read concurrently by mul\u2010\ntiple threads.\nAn append-only log seems wasteful at first glance: why don\u2019t you update the file in\nplace, overwriting the old value with the new value? But an append-only design turns\nout to be good for several reasons:\n\u2022 Appending and segment merging are sequential write operations, which are gen\u2010\nerally much faster than random writes, especially on magnetic spinning-disk\nhard drives. To some extent sequential writes are also preferable on flash-based\nsolid state drives (SSDs) [4]. We will discuss this issue further in \u201cComparing B-\nTrees and LSM-Trees\u201d on page 83.\n\u2022 Concurrency and crash recovery are much simpler if segment files are append-\nonly or immutable. For example, you don\u2019t have to worry about the case where a\ncrash happened while a value was being overwritten, leaving you with a file con\u2010\ntaining part of the old and part of the new value spliced together.\n\u2022 Merging old segments avoids the problem of data files getting fragmented over\ntime.\nHowever, the hash table index also has limitations:\n\u2022 The hash table must fit in memory, so if you have a very large number of keys,\nyou\u2019re out of luck. In principle, you could maintain a hash map on disk, but\nunfortunately it is difficult to make an on-disk hash map perform well. It\nrequires a lot of random access I/O, it is expensive to grow when it becomes full,\nand hash collisions require fiddly logic [5].\n\u2022 Range queries are not efficient. For example, you cannot easily scan over all keys\nbetween kitty00000 and kitty99999\u2014you\u2019d have to look up each key individu\u2010\nally in the hash maps.\nIn the next section we will look at an indexing structure that doesn\u2019t have those limi\u2010\ntations. \nData Structures That Power Your Database | 75", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb42dbde-a4c4-4548-b94d-be9f51b6f48b": {"__data__": {"id_": "cb42dbde-a4c4-4548-b94d-be9f51b6f48b", "embedding": null, "metadata": {"page_label": "76", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "537bc11f-d29d-40de-973f-98eaf9b3e3bf", "node_type": "4", "metadata": {"page_label": "76", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "011153cc607b565da8ec964fd094fe2558233c9ead631e07d77e44b56e588713", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SSTables and LSM-Trees\nIn Figure 3-3, each log-structured storage segment is a sequence of key-value pairs.\nThese pairs appear in the order that they were written, and values later in the log take\nprecedence over values for the same key earlier in the log. Apart from that, the order\nof key-value pairs in the file does not matter.\nNow we can make a simple change to the format of our segment files: we require that\nthe sequence of key-value pairs is sorted by key . At first glance, that requirement\nseems to break our ability to use sequential writes, but we\u2019ll get to that in a moment.\nWe call this format Sorted String Table , or SSTable for short. We also require that\neach key only appears once within each merged segment file (the compaction process\nalready ensures that). SSTables have several big advantages over log segments with\nhash indexes:\n1. Merging segments is simple and efficient, even if the files are bigger than the\navailable memory. The approach is like the one used in the mergesort algorithm\nand is illustrated in Figure 3-4: you start reading the input files side by side, look\nat the first key in each file, copy the lowest key (according to the sort order) to\nthe output file, and repeat. This produces a new merged segment file, also sorted\nby key.\nFigure 3-4. Merging several SSTable segments, retaining only the most recent value\nfor each key.\n76 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62fbab34-2776-4dc8-a1d5-edca9b67141c": {"__data__": {"id_": "62fbab34-2776-4dc8-a1d5-edca9b67141c", "embedding": null, "metadata": {"page_label": "77", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6476d238-12ee-4cf3-9af7-69a88bfcd53f", "node_type": "4", "metadata": {"page_label": "77", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8a29538f0fd5fca96b8a6ed1ec09fbe64eeca947af3ef10a2f51bbfa5af69862", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. If all keys and values had a fixed size, you could use binary search on a segment file and avoid the in-\nmemory index entirely. However, they are usually variable-length in practice, which makes it difficult to tell\nwhere one record ends and the next one starts if you don\u2019t have an index.\nWhat if the same key appears in several input segments? Remember that each\nsegment contains all the values written to the database during some period of\ntime. This means that all the values in one input segment must be more recent\nthan all the values in the other segment (assuming that we always merge adjacent\nsegments). When multiple segments contain the same key, we can keep the value\nfrom the most recent segment and discard the values in older segments.\n2. In order to find a particular key in the file, you no longer need to keep an index\nof all the keys in memory. See Figure 3-5 for an example: say you\u2019re looking for\nthe key handiwork, but you don\u2019t know the exact offset of that key in the segment\nfile. However, you do know the offsets for the keys handbag and handsome, and\nbecause of the sorting you know that handiwork must appear between those two.\nThis means you can jump to the offset for handbag and scan from there until you\nfind handiwork (or not, if the key is not present in the file).\nFigure 3-5. An SSTable with an in-memory index.\nYou still need an in-memory index to tell you the offsets for some of the keys, but\nit can be sparse: one key for every few kilobytes of segment file is sufficient,\nbecause a few kilobytes can be scanned very quickly.i\n3. Since read requests need to scan over several key-value pairs in the requested\nrange anyway, it is possible to group those records into a block and compress it\nbefore writing it to disk (indicated by the shaded area in Figure 3-5). Each entry\nof the sparse in-memory index then points at the start of a compressed block.\nBesides saving disk space, compression also reduces the I/O bandwidth use.\nData Structures That Power Your Database | 77", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9037245-9f85-4d43-802b-92141bc9dc7b": {"__data__": {"id_": "e9037245-9f85-4d43-802b-92141bc9dc7b", "embedding": null, "metadata": {"page_label": "78", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52691c9d-5a8d-49b2-88b6-bf5d28fff13f", "node_type": "4", "metadata": {"page_label": "78", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "35ad75176489ec8f0e0c3732abe32af5631eb3224caf47707e6753f2adb536c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Constructing and maintaining SSTables\nFine so far\u2014but how do you get your data to be sorted by key in the first place? Our\nincoming writes can occur in any order.\nMaintaining a sorted structure on disk is possible (see \u201cB-Trees\u201d on page 79), but\nmaintaining it in memory is much easier. There are plenty of well-known tree data\nstructures that you can use, such as red-black trees or AVL trees [ 2]. With these data\nstructures, you can insert keys in any order and read them back in sorted order.\nWe can now make our storage engine work as follows:\n\u2022 When a write comes in, add it to an in-memory balanced tree data structure (for\nexample, a red-black tree). This in-memory tree is sometimes called a memtable.\n\u2022 When the memtable gets bigger than some threshold\u2014typically a few megabytes\n\u2014write it out to disk as an SSTable file. This can be done efficiently because the\ntree already maintains the key-value pairs sorted by key. The new SSTable file\nbecomes the most recent segment of the database. While the SSTable is being\nwritten out to disk, writes can continue to a new memtable instance.\n\u2022 In order to serve a read request, first try to find the key in the memtable, then in\nthe most recent on-disk segment, then in the next-older segment, etc.\n\u2022 From time to time, run a merging and compaction process in the background to\ncombine segment files and to discard overwritten or deleted values.\nThis scheme works very well. It only suffers from one problem: if the database\ncrashes, the most recent writes (which are in the memtable but not yet written out to\ndisk) are lost. In order to avoid that problem, we can keep a separate log on disk to\nwhich every write is immediately appended, just like in the previous section. That log\nis not in sorted order, but that doesn\u2019t matter, because its only purpose is to restore\nthe memtable after a crash. Every time the memtable is written out to an SSTable, the\ncorresponding log can be discarded.\nMaking an LSM-tree out of SSTables\nThe algorithm described here is essentially what is used in LevelDB [ 6] and RocksDB\n[7], key-value storage engine libraries that are designed to be embedded into other\napplications. Among other things, LevelDB can be used in Riak as an alternative to\nBitcask. Similar storage engines are used in Cassandra and HBase [ 8], both of which\nwere inspired by Google\u2019s Bigtable paper [ 9] (which introduced the terms SSTable\nand memtable).\nOriginally this indexing structure was described by Patrick O\u2019Neil et al. under the\nname Log-Structured Merge-Tree  (or LSM-Tree) [ 10], building on earlier work on\n78 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e0ef7ca-e028-4fec-b94e-82efdaa58695": {"__data__": {"id_": "8e0ef7ca-e028-4fec-b94e-82efdaa58695", "embedding": null, "metadata": {"page_label": "79", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "362504aa-2f4a-460d-b953-e469544af73b", "node_type": "4", "metadata": {"page_label": "79", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dbcf90dcdd06febc539f3949dac24fd530ffd77e8ee5f7a3fdcf15e9a606ebf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "log-structured filesystems [ 11]. Storage engines that are based on this principle of\nmerging and compacting sorted files are often called LSM storage engines.\nLucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a\nsimilar method for storing its term dictionary [12, 13]. A full-text index is much more\ncomplex than a key-value index but is based on a similar idea: given a word in a\nsearch query, find all the documents (web pages, product descriptions, etc.) that\nmention the word. This is implemented with a key-value structure where the key is a\nword (a term) and the value is the list of IDs of all the documents that contain the\nword (the postings list). In Lucene, this mapping from term to postings list is kept in\nSSTable-like sorted files, which are merged in the background as needed [14].\nPerformance optimizations\nAs always, a lot of detail goes into making a storage engine perform well in practice.\nFor example, the LSM-tree algorithm can be slow when looking up keys that do not\nexist in the database: you have to check the memtable, then the segments all the way\nback to the oldest (possibly having to read from disk for each one) before you can be\nsure that the key does not exist. In order to optimize this kind of access, storage\nengines often use additional Bloom filters [15]. (A Bloom filter is a memory-efficient\ndata structure for approximating the contents of a set. It can tell you if a key does not\nappear in the database, and thus saves many unnecessary disk reads for nonexistent\nkeys.)\nThere are also different strategies to determine the order and timing of how SSTables\nare compacted and merged. The most common options are size-tiered and leveled\ncompaction. LevelDB and RocksDB use leveled compaction (hence the name of Lev\u2010\nelDB), HBase uses size-tiered, and Cassandra supports both [ 16]. In size-tiered com\u2010\npaction, newer and smaller SSTables are successively merged into older and larger\nSSTables. In leveled compaction, the key range is split up into smaller SSTables and\nolder data is moved into separate \u201clevels,\u201d which allows the compaction to proceed\nmore incrementally and use less disk space.\nEven though there are many subtleties, the basic idea of LSM-trees\u2014keeping a cas\u2010\ncade of SSTables that are merged in the background\u2014is simple and effective. Even\nwhen the dataset is much bigger than the available memory it continues to work well.\nSince data is stored in sorted order, you can efficiently perform range queries (scan\u2010\nning all keys above some minimum and up to some maximum), and because the disk\nwrites are sequential the LSM-tree can support remarkably high write throughput. \nB-Trees\nThe log-structured indexes we have discussed so far are gaining acceptance, but they\nare not the most common type of index. The most widely used indexing structure is\nquite different: the B-tree.\nData Structures That Power Your Database | 79", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16ed9bfb-cb76-4bd1-b6c2-6013827f9e0a": {"__data__": {"id_": "16ed9bfb-cb76-4bd1-b6c2-6013827f9e0a", "embedding": null, "metadata": {"page_label": "80", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d08ab820-f6d7-4188-9b8a-3d4b18ad6fc2", "node_type": "4", "metadata": {"page_label": "80", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4a57dd0d4637e599085708ccab3a51289a2111642e41bc8eec9812eb110158da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduced in 1970 [ 17] and called \u201cubiquitous\u201d less than 10 years later [ 18], B-trees\nhave stood the test of time very well. They remain the standard index implementation\nin almost all relational databases, and many nonrelational databases use them too.\nLike SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-\nvalue lookups and range queries. But that\u2019s where the similarity ends: B-trees have a\nvery different design philosophy.\nThe log-structured indexes we saw earlier break the database down into variable-size\nsegments, typically several megabytes or more in size, and always write a segment\nsequentially. By contrast, B-trees break the database down into fixed-size blocks or\npages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a\ntime. This design corresponds more closely to the underlying hardware, as disks are\nalso arranged in fixed-size blocks.\nEach page can be identified using an address or location, which allows one page to\nrefer to another\u2014similar to a pointer, but on disk instead of in memory. We can use\nthese page references to construct a tree of pages, as illustrated in Figure 3-6.\nFigure 3-6. Looking up a key using a B-tree index.\nOne page is designated as the root of the B-tree; whenever you want to look up a key\nin the index, you start here. The page contains several keys and references to child\npages. Each child is responsible for a continuous range of keys, and the keys between\nthe references indicate where the boundaries between those ranges lie.\nIn the example in Figure 3-6, we are looking for the key 251, so we know that we need\nto follow the page reference between the boundaries 200 and 300. That takes us to a\nsimilar-looking page that further breaks down the 200\u2013300 range into subranges.\n80 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38faacd5-3f7a-46ad-b06c-3068129eb40b": {"__data__": {"id_": "38faacd5-3f7a-46ad-b06c-3068129eb40b", "embedding": null, "metadata": {"page_label": "81", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33be2da0-97eb-4239-9ff7-d7d9e5a8de60", "node_type": "4", "metadata": {"page_label": "81", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "45019c61043a94a9811e2ba21eafc937389cbcdfa141ac89282d9a380d396e21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. Inserting a new key into a B-tree is reasonably intuitive, but deleting one (while keeping the tree balanced)\nis somewhat more involved [2].\nEventually we get down to a page containing individual keys (a leaf page ), which\neither contains the value for each key inline or contains references to the pages where\nthe values can be found.\nThe number of references to child pages in one page of the B-tree is called the\nbranching factor. For example, in Figure 3-6 the branching factor is six. In practice,\nthe branching factor depends on the amount of space required to store the page refer\u2010\nences and the range boundaries, but typically it is several hundred.\nIf you want to update the value for an existing key in a B-tree, you search for the leaf\npage containing that key, change the value in that page, and write the page back to\ndisk (any references to that page remain valid). If you want to add a new key, you\nneed to find the page whose range encompasses the new key and add it to that page.\nIf there isn\u2019t enough free space in the page to accommodate the new key, it is split\ninto two half-full pages, and the parent page is updated to account for the new subdi\u2010\nvision of key ranges\u2014see Figure 3-7.ii\nFigure 3-7. Growing a B-tree by splitting a page.\nThis algorithm ensures that the tree remains balanced: a B-tree with n keys always\nhas a depth of O(log n). Most databases can fit into a B-tree that is three or four levels\ndeep, so you don\u2019t need to follow many page references to find the page you are look\u2010\ning for. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to\n256 TB.)\nData Structures That Power Your Database | 81", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a460fc7c-baf3-41a4-b636-73848f105665": {"__data__": {"id_": "a460fc7c-baf3-41a4-b636-73848f105665", "embedding": null, "metadata": {"page_label": "82", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "749d3916-1057-456f-b57e-a87090801b78", "node_type": "4", "metadata": {"page_label": "82", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7a8f1804d3f0ccd4ce3293aafde5f7f121c8e274f3ba2cee1fd3b29eecad408b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Making B-trees reliable\nThe basic underlying write operation of a B-tree is to overwrite a page on disk with\nnew data. It is assumed that the overwrite does not change the location of the page;\ni.e., all references to that page remain intact when the page is overwritten. This is in\nstark contrast to log-structured indexes such as LSM-trees, which only append to files\n(and eventually delete obsolete files) but never modify files in place.\nYou can think of overwriting a page on disk as an actual hardware operation. On a\nmagnetic hard drive, this means moving the disk head to the right place, waiting for\nthe right position on the spinning platter to come around, and then overwriting the\nappropriate sector with new data. On SSDs, what happens is somewhat more compli\u2010\ncated, due to the fact that an SSD must erase and rewrite fairly large blocks of a stor\u2010\nage chip at a time [19].\nMoreover, some operations require several different pages to be overwritten. For\nexample, if you split a page because an insertion caused it to be overfull, you need to\nwrite the two pages that were split, and also overwrite their parent page to update the\nreferences to the two child pages. This is a dangerous operation, because if the data\u2010\nbase crashes after only some of the pages have been written, you end up with a cor\u2010\nrupted index (e.g., there may be an orphan page that is not a child of any parent).\nIn order to make the database resilient to crashes, it is common for B-tree implemen\u2010\ntations to include an additional data structure on disk: a write-ahead log (WAL, also\nknown as a redo log). This is an append-only file to which every B-tree modification\nmust be written before it can be applied to the pages of the tree itself. When the data\u2010\nbase comes back up after a crash, this log is used to restore the B-tree back to a con\u2010\nsistent state [5, 20].\nAn additional complication of updating pages in place is that careful concurrency\ncontrol is required if multiple threads are going to access the B-tree at the same time\n\u2014otherwise a thread may see the tree in an inconsistent state. This is typically done\nby protecting the tree\u2019s data structures with latches (lightweight locks). Log-\nstructured approaches are simpler in this regard, because they do all the merging in\nthe background without interfering with incoming queries and atomically swap old\nsegments for new segments from time to time.\nB-tree optimizations\nAs B-trees have been around for so long, it\u2019s not surprising that many optimizations\nhave been developed over the years. To mention just a few:\n\u2022 Instead of overwriting pages and maintaining a WAL for crash recovery, some\ndatabases (like LMDB) use a copy-on-write scheme [ 21]. A modified page is\nwritten to a different location, and a new version of the parent pages in the tree is\ncreated, pointing at the new location. This approach is also useful for concur\u2010\n82 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2925, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0815e79-e88e-4800-9399-feb5f7747389": {"__data__": {"id_": "c0815e79-e88e-4800-9399-feb5f7747389", "embedding": null, "metadata": {"page_label": "83", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "879c0ea2-9517-4705-ad8e-e422d4f0c535", "node_type": "4", "metadata": {"page_label": "83", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7ccc994ff780ee9363649364bd9672103abe68ae05c5b03080722fd938d74e7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. This variant is sometimes known as a B + tree, although the optimization is so common that it often isn\u2019t\ndistinguished from other B-tree variants.\nrency control, as we shall see in \u201cSnapshot Isolation and Repeatable Read\u201d on\npage 237.\n\u2022 We can save space in pages by not storing the entire key, but abbreviating it.\nEspecially in pages on the interior of the tree, keys only need to provide enough\ninformation to act as boundaries between key ranges. Packing more keys into a\npage allows the tree to have a higher branching factor, and thus fewer levels.iii\n\u2022 In general, pages can be positioned anywhere on disk; there is nothing requiring\npages with nearby key ranges to be nearby on disk. If a query needs to scan over a\nlarge part of the key range in sorted order, that page-by-page layout can be ineffi\u2010\ncient, because a disk seek may be required for every page that is read. Many B-\ntree implementations therefore try to lay out the tree so that leaf pages appear in\nsequential order on disk. However, it\u2019s difficult to maintain that order as the tree\ngrows. By contrast, since LSM-trees rewrite large segments of the storage in one\ngo during merging, it\u2019s easier for them to keep sequential keys close to each other\non disk.\n\u2022 Additional pointers have been added to the tree. For example, each leaf page may\nhave references to its sibling pages to the left and right, which allows scanning\nkeys in order without jumping back to parent pages.\n\u2022 B-tree variants such as fractal trees  [22] borrow some log-structured ideas to\nreduce disk seeks (and they have nothing to do with fractals). \nComparing B-Trees and LSM-Trees\nEven though B-tree implementations are generally more mature than LSM-tree\nimplementations, LSM-trees are also interesting due to their performance character\u2010\nistics. As a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees\nare thought to be faster for reads [ 23]. Reads are typically slower on LSM-trees\nbecause they have to check several different data structures and SSTables at different\nstages of compaction.\nHowever, benchmarks are often inconclusive and sensitive to details of the workload.\nYou need to test systems with your particular workload in order to make a valid com\u2010\nparison. In this section we will briefly discuss a few things that are worth considering\nwhen measuring the performance of a storage engine.\nData Structures That Power Your Database | 83", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b10d7e57-0256-411f-80ef-5cdf0f6bbb37": {"__data__": {"id_": "b10d7e57-0256-411f-80ef-5cdf0f6bbb37", "embedding": null, "metadata": {"page_label": "84", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f18ff42-25b2-4874-b9c1-711b6730375d", "node_type": "4", "metadata": {"page_label": "84", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4e3c74830787b714d65070836ad558dc34a7be5646ec95a4a91da992f09b306e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Advantages of LSM-trees\nA B-tree index must write every piece of data at least twice: once to the write-ahead\nlog, and once to the tree page itself (and perhaps again as pages are split). There is\nalso overhead from having to write an entire page at a time, even if only a few bytes in\nthat page changed. Some storage engines even overwrite the same page twice in order\nto avoid ending up with a partially updated page in the event of a power failure [ 24,\n25].\nLog-structured indexes also rewrite data multiple times due to repeated compaction\nand merging of SSTables. This effect\u2014one write to the database resulting in multiple\nwrites to the disk over the course of the database\u2019s lifetime\u2014is known as write ampli\u2010\nfication. It is of particular concern on SSDs, which can only overwrite blocks a limi\u2010\nted number of times before wearing out.\nIn write-heavy applications, the performance bottleneck might be the rate at which\nthe database can write to disk. In this case, write amplification has a direct perfor\u2010\nmance cost: the more that a storage engine writes to disk, the fewer writes per second\nit can handle within the available disk bandwidth.\nMoreover, LSM-trees are typically able to sustain higher write throughput than B-\ntrees, partly because they sometimes have lower write amplification (although this\ndepends on the storage engine configuration and workload), and partly because they\nsequentially write compact SSTable files rather than having to overwrite several pages\nin the tree [ 26]. This difference is particularly important on magnetic hard drives,\nwhere sequential writes are much faster than random writes.\nLSM-trees can be compressed better, and thus often produce smaller files on disk\nthan B-trees. B-tree storage engines leave some disk space unused due to fragmenta\u2010\ntion: when a page is split or when a row cannot fit into an existing page, some space\nin a page remains unused. Since LSM-trees are not page-oriented and periodically\nrewrite SSTables to remove fragmentation, they have lower storage overheads, espe\u2010\ncially when using leveled compaction [27].\nOn many SSDs, the firmware internally uses a log-structured algorithm to turn ran\u2010\ndom writes into sequential writes on the underlying storage chips, so the impact of\nthe storage engine\u2019s write pattern is less pronounced [ 19]. However, lower write\namplification and reduced fragmentation are still advantageous on SSDs: represent\u2010\ning data more compactly allows more read and write requests within the available I/O\nbandwidth.\nDownsides of LSM-trees\nA downside of log-structured storage is that the compaction process can sometimes\ninterfere with the performance of ongoing reads and writes. Even though storage\nengines try to perform compaction incrementally and without affecting concurrent\n84 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e001bf3c-f75c-4cb8-b2db-ecdc44e18bc0": {"__data__": {"id_": "e001bf3c-f75c-4cb8-b2db-ecdc44e18bc0", "embedding": null, "metadata": {"page_label": "85", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21d53f44-d41a-43a1-943c-f709044405f4", "node_type": "4", "metadata": {"page_label": "85", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d41b0365f6ad1cfc9174e3970551ce668e26e3f510b868e2a2d1235a233d54bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "access, disks have limited resources, so it can easily happen that a request needs to\nwait while the disk finishes an expensive compaction operation. The impact on\nthroughput and average response time is usually small, but at higher percentiles (see\n\u201cDescribing Performance\u201d on page 13) the response time of queries to log-structured\nstorage engines can sometimes be quite high, and B-trees can be more predictable\n[28].\nAnother issue with compaction arises at high write throughput: the disk\u2019s finite write\nbandwidth needs to be shared between the initial write (logging and flushing a\nmemtable to disk) and the compaction threads running in the background. When\nwriting to an empty database, the full disk bandwidth can be used for the initial write,\nbut the bigger the database gets, the more disk bandwidth is required for compaction.\nIf write throughput is high and compaction is not configured carefully, it can happen\nthat compaction cannot keep up with the rate of incoming writes. In this case, the\nnumber of unmerged segments on disk keeps growing until you run out of disk\nspace, and reads also slow down because they need to check more segment files. Typ\u2010\nically, SSTable-based storage engines do not throttle the rate of incoming writes, even\nif compaction cannot keep up, so you need explicit monitoring to detect this situa\u2010\ntion [29, 30].\nAn advantage of B-trees is that each key exists in exactly one place in the index,\nwhereas a log-structured storage engine may have multiple copies of the same key in\ndifferent segments. This aspect makes B-trees attractive in databases that want to\noffer strong transactional semantics: in many relational databases, transaction isola\u2010\ntion is implemented using locks on ranges of keys, and in a B-tree index, those locks\ncan be directly attached to the tree [5]. In Chapter 7 we will discuss this point in more\ndetail.\nB-trees are very ingrained in the architecture of databases and provide consistently\ngood performance for many workloads, so it\u2019s unlikely that they will go away anytime\nsoon. In new datastores, log-structured indexes are becoming increasingly popular.\nThere is no quick and easy rule for determining which type of storage engine is better\nfor your use case, so it is worth testing empirically. \nOther Indexing Structures\nSo far we have only discussed key-value indexes, which are like a primary key index in\nthe relational model. A primary key uniquely identifies one row in a relational table,\nor one document in a document database, or one vertex in a graph database. Other\nrecords in the database can refer to that row/document/vertex by its primary key (or\nID), and the index is used to resolve such references.\nIt is also very common to have secondary indexes . In relational databases, you can\ncreate several secondary indexes on the same table using the CREATE INDEX com\u2010\nData Structures That Power Your Database | 85", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74a863ca-a564-46a5-a900-9cb8332a30cb": {"__data__": {"id_": "74a863ca-a564-46a5-a900-9cb8332a30cb", "embedding": null, "metadata": {"page_label": "86", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c087c30-a793-40a8-98b7-f2b2709f5e77", "node_type": "4", "metadata": {"page_label": "86", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c2eb9d063b7c910aae6f0efa856fc6f29e228723359daec45c17b37b10f35893", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mand, and they are often crucial for performing joins efficiently. For example, in\nFigure 2-1  in Chapter 2  you would most likely have a secondary index on the\nuser_id columns so that you can find all the rows belonging to the same user in each\nof the tables.\nA secondary index can easily be constructed from a key-value index. The main differ\u2010\nence is that keys are not unique; i.e., there might be many rows (documents, vertices)\nwith the same key. This can be solved in two ways: either by making each value in the\nindex a list of matching row identifiers (like a postings list in a full-text index) or by\nmaking each key unique by appending a row identifier to it. Either way, both B-trees\nand log-structured indexes can be used as secondary indexes.\nStoring values within the index\nThe key in an index is the thing that queries search for, but the value can be one of\ntwo things: it could be the actual row (document, vertex) in question, or it could be a\nreference to the row stored elsewhere. In the latter case, the place where rows are\nstored is known as a heap file , and it stores data in no particular order (it may be\nappend-only, or it may keep track of deleted rows in order to overwrite them with\nnew data later). The heap file approach is common because it avoids duplicating data\nwhen multiple secondary indexes are present: each index just references a location in\nthe heap file, and the actual data is kept in one place.\nWhen updating a value without changing the key, the heap file approach can be quite\nefficient: the record can be overwritten in place, provided that the new value is not\nlarger than the old value. The situation is more complicated if the new value is larger,\nas it probably needs to be moved to a new location in the heap where there is enough\nspace. In that case, either all indexes need to be updated to point at the new heap\nlocation of the record, or a forwarding pointer is left behind in the old heap location\n[5].\nIn some situations, the extra hop from the index to the heap file is too much of a per\u2010\nformance penalty for reads, so it can be desirable to store the indexed row directly\nwithin an index. This is known as a clustered index . For example, in MySQL\u2019s\nInnoDB storage engine, the primary key of a table is always a clustered index, and\nsecondary indexes refer to the primary key (rather than a heap file location) [ 31]. In\nSQL Server, you can specify one clustered index per table [32].\nA compromise between a clustered index (storing all row data within the index) and\na nonclustered index (storing only references to the data within the index) is known\nas a covering index or index with included columns, which stores some of a table\u2019s col\u2010\numns within the index [ 33]. This allows some queries to be answered by using the\nindex alone (in which case, the index is said to cover the query) [32].\n86 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ccee254-1286-41ac-8fa1-1448015dd57d": {"__data__": {"id_": "1ccee254-1286-41ac-8fa1-1448015dd57d", "embedding": null, "metadata": {"page_label": "87", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77cf12e8-62e4-4986-af61-c9c8c1c412a0", "node_type": "4", "metadata": {"page_label": "87", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b7e9018f8f1e660406addebe300ca7e7aeddb6a397d24f389dd80af7d42cd10f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As with any kind of duplication of data, clustered and covering indexes can speed up\nreads, but they require additional storage and can add overhead on writes. Databases\nalso need to go to additional effort to enforce transactional guarantees, because appli\u2010\ncations should not see inconsistencies due to the duplication.\nMulti-column indexes\nThe indexes discussed so far only map a single key to a value. That is not sufficient if\nwe need to query multiple columns of a table (or multiple fields in a document)\nsimultaneously.\nThe most common type of multi-column index is called a concatenated index, which\nsimply combines several fields into one key by appending one column to another (the\nindex definition specifies in which order the fields are concatenated). This is like an\nold-fashioned paper phone book, which provides an index from ( lastname, first\u2010\nname) to phone number. Due to the sort order, the index can be used to find all the\npeople with a particular last name, or all the people with a particular lastname-\nfirstname combination. However, the index is useless if you want to find all the peo\u2010\nple with a particular first name.\nMulti-dimensional indexes are a more general way of querying several columns at\nonce, which is particularly important for geospatial data. For example, a restaurant-\nsearch website may have a database containing the latitude and longitude of each res\u2010\ntaurant. When a user is looking at the restaurants on a map, the website needs to\nsearch for all the restaurants within the rectangular map area that the user is cur\u2010\nrently viewing. This requires a two-dimensional range query like the following:\nSELECT * FROM restaurants WHERE latitude  > 51.4946 AND latitude  < 51.5079\n                            AND longitude > -0.1162 AND longitude < -0.1004;\nA standard B-tree or LSM-tree index is not able to answer that kind of query effi\u2010\nciently: it can give you either all the restaurants in a range of latitudes (but at any lon\u2010\ngitude), or all the restaurants in a range of longitudes (but anywhere between the\nNorth and South poles), but not both simultaneously.\nOne option is to translate a two-dimensional location into a single number using a\nspace-filling curve, and then to use a regular B-tree index [ 34]. More commonly, spe\u2010\ncialized spatial indexes such as R-trees are used. For example, PostGIS implements\ngeospatial indexes as R-trees using PostgreSQL\u2019s Generalized Search Tree indexing\nfacility [35]. We don\u2019t have space to describe R-trees in detail here, but there is plenty\nof literature on them.\nAn interesting idea is that multi-dimensional indexes are not just for geographic\nlocations. For example, on an ecommerce website you could use a three-dimensional\nindex on the dimensions (red, green, blue) to search for products in a certain range of\ncolors, or in a database of weather observations you could have a two-dimensional\nData Structures That Power Your Database | 87", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc55e92f-4ed7-4ad8-b5a2-dc697a698b37": {"__data__": {"id_": "bc55e92f-4ed7-4ad8-b5a2-dc697a698b37", "embedding": null, "metadata": {"page_label": "88", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d92d8bdb-8825-4a32-801b-f4742dcb6c39", "node_type": "4", "metadata": {"page_label": "88", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d432fac5a1153836366e1ff3218b80fee475a65e2c79dce43f7e620a4befedb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "index on ( date, temperature) in order to efficiently search for all the observations\nduring the year 2013 where the temperature was between 25 and 30 \u2103. With a one-\ndimensional index, you would have to either scan over all the records from 2013\n(regardless of temperature) and then filter them by temperature, or vice versa. A 2D\nindex could narrow down by timestamp and temperature simultaneously. This tech\u2010\nnique is used by HyperDex [36].\nFull-text search and fuzzy indexes\nAll the indexes discussed so far assume that you have exact data and allow you to\nquery for exact values of a key, or a range of values of a key with a sort order. What\nthey don\u2019t allow you to do is search for similar keys, such as misspelled words. Such\nfuzzy querying requires different techniques.\nFor example, full-text search engines commonly allow a search for one word to be\nexpanded to include synonyms of the word, to ignore grammatical variations of\nwords, and to search for occurrences of words near each other in the same document,\nand support various other features that depend on linguistic analysis of the text. To\ncope with typos in documents or queries, Lucene is able to search text for words\nwithin a certain edit distance (an edit distance of 1 means that one letter has been\nadded, removed, or replaced) [37].\nAs mentioned in \u201cMaking an LSM-tree out of SSTables\u201d on page 78, Lucene uses a\nSSTable-like structure for its term dictionary. This structure requires a small in-\nmemory index that tells queries at which offset in the sorted file they need to look for\na key. In LevelDB, this in-memory index is a sparse collection of some of the keys,\nbut in Lucene, the in-memory index is a finite state automaton over the characters in\nthe keys, similar to a trie [38]. This automaton can be transformed into a Levenshtein\nautomaton, which supports efficient search for words within a given edit distance\n[39].\nOther fuzzy search techniques go in the direction of document classification and\nmachine learning. See an information retrieval textbook for more detail [e.g., 40].\nKeeping everything in memory\nThe data structures discussed so far in this chapter have all been answers to the limi\u2010\ntations of disks. Compared to main memory, disks are awkward to deal with. With\nboth magnetic disks and SSDs, data on disk needs to be laid out carefully if you want\ngood performance on reads and writes. However, we tolerate this awkwardness\nbecause disks have two significant advantages: they are durable (their contents are\nnot lost if the power is turned off), and they have a lower cost per gigabyte than\nRAM.\nAs RAM becomes cheaper, the cost-per-gigabyte argument is eroded. Many datasets\nare simply not that big, so it\u2019s quite feasible to keep them entirely in memory, poten\u2010\n88 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bac6414-d57f-419d-bb1c-04f016018f4d": {"__data__": {"id_": "3bac6414-d57f-419d-bb1c-04f016018f4d", "embedding": null, "metadata": {"page_label": "89", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83d23e43-aee3-46b0-8eab-920371d96849", "node_type": "4", "metadata": {"page_label": "89", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4d5675abe8dfde413179060715999828edcc9ab873092c271bdbac2c22586f50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tially distributed across several machines. This has led to the development of in-\nmemory databases.\nSome in-memory key-value stores, such as Memcached, are intended for caching use\nonly, where it\u2019s acceptable for data to be lost if a machine is restarted. But other in-\nmemory databases aim for durability, which can be achieved with special hardware\n(such as battery-powered RAM), by writing a log of changes to disk, by writing peri\u2010\nodic snapshots to disk, or by replicating the in-memory state to other machines.\nWhen an in-memory database is restarted, it needs to reload its state, either from disk\nor over the network from a replica (unless special hardware is used). Despite writing\nto disk, it\u2019s still an in-memory database, because the disk is merely used as an\nappend-only log for durability, and reads are served entirely from memory. Writing\nto disk also has operational advantages: files on disk can easily be backed up,\ninspected, and analyzed by external utilities.\nProducts such as VoltDB, MemSQL, and Oracle TimesTen are in-memory databases\nwith a relational model, and the vendors claim that they can offer big performance\nimprovements by removing all the overheads associated with managing on-disk data\nstructures [ 41, 42]. RAMCloud is an open source, in-memory key-value store with\ndurability (using a log-structured approach for the data in memory as well as the data\non disk) [43]. Redis and Couchbase provide weak durability by writing to disk asyn\u2010\nchronously.\nCounterintuitively, the performance advantage of in-memory databases is not due to\nthe fact that they don\u2019t need to read from disk. Even a disk-based storage engine may\nnever need to read from disk if you have enough memory, because the operating sys\u2010\ntem caches recently used disk blocks in memory anyway. Rather, they can be faster\nbecause they can avoid the overheads of encoding in-memory data structures in a\nform that can be written to disk [44].\nBesides performance, another interesting area for in-memory databases is providing\ndata models that are difficult to implement with disk-based indexes. For example,\nRedis offers a database-like interface to various data structures such as priority\nqueues and sets. Because it keeps all data in memory, its implementation is compara\u2010\ntively simple.\nRecent research indicates that an in-memory database architecture could be extended\nto support datasets larger than the available memory, without bringing back the over\u2010\nheads of a disk-centric architecture [ 45]. The so-called anti-caching approach works\nby evicting the least recently used data from memory to disk when there is not\nenough memory, and loading it back into memory when it is accessed again in the\nfuture. This is similar to what operating systems do with virtual memory and swap\nfiles, but the database can manage memory more efficiently than the OS, as it can\nwork at the granularity of individual records rather than entire memory pages. This\nData Structures That Power Your Database | 89", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4e76447-c943-4995-986f-4fbe37003ffc": {"__data__": {"id_": "a4e76447-c943-4995-986f-4fbe37003ffc", "embedding": null, "metadata": {"page_label": "90", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6624e4f-7dbd-4716-b413-cd66f0055a69", "node_type": "4", "metadata": {"page_label": "90", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cf6c97d6f2801f3cda1a62a69421c9a0647ba2fe88adae1b7930dd97a968bd6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "approach still requires indexes to fit entirely in memory, though (like the Bitcask\nexample at the beginning of the chapter).\nFurther changes to storage engine design will probably be needed if non-volatile\nmemory (NVM) technologies become more widely adopted [ 46]. At present, this is a\nnew area of research, but it is worth keeping an eye on in the future. \nTransaction Processing or Analytics?\nIn the early days of business data processing, a write to the database typically corre\u2010\nsponded to a commercial transaction  taking place: making a sale, placing an order\nwith a supplier, paying an employee\u2019s salary, etc. As databases expanded into areas\nthat didn\u2019t involve money changing hands, the term transaction nevertheless stuck,\nreferring to a group of reads and writes that form a logical unit.\nA transaction needn\u2019t necessarily have ACID (atomicity, consis\u2010\ntency, isolation, and durability) properties. Transaction processing\njust means allowing clients to make low-latency reads and writes\u2014\nas opposed to batch processing  jobs, which only run periodically\n(for example, once per day). We discuss the ACID properties in\nChapter 7 and batch processing in Chapter 10.\nEven though databases started being used for many different kinds of data\u2014com\u2010\nments on blog posts, actions in a game, contacts in an address book, etc.\u2014the basic\naccess pattern remained similar to processing business transactions. An application\ntypically looks up a small number of records by some key, using an index. Records\nare inserted or updated based on the user\u2019s input. Because these applications are\ninteractive, the access pattern became known as online transaction processing\n(OLTP).\nHowever, databases also started being increasingly used for data analytics, which has\nvery different access patterns. Usually an analytic query needs to scan over a huge\nnumber of records, only reading a few columns per record, and calculates aggregate\nstatistics (such as count, sum, or average) rather than returning the raw data to the\nuser. For example, if your data is a table of sales transactions, then analytic queries\nmight be:\n\u2022 What was the total revenue of each of our stores in January?\n\u2022 How many more bananas than usual did we sell during our latest promotion?\n\u2022 Which brand of baby food is most often purchased together with brand X\ndiapers?\n90 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b12f8a0-7c7a-484d-8de8-4c7a1e66ec4f": {"__data__": {"id_": "0b12f8a0-7c7a-484d-8de8-4c7a1e66ec4f", "embedding": null, "metadata": {"page_label": "91", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "951c4c49-2d38-4604-a115-0f8dff56ed80", "node_type": "4", "metadata": {"page_label": "91", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "72096dc610aef49c0b325db4e1170d03193acf0e6d4b6f30053189ad8676fe25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. The meaning of online in OLAP is unclear; it probably refers to the fact that queries are not just for prede\u2010\nfined reports, but that analysts use the OLAP system interactively for explorative queries.\nThese queries are often written by business analysts, and feed into reports that help\nthe management of a company make better decisions ( business intelligence). In order\nto differentiate this pattern of using databases from transaction processing, it has\nbeen called online analytic processing  (OLAP) [ 47].iv The difference between OLTP\nand OLAP is not always clear-cut, but some typical characteristics are listed in\nTable 3-1.\nTable 3-1. Comparing characteristics of transaction processing versus analytic systems\nProperty Transaction processing systems (OLTP) Analytic systems (OLAP)\nMain read pattern Small number of records per query, fetched by key Aggregate over large number of records\nMain write pattern Random-access, low-latency writes from user input Bulk import (ETL) or event stream\nPrimarily used by End user/customer, via web application Internal analyst, for decision support\nWhat data represents Latest state of data (current point in time) History of events that happened over time\nDataset size Gigabytes to terabytes Terabytes to petabytes\nAt first, the same databases were used for both transaction processing and analytic\nqueries. SQL turned out to be quite flexible in this regard: it works well for OLTP-\ntype queries as well as OLAP-type queries. Nevertheless, in the late 1980s and early\n1990s, there was a trend for companies to stop using their OLTP systems for analytics\npurposes, and to run the analytics on a separate database instead. This separate data\u2010\nbase was called a data warehouse.\nData Warehousing\nAn enterprise may have dozens of different transaction processing systems: systems\npowering the customer-facing website, controlling point of sale (checkout) systems in\nphysical stores, tracking inventory in warehouses, planning routes for vehicles, man\u2010\naging suppliers, administering employees, etc. Each of these systems is complex and\nneeds a team of people to maintain it, so the systems end up operating mostly auton\u2010\nomously from each other.\nThese OLTP systems are usually expected to be highly available and to process trans\u2010\nactions with low latency, since they are often critical to the operation of the business.\nDatabase administrators therefore closely guard their OLTP databases. They are usu\u2010\nally reluctant to let business analysts run ad hoc analytic queries on an OLTP data\u2010\nbase, since those queries are often expensive, scanning large parts of the dataset,\nwhich can harm the performance of concurrently executing transactions.\nTransaction Processing or Analytics? | 91", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0250f6cd-9ffc-4d0a-8061-7d4677fda721": {"__data__": {"id_": "0250f6cd-9ffc-4d0a-8061-7d4677fda721", "embedding": null, "metadata": {"page_label": "92", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73b29697-d1cb-4732-a6d9-815a63da0d05", "node_type": "4", "metadata": {"page_label": "92", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1da3c8dba6c9116b664388960224a48fde61316f37cc5021eb4a758e830bb28f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A data warehouse, by contrast, is a separate database that analysts can query to their\nhearts\u2019 content, without affecting OLTP operations [ 48]. The data warehouse con\u2010\ntains a read-only copy of the data in all the various OLTP systems in the company.\nData is extracted from OLTP databases (using either a periodic data dump or a con\u2010\ntinuous stream of updates), transformed into an analysis-friendly schema, cleaned\nup, and then loaded into the data warehouse. This process of getting data into the\nwarehouse is known as Extract\u2013Transform\u2013Load (ETL) and is illustrated in\nFigure 3-8.\nFigure 3-8. Simplified outline of ETL into a data warehouse.\nData warehouses now exist in almost all large enterprises, but in small companies\nthey are almost unheard of. This is probably because most small companies don\u2019t\nhave so many different OLTP systems, and most small companies have a small\namount of data\u2014small enough that it can be queried in a conventional SQL database,\nor even analyzed in a spreadsheet. In a large company, a lot of heavy lifting is\nrequired to do something that is simple in a small company.\nA big advantage of using a separate data warehouse, rather than querying OLTP sys\u2010\ntems directly for analytics, is that the data warehouse can be optimized for analytic\naccess patterns. It turns out that the indexing algorithms discussed in the first half of\nthis chapter work well for OLTP, but are not very good at answering analytic queries.\n92 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "121ee863-9238-4561-99cd-46b3adbcd66e": {"__data__": {"id_": "121ee863-9238-4561-99cd-46b3adbcd66e", "embedding": null, "metadata": {"page_label": "93", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25ec912a-b6f7-45d4-b7d3-b03cefd03e07", "node_type": "4", "metadata": {"page_label": "93", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5c651de06a0b844677bf11bc0a0faadf6a8f51ca98f0cd9683c40133e8be4048", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the rest of this chapter we will look at storage engines that are optimized for ana\u2010\nlytics instead.\nThe divergence between OLTP databases and data warehouses\nThe data model of a data warehouse is most commonly relational, because SQL is\ngenerally a good fit for analytic queries. There are many graphical data analysis tools\nthat generate SQL queries, visualize the results, and allow analysts to explore the data\n(through operations such as drill-down and slicing and dicing).\nOn the surface, a data warehouse and a relational OLTP database look similar,\nbecause they both have a SQL query interface. However, the internals of the systems\ncan look quite different, because they are optimized for very different query patterns.\nMany database vendors now focus on supporting either transaction processing or\nanalytics workloads, but not both.\nSome databases, such as Microsoft SQL Server and SAP HANA, have support for\ntransaction processing and data warehousing in the same product. However, they are\nincreasingly becoming two separate storage and query engines, which happen to be\naccessible through a common SQL interface [49, 50, 51].\nData warehouse vendors such as Teradata, Vertica, SAP HANA, and ParAccel typi\u2010\ncally sell their systems under expensive commercial licenses. Amazon RedShift is a\nhosted version of ParAccel. More recently, a plethora of open source SQL-on-\nHadoop projects have emerged; they are young but aiming to compete with commer\u2010\ncial data warehouse systems. These include Apache Hive, Spark SQL, Cloudera\nImpala, Facebook Presto, Apache Tajo, and Apache Drill [ 52, 53]. Some of them are\nbased on ideas from Google\u2019s Dremel [54].\nStars and Snowflakes: Schemas for Analytics\nAs explored in Chapter 2, a wide range of different data models are used in the realm\nof transaction processing, depending on the needs of the application. On the other\nhand, in analytics, there is much less diversity of data models. Many data warehouses\nare used in a fairly formulaic style, known as a star schema  (also known as dimen\u2010\nsional modeling [55]).\nThe example schema in Figure 3-9 shows a data warehouse that might be found at a\ngrocery retailer. At the center of the schema is a so-called fact table (in this example,\nit is called fact_sales). Each row of the fact table represents an event that occurred\nat a particular time (here, each row represents a customer\u2019s purchase of a product). If\nwe were analyzing website traffic rather than retail sales, each row might represent a\npage view or a click by a user.\nTransaction Processing or Analytics? | 93", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9dd69bfe-0179-4c4b-984a-59f9361b9a22": {"__data__": {"id_": "9dd69bfe-0179-4c4b-984a-59f9361b9a22", "embedding": null, "metadata": {"page_label": "94", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb1b3245-8f85-498b-8c99-acfb26d5c969", "node_type": "4", "metadata": {"page_label": "94", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ffb0fbb94361b31b601856e5736cd5ff46b49f0a4b88c7d26ba595646e7cc68b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3-9. Example of a star schema for use in a data warehouse.\nUsually, facts are captured as individual events, because this allows maximum flexi\u2010\nbility of analysis later. However, this means that the fact table can become extremely\nlarge. A big enterprise like Apple, Walmart, or eBay may have tens of petabytes of\ntransaction history in its data warehouse, most of which is in fact tables [56].\nSome of the columns in the fact table are attributes, such as the price at which the\nproduct was sold and the cost of buying it from the supplier (allowing the profit mar\u2010\ngin to be calculated). Other columns in the fact table are foreign key references to\nother tables, called dimension tables. As each row in the fact table represents an event,\nthe dimensions represent the who, what, where, when, how, and why of the event.\nFor example, in Figure 3-9, one of the dimensions is the product that was sold. Each\nrow in the dim_product table represents one type of product that is for sale, including\n94 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce78b252-4ee3-4359-8dc7-e588c899ce7c": {"__data__": {"id_": "ce78b252-4ee3-4359-8dc7-e588c899ce7c", "embedding": null, "metadata": {"page_label": "95", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfd9bfb1-094d-4f66-be09-a912dcd9a4dc", "node_type": "4", "metadata": {"page_label": "95", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a1ef3480bf244985c5232024b01c438b6062e533e5842fa31dd9fbb9e6187ba9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "its stock-keeping unit (SKU), description, brand name, category, fat content, package\nsize, etc. Each row in the fact_sales table uses a foreign key to indicate which prod\u2010\nuct was sold in that particular transaction. (For simplicity, if the customer buys sev\u2010\neral different products at once, they are represented as separate rows in the fact\ntable.)\nEven date and time are often represented using dimension tables, because this allows\nadditional information about dates (such as public holidays) to be encoded, allowing\nqueries to differentiate between sales on holidays and non-holidays.\nThe name \u201cstar schema\u201d comes from the fact that when the table relationships are\nvisualized, the fact table is in the middle, surrounded by its dimension tables; the\nconnections to these tables are like the rays of a star.\nA variation of this template is known as the snowflake schema, where dimensions are\nfurther broken down into subdimensions. For example, there could be separate tables\nfor brands and product categories, and each row in the dim_product table could ref\u2010\nerence the brand and category as foreign keys, rather than storing them as strings in\nthe dim_product table. Snowflake schemas are more normalized than star schemas,\nbut star schemas are often preferred because they are simpler for analysts to work\nwith [55].\nIn a typical data warehouse, tables are often very wide: fact tables often have over 100\ncolumns, sometimes several hundred [51]. Dimension tables can also be very wide, as\nthey include all the metadata that may be relevant for analysis\u2014for example, the\ndim_store table may include details of which services are offered at each store,\nwhether it has an in-store bakery, the square footage, the date when the store was first\nopened, when it was last remodeled, how far it is from the nearest highway, etc. \nColumn-Oriented Storage\nIf you have trillions of rows and petabytes of data in your fact tables, storing and\nquerying them efficiently becomes a challenging problem. Dimension tables are usu\u2010\nally much smaller (millions of rows), so in this section we will concentrate primarily\non storage of facts.\nAlthough fact tables are often over 100 columns wide, a typical data warehouse query\nonly accesses 4 or 5 of them at one time ( \"SELECT *\" queries are rarely needed for\nanalytics) [ 51]. Take the query in Example 3-1 : it accesses a large number of rows\n(every occurrence of someone buying fruit or candy during the 2013 calendar year),\nbut it only needs to access three columns of the fact_sales table: date_key,\nproduct_sk, and quantity. The query ignores all other columns.\nColumn-Oriented Storage | 95", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "315d6c44-089f-4c97-a8df-c067e81bdbfe": {"__data__": {"id_": "315d6c44-089f-4c97-a8df-c067e81bdbfe", "embedding": null, "metadata": {"page_label": "96", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59e17e04-9380-4f53-9ac7-e9f2a0385485", "node_type": "4", "metadata": {"page_label": "96", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "98e07c6fa0ca0eea14893f119c9cd2ab2860d066e1718650c7c08aed2827c361", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example 3-1. Analyzing whether people are more inclined to buy fresh fruit or candy,\ndepending on the day of the week\nSELECT\n  dim_date.weekday, dim_product.category,\n  SUM(fact_sales.quantity) AS quantity_sold\nFROM fact_sales\n  JOIN dim_date    ON fact_sales.date_key   = dim_date.date_key\n  JOIN dim_product ON fact_sales.product_sk = dim_product.product_sk\nWHERE\n  dim_date.year = 2013 AND\n  dim_product.category IN ('Fresh fruit', 'Candy')\nGROUP BY\n  dim_date.weekday, dim_product.category;\nHow can we execute this query efficiently?\nIn most OLTP databases, storage is laid out in a row-oriented fashion: all the values\nfrom one row of a table are stored next to each other. Document databases are simi\u2010\nlar: an entire document is typically stored as one contiguous sequence of bytes. You\ncan see this in the CSV example of Figure 3-1.\nIn order to process a query like Example 3-1 , you may have indexes on\nfact_sales.date_key and/or fact_sales.product_sk that tell the storage engine\nwhere to find all the sales for a particular date or for a particular product. But then, a\nrow-oriented storage engine still needs to load all of those rows (each consisting of\nover 100 attributes) from disk into memory, parse them, and filter out those that\ndon\u2019t meet the required conditions. That can take a long time.\nThe idea behind column-oriented storage is simple: don\u2019t store all the values from one\nrow together, but store all the values from each column together instead. If each col\u2010\numn is stored in a separate file, a query only needs to read and parse those columns\nthat are used in that query, which can save a lot of work. This principle is illustrated\nin Figure 3-10. \nColumn storage is easiest to understand in a relational data model,\nbut it applies equally to nonrelational data. For example, Parquet\n[57] is a columnar storage format that supports a document data\nmodel, based on Google\u2019s Dremel [54].\n96 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94475ae2-be3f-4584-933d-a4ecd97b0bbe": {"__data__": {"id_": "94475ae2-be3f-4584-933d-a4ecd97b0bbe", "embedding": null, "metadata": {"page_label": "97", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4c8ef361-db9c-4853-a030-522ec6535aee", "node_type": "4", "metadata": {"page_label": "97", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cc33f2169b1b555490639ec9f9c123ffdfbb0c4e87e4fe698ed136489efe2fd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3-10. Storing relational data by column, rather than by row.\nThe column-oriented storage layout relies on each column file containing the rows in\nthe same order. Thus, if you need to reassemble an entire row, you can take the 23rd\nentry from each of the individual column files and put them together to form the\n23rd row of the table.\nColumn Compression\nBesides only loading those columns from disk that are required for a query, we can\nfurther reduce the demands on disk throughput by compressing data. Fortunately,\ncolumn-oriented storage often lends itself very well to compression.\nTake a look at the sequences of values for each column in Figure 3-10: they often look\nquite repetitive, which is a good sign for compression. Depending on the data in the\ncolumn, different compression techniques can be used. One technique that is particu\u2010\nlarly effective in data warehouses is bitmap encoding, illustrated in Figure 3-11.\nColumn-Oriented Storage | 97", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "479f0d68-dbf1-47a8-bf24-fe666cdbd224": {"__data__": {"id_": "479f0d68-dbf1-47a8-bf24-fe666cdbd224", "embedding": null, "metadata": {"page_label": "98", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d001133d-911d-4af4-9a0a-7ba952670862", "node_type": "4", "metadata": {"page_label": "98", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6c5d205160f35f606599d0e0bd327ad5ffdf4a032bef09807a4125f9644bf805", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3-11. Compressed, bitmap-indexed storage of a single column.\nOften, the number of distinct values in a column is small compared to the number of\nrows (for example, a retailer may have billions of sales transactions, but only 100,000\ndistinct products). We can now take a column with n distinct values and turn it into\nn separate bitmaps: one bitmap for each distinct value, with one bit for each row. The\nbit is 1 if the row has that value, and 0 if not.\nIf n is very small (for example, a country column may have approximately 200 dis\u2010\ntinct values), those bitmaps can be stored with one bit per row. But if n is bigger,\nthere will be a lot of zeros in most of the bitmaps (we say that they are sparse). In that\ncase, the bitmaps can additionally be run-length encoded, as shown at the bottom of\nFigure 3-11. This can make the encoding of a column remarkably compact.\nBitmap indexes such as these are very well suited for the kinds of queries that are\ncommon in a data warehouse. For example:\nWHERE product_sk IN (30, 68, 69):\nLoad the three bitmaps for product_sk = 30, product_sk = 68, and product_sk\n= 69, and calculate the bitwise OR of the three bitmaps, which can be done very\nefficiently.\n98 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28385301-280e-422e-b172-34d08728c15a": {"__data__": {"id_": "28385301-280e-422e-b172-34d08728c15a", "embedding": null, "metadata": {"page_label": "99", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f1b0618-4642-4184-9aeb-70e4323fa0fd", "node_type": "4", "metadata": {"page_label": "99", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8c6926791d7da10a8f5dabffdf95b66a14b28fc91daab7a068264d889d42166a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "WHERE product_sk = 31 AND store_sk = 3:\nLoad the bitmaps for product_sk = 31 and store_sk = 3, and calculate the bit\u2010\nwise AND. This works because the columns contain the rows in the same order,\nso the kth bit in one column\u2019s bitmap corresponds to the same row as the kth bit\nin another column\u2019s bitmap.\nThere are also various other compression schemes for different kinds of data, but we\nwon\u2019t go into them in detail\u2014see [58] for an overview.\nColumn-oriented storage and column families\nCassandra and HBase have a concept of column families , which\nthey inherited from Bigtable [ 9]. However, it is very misleading to\ncall them column-oriented: within each column family, they store\nall columns from a row together, along with a row key, and they do\nnot use column compression. Thus, the Bigtable model is still\nmostly row-oriented.\nMemory bandwidth and vectorized processing\nFor data warehouse queries that need to scan over millions of rows, a big bottleneck\nis the bandwidth for getting data from disk into memory. However, that is not the\nonly bottleneck. Developers of analytical databases also worry about efficiently using\nthe bandwidth from main memory into the CPU cache, avoiding branch mispredic\u2010\ntions and bubbles in the CPU instruction processing pipeline, and making use of\nsingle-instruction-multi-data (SIMD) instructions in modern CPUs [59, 60].\nBesides reducing the volume of data that needs to be loaded from disk, column-\noriented storage layouts are also good for making efficient use of CPU cycles. For\nexample, the query engine can take a chunk of compressed column data that fits\ncomfortably in the CPU\u2019s L1 cache and iterate through it in a tight loop (that is, with\nno function calls). A CPU can execute such a loop much faster than code that\nrequires a lot of function calls and conditions for each record that is processed. Col\u2010\numn compression allows more rows from a column to fit in the same amount of L1\ncache. Operators, such as the bitwise AND and OR described previously, can be\ndesigned to operate on such chunks of compressed column data directly. This techni\u2010\nque is known as vectorized processing [58, 49]. \nSort Order in Column Storage\nIn a column store, it doesn\u2019t necessarily matter in which order the rows are stored.\nIt\u2019s easiest to store them in the order in which they were inserted, since then inserting\na new row just means appending to each of the column files. However, we can choose\nto impose an order, like we did with SSTables previously, and use that as an indexing\nmechanism.\nColumn-Oriented Storage | 99", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d0639f9-d01e-4ed8-97bf-b4ed07d0bddf": {"__data__": {"id_": "8d0639f9-d01e-4ed8-97bf-b4ed07d0bddf", "embedding": null, "metadata": {"page_label": "100", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc798519-767a-430c-a81a-dc7c36638aa6", "node_type": "4", "metadata": {"page_label": "100", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "92171d41a3331f11c509acfcc38a127c365442b4db9f03f96b48893834d033b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that it wouldn\u2019t make sense to sort each column independently, because then\nwe would no longer know which items in the columns belong to the same row. We\ncan only reconstruct a row because we know that the kth item in one column belongs\nto the same row as the kth item in another column.\nRather, the data needs to be sorted an entire row at a time, even though it is stored by\ncolumn. The administrator of the database can choose the columns by which the\ntable should be sorted, using their knowledge of common queries. For example, if\nqueries often target date ranges, such as the last month, it might make sense to make\ndate_key the first sort key. Then the query optimizer can scan only the rows from the\nlast month, which will be much faster than scanning all rows.\nA second column can determine the sort order of any rows that have the same value\nin the first column. For example, if date_key is the first sort key in Figure 3-10 , it\nmight make sense for product_sk to be the second sort key so that all sales for the\nsame product on the same day are grouped together in storage. That will help queries\nthat need to group or filter sales by product within a certain date range.\nAnother advantage of sorted order is that it can help with compression of columns. If\nthe primary sort column does not have many distinct values, then after sorting, it will\nhave long sequences where the same value is repeated many times in a row. A simple\nrun-length encoding, like we used for the bitmaps in Figure 3-11 , could compress\nthat column down to a few kilobytes\u2014even if the table has billions of rows.\nThat compression effect is strongest on the first sort key. The second and third sort\nkeys will be more jumbled up, and thus not have such long runs of repeated values.\nColumns further down the sorting priority appear in essentially random order, so\nthey probably won\u2019t compress as well. But having the first few columns sorted is still\na win overall.\nSeveral different sort orders\nA clever extension of this idea was introduced in C-Store and adopted in the com\u2010\nmercial data warehouse Vertica [ 61, 62]. Different queries benefit from different sort\norders, so why not store the same data sorted in several different ways? Data needs to\nbe replicated to multiple machines anyway, so that you don\u2019t lose data if one machine\nfails. You might as well store that redundant data sorted in different ways so that\nwhen you\u2019re processing a query, you can use the version that best fits the query\npattern.\nHaving multiple sort orders in a column-oriented store is a bit similar to having mul\u2010\ntiple secondary indexes in a row-oriented store. But the big difference is that the row-\noriented store keeps every row in one place (in the heap file or a clustered index), and\nsecondary indexes just contain pointers to the matching rows. In a column store,\nthere normally aren\u2019t any pointers to data elsewhere, only columns containing values. \n100 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffdb2205-9058-41a4-b42b-663cdeae02d5": {"__data__": {"id_": "ffdb2205-9058-41a4-b42b-663cdeae02d5", "embedding": null, "metadata": {"page_label": "101", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "124b4c50-fdd5-4ebe-856d-839d9f1538b1", "node_type": "4", "metadata": {"page_label": "101", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6618e7720bb63a07fadd3625fd75cc73de1af30c341d6ae6935a660b98cb5e39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Writing to Column-Oriented Storage\nThese optimizations make sense in data warehouses, because most of the load con\u2010\nsists of large read-only queries run by analysts. Column-oriented storage, compres\u2010\nsion, and sorting all help to make those read queries faster. However, they have the\ndownside of making writes more difficult.\nAn update-in-place approach, like B-trees use, is not possible with compressed col\u2010\numns. If you wanted to insert a row in the middle of a sorted table, you would most\nlikely have to rewrite all the column files. As rows are identified by their position\nwithin a column, the insertion has to update all columns consistently.\nFortunately, we have already seen a good solution earlier in this chapter: LSM-trees.\nAll writes first go to an in-memory store, where they are added to a sorted structure\nand prepared for writing to disk. It doesn\u2019t matter whether the in-memory store is\nrow-oriented or column-oriented. When enough writes have accumulated, they are\nmerged with the column files on disk and written to new files in bulk. This is essen\u2010\ntially what Vertica does [62].\nQueries need to examine both the column data on disk and the recent writes in mem\u2010\nory, and combine the two. However, the query optimizer hides this distinction from\nthe user. From an analyst\u2019s point of view, data that has been modified with inserts,\nupdates, or deletes is immediately reflected in subsequent queries. \nAggregation: Data Cubes and Materialized Views\nNot every data warehouse is necessarily a column store: traditional row-oriented\ndatabases and a few other architectures are also used. However, columnar storage can\nbe significantly faster for ad hoc analytical queries, so it is rapidly gaining popularity\n[51, 63].\nAnother aspect of data warehouses that is worth mentioning briefly is materialized\naggregates. As discussed earlier, data warehouse queries often involve an aggregate\nfunction, such as COUNT, SUM, AVG, MIN, or MAX in SQL. If the same aggregates are used\nby many different queries, it can be wasteful to crunch through the raw data every\ntime. Why not cache some of the counts or sums that queries use most often?\nOne way of creating such a cache is a materialized view. In a relational data model, it\nis often defined like a standard (virtual) view: a table-like object whose contents are\nthe results of some query. The difference is that a materialized view is an actual copy\nof the query results, written to disk, whereas a virtual view is just a shortcut for writ\u2010\ning queries. When you read from a virtual view, the SQL engine expands it into the\nview\u2019s underlying query on the fly and then processes the expanded query.\nWhen the underlying data changes, a materialized view needs to be updated, because\nit is a denormalized copy of the data. The database can do that automatically, but\nColumn-Oriented Storage | 101", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3eb283ba-a6d0-4c5a-ad84-d7b83a5371bc": {"__data__": {"id_": "3eb283ba-a6d0-4c5a-ad84-d7b83a5371bc", "embedding": null, "metadata": {"page_label": "102", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fb080f3-6d65-484c-9181-880033258288", "node_type": "4", "metadata": {"page_label": "102", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6aca7b4ac8df92b2d83733e01a3daac0ca768179673c4ccf7adc1c87305ddf3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "such updates make writes more expensive, which is why materialized views are not\noften used in OLTP databases. In read-heavy data warehouses they can make more\nsense (whether or not they actually improve read performance depends on the indi\u2010\nvidual case).\nA common special case of a materialized view is known as a data cube or OLAP cube\n[64]. It is a grid of aggregates grouped by different dimensions. Figure 3-12 shows an\nexample.\nFigure 3-12. Two dimensions of a data cube, aggregating data by summing.\nImagine for now that each fact has foreign keys to only two dimension tables\u2014in\nFigure 3-12, these are date and product. You can now draw a two-dimensional table,\nwith dates along one axis and products along the other. Each cell contains the aggre\u2010\ngate (e.g., SUM) of an attribute (e.g., net_price) of all facts with that date-product\ncombination. Then you can apply the same aggregate along each row or column and\nget a summary that has been reduced by one dimension (the sales by product regard\u2010\nless of date, or the sales by date regardless of product).\nIn general, facts often have more than two dimensions. In Figure 3-9 there are five\ndimensions: date, product, store, promotion, and customer. It\u2019s a lot harder to imag\u2010\nine what a five-dimensional hypercube would look like, but the principle remains the\nsame: each cell contains the sales for a particular date-product-store-promotion-\ncustomer combination. These values can then repeatedly be summarized along each\nof the dimensions.\nThe advantage of a materialized data cube is that certain queries become very fast\nbecause they have effectively been precomputed. For example, if you want to know\n102 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ff95447-7a05-4d05-b932-2af443623148": {"__data__": {"id_": "2ff95447-7a05-4d05-b932-2af443623148", "embedding": null, "metadata": {"page_label": "103", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3302a6dd-22f8-4137-8488-8a627e163b02", "node_type": "4", "metadata": {"page_label": "103", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d183f22deab112ce2513a1a97e37535b28ebd6aeb01bdaf147bc89c466b7f071", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the total sales per store yesterday, you just need to look at the totals along the appro\u2010\npriate dimension\u2014no need to scan millions of rows.\nThe disadvantage is that a data cube doesn\u2019t have the same flexibility as querying the\nraw data. For example, there is no way of calculating which proportion of sales comes\nfrom items that cost more than $100, because the price isn\u2019t one of the dimensions.\nMost data warehouses therefore try to keep as much raw data as possible, and use\naggregates such as data cubes only as a performance boost for certain queries.\nSummary\nIn this chapter we tried to get to the bottom of how databases handle storage and\nretrieval. What happens when you store data in a database, and what does the data\u2010\nbase do when you query for the data again later?\nOn a high level, we saw that storage engines fall into two broad categories: those opti\u2010\nmized for transaction processing (OLTP), and those optimized for analytics (OLAP).\nThere are big differences between the access patterns in those use cases:\n\u2022 OLTP systems are typically user-facing, which means that they may see a huge\nvolume of requests. In order to handle the load, applications usually only touch a\nsmall number of records in each query. The application requests records using\nsome kind of key, and the storage engine uses an index to find the data for the\nrequested key. Disk seek time is often the bottleneck here.\n\u2022 Data warehouses and similar analytic systems are less well known, because they\nare primarily used by business analysts, not by end users. They handle a much\nlower volume of queries than OLTP systems, but each query is typically very\ndemanding, requiring many millions of records to be scanned in a short time.\nDisk bandwidth (not seek time) is often the bottleneck here, and column-\noriented storage is an increasingly popular solution for this kind of workload.\nOn the OLTP side, we saw storage engines from two main schools of thought:\n\u2022 The log-structured school, which only permits appending to files and deleting\nobsolete files, but never updates a file that has been written. Bitcask, SSTables,\nLSM-trees, LevelDB, Cassandra, HBase, Lucene, and others belong to this group.\n\u2022 The update-in-place school, which treats the disk as a set of fixed-size pages that\ncan be overwritten. B-trees are the biggest example of this philosophy, being used\nin all major relational databases and also many nonrelational ones.\nLog-structured storage engines are a comparatively recent development. Their key\nidea is that they systematically turn random-access writes into sequential writes on\ndisk, which enables higher write throughput due to the performance characteristics\nof hard drives and SSDs.\nSummary | 103", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da43aa1a-7381-4b58-9b35-6520c780183f": {"__data__": {"id_": "da43aa1a-7381-4b58-9b35-6520c780183f", "embedding": null, "metadata": {"page_label": "104", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8a9a860-2380-4bd7-842d-71fe3fdb8f6c", "node_type": "4", "metadata": {"page_label": "104", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3bb8a72e152794dd7c168a21c118a65e07d34e84c6011f04df835ab2cb5d9884", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finishing off the OLTP side, we did a brief tour through some more complicated\nindexing structures, and databases that are optimized for keeping all data in memory.\nWe then took a detour from the internals of storage engines to look at the high-level\narchitecture of a typical data warehouse. This background illustrated why analytic\nworkloads are so different from OLTP: when your queries require sequentially scan\u2010\nning across a large number of rows, indexes are much less relevant. Instead it\nbecomes important to encode data very compactly, to minimize the amount of data\nthat the query needs to read from disk. We discussed how column-oriented storage\nhelps achieve this goal.\nAs an application developer, if you\u2019re armed with this knowledge about the internals\nof storage engines, you are in a much better position to know which tool is best suited\nfor your particular application. If you need to adjust a database\u2019s tuning parameters,\nthis understanding allows you to imagine what effect a higher or a lower value may\nhave.\nAlthough this chapter couldn\u2019t make you an expert in tuning any one particular stor\u2010\nage engine, it has hopefully equipped you with enough vocabulary and ideas that you\ncan make sense of the documentation for the database of your choice. \nReferences\n[1] Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman: Data Structures and\nAlgorithms. Addison-Wesley, 1983. ISBN: 978-0-201-00023-8\n[2] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein:\nIntroduction to Algorithms, 3rd edition. MIT Press, 2009. ISBN: 978-0-262-53305-8\n[3] Justin Sheehy and David Smith: \u201c Bitcask: A Log-Structured Hash Table for Fast\nKey/Value Data,\u201d Basho Technologies, April 2010.\n[4] Yinan Li, Bingsheng He, Robin Jun Yang, et al.: \u201c Tree Indexing on Solid State\nDrives,\u201d Proceedings of the VLDB Endowment, volume 3, number 1, pages 1195\u20131206,\nSeptember 2010.\n[5] Goetz Graefe: \u201c Modern B-Tree Techniques ,\u201d Foundations and Trends in Data\u2010\nbases, volume 3, number 4, pages 203\u2013402, August 2011. doi:10.1561/1900000028\n[6] Jeffrey Dean and Sanjay Ghemawat: \u201c LevelDB Implementation Notes ,\u201d lev\u2010\neldb.googlecode.com.\n[7] Dhruba Borthakur: \u201c The History of RocksDB ,\u201d rocksdb.blogspot.com, November\n24, 2013.\n[8] Matteo Bertozzi: \u201cApache HBase I/O \u2013 HFile,\u201d blog.cloudera.com, June, 29 2012.\n104 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "803a94af-7bfd-438a-932e-914b7fdaa30d": {"__data__": {"id_": "803a94af-7bfd-438a-932e-914b7fdaa30d", "embedding": null, "metadata": {"page_label": "105", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12290d7f-3841-4666-a602-db5ddba1c7c2", "node_type": "4", "metadata": {"page_label": "105", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "89547e9dc400c22c64fc227f6cd35658c09f77bf84eb5dc9242a58f098dce6ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[9] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, et al.: \u201c Bigtable: A Distributed Stor\u2010\nage System for Structured Data ,\u201d at 7th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), November 2006.\n[10] Patrick O\u2019Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O\u2019Neil: \u201c The Log-\nStructured Merge-Tree (LSM-Tree) ,\u201d Acta Informatica, volume 33, number 4, pages\n351\u2013385, June 1996. doi:10.1007/s002360050048\n[11] Mendel Rosenblum and John K. Ousterhout: \u201c The Design and Implementation\nof a Log-Structured File System ,\u201d ACM Transactions on Computer Systems , volume\n10, number 1, pages 26\u201352, February 1992. doi:10.1145/146941.146943\n[12] Adrien Grand: \u201cWhat Is in a Lucene Index?,\u201d at Lucene/Solr Revolution, Novem\u2010\nber 14, 2013.\n[13] Deepak Kandepet: \u201c Hacking Lucene\u2014The Index Format ,\u201d hackerlabs.org, Octo\u2010\nber 1, 2011.\n[14] Michael McCandless: \u201cVisualizing Lucene\u2019s Segment Merges,\u201d blog.mikemccand\u2010\nless.com, February 11, 2011.\n[15] Burton H. Bloom: \u201c Space/Time Trade-offs in Hash Coding with Allowable\nErrors,\u201d Communications of the ACM , volume 13, number 7, pages 422\u2013426, July\n1970. doi:10.1145/362686.362692\n[16] \u201c Operating Cassandra: Compaction ,\u201d Apache Cassandra Documentation v4.0,\n2016.\n[17] Rudolf Bayer and Edward M. McCreight: \u201c Organization and Maintenance of\nLarge Ordered Indices ,\u201d Boeing Scientific Research Laboratories, Mathematical and\nInformation Sciences Laboratory, report no. 20, July 1970.\n[18] Douglas Comer: \u201cThe Ubiquitous B-Tree,\u201d ACM Computing Surveys, volume 11,\nnumber 2, pages 121\u2013137, June 1979. doi:10.1145/356770.356776\n[19] Emmanuel Goossaert: \u201cCoding for SSDs,\u201d codecapsule.com, February 12, 2014.\n[20] C. Mohan and Frank Levine: \u201c ARIES/IM: An Efficient and High Concurrency\nIndex Management Method Using Write-Ahead Logging ,\u201d at ACM International\nConference on Management of Data  (SIGMOD), June 1992. doi:\n10.1145/130283.130338\n[21] Howard Chu: \u201cLDAP at Lightning Speed,\u201d at Build Stuff \u201914, November 2014.\n[22] Bradley C. Kuszmaul: \u201c A Comparison of Fractal Trees to Log-Structured Merge\n(LSM) Trees,\u201d tokutek.com, April 22, 2014.\n[23] Manos Athanassoulis, Michael S. Kester, Lukas M. Maas, et al.: \u201c Designing\nAccess Methods: The RUM Conjecture ,\u201d at 19th International Conference on Extend\u2010\ning Database Technology (EDBT), March 2016. doi:10.5441/002/edbt.2016.42\nSummary | 105", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61d4e394-6479-469b-9d78-479679e71c70": {"__data__": {"id_": "61d4e394-6479-469b-9d78-479679e71c70", "embedding": null, "metadata": {"page_label": "106", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8de76ca1-1478-4cf3-b08c-af18a5ae2ff6", "node_type": "4", "metadata": {"page_label": "106", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "735d0001199a6c266307820aacada19e2861783d15d523e4400407866d2a8c3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[24] Peter Zaitsev: \u201cInnodb Double Write,\u201d percona.com, August 4, 2006.\n[25] Tomas Vondra: \u201c On the Impact of Full-Page Writes ,\u201d blog.2ndquadrant.com,\nNovember 23, 2016.\n[26] Mark Callaghan: \u201c The Advantages of an LSM vs a B-Tree ,\u201d smalldatum.blog\u2010\nspot.co.uk, January 19, 2016.\n[27] Mark Callaghan: \u201c Choosing Between Efficiency and Performance with\nRocksDB,\u201d at Code Mesh, November 4, 2016.\n[28] Michi Mutsuzaki: \u201cMySQL vs. LevelDB,\u201d github.com, August 2011.\n[29] Benjamin Coverston, Jonathan Ellis, et al.: \u201c CASSANDRA-1608: Redesigned\nCompaction, issues.apache.org, July 2011.\n[30] Igor Canadi, Siying Dong, and Mark Callaghan: \u201c RocksDB Tuning Guide ,\u201d git\u2010\nhub.com, 2016.\n[31] MySQL 5.7 Reference Manual. Oracle, 2014.\n[32] Books Online for SQL Server 2012. Microsoft, 2012.\n[33] Joe Webb: \u201c Using Covering Indexes to Improve Query Performance ,\u201d simple-\ntalk.com, 29 September 2008.\n[34] Frank Ramsak, Volker Markl, Robert Fenk, et al.: \u201c Integrating the UB-Tree into\na Database System Kernel ,\u201d at 26th International Conference on Very Large Data\nBases (VLDB), September 2000.\n[35] The PostGIS Development Group: \u201cPostGIS 2.1.2dev Manual,\u201d postgis.net, 2014.\n[36] Robert Escriva, Bernard Wong, and Emin G\u00fcn Sirer: \u201c HyperDex: A Distributed,\nSearchable Key-Value Store ,\u201d at ACM SIGCOMM Conference , August 2012. doi:\n10.1145/2377677.2377681\n[37] Michael McCandless: \u201c Lucene\u2019s FuzzyQuery Is 100 Times Faster in 4.0 ,\u201d\nblog.mikemccandless.com, March 24, 2011.\n[38] Steffen Heinz, Justin Zobel, and Hugh E. Williams: \u201cBurst Tries: A Fast, Efficient\nData Structure for String Keys ,\u201d ACM Transactions on Information Systems , volume\n20, number 2, pages 192\u2013223, April 2002. doi:10.1145/506309.506312\n[39] Klaus U. Schulz and Stoyan Mihov: \u201c Fast String Correction with Levenshtein\nAutomata,\u201d International Journal on Document Analysis and Recognition , volume 5,\nnumber 1, pages 67\u201385, November 2002. doi:10.1007/s10032-002-0082-8\n[40] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\u00fctze: Introduc\u2010\ntion to Information Retrieval . Cambridge University Press, 2008. ISBN:\n978-0-521-86571-5, available online at nlp.stanford.edu/IR-book\n106 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f251fcfa-904b-4e5d-846e-0e3fd9e9c99b": {"__data__": {"id_": "f251fcfa-904b-4e5d-846e-0e3fd9e9c99b", "embedding": null, "metadata": {"page_label": "107", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63fa029d-cf64-41f9-bd0d-d4bfc46210db", "node_type": "4", "metadata": {"page_label": "107", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7e1bd2f6c61cb8d607295d75c1dec9918080c066bd79a2eef06b55471a1ce50a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[41] Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.: \u201c The End of an\nArchitectural Era (It\u2019s Time for a Complete Rewrite) ,\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007.\n[42] \u201cVoltDB Technical Overview White Paper,\u201d VoltDB, 2014.\n[43] Stephen M. Rumble, Ankita Kejriwal, and John K. Ousterhout: \u201c Log-Structured\nMemory for DRAM-Based Storage ,\u201d at 12th USENIX Conference on File and Storage\nTechnologies (FAST), February 2014.\n[44] Stavros Harizopoulos, Daniel J. Abadi, Samuel Madden, and Michael Stone\u2010\nbraker: \u201c OLTP Through the Looking Glass, and What We Found There ,\u201d at ACM\nInternational Conference on Management of Data  (SIGMOD), June 2008. doi:\n10.1145/1376616.1376713\n[45] Justin DeBrabant, Andrew Pavlo, Stephen Tu, et al.: \u201c Anti-Caching: A New\nApproach to Database Management System Architecture ,\u201d Proceedings of the VLDB\nEndowment, volume 6, number 14, pages 1942\u20131953, September 2013.\n[46] Joy Arulraj, Andrew Pavlo, and Subramanya R. Dulloor: \u201c Let\u2019s Talk About Stor\u2010\nage & Recovery Methods for Non-Volatile Memory Database Systems ,\u201d at ACM\nInternational Conference on Management of Data  (SIGMOD), June 2015. doi:\n10.1145/2723372.2749441\n[47] Edgar F. Codd, S. B. Codd, and C. T. Salley: \u201c Providing OLAP to User-Analysts:\nAn IT Mandate,\u201d E. F. Codd Associates, 1993.\n[48] Surajit Chaudhuri and Umeshwar Dayal: \u201c An Overview of Data Warehousing\nand OLAP Technology,\u201d ACM SIGMOD Record, volume 26, number 1, pages 65\u201374,\nMarch 1997. doi:10.1145/248603.248616\n[49] Per-\u00c5ke Larson, Cipri Clinciu, Campbell Fraser, et al.: \u201c Enhancements to SQL\nServer Column Stores ,\u201d at ACM International Conference on Management of Data\n(SIGMOD), June 2013.\n[50] Franz F\u00e4rber, Norman May, Wolfgang Lehner, et al.: \u201cThe SAP HANA Database\n\u2013 An Architecture Overview,\u201d IEEE Data Engineering Bulletin, volume 35, number 1,\npages 28\u201333, March 2012.\n[51] Michael Stonebraker: \u201c The Traditional RDBMS Wisdom Is (Almost Certainly)\nAll Wrong,\u201d presentation at EPFL, May 2013.\n[52] Daniel J. Abadi: \u201cClassifying the SQL-on-Hadoop Solutions,\u201d hadapt.com, Octo\u2010\nber 2, 2013.\n[53] Marcel Kornacker, Alexander Behm, Victor Bittorf, et al.: \u201c Impala: A Modern,\nOpen-Source SQL Engine for Hadoop ,\u201d at 7th Biennial Conference on Innovative\nData Systems Research (CIDR), January 2015.\nSummary | 107", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2335, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d261c2a7-41d1-4a96-83ed-20c73daf661f": {"__data__": {"id_": "d261c2a7-41d1-4a96-83ed-20c73daf661f", "embedding": null, "metadata": {"page_label": "108", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35dfe2e9-f655-49f3-9b1c-fc17d09e9a01", "node_type": "4", "metadata": {"page_label": "108", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1df5cdedad991a08baa0c2a75f915668993319ef52fe114f6cdd62ec7017ebe6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[54] Sergey Melnik, Andrey Gubarev, Jing Jing Long, et al.: \u201c Dremel: Interactive\nAnalysis of Web-Scale Datasets,\u201d at 36th International Conference on Very Large Data\nBases (VLDB), pages 330\u2013339, September 2010.\n[55] Ralph Kimball and Margy Ross: The Data Warehouse Toolkit: The Definitive\nGuide to Dimensional Modeling , 3rd edition. John Wiley & Sons, July 2013. ISBN:\n978-1-118-53080-1\n[56] Derrick Harris: \u201cWhy Apple, eBay, and Walmart Have Some of the Biggest Data\nWarehouses You\u2019ve Ever Seen,\u201d gigaom.com, March 27, 2013.\n[57] Julien Le Dem: \u201c Dremel Made Simple with Parquet ,\u201d blog.twitter.com, Septem\u2010\nber 11, 2013.\n[58] Daniel J. Abadi, Peter Boncz, Stavros Harizopoulos, et al.: \u201c The Design and\nImplementation of Modern Column-Oriented Database Systems ,\u201d Foundations and\nTrends in Databases , volume 5, number 3, pages 197\u2013280, December 2013. doi:\n10.1561/1900000024\n[59] Peter Boncz, Marcin Zukowski, and Niels Nes: \u201c MonetDB/X100: Hyper-\nPipelining Query Execution,\u201d at 2nd Biennial Conference on Innovative Data Systems\nResearch (CIDR), January 2005.\n[60] Jingren Zhou and Kenneth A. Ross: \u201c Implementing Database Operations Using\nSIMD Instructions,\u201d at ACM International Conference on Management of Data  (SIG\u2010\nMOD), pages 145\u2013156, June 2002. doi:10.1145/564691.564709\n[61] Michael Stonebraker, Daniel J. Abadi, Adam Batkin, et al.: \u201c C-Store: A Column-\noriented DBMS,\u201d at 31st International Conference on Very Large Data Bases  (VLDB),\npages 553\u2013564, September 2005.\n[62] Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, et al.: \u201c The Vertica Ana\u2010\nlytic Database: C-Store 7 Years Later ,\u201d Proceedings of the VLDB Endowment , volume\n5, number 12, pages 1790\u20131801, August 2012.\n[63] Julien Le Dem and Nong Li: \u201c Efficient Data Storage for Analytics with Apache\nParquet 2.0,\u201d at Hadoop Summit, San Jose, June 2014.\n[64] Jim Gray, Surajit Chaudhuri, Adam Bosworth, et al.: \u201c Data Cube: A Relational\nAggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals ,\u201d Data\nMining and Knowledge Discovery , volume 1, number 1, pages 29\u201353, March 2007.\ndoi:10.1023/A:1009726021843\n108 | Chapter 3: Storage and Retrieval", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2955a4c-b889-4a6c-b478-078006eb11c5": {"__data__": {"id_": "e2955a4c-b889-4a6c-b478-078006eb11c5", "embedding": null, "metadata": {"page_label": "109", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eda0303e-0beb-49be-8d08-3ee5d9c8aea6", "node_type": "4", "metadata": {"page_label": "109", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48b3a700-4052-4141-a777-b7423a9f8998": {"__data__": {"id_": "48b3a700-4052-4141-a777-b7423a9f8998", "embedding": null, "metadata": {"page_label": "110", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b73c1a03-565b-4223-baa0-5d9d0417dcc7", "node_type": "4", "metadata": {"page_label": "110", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8261a33-323c-45fe-a8dd-40033faaf376": {"__data__": {"id_": "b8261a33-323c-45fe-a8dd-40033faaf376", "embedding": null, "metadata": {"page_label": "111", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "345a8a3d-4e15-4d26-82b8-ec307d36e7ba", "node_type": "4", "metadata": {"page_label": "111", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d608246226b90fc0eeb4e0514d88f274c79946586d848204030843dda8dc8eff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 4\nEncoding and Evolution\nEverything changes and nothing stands still.\n\u2014Heraclitus of Ephesus, as quoted by Plato in Cratylus (360 BCE)\nApplications inevitably change over time. Features are added or modified as new\nproducts are launched, user requirements become better understood, or business cir\u2010\ncumstances change. In Chapter 1 we introduced the idea of evolvability: we should\naim to build systems that make it easy to adapt to change (see \u201cEvolvability: Making\nChange Easy\u201d on page 21).\nIn most cases, a change to an application\u2019s features also requires a change to data that\nit stores: perhaps a new field or record type needs to be captured, or perhaps existing\ndata needs to be presented in a new way.\nThe data models we discussed in Chapter 2 have different ways of coping with such\nchange. Relational databases generally assume that all data in the database conforms\nto one schema: although that schema can be changed (through schema migrations;\ni.e., ALTER statements), there is exactly one schema in force at any one point in time.\nBy contrast, schema-on-read (\u201cschemaless\u201d) databases don\u2019t enforce a schema, so the\ndatabase can contain a mixture of older and newer data formats written at different\ntimes (see \u201cSchema flexibility in the document model\u201d on page 39).\nWhen a data format or schema changes, a corresponding change to application code\noften needs to happen (for example, you add a new field to a record, and the applica\u2010\ntion code starts reading and writing that field). However, in a large application, code\nchanges often cannot happen instantaneously:\n111", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "575fec38-86d0-4072-8f68-bf6c5817834e": {"__data__": {"id_": "575fec38-86d0-4072-8f68-bf6c5817834e", "embedding": null, "metadata": {"page_label": "112", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eed8740c-838b-4ea7-8d9e-24766f5de35a", "node_type": "4", "metadata": {"page_label": "112", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d4c09c57b4a493199aff5c194d4a796eda57cd9d2dba051cc93622cefa74a7f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 With server-side applications you may want to perform a rolling upgrade  (also\nknown as a staged rollout), deploying the new version to a few nodes at a time,\nchecking whether the new version is running smoothly, and gradually working\nyour way through all the nodes. This allows new versions to be deployed without\nservice downtime, and thus encourages more frequent releases and better evolva\u2010\nbility.\n\u2022 With client-side applications you\u2019re at the mercy of the user, who may not install\nthe update for some time.\nThis means that old and new versions of the code, and old and new data formats,\nmay potentially all coexist in the system at the same time. In order for the system to\ncontinue running smoothly, we need to maintain compatibility in both directions:\nBackward compatibility\nNewer code can read data that was written by older code.\nForward compatibility\nOlder code can read data that was written by newer code.\nBackward compatibility is normally not hard to achieve: as author of the newer code,\nyou know the format of data written by older code, and so you can explicitly handle it\n(if necessary by simply keeping the old code to read the old data). Forward compati\u2010\nbility can be trickier, because it requires older code to ignore additions made by a\nnewer version of the code.\nIn this chapter we will look at several formats for encoding data, including JSON,\nXML, Protocol Buffers, Thrift, and Avro. In particular, we will look at how they han\u2010\ndle schema changes and how they support systems where old and new data and code\nneed to coexist. We will then discuss how those formats are used for data storage and\nfor communication: in web services, Representational State Transfer (REST), and\nremote procedure calls (RPC), as well as message-passing systems such as actors and\nmessage queues.\nFormats for Encoding Data\nPrograms usually work with data in (at least) two different representations:\n1. In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so\non. These data structures are optimized for efficient access and manipulation by\nthe CPU (typically using pointers).\n2. When you want to write data to a file or send it over the network, you have to\nencode it as some kind of self-contained sequence of bytes (for example, a JSON\ndocument). Since a pointer wouldn\u2019t make sense to any other process, this\n112 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22500a9d-7f9d-4f63-bc64-e47857a76df4": {"__data__": {"id_": "22500a9d-7f9d-4f63-bc64-e47857a76df4", "embedding": null, "metadata": {"page_label": "113", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e186f9ad-7aeb-4bb8-839d-39009f37fbd4", "node_type": "4", "metadata": {"page_label": "113", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "80bd13eb333b54fb32ec2f59f6311b73253cfadd2fcbd60c52c1733e6885135c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. With the exception of some special cases, such as certain memory-mapped files or when operating directly\non compressed data (as described in \u201cColumn Compression\u201d on page 97).\nii. Note that encoding has nothing to do with encryption. We don\u2019t discuss encryption in this book.\nsequence-of-bytes representation looks quite different from the data structures\nthat are normally used in memory.i\nThus, we need some kind of translation between the two representations. The trans\u2010\nlation from the in-memory representation to a byte sequence is called encoding (also\nknown as serialization or marshalling), and the reverse is called decoding (parsing,\ndeserialization, unmarshalling).ii\nTerminology clash\nSerialization is unfortunately also used in the context of transac\u2010\ntions (see Chapter 7 ), with a completely different meaning. To\navoid overloading the word we\u2019ll stick with encoding in this book,\neven though serialization is perhaps a more common term.\nAs this is such a common problem, there are a myriad different libraries and encod\u2010\ning formats to choose from. Let\u2019s do a brief overview.\nLanguage-Specific Formats\nMany programming languages come with built-in support for encoding in-memory\nobjects into byte sequences. For example, Java has java.io.Serializable [1], Ruby\nhas Marshal [2], Python has pickle [3], and so on. Many third-party libraries also\nexist, such as Kryo for Java [4].\nThese encoding libraries are very convenient, because they allow in-memory objects\nto be saved and restored with minimal additional code. However, they also have a\nnumber of deep problems:\n\u2022 The encoding is often tied to a particular programming language, and reading\nthe data in another language is very difficult. If you store or transmit data in such\nan encoding, you are committing yourself to your current programming lan\u2010\nguage for potentially a very long time, and precluding integrating your systems\nwith those of other organizations (which may use different languages).\n\u2022 In order to restore data in the same object types, the decoding process needs to\nbe able to instantiate arbitrary classes. This is frequently a source of security\nproblems [5]: if an attacker can get your application to decode an arbitrary byte\nsequence, they can instantiate arbitrary classes, which in turn often allows them\nto do terrible things such as remotely executing arbitrary code [6, 7].\nFormats for Encoding Data | 113", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc4ee4f3-4aa7-4647-a69c-47593e9970cf": {"__data__": {"id_": "cc4ee4f3-4aa7-4647-a69c-47593e9970cf", "embedding": null, "metadata": {"page_label": "114", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a598210-0d54-44a9-9502-ec10dfed78e1", "node_type": "4", "metadata": {"page_label": "114", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2a3066227969f1c2ccdddd08e010743a67a71fecc119b2766d9dc7395ff130b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Versioning data is often an afterthought in these libraries: as they are intended\nfor quick and easy encoding of data, they often neglect the inconvenient prob\u2010\nlems of forward and backward compatibility.\n\u2022 Efficiency (CPU time taken to encode or decode, and the size of the encoded\nstructure) is also often an afterthought. For example, Java\u2019s built-in serialization\nis notorious for its bad performance and bloated encoding [8].\nFor these reasons it\u2019s generally a bad idea to use your language\u2019s built-in encoding for\nanything other than very transient purposes.\nJSON, XML, and Binary Variants\nMoving to standardized encodings that can be written and read by many program\u2010\nming languages, JSON and XML are the obvious contenders. They are widely known,\nwidely supported, and almost as widely disliked. XML is often criticized for being too\nverbose and unnecessarily complicated [ 9]. JSON\u2019s popularity is mainly due to its\nbuilt-in support in web browsers (by virtue of being a subset of JavaScript) and sim\u2010\nplicity relative to XML. CSV is another popular language-independent format, albeit\nless powerful.\nJSON, XML, and CSV are textual formats, and thus somewhat human-readable\n(although the syntax is a popular topic of debate). Besides the superficial syntactic\nissues, they also have some subtle problems:\n\u2022 There is a lot of ambiguity around the encoding of numbers. In XML and CSV,\nyou cannot distinguish between a number and a string that happens to consist of\ndigits (except by referring to an external schema). JSON distinguishes strings and\nnumbers, but it doesn\u2019t distinguish integers and floating-point numbers, and it\ndoesn\u2019t specify a precision.\nThis is a problem when dealing with large numbers; for example, integers greater\nthan 253 cannot be exactly represented in an IEEE 754 double-precision floating-\npoint number, so such numbers become inaccurate when parsed in a language\nthat uses floating-point numbers (such as JavaScript). An example of numbers\nlarger than 2 53 occurs on Twitter, which uses a 64-bit number to identify each\ntweet. The JSON returned by Twitter\u2019s API includes tweet IDs twice, once as a\nJSON number and once as a decimal string, to work around the fact that the\nnumbers are not correctly parsed by JavaScript applications [10].\n\u2022 JSON and XML have good support for Unicode character strings (i.e., human-\nreadable text), but they don\u2019t support binary strings (sequences of bytes without\na character encoding). Binary strings are a useful feature, so people get around\nthis limitation by encoding the binary data as text using Base64. The schema is\nthen used to indicate that the value should be interpreted as Base64-encoded.\nThis works, but it\u2019s somewhat hacky and increases the data size by 33%.\n114 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4794e613-bb4b-48c4-b6a9-21d09533b7a5": {"__data__": {"id_": "4794e613-bb4b-48c4-b6a9-21d09533b7a5", "embedding": null, "metadata": {"page_label": "115", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0936adf8-4efe-4e42-a51a-30e6254e307d", "node_type": "4", "metadata": {"page_label": "115", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d32e55e8d45db1f8961272137359a0920aac4e77a64b3d3ee86e61d46c2502e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 There is optional schema support for both XML [ 11] and JSON [ 12]. These\nschema languages are quite powerful, and thus quite complicated to learn and\nimplement. Use of XML schemas is fairly widespread, but many JSON-based\ntools don\u2019t bother using schemas. Since the correct interpretation of data (such\nas numbers and binary strings) depends on information in the schema, applica\u2010\ntions that don\u2019t use XML/JSON schemas need to potentially hardcode the appro\u2010\npriate encoding/decoding logic instead.\n\u2022 CSV does not have any schema, so it is up to the application to define the mean\u2010\ning of each row and column. If an application change adds a new row or column,\nyou have to handle that change manually. CSV is also a quite vague format (what\nhappens if a value contains a comma or a newline character?). Although its\nescaping rules have been formally specified [ 13], not all parsers implement them\ncorrectly.\nDespite these flaws, JSON, XML, and CSV are good enough for many purposes. It\u2019s\nlikely that they will remain popular, especially as data interchange formats (i.e., for\nsending data from one organization to another). In these situations, as long as people\nagree on what the format is, it often doesn\u2019t matter how pretty or efficient the format\nis. The difficulty of getting different organizations to agree on anything outweighs\nmost other concerns.\nBinary encoding\nFor data that is used only internally within your organization, there is less pressure to\nuse a lowest-common-denominator encoding format. For example, you could choose\na format that is more compact or faster to parse. For a small dataset, the gains are\nnegligible, but once you get into the terabytes, the choice of data format can have a\nbig impact.\nJSON is less verbose than XML, but both still use a lot of space compared to binary\nformats. This observation led to the development of a profusion of binary encodings\nfor JSON (MessagePack, BSON, BJSON, UBJSON, BISON, and Smile, to name a few)\nand for XML (WBXML and Fast Infoset, for example). These formats have been\nadopted in various niches, but none of them are as widely adopted as the textual ver\u2010\nsions of JSON and XML.\nSome of these formats extend the set of datatypes (e.g., distinguishing integers and\nfloating-point numbers, or adding support for binary strings), but otherwise they\nkeep the JSON/XML data model unchanged. In particular, since they don\u2019t prescribe\na schema, they need to include all the object field names within the encoded data.\nThat is, in a binary encoding of the JSON document in Example 4-1, they will need to\ninclude the strings userName, favoriteNumber, and interests somewhere.\nFormats for Encoding Data | 115", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e111f854-1f76-4cc5-b8d4-f25ddd2c98e2": {"__data__": {"id_": "e111f854-1f76-4cc5-b8d4-f25ddd2c98e2", "embedding": null, "metadata": {"page_label": "116", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c9b9f51-7f61-466d-b092-3e61f64ca6e4", "node_type": "4", "metadata": {"page_label": "116", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "617faaac945168159936018f621a3fb312accfbb6467cb081d0c68e5384eba2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example 4-1. Example record which we will encode in several binary formats in this\nchapter\n{\n    \"userName\": \"Martin\",\n    \"favoriteNumber\": 1337,\n    \"interests\": [\"daydreaming\", \"hacking\"]\n}\nLet\u2019s look at an example of MessagePack, a binary encoding for JSON. Figure 4-1\nshows the byte sequence that you get if you encode the JSON document in\nExample 4-1 with MessagePack [14]. The first few bytes are as follows:\n1. The first byte, 0x83, indicates that what follows is an object (top four bits = 0x80)\nwith three fields (bottom four bits = 0x03). (In case you\u2019re wondering what hap\u2010\npens if an object has more than 15 fields, so that the number of fields doesn\u2019t fit\nin four bits, it then gets a different type indicator, and the number of fields is\nencoded in two or four bytes.)\n2. The second byte, 0xa8, indicates that what follows is a string (top four bits =\n0xa0) that is eight bytes long (bottom four bits = 0x08).\n3. The next eight bytes are the field name userName in ASCII. Since the length was\nindicated previously, there\u2019s no need for any marker to tell us where the string\nends (or any escaping).\n4. The next seven bytes encode the six-letter string value Martin with a prefix 0xa6,\nand so on.\nThe binary encoding is 66 bytes long, which is only a little less than the 81 bytes taken\nby the textual JSON encoding (with whitespace removed). All the binary encodings of\nJSON are similar in this regard. It\u2019s not clear whether such a small space reduction\n(and perhaps a speedup in parsing) is worth the loss of human-readability.\nIn the following sections we will see how we can do much better, and encode the\nsame record in just 32 bytes.\n116 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "549c8289-fcb9-48f2-ae17-8a1aa78c18f5": {"__data__": {"id_": "549c8289-fcb9-48f2-ae17-8a1aa78c18f5", "embedding": null, "metadata": {"page_label": "117", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f63d7ccc-a7dd-4fea-9c51-4d82b8b42c59", "node_type": "4", "metadata": {"page_label": "117", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e7253550f07ccbbd8d3ae4a6d5c20cede7f01deee868440d99c46e1fc6471ce9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 4-1. Example record (Example 4-1) encoded using MessagePack.\nThrift and Protocol Buffers\nApache Thrift [15] and Protocol Buffers (protobuf) [16] are binary encoding libraries\nthat are based on the same principle. Protocol Buffers was originally developed at\nGoogle, Thrift was originally developed at Facebook, and both were made open\nsource in 2007\u201308 [17].\nBoth Thrift and Protocol Buffers require a schema for any data that is encoded. To\nencode the data in Example 4-1  in Thrift, you would describe the schema in the\nThrift interface definition language (IDL) like this:\nstruct Person {\n  1: required string       userName,\n  2: optional i64          favoriteNumber,\n  3: optional list<string> interests\n}\nFormats for Encoding Data | 117", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7792c5ed-984f-42d6-b4b3-05d9d506e0af": {"__data__": {"id_": "7792c5ed-984f-42d6-b4b3-05d9d506e0af", "embedding": null, "metadata": {"page_label": "118", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2ae9afc-ed09-4c1e-a1bf-ee09e7130048", "node_type": "4", "metadata": {"page_label": "118", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bead9a3b0c52ecf5acca182a074a6cc0bd455805b820615b63e6704529a920e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. Actually, it has three\u2014BinaryProtocol, CompactProtocol, and DenseProtocol\u2014although DenseProtocol\nis only supported by the C++ implementation, so it doesn\u2019t count as cross-language [18]. Besides those, it also\nhas two different JSON-based encoding formats [19]. What fun!\nThe equivalent schema definition for Protocol Buffers looks very similar:\nmessage Person {\n    required string user_name       = 1;\n    optional int64  favorite_number = 2;\n    repeated string interests       = 3;\n}\nThrift and Protocol Buffers each come with a code generation tool that takes a\nschema definition like the ones shown here, and produces classes that implement the\nschema in various programming languages [ 18]. Your application code can call this\ngenerated code to encode or decode records of the schema.\nWhat does data encoded with this schema look like? Confusingly, Thrift has two dif\u2010\nferent binary encoding formats,iii called BinaryProtocol and CompactProtocol, respec\u2010\ntively. Let\u2019s look at BinaryProtocol first. Encoding Example 4-1 in that format takes\n59 bytes, as shown in Figure 4-2 [19].\nFigure 4-2. Example record encoded using Thrift\u2019s BinaryProtocol.\n118 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afdcc41b-4d19-47b7-9cec-407925fa2ec6": {"__data__": {"id_": "afdcc41b-4d19-47b7-9cec-407925fa2ec6", "embedding": null, "metadata": {"page_label": "119", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ea71c8f-e875-4f17-9a99-d87468767bd7", "node_type": "4", "metadata": {"page_label": "119", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "85574e57321e58c7d28e345d058fdc0db85006923300388f1edd28a88c9f3433", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly to Figure 4-1, each field has a type annotation (to indicate whether it is a\nstring, integer, list, etc.) and, where required, a length indication (length of a string,\nnumber of items in a list). The strings that appear in the data (\u201cMartin\u201d, \u201cdaydream\u2010\ning\u201d, \u201chacking\u201d) are also encoded as ASCII (or rather, UTF-8), similar to before.\nThe big difference compared to Figure 4-1 is that there are no field names (userName,\nfavoriteNumber, interests). Instead, the encoded data contains field tags, which are\nnumbers (1, 2, and 3). Those are the numbers that appear in the schema definition.\nField tags are like aliases for fields\u2014they are a compact way of saying what field we\u2019re\ntalking about, without having to spell out the field name.\nThe Thrift CompactProtocol encoding is semantically equivalent to BinaryProtocol,\nbut as you can see in Figure 4-3, it packs the same information into only 34 bytes. It\ndoes this by packing the field type and tag number into a single byte, and by using\nvariable-length integers. Rather than using a full eight bytes for the number 1337, it is\nencoded in two bytes, with the top bit of each byte used to indicate whether there are\nstill more bytes to come. This means numbers between \u201364 and 63 are encoded in\none byte, numbers between \u20138192 and 8191 are encoded in two bytes, etc. Bigger\nnumbers use more bytes.\nFigure 4-3. Example record encoded using Thrift\u2019s CompactProtocol.\nFinally, Protocol Buffers (which has only one binary encoding format) encodes the\nsame data as shown in Figure 4-4. It does the bit packing slightly differently, but is\nFormats for Encoding Data | 119", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77bd090e-0514-4d8e-980b-de696eb48a71": {"__data__": {"id_": "77bd090e-0514-4d8e-980b-de696eb48a71", "embedding": null, "metadata": {"page_label": "120", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88dbeb51-81c5-46a7-8629-780d2e47b287", "node_type": "4", "metadata": {"page_label": "120", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "60c0976500a0b35aeee16e5b68bdb70ccaacee53dd12d2fb68949e4ab0fa17ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "otherwise very similar to Thrift\u2019s CompactProtocol. Protocol Buffers fits the same\nrecord in 33 bytes.\nFigure 4-4. Example record encoded using Protocol Buffers.\nOne detail to note: in the schemas shown earlier, each field was marked either\nrequired or optional, but this makes no difference to how the field is encoded\n(nothing in the binary data indicates whether a field was required). The difference is\nsimply that required enables a runtime check that fails if the field is not set, which\ncan be useful for catching bugs.\nField tags and schema evolution\nWe said previously that schemas inevitably need to change over time. We call this\nschema evolution. How do Thrift and Protocol Buffers handle schema changes while\nkeeping backward and forward compatibility?\nAs you can see from the examples, an encoded record is just the concatenation of its\nencoded fields. Each field is identified by its tag number (the numbers 1, 2, 3 in the\nsample schemas) and annotated with a datatype (e.g., string or integer). If a field\nvalue is not set, it is simply omitted from the encoded record. From this you can see\nthat field tags are critical to the meaning of the encoded data. You can change the\nname of a field in the schema, since the encoded data never refers to field names, but\nyou cannot change a field\u2019s tag, since that would make all existing encoded data\ninvalid.\n120 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb14270a-374d-41aa-994b-f95444f807e2": {"__data__": {"id_": "bb14270a-374d-41aa-994b-f95444f807e2", "embedding": null, "metadata": {"page_label": "121", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6e43daa-7ddc-4891-a997-03a0736954bc", "node_type": "4", "metadata": {"page_label": "121", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bb72f388e725bce5544a7ff302c5533d9e250f616ededeb3f337fbaef313b5a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can add new fields to the schema, provided that you give each field a new tag\nnumber. If old code (which doesn\u2019t know about the new tag numbers you added)\ntries to read data written by new code, including a new field with a tag number it\ndoesn\u2019t recognize, it can simply ignore that field. The datatype annotation allows the\nparser to determine how many bytes it needs to skip. This maintains forward com\u2010\npatibility: old code can read records that were written by new code.\nWhat about backward compatibility? As long as each field has a unique tag number,\nnew code can always read old data, because the tag numbers still have the same\nmeaning. The only detail is that if you add a new field, you cannot make it required.\nIf you were to add a field and make it required, that check would fail if new code read\ndata written by old code, because the old code will not have written the new field that\nyou added. Therefore, to maintain backward compatibility, every field you add after\nthe initial deployment of the schema must be optional or have a default value.\nRemoving a field is just like adding a field, with backward and forward compatibility\nconcerns reversed. That means you can only remove a field that is optional (a\nrequired field can never be removed), and you can never use the same tag number\nagain (because you may still have data written somewhere that includes the old tag\nnumber, and that field must be ignored by new code). \nDatatypes and schema evolution\nWhat about changing the datatype of a field? That may be possible\u2014check the docu\u2010\nmentation for details\u2014but there is a risk that values will lose precision or get trunca\u2010\nted. For example, say you change a 32-bit integer into a 64-bit integer. New code can\neasily read data written by old code, because the parser can fill in any missing bits\nwith zeros. However, if old code reads data written by new code, the old code is still\nusing a 32-bit variable to hold the value. If the decoded 64-bit value won\u2019t fit in 32\nbits, it will be truncated.\nA curious detail of Protocol Buffers is that it does not have a list or array datatype,\nbut instead has a repeated marker for fields (which is a third option alongside\nrequired and optional). As you can see in Figure 4-4, the encoding of a repeated\nfield is just what it says on the tin: the same field tag simply appears multiple times in\nthe record. This has the nice effect that it\u2019s okay to change an optional (single-\nvalued) field into a repeated (multi-valued) field. New code reading old data sees a\nlist with zero or one elements (depending on whether the field was present); old code\nreading new data sees only the last element of the list.\nThrift has a dedicated list datatype, which is parameterized with the datatype of the\nlist elements. This does not allow the same evolution from single-valued to multi-\nvalued as Protocol Buffers does, but it has the advantage of supporting nested lists. \nFormats for Encoding Data | 121", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2960, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a13a39c5-021e-47ce-86c7-6669dece0b96": {"__data__": {"id_": "a13a39c5-021e-47ce-86c7-6669dece0b96", "embedding": null, "metadata": {"page_label": "122", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60f0ca57-3352-4ff5-86df-948bd5358ff7", "node_type": "4", "metadata": {"page_label": "122", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6be75b05765a2f31560489ae085843d2d39ad56a21c5138640184e6f352a254d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Avro\nApache Avro [ 20] is another binary encoding format that is interestingly different\nfrom Protocol Buffers and Thrift. It was started in 2009 as a subproject of Hadoop, as\na result of Thrift not being a good fit for Hadoop\u2019s use cases [21].\nAvro also uses a schema to specify the structure of the data being encoded. It has two\nschema languages: one (Avro IDL) intended for human editing, and one (based on\nJSON) that is more easily machine-readable.\nOur example schema, written in Avro IDL, might look like this:\nrecord Person {\n    string               userName;\n    union { null, long } favoriteNumber = null;\n    array<string>        interests;\n}\nThe equivalent JSON representation of that schema is as follows:\n{\n    \"type\": \"record\",\n    \"name\": \"Person\",\n    \"fields\": [\n        {\"name\": \"userName\",       \"type\": \"string\"},\n        {\"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null},\n        {\"name\": \"interests\",      \"type\": {\"type\": \"array\", \"items\": \"string\"}}\n    ]\n}\nFirst of all, notice that there are no tag numbers in the schema. If we encode our\nexample record (Example 4-1) using this schema, the Avro binary encoding is just 32\nbytes long\u2014the most compact of all the encodings we have seen. The breakdown of\nthe encoded byte sequence is shown in Figure 4-5.\nIf you examine the byte sequence, you can see that there is nothing to identify fields\nor their datatypes. The encoding simply consists of values concatenated together. A\nstring is just a length prefix followed by UTF-8 bytes, but there\u2019s nothing in the enco\u2010\nded data that tells you that it is a string. It could just as well be an integer, or some\u2010\nthing else entirely. An integer is encoded using a variable-length encoding (the same\nas Thrift\u2019s CompactProtocol).\n122 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "745238bb-3789-4c80-afd6-4360f2a4edaf": {"__data__": {"id_": "745238bb-3789-4c80-afd6-4360f2a4edaf", "embedding": null, "metadata": {"page_label": "123", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95b20aef-433d-4a25-874a-ebe075d8b4b5", "node_type": "4", "metadata": {"page_label": "123", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ac1c7314ad1844603e7f2126f67c18b25403aca518439185697ddce5157228cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 4-5. Example record encoded using Avro.\nTo parse the binary data, you go through the fields in the order that they appear in\nthe schema and use the schema to tell you the datatype of each field. This means that\nthe binary data can only be decoded correctly if the code reading the data is using the\nexact same schema  as the code that wrote the data. Any mismatch in the schema\nbetween the reader and the writer would mean incorrectly decoded data.\nSo, how does Avro support schema evolution?\nThe writer\u2019s schema and the reader\u2019s schema\nWith Avro, when an application wants to encode some data (to write it to a file or\ndatabase, to send it over the network, etc.), it encodes the data using whatever version\nof the schema it knows about\u2014for example, that schema may be compiled into the\napplication. This is known as the writer\u2019s schema.\nWhen an application wants to decode some data (read it from a file or database,\nreceive it from the network, etc.), it is expecting the data to be in some schema, which\nis known as the reader\u2019s schema. That is the schema the application code is relying on\n\u2014code may have been generated from that schema during the application\u2019s build\nprocess.\nThe key idea with Avro is that the writer\u2019s schema and the reader\u2019s schema don\u2019t have\nto be the same \u2014they only need to be compatible. When data is decoded (read), the\nFormats for Encoding Data | 123", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02db40c4-fbf8-4be8-b877-95f6f6677d42": {"__data__": {"id_": "02db40c4-fbf8-4be8-b877-95f6f6677d42", "embedding": null, "metadata": {"page_label": "124", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4c09667-167e-4ce0-8055-26b96db88d24", "node_type": "4", "metadata": {"page_label": "124", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c497860618fde8f4dcf02c4b4669b83ecece38cd1ddde0a5272ed8f995f9cff0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Avro library resolves the differences by looking at the writer\u2019s schema and the\nreader\u2019s schema side by side and translating the data from the writer\u2019s schema into\nthe reader\u2019s schema. The Avro specification [ 20] defines exactly how this resolution\nworks, and it is illustrated in Figure 4-6.\nFor example, it\u2019s no problem if the writer\u2019s schema and the reader\u2019s schema have\ntheir fields in a different order, because the schema resolution matches up the fields\nby field name. If the code reading the data encounters a field that appears in the\nwriter\u2019s schema but not in the reader\u2019s schema, it is ignored. If the code reading the\ndata expects some field, but the writer\u2019s schema does not contain a field of that name,\nit is filled in with a default value declared in the reader\u2019s schema.\nFigure 4-6. An Avro reader resolves differences between the writer\u2019s schema and the\nreader\u2019s schema.\nSchema evolution rules\nWith Avro, forward compatibility means that you can have a new version of the\nschema as writer and an old version of the schema as reader. Conversely, backward\ncompatibility means that you can have a new version of the schema as reader and an\nold version as writer.\nTo maintain compatibility, you may only add or remove a field that has a default\nvalue. (The field favoriteNumber in our Avro schema has a default value of null.)\nFor example, say you add a field with a default value, so this new field exists in the\nnew schema but not the old one. When a reader using the new schema reads a record\nwritten with the old schema, the default value is filled in for the missing field.\nIf you were to add a field that has no default value, new readers wouldn\u2019t be able to\nread data written by old writers, so you would break backward compatibility. If you\nwere to remove a field that has no default value, old readers wouldn\u2019t be able to read\ndata written by new writers, so you would break forward compatibility.\n124 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8e13435-2962-42f5-8193-0b3979e8dfd0": {"__data__": {"id_": "c8e13435-2962-42f5-8193-0b3979e8dfd0", "embedding": null, "metadata": {"page_label": "125", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0037b18-1040-4e5d-ad92-d82bc18c1858", "node_type": "4", "metadata": {"page_label": "125", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c10ec0b7ab90e672bd02db9f09239ba77d4ca04979b94bec361dc6c428bd8c77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. To be precise, the default value must be of the type of the first branch of the union, although this is a\nspecific limitation of Avro, not a general feature of union types.\nIn some programming languages, null is an acceptable default for any variable, but\nthis is not the case in Avro: if you want to allow a field to be null, you have to use a\nunion type . For example, union { null, long, string } field;  indicates that\nfield can be a number, or a string, or null. You can only use null as a default value if\nit is one of the branches of the union.iv This is a little more verbose than having every\u2010\nthing nullable by default, but it helps prevent bugs by being explicit about what can\nand cannot be null [22].\nConsequently, Avro doesn\u2019t have optional and required markers in the same way as\nProtocol Buffers and Thrift do (it has union types and default values instead).\nChanging the datatype of a field is possible, provided that Avro can convert the type.\nChanging the name of a field is possible but a little tricky: the reader\u2019s schema can\ncontain aliases for field names, so it can match an old writer\u2019s schema field names\nagainst the aliases. This means that changing a field name is backward compatible but\nnot forward compatible. Similarly, adding a branch to a union type is backward com\u2010\npatible but not forward compatible.\nBut what is the writer\u2019s schema?\nThere is an important question that we\u2019ve glossed over so far: how does the reader\nknow the writer\u2019s schema with which a particular piece of data was encoded? We\ncan\u2019t just include the entire schema with every record, because the schema would\nlikely be much bigger than the encoded data, making all the space savings from the\nbinary encoding futile.\nThe answer depends on the context in which Avro is being used. To give a few exam\u2010\nples:\nLarge file with lots of records\nA common use for Avro\u2014especially in the context of Hadoop\u2014is for storing a\nlarge file containing millions of records, all encoded with the same schema. (We\nwill discuss this kind of situation in Chapter 10.) In this case, the writer of that\nfile can just include the writer\u2019s schema once at the beginning of the file. Avro\nspecifies a file format (object container files) to do this.\nDatabase with individually written records\nIn a database, different records may be written at different points in time using\ndifferent writer\u2019s schemas\u2014you cannot assume that all the records will have the\nsame schema. The simplest solution is to include a version number at the begin\u2010\nning of every encoded record, and to keep a list of schema versions in your data\u2010\nFormats for Encoding Data | 125", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b395a0c-ae9c-4b6c-ae07-641d93c5681a": {"__data__": {"id_": "6b395a0c-ae9c-4b6c-ae07-641d93c5681a", "embedding": null, "metadata": {"page_label": "126", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b90b9ca-ce91-44cc-a62a-2d3c5b43aa71", "node_type": "4", "metadata": {"page_label": "126", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "25a6f840415a8e57e8b2b3c9d2164be6176111333a12fdf109471893137a9e5c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "base. A reader can fetch a record, extract the version number, and then fetch the\nwriter\u2019s schema for that version number from the database. Using that writer\u2019s\nschema, it can decode the rest of the record. (Espresso [ 23] works this way, for\nexample.)\nSending records over a network connection\nWhen two processes are communicating over a bidirectional network connec\u2010\ntion, they can negotiate the schema version on connection setup and then use\nthat schema for the lifetime of the connection. The Avro RPC protocol (see\n\u201cDataflow Through Services: REST and RPC\u201d on page 131) works like this.\nA database of schema versions is a useful thing to have in any case, since it acts as\ndocumentation and gives you a chance to check schema compatibility [ 24]. As the\nversion number, you could use a simple incrementing integer, or you could use a\nhash of the schema.\nDynamically generated schemas\nOne advantage of Avro\u2019s approach, compared to Protocol Buffers and Thrift, is that\nthe schema doesn\u2019t contain any tag numbers. But why is this important? What\u2019s the\nproblem with keeping a couple of numbers in the schema?\nThe difference is that Avro is friendlier to dynamically generated schemas. For exam\u2010\nple, say you have a relational database whose contents you want to dump to a file, and\nyou want to use a binary format to avoid the aforementioned problems with textual\nformats (JSON, CSV, SQL). If you use Avro, you can fairly easily generate an Avro\nschema (in the JSON representation we saw earlier) from the relational schema and\nencode the database contents using that schema, dumping it all to an Avro object\ncontainer file [ 25]. You generate a record schema for each database table, and each\ncolumn becomes a field in that record. The column name in the database maps to the\nfield name in Avro.\nNow, if the database schema changes (for example, a table has one column added and\none column removed), you can just generate a new Avro schema from the updated\ndatabase schema and export data in the new Avro schema. The data export process\ndoes not need to pay any attention to the schema change\u2014it can simply do the\nschema conversion every time it runs. Anyone who reads the new data files will see\nthat the fields of the record have changed, but since the fields are identified by name,\nthe updated writer\u2019s schema can still be matched up with the old reader\u2019s schema.\nBy contrast, if you were using Thrift or Protocol Buffers for this purpose, the field\ntags would likely have to be assigned by hand: every time the database schema\nchanges, an administrator would have to manually update the mapping from data\u2010\nbase column names to field tags. (It might be possible to automate this, but the\nschema generator would have to be very careful to not assign previously used field\n126 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1555d286-0a8e-42b2-8551-a3c3443bc9fc": {"__data__": {"id_": "1555d286-0a8e-42b2-8551-a3c3443bc9fc", "embedding": null, "metadata": {"page_label": "127", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d66b00b1-e2da-4ec0-8683-65c1b2ea9b5b", "node_type": "4", "metadata": {"page_label": "127", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cedfcae1a809e876b36f8c926618976d0069b51616b63bfd9b5d338d7fe54144", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tags.) This kind of dynamically generated schema simply wasn\u2019t a design goal of\nThrift or Protocol Buffers, whereas it was for Avro.\nCode generation and dynamically typed languages\nThrift and Protocol Buffers rely on code generation: after a schema has been defined,\nyou can generate code that implements this schema in a programming language of\nyour choice. This is useful in statically typed languages such as Java, C++, or C#,\nbecause it allows efficient in-memory structures to be used for decoded data, and it\nallows type checking and autocompletion in IDEs when writing programs that access\nthe data structures.\nIn dynamically typed programming languages such as JavaScript, Ruby, or Python,\nthere is not much point in generating code, since there is no compile-time type\nchecker to satisfy. Code generation is often frowned upon in these languages, since\nthey otherwise avoid an explicit compilation step. Moreover, in the case of a dynami\u2010\ncally generated schema (such as an Avro schema generated from a database table),\ncode generation is an unnecessarily obstacle to getting to the data.\nAvro provides optional code generation for statically typed programming languages,\nbut it can be used just as well without any code generation. If you have an object con\u2010\ntainer file (which embeds the writer\u2019s schema), you can simply open it using the Avro\nlibrary and look at the data in the same way as you could look at a JSON file. The file\nis self-describing since it includes all the necessary metadata.\nThis property is especially useful in conjunction with dynamically typed data pro\u2010\ncessing languages like Apache Pig [ 26]. In Pig, you can just open some Avro files,\nstart analyzing them, and write derived datasets to output files in Avro format\nwithout even thinking about schemas. \nThe Merits of Schemas\nAs we saw, Protocol Buffers, Thrift, and Avro all use a schema to describe a binary\nencoding format. Their schema languages are much simpler than XML Schema or\nJSON Schema, which support much more detailed validation rules (e.g., \u201cthe string\nvalue of this field must match this regular expression\u201d or \u201cthe integer value of this\nfield must be between 0 and 100\u201d). As Protocol Buffers, Thrift, and Avro are simpler\nto implement and simpler to use, they have grown to support a fairly wide range of\nprogramming languages.\nThe ideas on which these encodings are based are by no means new. For example,\nthey have a lot in common with ASN.1, a schema definition language that was first\nstandardized in 1984 [ 27]. It was used to define various network protocols, and its\nbinary encoding (DER) is still used to encode SSL certificates (X.509), for example\n[28]. ASN.1 supports schema evolution using tag numbers, similar to Protocol Buf\u2010\nFormats for Encoding Data | 127", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "93bd6082-5127-4f7c-86ac-ca615f90047f": {"__data__": {"id_": "93bd6082-5127-4f7c-86ac-ca615f90047f", "embedding": null, "metadata": {"page_label": "128", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c559614-084b-43ff-9cfc-7ead488ee6cc", "node_type": "4", "metadata": {"page_label": "128", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6c1f05e7f1b7c2876437440ddf9a0024e1a19cc9946cd670387afbca02b4930f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "fers and Thrift [ 29]. However, it\u2019s also very complex and badly documented, so\nASN.1 is probably not a good choice for new applications.\nMany data systems also implement some kind of proprietary binary encoding for\ntheir data. For example, most relational databases have a network protocol over\nwhich you can send queries to the database and get back responses. Those protocols\nare generally specific to a particular database, and the database vendor provides a\ndriver (e.g., using the ODBC or JDBC APIs) that decodes responses from the data\u2010\nbase\u2019s network protocol into in-memory data structures.\nSo, we can see that although textual data formats such as JSON, XML, and CSV are\nwidespread, binary encodings based on schemas are also a viable option. They have a\nnumber of nice properties:\n\u2022 They can be much more compact than the various \u201cbinary JSON\u201d variants, since\nthey can omit field names from the encoded data.\n\u2022 The schema is a valuable form of documentation, and because the schema is\nrequired for decoding, you can be sure that it is up to date (whereas manually\nmaintained documentation may easily diverge from reality).\n\u2022 Keeping a database of schemas allows you to check forward and backward com\u2010\npatibility of schema changes, before anything is deployed.\n\u2022 For users of statically typed programming languages, the ability to generate code\nfrom the schema is useful, since it enables type checking at compile time.\nIn summary, schema evolution allows the same kind of flexibility as schemaless/\nschema-on-read JSON databases provide (see \u201cSchema flexibility in the document\nmodel\u201d on page 39), while also providing better guarantees about your data and bet\u2010\nter tooling. \nModes of Dataflow\nAt the beginning of this chapter we said that whenever you want to send some data to\nanother process with which you don\u2019t share memory\u2014for example, whenever you\nwant to send data over the network or write it to a file\u2014you need to encode it as a\nsequence of bytes. We then discussed a variety of different encodings for doing this.\nWe talked about forward and backward compatibility, which are important for evolv\u2010\nability (making change easy by allowing you to upgrade different parts of your system\nindependently, and not having to change everything at once). Compatibility is a rela\u2010\ntionship between one process that encodes the data, and another process that decodes\nit.\n128 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24ec6315-6c4c-4ca0-97e4-8d51191731b8": {"__data__": {"id_": "24ec6315-6c4c-4ca0-97e4-8d51191731b8", "embedding": null, "metadata": {"page_label": "129", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ce965e6-4a71-4430-a8ef-acb6044f9a42", "node_type": "4", "metadata": {"page_label": "129", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "280b5574e9a01aace11b0096b12de4a0642916c2191f432e4c22884ccb5d7900", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That\u2019s a fairly abstract idea\u2014there are many ways data can flow from one process to\nanother. Who encodes the data, and who decodes it? In the rest of this chapter we\nwill explore some of the most common ways how data flows between processes:\n\u2022 Via databases (see \u201cDataflow Through Databases\u201d on page 129)\n\u2022 Via service calls (see \u201cDataflow Through Services: REST and RPC\u201d on page 131)\n\u2022 Via asynchronous message passing (see \u201cMessage-Passing Dataflow\u201d on page 136)\nDataflow Through Databases\nIn a database, the process that writes to the database encodes the data, and the pro\u2010\ncess that reads from the database decodes it. There may just be a single process\naccessing the database, in which case the reader is simply a later version of the same\nprocess\u2014in that case you can think of storing something in the database as sending a\nmessage to your future self.\nBackward compatibility is clearly necessary here; otherwise your future self won\u2019t be\nable to decode what you previously wrote.\nIn general, it\u2019s common for several different processes to be accessing a database at\nthe same time. Those processes might be several different applications or services, or\nthey may simply be several instances of the same service (running in parallel for scal\u2010\nability or fault tolerance). Either way, in an environment where the application is\nchanging, it is likely that some processes accessing the database will be running newer\ncode and some will be running older code\u2014for example because a new version is cur\u2010\nrently being deployed in a rolling upgrade, so some instances have been updated\nwhile others haven\u2019t yet.\nThis means that a value in the database may be written by a newer version of the\ncode, and subsequently read by an older version of the code that is still running.\nThus, forward compatibility is also often required for databases.\nHowever, there is an additional snag. Say you add a field to a record schema, and the\nnewer code writes a value for that new field to the database. Subsequently, an older\nversion of the code (which doesn\u2019t yet know about the new field) reads the record,\nupdates it, and writes it back. In this situation, the desirable behavior is usually for\nthe old code to keep the new field intact, even though it couldn\u2019t be interpreted.\nThe encoding formats discussed previously support such preservation of unknown\nfields, but sometimes you need to take care at an application level, as illustrated in\nFigure 4-7 . For example, if you decode a database value into model objects in the\napplication, and later reencode those model objects, the unknown field might be lost\nin that translation process. Solving this is not a hard problem; you just need to be\naware of it.\nModes of Dataflow | 129", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67d26cdb-8f65-42aa-8921-1204673e96c0": {"__data__": {"id_": "67d26cdb-8f65-42aa-8921-1204673e96c0", "embedding": null, "metadata": {"page_label": "130", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67076165-aa9b-411b-9022-003bf9b5318a", "node_type": "4", "metadata": {"page_label": "130", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3bc9b116bcae6840be49e065098959057c27108392285bbbec68d3a635bdd9f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "v. Except for MySQL, which often rewrites an entire table even though it is not strictly necessary, as men\u2010\ntioned in \u201cSchema flexibility in the document model\u201d on page 39.\nFigure 4-7. When an older version of the application updates data previously written\nby a newer version of the application, data may be lost if you\u2019re not careful.\nDifferent values written at different times\nA database generally allows any value to be updated at any time. This means that\nwithin a single database you may have some values that were written five milli\u2010\nseconds ago, and some values that were written five years ago.\nWhen you deploy a new version of your application (of a server-side application, at\nleast), you may entirely replace the old version with the new version within a few\nminutes. The same is not true of database contents: the five-year-old data will still be\nthere, in the original encoding, unless you have explicitly rewritten it since then. This\nobservation is sometimes summed up as data outlives code.\nRewriting (migrating) data into a new schema is certainly possible, but it\u2019s an expen\u2010\nsive thing to do on a large dataset, so most databases avoid it if possible. Most rela\u2010\ntional databases allow simple schema changes, such as adding a new column with a\nnull default value, without rewriting existing data. v When an old row is read, the\ndatabase fills in nulls for any columns that are missing from the encoded data on\ndisk. LinkedIn\u2019s document database Espresso uses Avro for storage, allowing it to use\nAvro\u2019s schema evolution rules [23].\n130 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e930f6d-603c-4e78-abd2-61b3fc0385e4": {"__data__": {"id_": "6e930f6d-603c-4e78-abd2-61b3fc0385e4", "embedding": null, "metadata": {"page_label": "131", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af130a67-c49e-43bf-9b58-50a950c1ff08", "node_type": "4", "metadata": {"page_label": "131", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d82ecf051691dc5d643980363f56c94ed5bbe9a9afb712349fe3deae2a7b339f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Schema evolution thus allows the entire database to appear as if it was encoded with a\nsingle schema, even though the underlying storage may contain records encoded with\nvarious historical versions of the schema.\nArchival storage\nPerhaps you take a snapshot of your database from time to time, say for backup pur\u2010\nposes or for loading into a data warehouse (see \u201cData Warehousing\u201d on page 91). In\nthis case, the data dump will typically be encoded using the latest schema, even if the\noriginal encoding in the source database contained a mixture of schema versions\nfrom different eras. Since you\u2019re copying the data anyway, you might as well encode\nthe copy of the data consistently.\nAs the data dump is written in one go and is thereafter immutable, formats like Avro\nobject container files are a good fit. This is also a good opportunity to encode the data\nin an analytics-friendly column-oriented format such as Parquet (see \u201cColumn Com\u2010\npression\u201d on page 97).\nIn Chapter 10 we will talk more about using data in archival storage. \nDataflow Through Services: REST and RPC\nWhen you have processes that need to communicate over a network, there are a few\ndifferent ways of arranging that communication. The most common arrangement is\nto have two roles: clients and servers. The servers expose an API over the network,\nand the clients can connect to the servers to make requests to that API. The API\nexposed by the server is known as a service.\nThe web works this way: clients (web browsers) make requests to web servers, mak\u2010\ning GET requests to download HTML, CSS, JavaScript, images, etc., and making POST\nrequests to submit data to the server. The API consists of a standardized set of proto\u2010\ncols and data formats (HTTP, URLs, SSL/TLS, HTML, etc.). Because web browsers,\nweb servers, and website authors mostly agree on these standards, you can use any\nweb browser to access any website (at least in theory!).\nWeb browsers are not the only type of client. For example, a native app running on a\nmobile device or a desktop computer can also make network requests to a server, and\na client-side JavaScript application running inside a web browser can use\nXMLHttpRequest to become an HTTP client (this technique is known as Ajax [30]).\nIn this case, the server\u2019s response is typically not HTML for displaying to a human,\nbut rather data in an encoding that is convenient for further processing by the client-\nside application code (such as JSON). Although HTTP may be used as the transport\nprotocol, the API implemented on top is application-specific, and the client and\nserver need to agree on the details of that API.\nModes of Dataflow | 131", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "66329508-b3e0-4025-8e62-5fa7fd9cd628": {"__data__": {"id_": "66329508-b3e0-4025-8e62-5fa7fd9cd628", "embedding": null, "metadata": {"page_label": "132", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb13fee7-e50f-4d0d-9f2a-4ba29c7bc046", "node_type": "4", "metadata": {"page_label": "132", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ec166b8411d540a92563c4b06a1efc8e04969c82c1deeb5e365767306b969a50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, a server can itself be a client to another service (for example, a typical web\napp server acts as client to a database). This approach is often used to decompose a\nlarge application into smaller services by area of functionality, such that one service\nmakes a request to another when it requires some functionality or data from that\nother service. This way of building applications has traditionally been called a service-\noriented architecture  (SOA), more recently refined and rebranded as microservices\narchitecture [31, 32].\nIn some ways, services are similar to databases: they typically allow clients to submit\nand query data. However, while databases allow arbitrary queries using the query lan\u2010\nguages we discussed in Chapter 2 , services expose an application-specific API that\nonly allows inputs and outputs that are predetermined by the business logic (applica\u2010\ntion code) of the service [ 33]. This restriction provides a degree of encapsulation:\nservices can impose fine-grained restrictions on what clients can and cannot do.\nA key design goal of a service-oriented/microservices architecture is to make the\napplication easier to change and maintain by making services independently deploya\u2010\nble and evolvable. For example, each service should be owned by one team, and that\nteam should be able to release new versions of the service frequently, without having\nto coordinate with other teams. In other words, we should expect old and new ver\u2010\nsions of servers and clients to be running at the same time, and so the data encoding\nused by servers and clients must be compatible across versions of the service API\u2014\nprecisely what we\u2019ve been talking about in this chapter.\nWeb services\nWhen HTTP is used as the underlying protocol for talking to the service, it is called a\nweb service. This is perhaps a slight misnomer, because web services are not only used\non the web, but in several different contexts. For example:\n1. A client application running on a user\u2019s device (e.g., a native app on a mobile\ndevice, or JavaScript web app using Ajax) making requests to a service over\nHTTP. These requests typically go over the public internet.\n2. One service making requests to another service owned by the same organization,\noften located within the same datacenter, as part of a service-oriented/microser\u2010\nvices architecture. (Software that supports this kind of use case is sometimes\ncalled middleware.)\n3. One service making requests to a service owned by a different organization, usu\u2010\nally via the internet. This is used for data exchange between different organiza\u2010\ntions\u2019 backend systems. This category includes public APIs provided by online\nservices, such as credit card processing systems, or OAuth for shared access to\nuser data.\n132 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54fac147-2b7d-4043-9fce-fdfb8d1128f9": {"__data__": {"id_": "54fac147-2b7d-4043-9fce-fdfb8d1128f9", "embedding": null, "metadata": {"page_label": "133", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e73266c0-a56a-46f5-aca6-5ae3e27fa699", "node_type": "4", "metadata": {"page_label": "133", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "81e33e5488240317462306bbb716966e14c7f1549b7f43f847319d45a987fc7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vi. Even within each camp there are plenty of arguments. For example, HATEOAS ( hypermedia as the engine\nof application state), often provokes discussions [35].\nvii. Despite the similarity of acronyms, SOAP is not a requirement for SOA. SOAP is a particular technology,\nwhereas SOA is a general approach to building systems.\nThere are two popular approaches to web services: REST and SOAP. They are almost\ndiametrically opposed in terms of philosophy, and often the subject of heated debate\namong their respective proponents.vi\nREST is not a protocol, but rather a design philosophy that builds upon the principles\nof HTTP [ 34, 35]. It emphasizes simple data formats, using URLs for identifying\nresources and using HTTP features for cache control, authentication, and content\ntype negotiation. REST has been gaining popularity compared to SOAP, at least in\nthe context of cross-organizational service integration [ 36], and is often associated\nwith microservices [ 31]. An API designed according to the principles of REST is\ncalled RESTful.\nBy contrast, SOAP is an XML-based protocol for making network API requests. vii\nAlthough it is most commonly used over HTTP, it aims to be independent from\nHTTP and avoids using most HTTP features. Instead, it comes with a sprawling and\ncomplex multitude of related standards (the web service framework, known as WS-*)\nthat add various features [37].\nThe API of a SOAP web service is described using an XML-based language called the\nWeb Services Description Language, or WSDL. WSDL enables code generation so\nthat a client can access a remote service using local classes and method calls (which\nare encoded to XML messages and decoded again by the framework). This is useful in\nstatically typed programming languages, but less so in dynamically typed ones (see\n\u201cCode generation and dynamically typed languages\u201d on page 127).\nAs WSDL is not designed to be human-readable, and as SOAP messages are often too\ncomplex to construct manually, users of SOAP rely heavily on tool support, code\ngeneration, and IDEs [ 38]. For users of programming languages that are not sup\u2010\nported by SOAP vendors, integration with SOAP services is difficult.\nEven though SOAP and its various extensions are ostensibly standardized, interoper\u2010\nability between different vendors\u2019 implementations often causes problems [ 39]. For\nall of these reasons, although SOAP is still used in many large enterprises, it has fallen\nout of favor in most smaller companies.\nRESTful APIs tend to favor simpler approaches, typically involving less code genera\u2010\ntion and automated tooling. A definition format such as OpenAPI, also known as\nSwagger [40], can be used to describe RESTful APIs and produce documentation.\nModes of Dataflow | 133", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcff0f9c-6df2-447e-aef0-7b572c15d24c": {"__data__": {"id_": "fcff0f9c-6df2-447e-aef0-7b572c15d24c", "embedding": null, "metadata": {"page_label": "134", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e8b2615-740c-4739-9cad-9d4bc88a465b", "node_type": "4", "metadata": {"page_label": "134", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3776d8a44f0f89210f9796c57aea2a1f01e407ab96f51957ae27f173bba9186d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The problems with remote procedure calls (RPCs)\nWeb services are merely the latest incarnation of a long line of technologies for mak\u2010\ning API requests over a network, many of which received a lot of hype but have seri\u2010\nous problems. Enterprise JavaBeans (EJB) and Java\u2019s Remote Method Invocation\n(RMI) are limited to Java. The Distributed Component Object Model (DCOM) is\nlimited to Microsoft platforms. The Common Object Request Broker Architecture\n(CORBA) is excessively complex, and does not provide backward or forward compat\u2010\nibility [41].\nAll of these are based on the idea of a remote procedure call  (RPC), which has been\naround since the 1970s [ 42]. The RPC model tries to make a request to a remote net\u2010\nwork service look the same as calling a function or method in your programming lan\u2010\nguage, within the same process (this abstraction is called location transparency ).\nAlthough RPC seems convenient at first, the approach is fundamentally flawed [ 43,\n44]. A network request is very different from a local function call: \n\u2022 A local function call is predictable and either succeeds or fails, depending only on\nparameters that are under your control. A network request is unpredictable: the\nrequest or response may be lost due to a network problem, or the remote\nmachine may be slow or unavailable, and such problems are entirely outside of\nyour control. Network problems are common, so you have to anticipate them,\nfor example by retrying a failed request.\n\u2022 A local function call either returns a result, or throws an exception, or never\nreturns (because it goes into an infinite loop or the process crashes). A network\nrequest has another possible outcome: it may return without a result, due to a\ntimeout. In that case, you simply don\u2019t know what happened: if you don\u2019t get a\nresponse from the remote service, you have no way of knowing whether the\nrequest got through or not. (We discuss this issue in more detail in Chapter 8.)\n\u2022 If you retry a failed network request, it could happen that the requests are\nactually getting through, and only the responses are getting lost.  In that case,\nretrying will cause the action to be performed multiple times, unless you build a\nmechanism for deduplication ( idempotence) into the protocol. Local function\ncalls don\u2019t have this problem. (We discuss idempotence in more detail in Chap\u2010\nter 11.)\n\u2022 Every time you call a local function, it normally takes about the same time to exe\u2010\ncute. A network request is much slower than a function call, and its latency is\nalso wildly variable: at good times it may complete in less than a millisecond, but\nwhen the network is congested or the remote service is overloaded it may take\nmany seconds to do exactly the same thing.\n\u2022 When you call a local function, you can efficiently pass it references (pointers) to\nobjects in local memory. When you make a network request, all those parameters\n134 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2931, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07ecd657-02aa-4eaa-9692-67144c0ad1e1": {"__data__": {"id_": "07ecd657-02aa-4eaa-9692-67144c0ad1e1", "embedding": null, "metadata": {"page_label": "135", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "714d2d41-4756-48a3-86a5-64b8f6f54d97", "node_type": "4", "metadata": {"page_label": "135", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d9d75f1f47b6a46caff1c3be7c7369490fc2de73778173c0376cfc2899f2c5e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "need to be encoded into a sequence of bytes that can be sent over the network.\nThat\u2019s okay if the parameters are primitives like numbers or strings, but quickly\nbecomes problematic with larger objects.\n\u2022 The client and the service may be implemented in different programming lan\u2010\nguages, so the RPC framework must translate datatypes from one language into\nanother. This can end up ugly, since not all languages have the same types\u2014\nrecall JavaScript\u2019s problems with numbers greater than 2 53, for example (see\n\u201cJSON, XML, and Binary Variants\u201d on page 114). This problem doesn\u2019t exist in a\nsingle process written in a single language.\nAll of these factors mean that there\u2019s no point trying to make a remote service look\ntoo much like a local object in your programming language, because it\u2019s a fundamen\u2010\ntally different thing. Part of the appeal of REST is that it doesn\u2019t try to hide the fact\nthat it\u2019s a network protocol (although this doesn\u2019t seem to stop people from building\nRPC libraries on top of REST).\nCurrent directions for RPC\nDespite all these problems, RPC isn\u2019t going away. Various RPC frameworks have\nbeen built on top of all the encodings mentioned in this chapter: for example, Thrift\nand Avro come with RPC support included, gRPC is an RPC implementation using\nProtocol Buffers, Finagle also uses Thrift, and Rest.li uses JSON over HTTP.\nThis new generation of RPC frameworks is more explicit about the fact that a remote\nrequest is different from a local function call. For example, Finagle and Rest.li use\nfutures (promises) to encapsulate asynchronous actions that may fail. Futures also\nsimplify situations where you need to make requests to multiple services in parallel,\nand combine their results [ 45]. gRPC supports streams, where a call consists of not\njust one request and one response, but a series of requests and responses over time\n[46].\nSome of these frameworks also provide service discovery\u2014that is, allowing a client to\nfind out at which IP address and port number it can find a particular service. We will\nreturn to this topic in \u201cRequest Routing\u201d on page 214.\nCustom RPC protocols with a binary encoding format can achieve better perfor\u2010\nmance than something generic like JSON over REST. However, a RESTful API has\nother significant advantages: it is good for experimentation and debugging (you can\nsimply make requests to it using a web browser or the command-line tool curl,\nwithout any code generation or software installation), it is supported by all main\u2010\nstream programming languages and platforms, and there is a vast ecosystem of tools\navailable (servers, caches, load balancers, proxies, firewalls, monitoring, debugging\ntools, testing tools, etc.).\nModes of Dataflow | 135", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb60e919-c254-4cd3-8f0c-21441eee671c": {"__data__": {"id_": "fb60e919-c254-4cd3-8f0c-21441eee671c", "embedding": null, "metadata": {"page_label": "136", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db0cebe4-09ec-4fcf-9523-bb6f5fe36294", "node_type": "4", "metadata": {"page_label": "136", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "49bb643a7c9458cef15222ca3d8b7cc8ba89df229fe2b3f895e33c43631b0d93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For these reasons, REST seems to be the predominant style for public APIs. The main\nfocus of RPC frameworks is on requests between services owned by the same organi\u2010\nzation, typically within the same datacenter.\nData encoding and evolution for RPC\nFor evolvability, it is important that RPC clients and servers can be changed and\ndeployed independently. Compared to data flowing through databases (as described\nin the last section), we can make a simplifying assumption in the case of dataflow\nthrough services: it is reasonable to assume that all the servers will be updated first,\nand all the clients second. Thus, you only need backward compatibility on requests,\nand forward compatibility on responses.\nThe backward and forward compatibility properties of an RPC scheme are inherited\nfrom whatever encoding it uses:\n\u2022 Thrift, gRPC (Protocol Buffers), and Avro RPC can be evolved according to the\ncompatibility rules of the respective encoding format.\n\u2022 In SOAP, requests and responses are specified with XML schemas. These can be\nevolved, but there are some subtle pitfalls [47].\n\u2022 RESTful APIs most commonly use JSON (without a formally specified schema)\nfor responses, and JSON or URI-encoded/form-encoded request parameters for\nrequests. Adding optional request parameters and adding new fields to response\nobjects are usually considered changes that maintain compatibility.\nService compatibility is made harder by the fact that RPC is often used for communi\u2010\ncation across organizational boundaries, so the provider of a service often has no\ncontrol over its clients and cannot force them to upgrade. Thus, compatibility needs\nto be maintained for a long time, perhaps indefinitely. If a compatibility-breaking\nchange is required, the service provider often ends up maintaining multiple versions\nof the service API side by side.\nThere is no agreement on how API versioning should work (i.e., how a client can\nindicate which version of the API it wants to use [ 48]). For RESTful APIs, common\napproaches are to use a version number in the URL or in the HTTP Accept header.\nFor services that use API keys to identify a particular client, another option is to store\na client\u2019s requested API version on the server and to allow this version selection to be\nupdated through a separate administrative interface [49]. \nMessage-Passing Dataflow\nWe have been looking at the different ways encoded data flows from one process to\nanother. So far, we\u2019ve discussed REST and RPC (where one process sends a request\nover the network to another process and expects a response as quickly as possible),\n136 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a89947bb-55f3-4541-96b1-0302003c95ef": {"__data__": {"id_": "a89947bb-55f3-4541-96b1-0302003c95ef", "embedding": null, "metadata": {"page_label": "137", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d35650d0-7ec1-403f-b4fc-8b39878e25f7", "node_type": "4", "metadata": {"page_label": "137", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9e83ddcbe38dcfd54d75f73b219a651cb68dceb5f463dab302a30ecfffdc7081", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and databases (where one process writes encoded data, and another process reads it\nagain sometime in the future).\nIn this final section, we will briefly look at asynchronous message-passing  systems,\nwhich are somewhere between RPC and databases. They are similar to RPC in that a\nclient\u2019s request (usually called a message) is delivered to another process with low\nlatency. They are similar to databases in that the message is not sent via a direct net\u2010\nwork connection, but goes via an intermediary called a message broker (also called a\nmessage queue or message-oriented middleware), which stores the message temporar\u2010\nily.\nUsing a message broker has several advantages compared to direct RPC:\n\u2022 It can act as a buffer if the recipient is unavailable or overloaded, and thus\nimprove system reliability.\n\u2022 It can automatically redeliver messages to a process that has crashed, and thus\nprevent messages from being lost.\n\u2022 It avoids the sender needing to know the IP address and port number of the\nrecipient (which is particularly useful in a cloud deployment where virtual\nmachines often come and go).\n\u2022 It allows one message to be sent to several recipients.\n\u2022 It logically decouples the sender from the recipient (the sender just publishes\nmessages and doesn\u2019t care who consumes them).\nHowever, a difference compared to RPC is that message-passing communication is\nusually one-way: a sender normally doesn\u2019t expect to receive a reply to its messages. It\nis possible for a process to send a response, but this would usually be done on a sepa\u2010\nrate channel. This communication pattern is asynchronous: the sender doesn\u2019t wait\nfor the message to be delivered, but simply sends it and then forgets about it.\nMessage brokers\nIn the past, the landscape of message brokers was dominated by commercial enter\u2010\nprise software from companies such as TIBCO, IBM WebSphere, and webMethods.\nMore recently, open source implementations such as RabbitMQ, ActiveMQ, Hor\u2010\nnetQ, NATS, and Apache Kafka have become popular. We will compare them in\nmore detail in Chapter 11.\nThe detailed delivery semantics vary by implementation and configuration, but in\ngeneral, message brokers are used as follows: one process sends a message to a named\nqueue or topic, and the broker ensures that the message is delivered to one or more\nconsumers of or subscribers to that queue or topic. There can be many producers and\nmany consumers on the same topic.\nModes of Dataflow | 137", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53373e60-a5fe-4ab1-b552-6cbfa9f8ea4a": {"__data__": {"id_": "53373e60-a5fe-4ab1-b552-6cbfa9f8ea4a", "embedding": null, "metadata": {"page_label": "138", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75584461-ea9d-4cc5-92f7-49cc25cdd0e9", "node_type": "4", "metadata": {"page_label": "138", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8d69e49511326e485f537b8fc7193a03ca7e52c28cf2afb4f067318d932f7472", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A topic provides only one-way dataflow. However, a consumer may itself publish\nmessages to another topic (so you can chain them together, as we shall see in Chap\u2010\nter 11), or to a reply queue that is consumed by the sender of the original message\n(allowing a request/response dataflow, similar to RPC).\nMessage brokers typically don\u2019t enforce any particular data model\u2014a message is just\na sequence of bytes with some metadata, so you can use any encoding format. If the\nencoding is backward and forward compatible, you have the greatest flexibility to\nchange publishers and consumers independently and deploy them in any order.\nIf a consumer republishes messages to another topic, you may need to be careful to\npreserve unknown fields, to prevent the issue described previously in the context of\ndatabases (Figure 4-7).\nDistributed actor frameworks\nThe actor model is a programming model for concurrency in a single process. Rather\nthan dealing directly with threads (and the associated problems of race conditions,\nlocking, and deadlock), logic is encapsulated in actors. Each actor typically represents\none client or entity, it may have some local state (which is not shared with any other\nactor), and it communicates with other actors by sending and receiving asynchro\u2010\nnous messages. Message delivery is not guaranteed: in certain error scenarios, mes\u2010\nsages will be lost. Since each actor processes only one message at a time, it doesn\u2019t\nneed to worry about threads, and each actor can be scheduled independently by the\nframework.\nIn distributed actor frameworks, this programming model is used to scale an applica\u2010\ntion across multiple nodes. The same message-passing mechanism is used, no matter\nwhether the sender and recipient are on the same node or different nodes. If they are\non different nodes, the message is transparently encoded into a byte sequence, sent\nover the network, and decoded on the other side.\nLocation transparency works better in the actor model than in RPC, because the actor\nmodel already assumes that messages may be lost, even within a single process.\nAlthough latency over the network is likely higher than within the same process,\nthere is less of a fundamental mismatch between local and remote communication\nwhen using the actor model.\nA distributed actor framework essentially integrates a message broker and the actor\nprogramming model into a single framework. However, if you want to perform roll\u2010\ning upgrades of your actor-based application, you still have to worry about forward\nand backward compatibility, as messages may be sent from a node running the new\nversion to a node running the old version, and vice versa.\nThree popular distributed actor frameworks handle message encoding as follows:\n138 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46bac0e0-f70c-4c81-86a2-1dbf66d1127d": {"__data__": {"id_": "46bac0e0-f70c-4c81-86a2-1dbf66d1127d", "embedding": null, "metadata": {"page_label": "139", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85b9f8fe-8a24-432d-b7db-a45a78223a81", "node_type": "4", "metadata": {"page_label": "139", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9ba3dd467b0a18b9057861959f32bec91ecce3ece98cc53711f0f402e36946bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Akka uses Java\u2019s built-in serialization by default, which does not provide forward\nor backward compatibility. However, you can replace it with something like Pro\u2010\ntocol Buffers, and thus gain the ability to do rolling upgrades [50].\n\u2022 Orleans by default uses a custom data encoding format that does not support\nrolling upgrade deployments; to deploy a new version of your application, you\nneed to set up a new cluster, move traffic from the old cluster to the new one, and\nshut down the old one [51, 52]. Like with Akka, custom serialization plug-ins can\nbe used.\n\u2022 In Erlang OTP it is surprisingly hard to make changes to record schemas (despite\nthe system having many features designed for high availability); rolling upgrades\nare possible but need to be planned carefully [ 53]. An experimental new maps\ndatatype (a JSON-like structure, introduced in Erlang R17 in 2014) may make\nthis easier in the future [54]. \nSummary\nIn this chapter we looked at several ways of turning data structures into bytes on the\nnetwork or bytes on disk. We saw how the details of these encodings affect not only\ntheir efficiency, but more importantly also the architecture of applications and your\noptions for deploying them.\nIn particular, many services need to support rolling upgrades, where a new version of\na service is gradually deployed to a few nodes at a time, rather than deploying to all\nnodes simultaneously. Rolling upgrades allow new versions of a service to be released\nwithout downtime (thus encouraging frequent small releases over rare big releases)\nand make deployments less risky (allowing faulty releases to be detected and rolled\nback before they affect a large number of users). These properties are hugely benefi\u2010\ncial for evolvability, the ease of making changes to an application.\nDuring rolling upgrades, or for various other reasons, we must assume that different\nnodes are running the different versions of our application\u2019s code. Thus, it is impor\u2010\ntant that all data flowing around the system is encoded in a way that provides back\u2010\nward compatibility (new code can read old data) and forward compatibility (old code\ncan read new data).\nWe discussed several data encoding formats and their compatibility properties:\n\u2022 Programming language\u2013specific encodings are restricted to a single program\u2010\nming language and often fail to provide forward and backward compatibility.\n\u2022 Textual formats like JSON, XML, and CSV are widespread, and their compatibil\u2010\nity depends on how you use them. They have optional schema languages, which\nare sometimes helpful and sometimes a hindrance. These formats are somewhat\nSummary | 139", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2631, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13df25be-addf-4d90-adf8-c00769ded84d": {"__data__": {"id_": "13df25be-addf-4d90-adf8-c00769ded84d", "embedding": null, "metadata": {"page_label": "140", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46b57060-f7ac-4571-a868-c6e91749782b", "node_type": "4", "metadata": {"page_label": "140", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dcce4cd18082563e4b5e7d59399339c09c6351fc3a2af127b2bbe13ea809e43c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vague about datatypes, so you have to be careful with things like numbers and\nbinary strings.\n\u2022 Binary schema\u2013driven formats like Thrift, Protocol Buffers, and Avro allow\ncompact, efficient encoding with clearly defined forward and backward compati\u2010\nbility semantics. The schemas can be useful for documentation and code genera\u2010\ntion in statically typed languages. However, they have the downside that data\nneeds to be decoded before it is human-readable.\nWe also discussed several modes of dataflow, illustrating different scenarios in which\ndata encodings are important:\n\u2022 Databases, where the process writing to the database encodes the data and the\nprocess reading from the database decodes it\n\u2022 RPC and REST APIs, where the client encodes a request, the server decodes the\nrequest and encodes a response, and the client finally decodes the response\n\u2022 Asynchronous message passing (using message brokers or actors), where nodes\ncommunicate by sending each other messages that are encoded by the sender\nand decoded by the recipient\nWe can conclude that with a bit of care, backward/forward compatibility and rolling\nupgrades are quite achievable. May your application\u2019s evolution be rapid and your\ndeployments be frequent.\nReferences\n[1] \u201cJava Object Serialization Specification,\u201d docs.oracle.com, 2010.\n[2] \u201cRuby 2.2.0 API Documentation,\u201d ruby-doc.org, Dec 2014.\n[3] \u201cThe Python 3.4.3 Standard Library Reference Manual ,\u201d docs.python.org, Febru\u2010\nary 2015.\n[4] \u201cEsotericSoftware/kryo,\u201d github.com, October 2014.\n[5] \u201c CWE-502: Deserialization of Untrusted Data ,\u201d Common Weakness Enumera\u2010\ntion, cwe.mitre.org, July 30, 2014.\n[6] Steve Breen: \u201c What Do WebLogic, WebSphere, JBoss, Jenkins, OpenNMS, and\nYour Application Have in Common? This Vulnerability ,\u201d foxglovesecurity.com,\nNovember 6, 2015.\n[7] Patrick McKenzie: \u201cWhat the Rails Security Issue Means for Your Startup,\u201d kalzu\u2010\nmeus.com, January 31, 2013.\n[8] Eishay Smith: \u201cjvm-serializers wiki,\u201d github.com, November 2014.\n140 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "089f6d1c-aa88-442a-a81a-38c9a3f9b675": {"__data__": {"id_": "089f6d1c-aa88-442a-a81a-38c9a3f9b675", "embedding": null, "metadata": {"page_label": "141", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb09f4e1-d0af-4f9d-9124-6ef7f7280147", "node_type": "4", "metadata": {"page_label": "141", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4b022190d4242b181062e3060fd3168fa8fe560562fb56e7599d377aec7bf083", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[9] \u201cXML Is a Poor Copy of S-Expressions,\u201d c2.com wiki.\n[10] Matt Harris: \u201c Snowflake: An Update and Some Very Important Information ,\u201d\nemail to Twitter Development Talk mailing list, October 19, 2010.\n[11] Shudi (Sandy) Gao, C. M. Sperberg-McQueen, and Henry S. Thompson: \u201c XML\nSchema 1.1,\u201d W3C Recommendation, May 2001.\n[12] Francis Galiegue, Kris Zyp, and Gary Court: \u201c JSON Schema ,\u201d IETF Internet-\nDraft, February 2013.\n[13] Yakov Shafranovich: \u201c RFC 4180: Common Format and MIME Type for\nComma-Separated Values (CSV) Files,\u201d October 2005.\n[14] \u201cMessagePack Specification,\u201d msgpack.org.\n[15] Mark Slee, Aditya Agarwal, and Marc Kwiatkowski: \u201c Thrift: Scalable Cross-\nLanguage Services Implementation,\u201d Facebook technical report, April 2007.\n[16] \u201cProtocol Buffers Developer Guide,\u201d Google, Inc., developers.google.com.\n[17] Igor Anishchenko: \u201c Thrift vs Protocol Buffers vs Avro - Biased Comparison ,\u201d\nslideshare.net, September 17, 2012.\n[18] \u201c A Matrix of the Features Each Individual Language Library Supports ,\u201d\nwiki.apache.org.\n[19] Martin Kleppmann: \u201c Schema Evolution in Avro, Protocol Buffers and Thrift ,\u201d\nmartin.kleppmann.com, December 5, 2012.\n[20] \u201cApache Avro 1.7.7 Documentation,\u201d avro.apache.org, July 2014.\n[21] Doug Cutting, Chad Walters, Jim Kellerman, et al.: \u201c [PROPOSAL] New Subpro\u2010\nject: Avro ,\u201d email thread on hadoop-general mailing list, mail-archives.apache.org,\nApril 2009.\n[22] Tony Hoare: \u201c Null References: The Billion Dollar Mistake ,\u201d at QCon London ,\nMarch 2009.\n[23] Aditya Auradkar and Tom Quiggle: \u201c Introducing Espresso\u2014LinkedIn\u2019s Hot\nNew Distributed Document Store,\u201d engineering.linkedin.com, January 21, 2015.\n[24] Jay Kreps: \u201cPutting Apache Kafka to Use: A Practical Guide to Building a Stream\nData Platform (Part 2),\u201d blog.confluent.io, February 25, 2015.\n[25] Gwen Shapira: \u201cThe Problem of Managing Schemas,\u201d radar.oreilly.com, Novem\u2010\nber 4, 2014.\n[26] \u201cApache Pig 0.14.0 Documentation,\u201d pig.apache.org, November 2014.\n[27] John Larmouth: ASN.1 Complete . Morgan Kaufmann, 1999. ISBN:\n978-0-122-33435-1\nSummary | 141", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2062, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f94e1014-f7be-4e97-b2e5-8029c6d82de0": {"__data__": {"id_": "f94e1014-f7be-4e97-b2e5-8029c6d82de0", "embedding": null, "metadata": {"page_label": "142", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "919841c8-0688-4df9-acc7-e86a7f2ae6d8", "node_type": "4", "metadata": {"page_label": "142", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b7403d8d1645c41b03ac990279dfc219289643b89c526a4ab32558c3fce9fd19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[28] Russell Housley, Warwick Ford, Tim Polk, and David Solo: \u201c RFC 2459: Internet\nX.509 Public Key Infrastructure: Certificate and CRL Profile ,\u201d IETF Network Work\u2010\ning Group, Standards Track, January 1999.\n[29] Lev Walkin: \u201c Question: Extensibility and Dropping Fields ,\u201d lionet.info, Septem\u2010\nber 21, 2010.\n[30] Jesse James Garrett: \u201c Ajax: A New Approach to Web Applications ,\u201d adaptive\u2010\npath.com, February 18, 2005.\n[31] Sam Newman: Building Microservices . O\u2019Reilly Media, 2015. ISBN:\n978-1-491-95035-7\n[32] Chris Richardson: \u201c Microservices: Decomposing Applications for Deployability\nand Scalability,\u201d infoq.com, May 25, 2014.\n[33] Pat Helland: \u201c Data on the Outside Versus Data on the Inside ,\u201d at 2nd Biennial\nConference on Innovative Data Systems Research (CIDR), January 2005.\n[34] Roy Thomas Fielding: \u201c Architectural Styles and the Design of Network-Based\nSoftware Architectures,\u201d PhD Thesis, University of California, Irvine, 2000.\n[35] Roy Thomas Fielding: \u201c REST APIs Must Be Hypertext-Driven ,\u201d roy.gbiv.com,\nOctober 20 2008.\n[36] \u201cREST in Peace, SOAP,\u201d royal.pingdom.com, October 15, 2010.\n[37] \u201cWeb Services Standards as of Q1 2007,\u201d innoq.com, February 2007.\n[38] Pete Lacey: \u201cThe S Stands for Simple,\u201d harmful.cat-v.org, November 15, 2006.\n[39] Stefan Tilkov: \u201c Interview: Pete Lacey Criticizes Web Services ,\u201d infoq.com,\nDecember 12, 2006.\n[40] \u201c OpenAPI Specification (fka Swagger RESTful API Documentation Specifica\u2010\ntion) Version 2.0,\u201d swagger.io, September 8, 2014.\n[41] Michi Henning: \u201cThe Rise and Fall of CORBA,\u201d ACM Queue, volume 4, number\n5, pages 28\u201334, June 2006. doi:10.1145/1142031.1142044\n[42] Andrew D. Birrell and Bruce Jay Nelson: \u201c Implementing Remote Procedure\nCalls,\u201d ACM Transactions on Computer Systems  (TOCS), volume 2, number 1, pages\n39\u201359, February 1984. doi:10.1145/2080.357392\n[43] Jim Waldo, Geoff Wyant, Ann Wollrath, and Sam Kendall: \u201c A Note on Dis\u2010\ntributed Computing ,\u201d Sun Microsystems Laboratories, Inc., Technical Report\nTR-94-29, November 1994.\n[44] Steve Vinoski: \u201c Convenience over Correctness ,\u201d IEEE Internet Computing , vol\u2010\nume 12, number 4, pages 89\u201392, July 2008. doi:10.1109/MIC.2008.75\n142 | Chapter 4: Encoding and Evolution", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2cde020-300d-4fbd-aab6-9e7ca7c1c4de": {"__data__": {"id_": "f2cde020-300d-4fbd-aab6-9e7ca7c1c4de", "embedding": null, "metadata": {"page_label": "143", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03f2be03-5506-49cd-b146-24fcbe01f957", "node_type": "4", "metadata": {"page_label": "143", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a8e53c90e7eb52ac96fa3598b1804aeba5cdccbf837062486c95db7a2a25ef2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[45] Marius Eriksen: \u201c Your Server as a Function ,\u201d at 7th Workshop on Programming\nLanguages and Operating Systems  (PLOS), November 2013. doi:\n10.1145/2525528.2525538\n[46] \u201cgrpc-common Documentation,\u201d Google, Inc., github.com, February 2015.\n[47] Aditya Narayan and Irina Singh: \u201c Designing and Versioning Compatible Web\nServices,\u201d ibm.com, March 28, 2007.\n[48] Troy Hunt: \u201cYour API Versioning Is Wrong, Which Is Why I Decided to Do It 3\nDifferent Wrong Ways,\u201d troyhunt.com, February 10, 2014.\n[49] \u201cAPI Upgrades,\u201d Stripe, Inc., April 2015.\n[50] Jonas Bon\u00e9r: \u201cUpgrade in an Akka Cluster,\u201d email to akka-user mailing list, grok\u2010\nbase.com, August 28, 2013.\n[51] Philip A. Bernstein, Sergey Bykov, Alan Geller, et al.: \u201c Orleans: Distributed Vir\u2010\ntual Actors for Programmability and Scalability ,\u201d Microsoft Research Technical\nReport MSR-TR-2014-41, March 2014.\n[52] \u201c Microsoft Project Orleans Documentation ,\u201d Microsoft Research, dotnet.git\u2010\nhub.io, 2015.\n[53] David Mercer, Sean Hinde, Yinso Chen, and Richard A O\u2019Keefe: \u201c beginner:\nUpdating Data Structures,\u201d email thread on erlang-questions mailing list, erlang.com,\nOctober 29, 2007.\n[54] Fred Hebert: \u201cPostscript: Maps,\u201d learnyousomeerlang.com, April 9, 2014.\nSummary | 143", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f265f3e8-dd1c-4ba1-8040-a76fd1de876c": {"__data__": {"id_": "f265f3e8-dd1c-4ba1-8040-a76fd1de876c", "embedding": null, "metadata": {"page_label": "144", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b40ad265-d181-4cef-a479-eaec0cd332c6", "node_type": "4", "metadata": {"page_label": "144", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d11501cf-dc33-4b0b-a783-87d9d2fff5da": {"__data__": {"id_": "d11501cf-dc33-4b0b-a783-87d9d2fff5da", "embedding": null, "metadata": {"page_label": "145", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "593a8ed3-5fa6-493c-b5ad-513122c2e444", "node_type": "4", "metadata": {"page_label": "145", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "27a204114b43ae7cccd5fe70a338fc4f2ec2afe5152f1d574839f6615f7a8ff6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PART II\nDistributed Data\nFor a successful technology, reality must take precedence over public relations, for nature\ncannot be fooled.\n\u2014Richard Feynman, Rogers Commission Report (1986)\nIn Part I of this book, we discussed aspects of data systems that apply when data is\nstored on a single machine. Now, in Part II, we move up a level and ask: what hap\u2010\npens if multiple machines are involved in storage and retrieval of data?\nThere are various reasons why you might want to distribute a database across multi\u2010\nple machines:\nScalability\nIf your data volume, read load, or write load grows bigger than a single machine\ncan handle, you can potentially spread the load across multiple machines.\nFault tolerance/high availability\nIf your application needs to continue working even if one machine (or several\nmachines, or the network, or an entire datacenter) goes down, you can use multi\u2010\nple machines to give you redundancy. When one fails, another one can take over.\nLatency\nIf you have users around the world, you might want to have servers at various\nlocations worldwide so that each user can be served from a datacenter that is geo\u2010\ngraphically close to them. That avoids the users having to wait for network pack\u2010\nets to travel halfway around the world.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b590a616-c38e-48ee-ad45-fe1849e1500f": {"__data__": {"id_": "b590a616-c38e-48ee-ad45-fe1849e1500f", "embedding": null, "metadata": {"page_label": "146", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95768b69-1d3e-4936-b120-59e38eabf579", "node_type": "4", "metadata": {"page_label": "146", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d92d8e5ee51615bd2dde91f6e168e1e432940dd11a671fed36fc53aa53cde9ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. In a large machine, although any CPU can access any part of memory, some banks of memory are closer to\none CPU than to others (this is called nonuniform memory access, or NUMA [1]). To make efficient use of\nthis architecture, the processing needs to be broken down so that each CPU mostly accesses memory that is\nnearby\u2014which means that partitioning is still required, even when ostensibly running on one machine.\nii. Network Attached Storage (NAS) or Storage Area Network (SAN).\nScaling to Higher Load\nIf all you need is to scale to higher load, the simplest approach is to buy a more pow\u2010\nerful machine (sometimes called vertical scaling  or scaling up ). Many CPUs, many\nRAM chips, and many disks can be joined together under one operating system, and\na fast interconnect allows any CPU to access any part of the memory or disk. In this\nkind of shared-memory architecture , all the components can be treated as a single\nmachine [1].i\nThe problem with a shared-memory approach is that the cost grows faster than line\u2010\narly: a machine with twice as many CPUs, twice as much RAM, and twice as much\ndisk capacity as another typically costs significantly more than twice as much. And\ndue to bottlenecks, a machine twice the size cannot necessarily handle twice the load.\nA shared-memory architecture may offer limited fault tolerance\u2014high-end machines\nhave hot-swappable components (you can replace disks, memory modules, and even\nCPUs without shutting down the machines)\u2014but it is definitely limited to a single\ngeographic location.\nAnother approach is the shared-disk architecture, which uses several machines with\nindependent CPUs and RAM, but stores data on an array of disks that is shared\nbetween the machines, which are connected via a fast network. ii This architecture is\nused for some data warehousing workloads, but contention and the overhead of lock\u2010\ning limit the scalability of the shared-disk approach [2].\nShared-Nothing Architectures\nBy contrast, shared-nothing architectures [3] (sometimes called horizontal scaling or\nscaling out) have gained a lot of popularity. In this approach, each machine or virtual\nmachine running the database software is called a node. Each node uses its CPUs,\nRAM, and disks independently. Any coordination between nodes is done at the soft\u2010\nware level, using a conventional network.\nNo special hardware is required by a shared-nothing system, so you can use whatever\nmachines have the best price/performance ratio. You can potentially distribute data\nacross multiple geographic regions, and thus reduce latency for users and potentially\nbe able to survive the loss of an entire datacenter. With cloud deployments of virtual", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd639d1d-f3a2-4074-aada-647a6da30957": {"__data__": {"id_": "dd639d1d-f3a2-4074-aada-647a6da30957", "embedding": null, "metadata": {"page_label": "147", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b6f7b7b-52dc-477e-ae8b-97277996c19d", "node_type": "4", "metadata": {"page_label": "147", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7adbc61b89623cbb381086eb5c109017f13dbbd8bf897ce85620476006dfd439", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "machines, you don\u2019t need to be operating at Google scale: even for small companies,\na multi-region distributed architecture is now feasible.\nIn this part of the book, we focus on shared-nothing architectures\u2014not because they\nare necessarily the best choice for every use case, but rather because they require the\nmost caution from you, the application developer. If your data is distributed across\nmultiple nodes, you need to be aware of the constraints and trade-offs that occur in\nsuch a distributed system\u2014the database cannot magically hide these from you.\nWhile a distributed shared-nothing architecture has many advantages, it usually also\nincurs additional complexity for applications and sometimes limits the expressive\u2010\nness of the data models you can use. In some cases, a simple single-threaded program\ncan perform significantly better than a cluster with over 100 CPU cores [ 4]. On the\nother hand, shared-nothing systems can be very powerful. The next few chapters go\ninto details on the issues that arise when data is distributed. \nReplication Versus Partitioning\nThere are two common ways data is distributed across multiple nodes:\nReplication\nKeeping a copy of the same data on several different nodes, potentially in differ\u2010\nent locations. Replication provides redundancy: if some nodes are unavailable,\nthe data can still be served from the remaining nodes. Replication can also help\nimprove performance. We discuss replication in Chapter 5.\nPartitioning\nSplitting a big database into smaller subsets called partitions so that different par\u2010\ntitions can be assigned to different nodes (also known as sharding). We discuss\npartitioning in Chapter 6.\nThese are separate mechanisms, but they often go hand in hand, as illustrated in\nFigure II-1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1759, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "689277d6-465d-42ba-9d9f-c32d41e7bf3f": {"__data__": {"id_": "689277d6-465d-42ba-9d9f-c32d41e7bf3f", "embedding": null, "metadata": {"page_label": "148", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "195164b3-d902-4e34-ab9c-488282e01f97", "node_type": "4", "metadata": {"page_label": "148", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b9e4a9d6c778ebbaf005bd6162b161d5776cea15c1c4ba979dc64111bca70d0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure II-1. A database split into two partitions, with two replicas per partition.\nWith an understanding of those concepts, we can discuss the difficult trade-offs that\nyou need to make in a distributed system. We\u2019ll discuss transactions in Chapter 7, as\nthat will help you understand all the many things that can go wrong in a data system,\nand what you can do about them. We\u2019ll conclude this part of the book by discussing\nthe fundamental limitations of distributed systems in Chapters 8 and 9.\nLater, in Part III of this book, we will discuss how you can take several (potentially\ndistributed) datastores and integrate them into a larger system, satisfying the needs of\na complex application. But first, let\u2019s talk about distributed data.\nReferences\n[1] Ulrich Drepper: \u201c What Every Programmer Should Know About Memory ,\u201d akka\u2010\ndia.org, November 21, 2007.\n[2] Ben Stopford: \u201c Shared Nothing vs. Shared Disk Architectures: An Independent\nView,\u201d benstopford.com, November 24, 2009.\n[3] Michael Stonebraker: \u201cThe Case for Shared Nothing,\u201d IEEE Database Engineering\nBulletin, volume 9, number 1, pages 4\u20139, March 1986.\n[4] Frank McSherry, Michael Isard, and Derek G. Murray: \u201c Scalability! But at What\nCOST?,\u201d at 15th USENIX Workshop on Hot Topics in Operating Systems  (HotOS),\nMay 2015.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d844c2cd-126c-4f79-9fa0-70802e22894c": {"__data__": {"id_": "d844c2cd-126c-4f79-9fa0-70802e22894c", "embedding": null, "metadata": {"page_label": "149", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c921a356-bdc5-4096-942b-d82378df18b4", "node_type": "4", "metadata": {"page_label": "149", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8949d0cb-c011-4dbb-9327-1e40d32eb000": {"__data__": {"id_": "8949d0cb-c011-4dbb-9327-1e40d32eb000", "embedding": null, "metadata": {"page_label": "150", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e8bb51e-8cbe-4634-ac6c-98bd7211de12", "node_type": "4", "metadata": {"page_label": "150", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1c16112-5316-45ce-9559-a73cf5a73dc2": {"__data__": {"id_": "b1c16112-5316-45ce-9559-a73cf5a73dc2", "embedding": null, "metadata": {"page_label": "151", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b38ed499-9796-446d-ad50-333c5414f666", "node_type": "4", "metadata": {"page_label": "151", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "778eef1dbb2935c4dd8bdf2b7e9fc466be1dc6531aca6421ad429af0cb502fc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 5\nReplication\nThe major difference between a thing that might go wrong and a thing that cannot possibly\ngo wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out\nto be impossible to get at or repair.\n\u2014Douglas Adams, Mostly Harmless (1992)\nReplication means keeping a copy of the same data on multiple machines that are\nconnected via a network. As discussed in the introduction to Part II, there are several\nreasons why you might want to replicate data:\n\u2022 To keep data geographically close to your users (and thus reduce latency)\n\u2022 To allow the system to continue working even if some of its parts have failed\n(and thus increase availability)\n\u2022 To scale out the number of machines that can serve read queries (and thus\nincrease read throughput)\nIn this chapter we will assume that your dataset is so small that each machine can\nhold a copy of the entire dataset. In Chapter 6 we will relax that assumption and dis\u2010\ncuss partitioning (sharding) of datasets that are too big for a single machine. In later\nchapters we will discuss various kinds of faults that can occur in a replicated data sys\u2010\ntem, and how to deal with them.\nIf the data that you\u2019re replicating does not change over time, then replication is easy:\nyou just need to copy the data to every node once, and you\u2019re done. All of the diffi\u2010\nculty in replication lies in handling changes to replicated data, and that\u2019s what this\nchapter is about. We will discuss three popular algorithms for replicating changes\nbetween nodes: single-leader, multi-leader, and leaderless replication. Almost all dis\u2010\ntributed databases use one of these three approaches. They all have various pros and\ncons, which we will examine in detail.\n151", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3af368f9-a3e5-4145-97d9-90efff86a7ff": {"__data__": {"id_": "3af368f9-a3e5-4145-97d9-90efff86a7ff", "embedding": null, "metadata": {"page_label": "152", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d2bf0bf-8cb5-4476-aae5-3b1d76c63966", "node_type": "4", "metadata": {"page_label": "152", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4dc4dd40d18e791bcb90ec8a855e477a2463c010053abfa3b9ae645a98d25938", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. Different people have different definitions for hot, warm, and cold standby servers. In PostgreSQL, for\nexample, hot standby is used to refer to a replica that accepts reads from clients, whereas a warm standby\nprocesses changes from the leader but doesn\u2019t process any queries from clients. For purposes of this book, the\ndifference isn\u2019t important.\nThere are many trade-offs to consider with replication: for example, whether to use\nsynchronous or asynchronous replication, and how to handle failed replicas. Those\nare often configuration options in databases, and although the details vary by data\u2010\nbase, the general principles are similar across many different implementations. We\nwill discuss the consequences of such choices in this chapter.\nReplication of databases is an old topic\u2014the principles haven\u2019t changed much since\nthey were studied in the 1970s [ 1], because the fundamental constraints of networks\nhave remained the same. However, outside of research, many developers continued\nto assume for a long time that a database consisted of just one node. Mainstream use\nof distributed databases is more recent. Since many application developers are new to\nthis area, there has been a lot of misunderstanding around issues such as eventual\nconsistency. In \u201cProblems with Replication Lag\u201d on page 161 we will get more precise\nabout eventual consistency and discuss things like the read-your-writes and mono\u2010\ntonic reads guarantees.\nLeaders and Followers\nEach node that stores a copy of the database is called a replica. With multiple replicas,\na question inevitably arises: how do we ensure that all the data ends up on all the rep\u2010\nlicas?\nEvery write to the database needs to be processed by every replica; otherwise, the rep\u2010\nlicas would no longer contain the same data. The most common solution for this is\ncalled leader-based replication (also known as active/passive or master\u2013slave replica\u2010\ntion) and is illustrated in Figure 5-1. It works as follows:\n1. One of the replicas is designated the leader (also known as master or primary).\nWhen clients want to write to the database, they must send their requests to the\nleader, which first writes the new data to its local storage.\n2. The other replicas are known as followers (read replicas, slaves, secondaries, or hot\nstandbys).i Whenever the leader writes new data to its local storage, it also sends\nthe data change to all of its followers as part of a replication log or change stream.\nEach follower takes the log from the leader and updates its local copy of the data\u2010\nbase accordingly, by applying all writes in the same order as they were processed\non the leader.\n152 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2664, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "538ebd86-7ef1-4052-8b60-0156284a8acc": {"__data__": {"id_": "538ebd86-7ef1-4052-8b60-0156284a8acc", "embedding": null, "metadata": {"page_label": "153", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9131b24f-6d97-4bd8-a5a0-355b25b4db76", "node_type": "4", "metadata": {"page_label": "153", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "08d8c535becaf05266bdf65886014d466ee6b86e372f106417dbfa594403efdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. When a client wants to read from the database, it can query either the leader or\nany of the followers. However, writes are only accepted on the leader (the follow\u2010\ners are read-only from the client\u2019s point of view).\nFigure 5-1. Leader-based (master\u2013slave) replication.\nThis mode of replication is a built-in feature of many relational databases, such as\nPostgreSQL (since version 9.0), MySQL, Oracle Data Guard [ 2], and SQL Server\u2019s\nAlwaysOn Availability Groups [ 3]. It is also used in some nonrelational databases,\nincluding MongoDB, RethinkDB, and Espresso [ 4]. Finally, leader-based replication\nis not restricted to only databases: distributed message brokers such as Kafka [ 5] and\nRabbitMQ highly available queues [ 6] also use it. Some network filesystems and\nreplicated block devices such as DRBD are similar.\nSynchronous Versus Asynchronous Replication\nAn important detail of a replicated system is whether the replication happens syn\u2010\nchronously or asynchronously. (In relational databases, this is often a configurable\noption; other systems are often hardcoded to be either one or the other.)\nThink about what happens in Figure 5-1, where the user of a website updates their\nprofile image. At some point in time, the client sends the update request to the leader;\nshortly afterward, it is received by the leader. At some point, the leader forwards the\ndata change to the followers. Eventually, the leader notifies the client that the update\nwas successful.\nFigure 5-2 shows the communication between various components of the system: the\nuser\u2019s client, the leader, and two followers. Time flows from left to right. A request or\nresponse message is shown as a thick arrow.\nLeaders and Followers | 153", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42f75abf-b44e-4618-8e63-ccc94cad1d8a": {"__data__": {"id_": "42f75abf-b44e-4618-8e63-ccc94cad1d8a", "embedding": null, "metadata": {"page_label": "154", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bec3700a-1655-4ff5-9f03-c325b45911aa", "node_type": "4", "metadata": {"page_label": "154", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fd6e4c7051996e2ec551c8b79336d8d3f3c16bbd0b38364edc2ac448b7596474", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5-2. Leader-based replication with one synchronous and one asynchronous fol\u2010\nlower.\nIn the example of Figure 5-2, the replication to follower 1 is synchronous: the leader\nwaits until follower 1 has confirmed that it received the write before reporting success\nto the user, and before making the write visible to other clients. The replication to\nfollower 2 is asynchronous: the leader sends the message, but doesn\u2019t wait for a\nresponse from the follower.\nThe diagram shows that there is a substantial delay before follower 2 processes the\nmessage. Normally, replication is quite fast: most database systems apply changes to\nfollowers in less than a second. However, there is no guarantee of how long it might\ntake. There are circumstances when followers might fall behind the leader by several\nminutes or more; for example, if a follower is recovering from a failure, if the system\nis operating near maximum capacity, or if there are network problems between the\nnodes.\nThe advantage of synchronous replication is that the follower is guaranteed to have\nan up-to-date copy of the data that is consistent with the leader. If the leader sud\u2010\ndenly fails, we can be sure that the data is still available on the follower. The disad\u2010\nvantage is that if the synchronous follower doesn\u2019t respond (because it has crashed,\nor there is a network fault, or for any other reason), the write cannot be processed.\nThe leader must block all writes and wait until the synchronous replica is available\nagain.\nFor that reason, it is impractical for all followers to be synchronous: any one node\noutage would cause the whole system to grind to a halt. In practice, if you enable syn\u2010\nchronous replication on a database, it usually means that one of the followers is syn\u2010\nchronous, and the others are asynchronous. If the synchronous follower becomes\nunavailable or slow, one of the asynchronous followers is made synchronous. This\nguarantees that you have an up-to-date copy of the data on at least two nodes: the\n154 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2030, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9ed0a3e-b517-4224-8883-e2cea1a0296a": {"__data__": {"id_": "f9ed0a3e-b517-4224-8883-e2cea1a0296a", "embedding": null, "metadata": {"page_label": "155", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b31b14b2-30c3-4d47-ae4d-c5fa2eeef456", "node_type": "4", "metadata": {"page_label": "155", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6b71f80b1bdf7a41d2262aac1a2c5918436b241965c23f7a225552e486c5c6dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "leader and one synchronous follower. This configuration is sometimes also called\nsemi-synchronous [7].\nOften, leader-based replication is configured to be completely asynchronous. In this\ncase, if the leader fails and is not recoverable, any writes that have not yet been repli\u2010\ncated to followers are lost. This means that a write is not guaranteed to be durable,\neven if it has been confirmed to the client. However, a fully asynchronous configura\u2010\ntion has the advantage that the leader can continue processing writes, even if all of its\nfollowers have fallen behind.\nWeakening durability may sound like a bad trade-off, but asynchronous replication is\nnevertheless widely used, especially if there are many followers or if they are geo\u2010\ngraphically distributed. We will return to this issue in \u201cProblems with Replication\nLag\u201d on page 161.\nResearch on Replication\nIt can be a serious problem for asynchronously replicated systems to lose data if the\nleader fails, so researchers have continued investigating replication methods that do\nnot lose data but still provide good performance and availability. For example, chain\nreplication [8, 9] is a variant of synchronous replication that has been successfully\nimplemented in a few systems such as Microsoft Azure Storage [10, 11].\nThere is a strong connection between consistency of replication and consensus (get\u2010\nting several nodes to agree on a value), and we will explore this area of theory in more\ndetail in Chapter 9. In this chapter we will concentrate on the simpler forms of repli\u2010\ncation that are most commonly used in databases in practice. \nSetting Up New Followers\nFrom time to time, you need to set up new followers\u2014perhaps to increase the num\u2010\nber of replicas, or to replace failed nodes. How do you ensure that the new follower\nhas an accurate copy of the leader\u2019s data?\nSimply copying data files from one node to another is typically not sufficient: clients\nare constantly writing to the database, and the data is always in flux, so a standard file\ncopy would see different parts of the database at different points in time. The result\nmight not make any sense.\nYou could make the files on disk consistent by locking the database (making it\nunavailable for writes), but that would go against our goal of high availability. Fortu\u2010\nnately, setting up a follower can usually be done without downtime. Conceptually,\nthe process looks like this:\nLeaders and Followers | 155", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6dca04c-7115-4346-af1b-ca27ec7d12af": {"__data__": {"id_": "a6dca04c-7115-4346-af1b-ca27ec7d12af", "embedding": null, "metadata": {"page_label": "156", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7a48cd7-8ab2-4550-9ebf-8f7baa43058a", "node_type": "4", "metadata": {"page_label": "156", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2f9d200d8ad0bb346e4401fd889aa0ea7d54bb1bed9d8d938c2b481442f7d453", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Take a consistent snapshot of the leader\u2019s database at some point in time\u2014if pos\u2010\nsible, without taking a lock on the entire database. Most databases have this fea\u2010\nture, as it is also required for backups. In some cases, third-party tools are\nneeded, such as innobackupex for MySQL [12].\n2. Copy the snapshot to the new follower node.\n3. The follower connects to the leader and requests all the data changes that have\nhappened since the snapshot was taken. This requires that the snapshot is associ\u2010\nated with an exact position in the leader\u2019s replication log. That position has vari\u2010\nous names: for example, PostgreSQL calls it the log sequence number , and\nMySQL calls it the binlog coordinates.\n4. When the follower has processed the backlog of data changes since the snapshot,\nwe say it has caught up. It can now continue to process data changes from the\nleader as they happen.\nThe practical steps of setting up a follower vary significantly by database. In some\nsystems the process is fully automated, whereas in others it can be a somewhat arcane\nmulti-step workflow that needs to be manually performed by an administrator.\nHandling Node Outages\nAny node in the system can go down, perhaps unexpectedly due to a fault, but just as\nlikely due to planned maintenance (for example, rebooting a machine to install a ker\u2010\nnel security patch). Being able to reboot individual nodes without downtime is a big\nadvantage for operations and maintenance. Thus, our goal is to keep the system as a\nwhole running despite individual node failures, and to keep the impact of a node out\u2010\nage as small as possible.\nHow do you achieve high availability with leader-based replication?\nFollower failure: Catch-up recovery\nOn its local disk, each follower keeps a log of the data changes it has received from\nthe leader. If a follower crashes and is restarted, or if the network between the leader\nand the follower is temporarily interrupted, the follower can recover quite easily:\nfrom its log, it knows the last transaction that was processed before the fault occur\u2010\nred. Thus, the follower can connect to the leader and request all the data changes that\noccurred during the time when the follower was disconnected. When it has applied\nthese changes, it has caught up to the leader and can continue receiving a stream of\ndata changes as before.\n156 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2364, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06591038-061d-4627-92a0-45a02758b81b": {"__data__": {"id_": "06591038-061d-4627-92a0-45a02758b81b", "embedding": null, "metadata": {"page_label": "157", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2c8bbd8-bcad-479e-816e-199bff35e224", "node_type": "4", "metadata": {"page_label": "157", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "74ee7276d64933a5ecb62778fc1939d45e6e9fd9ce36cf5b04894ce3de175b85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Leader failure: Failover\nHandling a failure of the leader is trickier: one of the followers needs to be promoted\nto be the new leader, clients need to be reconfigured to send their writes to the new\nleader, and the other followers need to start consuming data changes from the new\nleader. This process is called failover.\nFailover can happen manually (an administrator is notified that the leader has failed\nand takes the necessary steps to make a new leader) or automatically. An automatic\nfailover process usually consists of the following steps:\n1. Determining that the leader has failed.  There are many things that could poten\u2010\ntially go wrong: crashes, power outages, network issues, and more. There is no\nfoolproof way of detecting what has gone wrong, so most systems simply use a\ntimeout: nodes frequently bounce messages back and forth between each other,\nand if a node doesn\u2019t respond for some period of time\u2014say, 30 seconds\u2014it is\nassumed to be dead. (If the leader is deliberately taken down for planned mainte\u2010\nnance, this doesn\u2019t apply.)\n2. Choosing a new leader.  This could be done through an election process (where\nthe leader is chosen by a majority of the remaining replicas), or a new leader\ncould be appointed by a previously elected controller node. The best candidate for\nleadership is usually the replica with the most up-to-date data changes from the\nold leader (to minimize any data loss). Getting all the nodes to agree on a new\nleader is a consensus problem, discussed in detail in Chapter 9.\n3. Reconfiguring the system to use the new leader.  Clients now need to send\ntheir write requests to the new leader (we discuss this in \u201cRequest Routing\u201d on\npage 214). If the old leader comes back, it might still believe that it is the leader,\nnot realizing that the other replicas have forced it to step down. The system\nneeds to ensure that the old leader becomes a follower and recognizes the new\nleader.\nFailover is fraught with things that can go wrong:\n\u2022 If asynchronous replication is used, the new leader may not have received all the\nwrites from the old leader before it failed. If the former leader rejoins the cluster\nafter a new leader has been chosen, what should happen to those writes? The new\nleader may have received conflicting writes in the meantime. The most common\nsolution is for the old leader\u2019s unreplicated writes to simply be discarded, which\nmay violate clients\u2019 durability expectations.\n\u2022 Discarding writes is especially dangerous if other storage systems outside of the\ndatabase need to be coordinated with the database contents. For example, in one\nincident at GitHub [13], an out-of-date MySQL follower was promoted to leader.\nThe database used an autoincrementing counter to assign primary keys to new\nLeaders and Followers | 157", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc09b4e5-fd95-44fb-8eda-33365f120b39": {"__data__": {"id_": "cc09b4e5-fd95-44fb-8eda-33365f120b39", "embedding": null, "metadata": {"page_label": "158", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5605c88d-26f9-4dd9-ae25-29adf6c839c9", "node_type": "4", "metadata": {"page_label": "158", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "23e7a018b6a042beaae95d897e2216de67846a4da58c45fb37745e01cde92b2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. This approach is known as fencing or, more emphatically, Shoot The Other Node In The Head (STONITH).\nWe will discuss fencing in more detail in \u201cThe leader and the lock\u201d on page 301.\nrows, but because the new leader\u2019s counter lagged behind the old leader\u2019s, it\nreused some primary keys that were previously assigned by the old leader. These\nprimary keys were also used in a Redis store, so the reuse of primary keys resul\u2010\nted in inconsistency between MySQL and Redis, which caused some private data\nto be disclosed to the wrong users.\n\u2022 In certain fault scenarios (see Chapter 8), it could happen that two nodes both\nbelieve that they are the leader. This situation is called split brain, and it is dan\u2010\ngerous: if both leaders accept writes, and there is no process for resolving con\u2010\nflicts (see \u201cMulti-Leader Replication\u201d on page 168), data is likely to be lost or\ncorrupted. As a safety catch, some systems have a mechanism to shut down one\nnode if two leaders are detected. ii However, if this mechanism is not carefully\ndesigned, you can end up with both nodes being shut down [14].\n\u2022 What is the right timeout before the leader is declared dead? A longer timeout\nmeans a longer time to recovery in the case where the leader fails. However, if the\ntimeout is too short, there could be unnecessary failovers. For example, a tempo\u2010\nrary load spike could cause a node\u2019s response time to increase above the timeout,\nor a network glitch could cause delayed packets. If the system is already strug\u2010\ngling with high load or network problems, an unnecessary failover is likely to\nmake the situation worse, not better.\nThere are no easy solutions to these problems. For this reason, some operations\nteams prefer to perform failovers manually, even if the software supports automatic\nfailover.\nThese issues\u2014node failures; unreliable networks; and trade-offs around replica con\u2010\nsistency, durability, availability, and latency\u2014are in fact fundamental problems in\ndistributed systems. In Chapter 8  and Chapter 9  we will discuss them in greater\ndepth.\nImplementation of Replication Logs\nHow does leader-based replication work under the hood? Several different replica\u2010\ntion methods are used in practice, so let\u2019s look at each one briefly.\nStatement-based replication\nIn the simplest case, the leader logs every write request ( statement) that it executes\nand sends that statement log to its followers. For a relational database, this means\nthat every INSERT, UPDATE, or DELETE statement is forwarded to followers, and each\n158 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1be1323b-77fb-4bc0-ba12-80dfce49d216": {"__data__": {"id_": "1be1323b-77fb-4bc0-ba12-80dfce49d216", "embedding": null, "metadata": {"page_label": "159", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de4c517d-cab1-4e88-9e98-32bd65f36774", "node_type": "4", "metadata": {"page_label": "159", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3769a73e6a6a9acab2fc444a0de7bdab672beedb4f0a70699d91b6e66ba6a97c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "follower parses and executes that SQL statement as if it had been received from a\nclient.\nAlthough this may sound reasonable, there are various ways in which this approach\nto replication can break down:\n\u2022 Any statement that calls a nondeterministic function, such as NOW() to get the\ncurrent date and time or RAND() to get a random number, is likely to generate a\ndifferent value on each replica.\n\u2022 If statements use an autoincrementing column, or if they depend on the existing\ndata in the database (e.g., UPDATE \u2026 WHERE <some condition>), they must be\nexecuted in exactly the same order on each replica, or else they may have a differ\u2010\nent effect. This can be limiting when there are multiple concurrently executing\ntransactions.\n\u2022 Statements that have side effects (e.g., triggers, stored procedures, user-defined\nfunctions) may result in different side effects occurring on each replica, unless\nthe side effects are absolutely deterministic.\nIt is possible to work around those issues\u2014for example, the leader can replace any\nnondeterministic function calls with a fixed return value when the statement is log\u2010\nged so that the followers all get the same value. However, because there are so many\nedge cases, other replication methods are now generally preferred.\nStatement-based replication was used in MySQL before version 5.1. It is still some\u2010\ntimes used today, as it is quite compact, but by default MySQL now switches to row-\nbased replication (discussed shortly) if there is any nondeterminism in a statement.\nVoltDB uses statement-based replication, and makes it safe by requiring transactions\nto be deterministic [15].\nWrite-ahead log (WAL) shipping\nIn Chapter 3 we discussed how storage engines represent data on disk, and we found\nthat usually every write is appended to a log:\n\u2022 In the case of a log-structured storage engine (see \u201cSSTables and LSM-Trees\u201d on\npage 76), this log is the main place for storage. Log segments are compacted and\ngarbage-collected in the background.\n\u2022 In the case of a B-tree (see \u201cB-Trees\u201d on page 79), which overwrites individual\ndisk blocks, every modification is first written to a write-ahead log so that the\nindex can be restored to a consistent state after a crash.\nIn either case, the log is an append-only sequence of bytes containing all writes to the\ndatabase. We can use the exact same log to build a replica on another node: besides\nwriting the log to disk, the leader also sends it across the network to its followers.\nLeaders and Followers | 159", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d03c1fc6-30d7-4b3b-a4b6-9c979fd2ce56": {"__data__": {"id_": "d03c1fc6-30d7-4b3b-a4b6-9c979fd2ce56", "embedding": null, "metadata": {"page_label": "160", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5b9410c-6060-461f-80c9-1a079d431dff", "node_type": "4", "metadata": {"page_label": "160", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "806101500f2201dd4026dd1a7c9a643aff5133a54e08e57fa3bcaab6d950cf46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When the follower processes this log, it builds a copy of the exact same data struc\u2010\ntures as found on the leader.\nThis method of replication is used in PostgreSQL and Oracle, among others [16]. The\nmain disadvantage is that the log describes the data on a very low level: a WAL con\u2010\ntains details of which bytes were changed in which disk blocks. This makes replica\u2010\ntion closely coupled to the storage engine. If the database changes its storage format\nfrom one version to another, it is typically not possible to run different versions of\nthe database software on the leader and the followers.\nThat may seem like a minor implementation detail, but it can have a big operational\nimpact. If the replication protocol allows the follower to use a newer software version\nthan the leader, you can perform a zero-downtime upgrade of the database software\nby first upgrading the followers and then performing a failover to make one of the\nupgraded nodes the new leader. If the replication protocol does not allow this version\nmismatch, as is often the case with WAL shipping, such upgrades require downtime.\nLogical (row-based) log replication\nAn alternative is to use different log formats for replication and for the storage\nengine, which allows the replication log to be decoupled from the storage engine\ninternals. This kind of replication log is called a logical log, to distinguish it from the\nstorage engine\u2019s (physical) data representation.\nA logical log for a relational database is usually a sequence of records describing\nwrites to database tables at the granularity of a row:\n\u2022 For an inserted row, the log contains the new values of all columns.\n\u2022 For a deleted row, the log contains enough information to uniquely identify the\nrow that was deleted. Typically this would be the primary key, but if there is no\nprimary key on the table, the old values of all columns need to be logged.\n\u2022 For an updated row, the log contains enough information to uniquely identify\nthe updated row, and the new values of all columns (or at least the new values of\nall columns that changed).\nA transaction that modifies several rows generates several such log records, followed\nby a record indicating that the transaction was committed. MySQL\u2019s binlog (when\nconfigured to use row-based replication) uses this approach [17].\nSince a logical log is decoupled from the storage engine internals, it can more easily\nbe kept backward compatible, allowing the leader and the follower to run different\nversions of the database software, or even different storage engines.\nA logical log format is also easier for external applications to parse. This aspect is use\u2010\nful if you want to send the contents of a database to an external system, such as a data\n160 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5995a41e-4da1-4b08-b385-db4e083057ac": {"__data__": {"id_": "5995a41e-4da1-4b08-b385-db4e083057ac", "embedding": null, "metadata": {"page_label": "161", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ab61b82-4266-45c7-a720-45f93c985cf6", "node_type": "4", "metadata": {"page_label": "161", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b5bf242a562a97b394caf38f8ca858972ab5f1c11afbb428b23daa3cb14e88a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "warehouse for offline analysis, or for building custom indexes and caches [ 18]. This\ntechnique is called change data capture, and we will return to it in Chapter 11.\nTrigger-based replication\nThe replication approaches described so far are implemented by the database system,\nwithout involving any application code. In many cases, that\u2019s what you want\u2014but\nthere are some circumstances where more flexibility is needed. For example, if you\nwant to only replicate a subset of the data, or want to replicate from one kind of\ndatabase to another, or if you need conflict resolution logic (see \u201cHandling Write\nConflicts\u201d on page 171), then you may need to move replication up to the application\nlayer.\nSome tools, such as Oracle GoldenGate [ 19], can make data changes available to an\napplication by reading the database log. An alternative is to use features that are\navailable in many relational databases: triggers and stored procedures.\nA trigger lets you register custom application code that is automatically executed\nwhen a data change (write transaction) occurs in a database system. The trigger has\nthe opportunity to log this change into a separate table, from which it can be read by\nan external process. That external process can then apply any necessary application\nlogic and replicate the data change to another system. Databus for Oracle [ 20] and\nBucardo for Postgres [21] work like this, for example.\nTrigger-based replication typically has greater overheads than other replication\nmethods, and is more prone to bugs and limitations than the database\u2019s built-in repli\u2010\ncation. However, it can nevertheless be useful due to its flexibility. \nProblems with Replication Lag\nBeing able to tolerate node failures is just one reason for wanting replication. As\nmentioned in the introduction to Part II , other reasons are scalability (processing\nmore requests than a single machine can handle) and latency (placing replicas geo\u2010\ngraphically closer to users).\nLeader-based replication requires all writes to go through a single node, but read-\nonly queries can go to any replica. For workloads that consist of mostly reads and\nonly a small percentage of writes (a common pattern on the web), there is an attrac\u2010\ntive option: create many followers, and distribute the read requests across those fol\u2010\nlowers. This removes load from the leader and allows read requests to be served by\nnearby replicas.\nIn this read-scaling architecture, you can increase the capacity for serving read-only\nrequests simply by adding more followers. However, this approach only realistically\nworks with asynchronous replication\u2014if you tried to synchronously replicate to all\nfollowers, a single node failure or network outage would make the entire system\nProblems with Replication Lag | 161", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7d43dc1-4b4c-4e91-9985-48825f58d304": {"__data__": {"id_": "a7d43dc1-4b4c-4e91-9985-48825f58d304", "embedding": null, "metadata": {"page_label": "162", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f778f31-41aa-4d34-a48e-9e3b4cf2a339", "node_type": "4", "metadata": {"page_label": "162", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "88bff7f35bb343cccc9e105ccbf382d393517b41afda24c9a45083ab0c4669b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. The term eventual consistency was coined by Douglas Terry et al. [24], popularized by Werner Vogels\n[22], and became the battle cry of many NoSQL projects. However, not only NoSQL databases are eventually\nconsistent: followers in an asynchronously replicated relational database have the same characteristics.\nunavailable for writing. And the more nodes you have, the likelier it is that one will\nbe down, so a fully synchronous configuration would be very unreliable.\nUnfortunately, if an application reads from an asynchronous follower, it may see out\u2010\ndated information if the follower has fallen behind. This leads to apparent inconsis\u2010\ntencies in the database: if you run the same query on the leader and a follower at the\nsame time, you may get different results, because not all writes have been reflected in\nthe follower. This inconsistency is just a temporary state\u2014if you stop writing to the\ndatabase and wait a while, the followers will eventually catch up and become consis\u2010\ntent with the leader. For that reason, this effect is known as eventual consistency [22,\n23].iii\nThe term \u201ceventually\u201d is deliberately vague: in general, there is no limit to how far a\nreplica can fall behind. In normal operation, the delay between a write happening on\nthe leader and being reflected on a follower\u2014the replication lag\u2014may be only a frac\u2010\ntion of a second, and not noticeable in practice. However, if the system is operating\nnear capacity or if there is a problem in the network, the lag can easily increase to\nseveral seconds or even minutes.\nWhen the lag is so large, the inconsistencies it introduces are not just a theoretical\nissue but a real problem for applications. In this section we will highlight three exam\u2010\nples of problems that are likely to occur when there is replication lag and outline\nsome approaches to solving them.\nReading Your Own Writes\nMany applications let the user submit some data and then view what they have sub\u2010\nmitted. This might be a record in a customer database, or a comment on a discussion\nthread, or something else of that sort. When new data is submitted, it must be sent to\nthe leader, but when the user views the data, it can be read from a follower. This is\nespecially appropriate if data is frequently viewed but only occasionally written.\nWith asynchronous replication, there is a problem, illustrated in Figure 5-3 : if the\nuser views the data shortly after making a write, the new data may not yet have\nreached the replica. To the user, it looks as though the data they submitted was lost,\nso they will be understandably unhappy.\n162 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e95e850-091a-4987-a3cb-f0c51a045e15": {"__data__": {"id_": "2e95e850-091a-4987-a3cb-f0c51a045e15", "embedding": null, "metadata": {"page_label": "163", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6911630-7e84-41ec-8d57-941199bb45ea", "node_type": "4", "metadata": {"page_label": "163", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8e87a41dea633d2397569ee9fceb5783f96b7bc201732ad5b30a4601c12fad60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5-3. A user makes a write, followed by a read from a stale replica. To prevent\nthis anomaly, we need read-after-write consistency.\nIn this situation, we need read-after-write consistency, also known as read-your-writes\nconsistency [24]. This is a guarantee that if the user reloads the page, they will always\nsee any updates they submitted themselves. It makes no promises about other users:\nother users\u2019 updates may not be visible until some later time. However, it reassures\nthe user that their own input has been saved correctly.\nHow can we implement read-after-write consistency in a system with leader-based\nreplication? There are various possible techniques. To mention a few:\n\u2022 When reading something that the user may have modified, read it from the\nleader; otherwise, read it from a follower. This requires that you have some way\nof knowing whether something might have been modified, without actually\nquerying it. For example, user profile information on a social network is nor\u2010\nmally only editable by the owner of the profile, not by anybody else. Thus, a sim\u2010\nple rule is: always read the user\u2019s own profile from the leader, and any other\nusers\u2019 profiles from a follower.\n\u2022 If most things in the application are potentially editable by the user, that\napproach won\u2019t be effective, as most things would have to be read from the\nleader (negating the benefit of read scaling). In that case, other criteria may be\nused to decide whether to read from the leader. For example, you could track the\ntime of the last update and, for one minute after the last update, make all reads\nfrom the leader. You could also monitor the replication lag on followers and pre\u2010\nvent queries on any follower that is more than one minute behind the leader.\n\u2022 The client can remember the timestamp of its most recent write\u2014then the sys\u2010\ntem can ensure that the replica serving any reads for that user reflects updates at\nleast until that timestamp. If a replica is not sufficiently up to date, either the read\ncan be handled by another replica or the query can wait until the replica has\nProblems with Replication Lag | 163", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e7f94ab-4aa8-4f02-8675-1c62edda9592": {"__data__": {"id_": "8e7f94ab-4aa8-4f02-8675-1c62edda9592", "embedding": null, "metadata": {"page_label": "164", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "843e01eb-cd13-46f1-9318-12dab5e46341", "node_type": "4", "metadata": {"page_label": "164", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c91d7d2e8b5125ad502d88a6b49eb3555b5fb48631d39c8434e4c0a3022c53dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "caught up. The timestamp could be a logical timestamp (something that indicates\nordering of writes, such as the log sequence number) or the actual system clock\n(in which case clock synchronization becomes critical; see \u201cUnreliable Clocks\u201d\non page 287).\n\u2022 If your replicas are distributed across multiple datacenters (for geographical\nproximity to users or for availability), there is additional complexity. Any request\nthat needs to be served by the leader must be routed to the datacenter that con\u2010\ntains the leader.\nAnother complication arises when the same user is accessing your service from mul\u2010\ntiple devices, for example a desktop web browser and a mobile app. In this case you\nmay want to provide cross-device read-after-write consistency: if the user enters some\ninformation on one device and then views it on another device, they should see the\ninformation they just entered.\nIn this case, there are some additional issues to consider:\n\u2022 Approaches that require remembering the timestamp of the user\u2019s last update\nbecome more difficult, because the code running on one device doesn\u2019t know\nwhat updates have happened on the other device. This metadata will need to be\ncentralized.\n\u2022 If your replicas are distributed across different datacenters, there is no guarantee\nthat connections from different devices will be routed to the same datacenter.\n(For example, if the user\u2019s desktop computer uses the home broadband connec\u2010\ntion and their mobile device uses the cellular data network, the devices\u2019 network\nroutes may be completely different.) If your approach requires reading from the\nleader, you may first need to route requests from all of a user\u2019s devices to the\nsame datacenter. \nMonotonic Reads\nOur second example of an anomaly that can occur when reading from asynchronous\nfollowers is that it\u2019s possible for a user to see things moving backward in time.\nThis can happen if a user makes several reads from different replicas. For example,\nFigure 5-4 shows user 2345 making the same query twice, first to a follower with little\nlag, then to a follower with greater lag. (This scenario is quite likely if the user\nrefreshes a web page, and each request is routed to a random server.) The first query\nreturns a comment that was recently added by user 1234, but the second query\ndoesn\u2019t return anything because the lagging follower has not yet picked up that write.\nIn effect, the second query is observing the system at an earlier point in time than the\nfirst query. This wouldn\u2019t be so bad if the first query hadn\u2019t returned anything,\nbecause user 2345 probably wouldn\u2019t know that user 1234 had recently added a com\u2010\n164 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a3e0d76-d20c-42ab-8265-d6d8e1a86b88": {"__data__": {"id_": "9a3e0d76-d20c-42ab-8265-d6d8e1a86b88", "embedding": null, "metadata": {"page_label": "165", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9f956fb-1413-4767-b5c3-3c2a1ea68001", "node_type": "4", "metadata": {"page_label": "165", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3cdc5d61924f9e1fd877d6b9f7a377ae8acbf27ccf0665674951f3ef7cd7329b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ment. However, it\u2019s very confusing for user 2345 if they first see user 1234\u2019s comment\nappear, and then see it disappear again.\nFigure 5-4. A user first reads from a fresh replica, then from a stale replica. Time\nappears to go backward. To prevent this anomaly, we need monotonic reads.\nMonotonic reads [23] is a guarantee that this kind of anomaly does not happen. It\u2019s a\nlesser guarantee than strong consistency, but a stronger guarantee than eventual con\u2010\nsistency. When you read data, you may see an old value; monotonic reads only means\nthat if one user makes several reads in sequence, they will not see time go backward\u2014\ni.e., they will not read older data after having previously read newer data.\nOne way of achieving monotonic reads is to make sure that each user always makes\ntheir reads from the same replica (different users can read from different replicas).\nFor example, the replica can be chosen based on a hash of the user ID, rather than\nrandomly. However, if that replica fails, the user\u2019s queries will need to be rerouted to\nanother replica. \nConsistent Prefix Reads\nOur third example of replication lag anomalies concerns violation of causality. Imag\u2010\nine the following short dialog between Mr. Poons and Mrs. Cake:\nMr. Poons\nHow far into the future can you see, Mrs. Cake?\nMrs. Cake\nAbout ten seconds usually, Mr. Poons.\nProblems with Replication Lag | 165", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63d9fd76-b0f9-41f3-9bbc-8b495ea19308": {"__data__": {"id_": "63d9fd76-b0f9-41f3-9bbc-8b495ea19308", "embedding": null, "metadata": {"page_label": "166", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11e11b6a-99b7-4449-9e82-3691af2e3ae6", "node_type": "4", "metadata": {"page_label": "166", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6586e1405ede3351f729965bf60b5493eb3ffea954771b667475d7384bdaa5c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There is a causal dependency between those two sentences: Mrs. Cake heard Mr.\nPoons\u2019s question and answered it.\nNow, imagine a third person is listening to this conversation through followers. The\nthings said by Mrs. Cake go through a follower with little lag, but the things said by\nMr. Poons have a longer replication lag (see Figure 5-5). This observer would hear\nthe following:\nMrs. Cake\nAbout ten seconds usually, Mr. Poons.\nMr. Poons\nHow far into the future can you see, Mrs. Cake?\nTo the observer it looks as though Mrs. Cake is answering the question before Mr.\nPoons has even asked it. Such psychic powers are impressive, but very confusing [25].\nFigure 5-5. If some partitions are replicated slower than others, an observer may see the\nanswer before they see the question.\nPreventing this kind of anomaly requires another type of guarantee: consistent prefix\nreads [23]. This guarantee says that if a sequence of writes happens in a certain order,\nthen anyone reading those writes will see them appear in the same order.\nThis is a particular problem in partitioned (sharded) databases, which we will discuss\nin Chapter 6. If the database always applies writes in the same order, reads always see\na consistent prefix, so this anomaly cannot happen. However, in many distributed\n166 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8633b959-2abd-4958-9d23-3321ddd50ec6": {"__data__": {"id_": "8633b959-2abd-4958-9d23-3321ddd50ec6", "embedding": null, "metadata": {"page_label": "167", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b5388e7-9b69-478c-9d74-5deece8363bd", "node_type": "4", "metadata": {"page_label": "167", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7a095df300531c30ceacee25e7a1cf3563d1c2108fa2f14635c05527ad4f4279", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "databases, different partitions operate independently, so there is no global ordering of\nwrites: when a user reads from the database, they may see some parts of the database\nin an older state and some in a newer state.\nOne solution is to make sure that any writes that are causally related to each other are\nwritten to the same partition\u2014but in some applications that cannot be done effi\u2010\nciently. There are also algorithms that explicitly keep track of causal dependencies, a\ntopic that we will return to in \u201cThe \u201chappens-before\u201d relationship and concurrency\u201d\non page 186. \nSolutions for Replication Lag\nWhen working with an eventually consistent system, it is worth thinking about how\nthe application behaves if the replication lag increases to several minutes or even\nhours. If the answer is \u201cno problem,\u201d that\u2019s great. However, if the result is a bad expe\u2010\nrience for users, it\u2019s important to design the system to provide a stronger guarantee,\nsuch as read-after-write. Pretending that replication is synchronous when in fact it is\nasynchronous is a recipe for problems down the line.\nAs discussed earlier, there are ways in which an application can provide a stronger\nguarantee than the underlying database\u2014for example, by performing certain kinds of\nreads on the leader. However, dealing with these issues in application code is com\u2010\nplex and easy to get wrong.\nIt would be better if application developers didn\u2019t have to worry about subtle replica\u2010\ntion issues and could just trust their databases to \u201cdo the right thing.\u201d This is why\ntransactions exist: they are a way for a database to provide stronger guarantees so that\nthe application can be simpler.\nSingle-node transactions have existed for a long time. However, in the move to dis\u2010\ntributed (replicated and partitioned) databases, many systems have abandoned them,\nclaiming that transactions are too expensive in terms of performance and availability,\nand asserting that eventual consistency is inevitable in a scalable system. There is\nsome truth in that statement, but it is overly simplistic, and we will develop a more\nnuanced view over the course of the rest of this book. We will return to the topic of\ntransactions in Chapters 7 and 9, and we will discuss some alternative mechanisms in\nPart III. \nProblems with Replication Lag | 167", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2305, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "369a1355-c2b2-4637-a58b-52ff004c673e": {"__data__": {"id_": "369a1355-c2b2-4637-a58b-52ff004c673e", "embedding": null, "metadata": {"page_label": "168", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59c620fa-c73e-44a5-8f52-bcc51834e567", "node_type": "4", "metadata": {"page_label": "168", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a2dbfb906228e8531f837acec688ff1f32ad6a0cfd618c41a7883c38752605cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. If the database is partitioned (see Chapter 6), each partition has one leader. Different partitions may have\ntheir leaders on different nodes, but each partition must nevertheless have one leader node.\nMulti-Leader Replication\nSo far in this chapter we have only considered replication architectures using a single\nleader. Although that is a common approach, there are interesting alternatives.\nLeader-based replication has one major downside: there is only one leader, and all\nwrites must go through it. iv If you can\u2019t connect to the leader for any reason, for\nexample due to a network interruption between you and the leader, you can\u2019t write to\nthe database.\nA natural extension of the leader-based replication model is to allow more than one\nnode to accept writes. Replication still happens in the same way: each node that pro\u2010\ncesses a write must forward that data change to all the other nodes. We call this a\nmulti-leader configuration (also known as master\u2013master or active/active replication).\nIn this setup, each leader simultaneously acts as a follower to the other leaders.\nUse Cases for Multi-Leader Replication\nIt rarely makes sense to use a multi-leader setup within a single datacenter, because\nthe benefits rarely outweigh the added complexity. However, there are some situa\u2010\ntions in which this configuration is reasonable.\nMulti-datacenter operation\nImagine you have a database with replicas in several different datacenters (perhaps so\nthat you can tolerate failure of an entire datacenter, or perhaps in order to be closer\nto your users). With a normal leader-based replication setup, the leader has to be in\none of the datacenters, and all writes must go through that datacenter.\nIn a multi-leader configuration, you can have a leader in each datacenter. Figure 5-6\nshows what this architecture might look like. Within each datacenter, regular leader\u2013\nfollower replication is used; between datacenters, each datacenter\u2019s leader replicates\nits changes to the leaders in other datacenters.\n168 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51c4f586-792b-4d4a-a513-d2d145c65833": {"__data__": {"id_": "51c4f586-792b-4d4a-a513-d2d145c65833", "embedding": null, "metadata": {"page_label": "169", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "384aa737-d647-432e-a3e6-687a03a38726", "node_type": "4", "metadata": {"page_label": "169", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3376083f6b178da5f685ea6967ff28a77fffda3867151b07304fdbb956de1898", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5-6. Multi-leader replication across multiple datacenters.\nLet\u2019s compare how the single-leader and multi-leader configurations fare in a multi-\ndatacenter deployment:\nPerformance\nIn a single-leader configuration, every write must go over the internet to the\ndatacenter with the leader. This can add significant latency to writes and might\ncontravene the purpose of having multiple datacenters in the first place. In a\nmulti-leader configuration, every write can be processed in the local datacenter\nand is replicated asynchronously to the other datacenters. Thus, the inter-\ndatacenter network delay is hidden from users, which means the perceived per\u2010\nformance may be better.\nTolerance of datacenter outages\nIn a single-leader configuration, if the datacenter with the leader fails, failover\ncan promote a follower in another datacenter to be leader. In a multi-leader con\u2010\nfiguration, each datacenter can continue operating independently of the others,\nand replication catches up when the failed datacenter comes back online.\nTolerance of network problems\nTraffic between datacenters usually goes over the public internet, which may be\nless reliable than the local network within a datacenter. A single-leader configu\u2010\nration is very sensitive to problems in this inter-datacenter link, because writes\nare made synchronously over this link. A multi-leader configuration with asyn\u2010\nchronous replication can usually tolerate network problems better: a temporary\nnetwork interruption does not prevent writes being processed.\nMulti-Leader Replication | 169", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e53834e-91c3-4f62-8285-2d0b345de351": {"__data__": {"id_": "2e53834e-91c3-4f62-8285-2d0b345de351", "embedding": null, "metadata": {"page_label": "170", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7505b684-5e5d-4c1c-a0dd-c21bb425195d", "node_type": "4", "metadata": {"page_label": "170", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8b0f576d98d21bf1634282d1942437bec1455e63d1886120a89a13553e987382", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some databases support multi-leader configurations by default, but it is also often\nimplemented with external tools, such as Tungsten Replicator for MySQL [ 26], BDR\nfor PostgreSQL [27], and GoldenGate for Oracle [19].\nAlthough multi-leader replication has advantages, it also has a big downside: the\nsame data may be concurrently modified in two different datacenters, and those write\nconflicts must be resolved (indicated as \u201cconflict resolution\u201d in Figure 5-6). We will\ndiscuss this issue in \u201cHandling Write Conflicts\u201d on page 171.\nAs multi-leader replication is a somewhat retrofitted feature in many databases, there\nare often subtle configuration pitfalls and surprising interactions with other database\nfeatures. For example, autoincrementing keys, triggers, and integrity constraints can\nbe problematic. For this reason, multi-leader replication is often considered danger\u2010\nous territory that should be avoided if possible [28].\nClients with offline operation\nAnother situation in which multi-leader replication is appropriate is if you have an\napplication that needs to continue to work while it is disconnected from the internet.\nFor example, consider the calendar apps on your mobile phone, your laptop, and\nother devices. You need to be able to see your meetings (make read requests) and\nenter new meetings (make write requests) at any time, regardless of whether your\ndevice currently has an internet connection. If you make any changes while you are\noffline, they need to be synced with a server and your other devices when the device\nis next online.\nIn this case, every device has a local database that acts as a leader (it accepts write\nrequests), and there is an asynchronous multi-leader replication process (sync)\nbetween the replicas of your calendar on all of your devices. The replication lag may\nbe hours or even days, depending on when you have internet access available.\nFrom an architectural point of view, this setup is essentially the same as multi-leader\nreplication between datacenters, taken to the extreme: each device is a \u201cdatacenter,\u201d\nand the network connection between them is extremely unreliable. As the rich his\u2010\ntory of broken calendar sync implementations demonstrates, multi-leader replication\nis a tricky thing to get right.\nThere are tools that aim to make this kind of multi-leader configuration easier. For\nexample, CouchDB is designed for this mode of operation [29].\nCollaborative editing\nReal-time collaborative editing  applications allow several people to edit a document\nsimultaneously. For example, Etherpad [ 30] and Google Docs [ 31] allow multiple\npeople to concurrently edit a text document or spreadsheet (the algorithm is briefly\ndiscussed in \u201cAutomatic Conflict Resolution\u201d on page 174).\n170 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "008e0dfd-2f90-48f7-a1c1-0357f872896f": {"__data__": {"id_": "008e0dfd-2f90-48f7-a1c1-0357f872896f", "embedding": null, "metadata": {"page_label": "171", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95399a02-52c2-46bb-8915-1ae0772afbb1", "node_type": "4", "metadata": {"page_label": "171", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2abc3c7a8653f3749490e89440ab348ef933a0084237b4b0b45cd44bba3d92fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We don\u2019t usually think of collaborative editing as a database replication problem, but\nit has a lot in common with the previously mentioned offline editing use case. When\none user edits a document, the changes are instantly applied to their local replica (the\nstate of the document in their web browser or client application) and asynchronously\nreplicated to the server and any other users who are editing the same document.\nIf you want to guarantee that there will be no editing conflicts, the application must\nobtain a lock on the document before a user can edit it. If another user wants to edit\nthe same document, they first have to wait until the first user has committed their\nchanges and released the lock. This collaboration model is equivalent to single-leader\nreplication with transactions on the leader.\nHowever, for faster collaboration, you may want to make the unit of change very\nsmall (e.g., a single keystroke) and avoid locking. This approach allows multiple users\nto edit simultaneously, but it also brings all the challenges of multi-leader replication,\nincluding requiring conflict resolution [32].\nHandling Write Conflicts\nThe biggest problem with multi-leader replication is that write conflicts can occur,\nwhich means that conflict resolution is required.\nFor example, consider a wiki page that is simultaneously being edited by two users, as\nshown in Figure 5-7 . User 1 changes the title of the page from A to B, and user 2\nchanges the title from A to C at the same time. Each user\u2019s change is successfully\napplied to their local leader. However, when the changes are asynchronously replica\u2010\nted, a conflict is detected [ 33]. This problem does not occur in a single-leader data\u2010\nbase.\nFigure 5-7. A write conflict caused by two leaders concurrently updating the same\nrecord.\nMulti-Leader Replication | 171", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "040ea64c-368e-4513-baf7-2ed2063a6898": {"__data__": {"id_": "040ea64c-368e-4513-baf7-2ed2063a6898", "embedding": null, "metadata": {"page_label": "172", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a0593a2-5395-4885-b6a2-735b2177eb55", "node_type": "4", "metadata": {"page_label": "172", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4d93a0ef3ee7081c72032a5adfb427fd17444b1785812771f0f3525e0d40e333", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Synchronous versus asynchronous conflict detection\nIn a single-leader database, the second writer will either block and wait for the first\nwrite to complete, or abort the second write transaction, forcing the user to retry the\nwrite. On the other hand, in a multi-leader setup, both writes are successful, and the\nconflict is only detected asynchronously at some later point in time. At that time, it\nmay be too late to ask the user to resolve the conflict.\nIn principle, you could make the conflict detection synchronous\u2014i.e., wait for the\nwrite to be replicated to all replicas before telling the user that the write was success\u2010\nful. However, by doing so, you would lose the main advantage of multi-leader repli\u2010\ncation: allowing each replica to accept writes independently. If you want synchronous\nconflict detection, you might as well just use single-leader replication.\nConflict avoidance\nThe simplest strategy for dealing with conflicts is to avoid them: if the application can\nensure that all writes for a particular record go through the same leader, then con\u2010\nflicts cannot occur. Since many implementations of multi-leader replication handle\nconflicts quite poorly, avoiding conflicts is a frequently recommended approach [34].\nFor example, in an application where a user can edit their own data, you can ensure\nthat requests from a particular user are always routed to the same datacenter and use\nthe leader in that datacenter for reading and writing. Different users may have differ\u2010\nent \u201chome\u201d datacenters (perhaps picked based on geographic proximity to the user),\nbut from any one user\u2019s point of view the configuration is essentially single-leader.\nHowever, sometimes you might want to change the designated leader for a record\u2014\nperhaps because one datacenter has failed and you need to reroute traffic to another\ndatacenter, or perhaps because a user has moved to a different location and is now\ncloser to a different datacenter. In this situation, conflict avoidance breaks down, and\nyou have to deal with the possibility of concurrent writes on different leaders.\nConverging toward a consistent state\nA single-leader database applies writes in a sequential order: if there are several\nupdates to the same field, the last write determines the final value of the field.\nIn a multi-leader configuration, there is no defined ordering of writes, so it\u2019s not clear\nwhat the final value should be. In Figure 5-7, at leader 1 the title is first updated to B\nand then to C; at leader 2 it is first updated to C and then to B. Neither order is \u201cmore\ncorrect\u201d than the other.\nIf each replica simply applied writes in the order that it saw the writes, the database\nwould end up in an inconsistent state: the final value would be C at leader 1 and B at\nleader 2. That is not acceptable\u2014every replication scheme must ensure that the data\nis eventually the same in all replicas. Thus, the database must resolve the conflict in a\n172 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ce47405-cd92-4f1c-946d-e335e1b80d5f": {"__data__": {"id_": "8ce47405-cd92-4f1c-946d-e335e1b80d5f", "embedding": null, "metadata": {"page_label": "173", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6d5b278-e94f-4115-8c0f-1bdf21f37202", "node_type": "4", "metadata": {"page_label": "173", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d569f2201f7fa8c2e0344132e454ff50438a122450ebf119f3dfa5e55616618e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "convergent way, which means that all replicas must arrive at the same final value\nwhen all changes have been replicated.\nThere are various ways of achieving convergent conflict resolution:\n\u2022 Give each write a unique ID (e.g., a timestamp, a long random number, a UUID,\nor a hash of the key and value), pick the write with the highest ID as the winner,\nand throw away the other writes. If a timestamp is used, this technique is known\nas last write wins  (LWW). Although this approach is popular, it is dangerously\nprone to data loss [ 35]. We will discuss LWW in more detail at the end of this\nchapter (\u201cDetecting Concurrent Writes\u201d on page 184).\n\u2022 Give each replica a unique ID, and let writes that originated at a higher-\nnumbered replica always take precedence over writes that originated at a lower-\nnumbered replica. This approach also implies data loss.\n\u2022 Somehow merge the values together\u2014e.g., order them alphabetically and then\nconcatenate them (in Figure 5-7 , the merged title might be something like\n\u201cB/C\u201d).\n\u2022 Record the conflict in an explicit data structure that preserves all information,\nand write application code that resolves the conflict at some later time (perhaps\nby prompting the user).\nCustom conflict resolution logic\nAs the most appropriate way of resolving a conflict may depend on the application,\nmost multi-leader replication tools let you write conflict resolution logic using appli\u2010\ncation code. That code may be executed on write or on read:\nOn write\nAs soon as the database system detects a conflict in the log of replicated changes,\nit calls the conflict handler. For example, Bucardo allows you to write a snippet of\nPerl for this purpose. This handler typically cannot prompt a user\u2014it runs in a\nbackground process and it must execute quickly.\nOn read\nWhen a conflict is detected, all the conflicting writes are stored. The next time\nthe data is read, these multiple versions of the data are returned to the applica\u2010\ntion. The application may prompt the user or automatically resolve the conflict,\nand write the result back to the database. CouchDB works this way, for example.\nNote that conflict resolution usually applies at the level of an individual row or docu\u2010\nment, not for an entire transaction [ 36]. Thus, if you have a transaction that atomi\u2010\ncally makes several different writes (see Chapter 7 ), each write is still considered\nseparately for the purposes of conflict resolution.\nMulti-Leader Replication | 173", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bef59ae7-e996-4b0e-b713-20c54b3cc47d": {"__data__": {"id_": "bef59ae7-e996-4b0e-b713-20c54b3cc47d", "embedding": null, "metadata": {"page_label": "174", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "483ae7a9-035f-4a75-a3e1-fbe28e3109a8", "node_type": "4", "metadata": {"page_label": "174", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "eb7eef0a5efb312921b84d0c1a6d8e65d8718ec0d8ce032fa867fd29293866ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Automatic Conflict Resolution\nConflict resolution rules can quickly become complicated, and custom code can be\nerror-prone. Amazon is a frequently cited example of surprising effects due to a con\u2010\nflict resolution handler: for some time, the conflict resolution logic on the shopping\ncart would preserve items added to the cart, but not items removed from the cart.\nThus, customers would sometimes see items reappearing in their carts even though\nthey had previously been removed [37].\nThere has been some interesting research into automatically resolving conflicts\ncaused by concurrent data modifications. A few lines of research are worth mention\u2010\ning:\n\u2022 Conflict-free replicated datatypes (CRDTs) [32, 38] are a family of data structures\nfor sets, maps, ordered lists, counters, etc. that can be concurrently edited by\nmultiple users, and which automatically resolve conflicts in sensible ways. Some\nCRDTs have been implemented in Riak 2.0 [39, 40].\n\u2022 Mergeable persistent data structures  [41] track history explicitly, similarly to the\nGit version control system, and use a three-way merge function (whereas CRDTs\nuse two-way merges).\n\u2022 Operational transformation [42] is the conflict resolution algorithm behind col\u2010\nlaborative editing applications such as Etherpad [ 30] and Google Docs [ 31]. It\nwas designed particularly for concurrent editing of an ordered list of items, such\nas the list of characters that constitute a text document.\nImplementations of these algorithms in databases are still young, but it\u2019s likely that\nthey will be integrated into more replicated data systems in the future. Automatic\nconflict resolution could make multi-leader data synchronization much simpler for\napplications to deal with. \nWhat is a conflict?\nSome kinds of conflict are obvious. In the example in Figure 5-7, two writes concur\u2010\nrently modified the same field in the same record, setting it to two different values.\nThere is little doubt that this is a conflict.\nOther kinds of conflict can be more subtle to detect. For example, consider a meeting\nroom booking system: it tracks which room is booked by which group of people at\nwhich time. This application needs to ensure that each room is only booked by one\ngroup of people at any one time (i.e., there must not be any overlapping bookings for\nthe same room). In this case, a conflict may arise if two different bookings are created\nfor the same room at the same time. Even if the application checks availability before\n174 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2503, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61444208-b3a9-4ac7-b4bc-8970843cb03b": {"__data__": {"id_": "61444208-b3a9-4ac7-b4bc-8970843cb03b", "embedding": null, "metadata": {"page_label": "175", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504a606a-c84d-49ae-8b52-47873bd0cb30", "node_type": "4", "metadata": {"page_label": "175", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "30352f0514c1a493c1bf201c6ecc682228f190bdcf88d929d630008abde5bc86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "v. Not to be confused with a star schema (see \u201cStars and Snowflakes: Schemas for Analytics\u201d on page 93),\nwhich describes the structure of a data model, not the communication topology between nodes.\nallowing a user to make a booking, there can be a conflict if the two bookings are\nmade on two different leaders.\nThere isn\u2019t a quick ready-made answer, but in the following chapters we will trace a\npath toward a good understanding of this problem. We will see some more examples\nof conflicts in Chapter 7, and in Chapter 12 we will discuss scalable approaches for\ndetecting and resolving conflicts in a replicated system. \nMulti-Leader Replication Topologies\nA replication topology  describes the communication paths along which writes are\npropagated from one node to another. If you have two leaders, like in Figure 5-7 ,\nthere is only one plausible topology: leader 1 must send all of its writes to leader 2,\nand vice versa. With more than two leaders, various different topologies are possible.\nSome examples are illustrated in Figure 5-8.\nFigure 5-8. Three example topologies in which multi-leader replication can be set up.\nThe most general topology is all-to-all (Figure 5-8 [c]), in which every leader sends its\nwrites to every other leader. However, more restricted topologies are also used: for\nexample, MySQL by default supports only a circular topology  [34], in which each\nnode receives writes from one node and forwards those writes (plus any writes of its\nown) to one other node. Another popular topology has the shape of a star:v one desig\u2010\nnated root node forwards writes to all of the other nodes. The star topology can be\ngeneralized to a tree.\nIn circular and star topologies, a write may need to pass through several nodes before\nit reaches all replicas. Therefore, nodes need to forward data changes they receive\nfrom other nodes. To prevent infinite replication loops, each node is given a unique\nidentifier, and in the replication log, each write is tagged with the identifiers of all the\nnodes it has passed through [ 43]. When a node receives a data change that is tagged\nMulti-Leader Replication | 175", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4068028f-2ed8-4790-b6ca-a65a277cb6e8": {"__data__": {"id_": "4068028f-2ed8-4790-b6ca-a65a277cb6e8", "embedding": null, "metadata": {"page_label": "176", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d45a1def-af70-4f6a-8155-9f98e7e5ca47", "node_type": "4", "metadata": {"page_label": "176", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4d2b0babbd1e2fd16bb7e50fe0c8f398e620f79fb6b7665755e3bf45d2a52bc0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with its own identifier, that data change is ignored, because the node knows that it\nhas already been processed.\nA problem with circular and star topologies is that if just one node fails, it can inter\u2010\nrupt the flow of replication messages between other nodes, causing them to be unable\nto communicate until the node is fixed. The topology could be reconfigured to work\naround the failed node, but in most deployments such reconfiguration would have to\nbe done manually. The fault tolerance of a more densely connected topology (such as\nall-to-all) is better because it allows messages to travel along different paths, avoiding\na single point of failure.\nOn the other hand, all-to-all topologies can have issues too. In particular, some net\u2010\nwork links may be faster than others (e.g., due to network congestion), with the result\nthat some replication messages may \u201covertake\u201d others, as illustrated in Figure 5-9.\nFigure 5-9. With multi-leader replication, writes may arrive in the wrong order at some\nreplicas.\nIn Figure 5-9, client A inserts a row into a table on leader 1, and client B updates that\nrow on leader 3. However, leader 2 may receive the writes in a different order: it may\nfirst receive the update (which, from its point of view, is an update to a row that does\nnot exist in the database) and only later receive the corresponding insert (which\nshould have preceded the update).\nThis is a problem of causality, similar to the one we saw in \u201cConsistent Prefix Reads\u201d\non page 165: the update depends on the prior insert, so we need to make sure that all\nnodes process the insert first, and then the update. Simply attaching a timestamp to\n176 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eec6fcbe-c5bb-4104-838c-f384f5a7f3c9": {"__data__": {"id_": "eec6fcbe-c5bb-4104-838c-f384f5a7f3c9", "embedding": null, "metadata": {"page_label": "177", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a9c34b7-7bfb-4d69-82cd-92ea5ce978ad", "node_type": "4", "metadata": {"page_label": "177", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "273e8a457ebce4d86a74d215a14bb58baaa3abda355add950878241e191c08b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vi. Dynamo is not available to users outside of Amazon. Confusingly, AWS offers a hosted database product\ncalled DynamoDB, which uses a completely different architecture: it is based on single-leader replication.\nevery write is not sufficient, because clocks cannot be trusted to be sufficiently in sync\nto correctly order these events at leader 2 (see Chapter 8).\nTo order these events correctly, a technique called version vectors can be used, which\nwe will discuss later in this chapter (see \u201cDetecting Concurrent Writes\u201d on page 184).\nHowever, conflict detection techniques are poorly implemented in many multi-leader\nreplication systems. For example, at the time of writing, PostgreSQL BDR does not\nprovide causal ordering of writes [ 27], and Tungsten Replicator for MySQL doesn\u2019t\neven try to detect conflicts [34].\nIf you are using a system with multi-leader replication, it is worth being aware of\nthese issues, carefully reading the documentation, and thoroughly testing your data\u2010\nbase to ensure that it really does provide the guarantees you believe it to have. \nLeaderless Replication\nThe replication approaches we have discussed so far in this chapter\u2014single-leader\nand multi-leader replication\u2014are based on the idea that a client sends a write request\nto one node (the leader), and the database system takes care of copying that write to\nthe other replicas. A leader determines the order in which writes should be processed,\nand followers apply the leader\u2019s writes in the same order.\nSome data storage systems take a different approach, abandoning the concept of a\nleader and allowing any replica to directly accept writes from clients. Some of the ear\u2010\nliest replicated data systems were leaderless [ 1, 44], but the idea was mostly forgotten\nduring the era of dominance of relational databases. It once again became a fashiona\u2010\nble architecture for databases after Amazon used it for its in-house Dynamo system\n[37].vi Riak, Cassandra, and Voldemort are open source datastores with leaderless\nreplication models inspired by Dynamo, so this kind of database is also known as\nDynamo-style.\nIn some leaderless implementations, the client directly sends its writes to several rep\u2010\nlicas, while in others, a coordinator node does this on behalf of the client. However,\nunlike a leader database, that coordinator does not enforce a particular ordering of\nwrites. As we shall see, this difference in design has profound consequences for the\nway the database is used.\nWriting to the Database When a Node Is Down\nImagine you have a database with three replicas, and one of the replicas is currently\nunavailable\u2014perhaps it is being rebooted to install a system update. In a leader-based\nLeaderless Replication | 177", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6dcb78a9-b7e0-4b39-bd02-ca962a5f6b47": {"__data__": {"id_": "6dcb78a9-b7e0-4b39-bd02-ca962a5f6b47", "embedding": null, "metadata": {"page_label": "178", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7389cd1-68cc-4697-b77c-eb76d0ddcba1", "node_type": "4", "metadata": {"page_label": "178", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6438597c6e753302d8e9ff4c17679309cafdaf818a1a3db59d73f93f137d039f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "configuration, if you want to continue processing writes, you may need to perform a\nfailover (see \u201cHandling Node Outages\u201d on page 156).\nOn the other hand, in a leaderless configuration, failover does not exist. Figure 5-10\nshows what happens: the client (user 1234) sends the write to all three replicas in par\u2010\nallel, and the two available replicas accept the write but the unavailable replica misses\nit. Let\u2019s say that it\u2019s sufficient for two out of three replicas to acknowledge the write:\nafter user 1234 has received two ok responses, we consider the write to be successful.\nThe client simply ignores the fact that one of the replicas missed the write.\nFigure 5-10. A quorum write, quorum read, and read repair after a node outage.\nNow imagine that the unavailable node comes back online, and clients start reading\nfrom it. Any writes that happened while the node was down are missing from that\nnode. Thus, if you read from that node, you may get stale (outdated) values as\nresponses.\nTo solve that problem, when a client reads from the database, it doesn\u2019t just send its\nrequest to one replica: read requests are also sent to several nodes in parallel . The cli\u2010\nent may get different responses from different nodes; i.e., the up-to-date value from\none node and a stale value from another. Version numbers are used to determine\nwhich value is newer (see \u201cDetecting Concurrent Writes\u201d on page 184).\nRead repair and anti-entropy\nThe replication scheme should ensure that eventually all the data is copied to every\nreplica. After an unavailable node comes back online, how does it catch up on the\nwrites that it missed?\n178 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1651, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "824e6936-163e-4fe3-9f48-0a4219069a0e": {"__data__": {"id_": "824e6936-163e-4fe3-9f48-0a4219069a0e", "embedding": null, "metadata": {"page_label": "179", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d52ea854-7804-4c8a-81d1-d3545322ed7a", "node_type": "4", "metadata": {"page_label": "179", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4ce1d1aed7b05d6477f06817f8e62f9772324f2e444da0ff69b753af91db18d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vii. Sometimes this kind of quorum is called a strict quorum, to contrast with sloppy quorums (discussed in\n\u201cSloppy Quorums and Hinted Handoff\u201d on page 183).\nTwo mechanisms are often used in Dynamo-style datastores:\nRead repair\nWhen a client makes a read from several nodes in parallel, it can detect any stale\nresponses. For example, in Figure 5-10, user 2345 gets a version 6 value from rep\u2010\nlica 3 and a version 7 value from replicas 1 and 2. The client sees that replica 3\nhas a stale value and writes the newer value back to that replica. This approach\nworks well for values that are frequently read.\nAnti-entropy process\nIn addition, some datastores have a background process that constantly looks for\ndifferences in the data between replicas and copies any missing data from one\nreplica to another. Unlike the replication log in leader-based replication, this\nanti-entropy process does not copy writes in any particular order, and there may\nbe a significant delay before data is copied.\nNot all systems implement both of these; for example, Voldemort currently does not\nhave an anti-entropy process. Note that without an anti-entropy process, values that\nare rarely read may be missing from some replicas and thus have reduced durability,\nbecause read repair is only performed when a value is read by the application.\nQuorums for reading and writing\nIn the example of Figure 5-10, we considered the write to be successful even though it\nwas only processed on two out of three replicas. What if only one out of three replicas\naccepted the write? How far can we push this?\nIf we know that every successful write is guaranteed to be present on at least two out\nof three replicas, that means at most one replica can be stale. Thus, if we read from at\nleast two replicas, we can be sure that at least one of the two is up to date. If the third\nreplica is down or slow to respond, reads can nevertheless continue returning an up-\nto-date value.\nMore generally, if there are n replicas, every write must be confirmed by w nodes to\nbe considered successful, and we must query at least r nodes for each read. (In our\nexample, n = 3, w = 2, r = 2.) As long as w + r > n, we expect to get an up-to-date\nvalue when reading, because at least one of the r nodes we\u2019re reading from must be\nup to date. Reads and writes that obey these r and w values are called quorum reads\nand writes [44].vii You can think of r and w as the minimum number of votes required\nfor the read or write to be valid.\nLeaderless Replication | 179", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2309a15-2930-4f20-869a-c695476365c0": {"__data__": {"id_": "b2309a15-2930-4f20-869a-c695476365c0", "embedding": null, "metadata": {"page_label": "180", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00f86e02-8911-4ead-8cc7-13ffb0337b37", "node_type": "4", "metadata": {"page_label": "180", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "49fc069429abe6cbf5d8d55123bf0f9849b8aa27ee4849c3159c8502b6e6a885", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Dynamo-style databases, the parameters n, w, and r are typically configurable. A\ncommon choice is to make n an odd number (typically 3 or 5) and to set w = r =\n(n + 1) / 2 (rounded up). However, you can vary the numbers as you see fit. For\nexample, a workload with few writes and many reads may benefit from setting w = n\nand r = 1. This makes reads faster, but has the disadvantage that just one failed node\ncauses all database writes to fail.\nThere may be more than n nodes in the cluster, but any given value\nis stored only on n nodes. This allows the dataset to be partitioned,\nsupporting datasets that are larger than you can fit on one node.\nWe will return to partitioning in Chapter 6.\nThe quorum condition, w + r > n, allows the system to tolerate unavailable nodes as\nfollows:\n\u2022 If w < n, we can still process writes if a node is unavailable.\n\u2022 If r < n, we can still process reads if a node is unavailable.\n\u2022 With n = 3, w = 2, r = 2 we can tolerate one unavailable node.\n\u2022 With n = 5, w = 3, r = 3 we can tolerate two unavailable nodes. This case is illus\u2010\ntrated in Figure 5-11.\n\u2022 Normally, reads and writes are always sent to all n replicas in parallel. The\nparameters w and r determine how many nodes we wait for\u2014i.e., how many of\nthe n nodes need to report success before we consider the read or write to be suc\u2010\ncessful.\nFigure 5-11. If w + r > n, at least one of the r replicas you read from must have seen the\nmost recent successful write.\n180 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88cb5cb9-9069-444d-9a07-3fb26d80a2f7": {"__data__": {"id_": "88cb5cb9-9069-444d-9a07-3fb26d80a2f7", "embedding": null, "metadata": {"page_label": "181", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05ea65a2-5eeb-4a89-b854-7eafe2180cf9", "node_type": "4", "metadata": {"page_label": "181", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4377183b8ff0c46707a34cbe526eb425e6308eaa93e295b65b3977cd6a0e5f0e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If fewer than the required w or r nodes are available, writes or reads return an error.\nA node could be unavailable for many reasons: because the node is down (crashed,\npowered down), due to an error executing the operation (can\u2019t write because the disk\nis full), due to a network interruption between the client and the node, or for any\nnumber of other reasons. We only care whether the node returned a successful\nresponse and don\u2019t need to distinguish between different kinds of fault.\nLimitations of Quorum Consistency\nIf you have n replicas, and you choose w and r such that w + r > n, you can generally\nexpect every read to return the most recent value written for a key. This is the case\nbecause the set of nodes to which you\u2019ve written and the set of nodes from which\nyou\u2019ve read must overlap. That is, among the nodes you read there must be at least\none node with the latest value (illustrated in Figure 5-11).\nOften, r and w are chosen to be a majority (more than n/2) of nodes, because that\nensures w + r > n while still tolerating up to n/2 node failures. But quorums are not\nnecessarily majorities\u2014it only matters that the sets of nodes used by the read and\nwrite operations overlap in at least one node. Other quorum assignments are possi\u2010\nble, which allows some flexibility in the design of distributed algorithms [45].\nYou may also set w and r to smaller numbers, so that w + r \u2264 n (i.e., the quorum con\u2010\ndition is not satisfied). In this case, reads and writes will still be sent to n nodes, but a\nsmaller number of successful responses is required for the operation to succeed.\nWith a smaller w and r you are more likely to read stale values, because it\u2019s more\nlikely that your read didn\u2019t include the node with the latest value. On the upside, this\nconfiguration allows lower latency and higher availability: if there is a network inter\u2010\nruption and many replicas become unreachable, there\u2019s a higher chance that you can\ncontinue processing reads and writes. Only after the number of reachable replicas\nfalls below w or r does the database become unavailable for writing or reading,\nrespectively.\nHowever, even with w + r > n, there are likely to be edge cases where stale values are\nreturned. These depend on the implementation, but possible scenarios include:\n\u2022 If a sloppy quorum is used (see \u201cSloppy Quorums and Hinted Handoff\u201d  on page\n183), the w writes may end up on different nodes than the r reads, so there is no\nlonger a guaranteed overlap between the r nodes and the w nodes [46].\n\u2022 If two writes occur concurrently, it is not clear which one happened first. In this\ncase, the only safe solution is to merge the concurrent writes (see \u201cHandling\nWrite Conflicts\u201d on page 171). If a winner is picked based on a timestamp (last\nwrite wins), writes can be lost due to clock skew [ 35]. We will return to this topic\nin \u201cDetecting Concurrent Writes\u201d on page 184.\nLeaderless Replication | 181", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3d89ad7-ef1b-4122-be2c-abd4aced6d83": {"__data__": {"id_": "b3d89ad7-ef1b-4122-be2c-abd4aced6d83", "embedding": null, "metadata": {"page_label": "182", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "034d0216-d8c6-4196-9f1e-82d36a15ad8c", "node_type": "4", "metadata": {"page_label": "182", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dbec8584b371d163458044054a2aeecfce6096ab3cc65532313f90c79d2cc3da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If a write happens concurrently with a read, the write may be reflected on only\nsome of the replicas. In this case, it\u2019s undetermined whether the read returns the\nold or the new value.\n\u2022 If a write succeeded on some replicas but failed on others (for example because\nthe disks on some nodes are full), and overall succeeded on fewer than w replicas,\nit is not rolled back on the replicas where it succeeded. This means that if a write\nwas reported as failed, subsequent reads may or may not return the value from\nthat write [47].\n\u2022 If a node carrying a new value fails, and its data is restored from a replica carry\u2010\ning an old value, the number of replicas storing the new value may fall below w,\nbreaking the quorum condition.\n\u2022 Even if everything is working correctly, there are edge cases in which you can get\nunlucky with the timing, as we shall see in \u201cLinearizability and quorums\u201d on\npage 334.\nThus, although quorums appear to guarantee that a read returns the latest written\nvalue, in practice it is not so simple. Dynamo-style databases are generally optimized\nfor use cases that can tolerate eventual consistency. The parameters w and r allow you\nto adjust the probability of stale values being read, but it\u2019s wise to not take them as\nabsolute guarantees.\nIn particular, you usually do not get the guarantees discussed in \u201cProblems with Rep\u2010\nlication Lag\u201d on page 161 (reading your writes, monotonic reads, or consistent prefix\nreads), so the previously mentioned anomalies can occur in applications. Stronger\nguarantees generally require transactions or consensus. We will return to these topics\nin Chapter 7 and Chapter 9. \nMonitoring staleness\nFrom an operational perspective, it\u2019s important to monitor whether your databases\nare returning up-to-date results. Even if your application can tolerate stale reads, you\nneed to be aware of the health of your replication. If it falls behind significantly, it\nshould alert you so that you can investigate the cause (for example, a problem in the\nnetwork or an overloaded node).\nFor leader-based replication, the database typically exposes metrics for the replication\nlag, which you can feed into a monitoring system. This is possible because writes are\napplied to the leader and to followers in the same order, and each node has a position\nin the replication log (the number of writes it has applied locally). By subtracting a\nfollower\u2019s current position from the leader\u2019s current position, you can measure the\namount of replication lag.\nHowever, in systems with leaderless replication, there is no fixed order in which\nwrites are applied, which makes monitoring more difficult. Moreover, if the database\n182 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "115be742-f81f-41c1-9074-bb633d086ace": {"__data__": {"id_": "115be742-f81f-41c1-9074-bb633d086ace", "embedding": null, "metadata": {"page_label": "183", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c4fab6e-1390-46bd-a4d6-2fd115973002", "node_type": "4", "metadata": {"page_label": "183", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3bc591f9d4d8881a254bed0e0763eab2c387609749a9a69ebfc1eadf345baedc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "only uses read repair (no anti-entropy), there is no limit to how old a value might be\n\u2014if a value is only infrequently read, the value returned by a stale replica may be\nancient.\nThere has been some research on measuring replica staleness in databases with lead\u2010\nerless replication and predicting the expected percentage of stale reads depending on\nthe parameters n, w, and r [48]. This is unfortunately not yet common practice, but it\nwould be good to include staleness measurements in the standard set of metrics for\ndatabases. Eventual consistency is a deliberately vague guarantee, but for operability\nit\u2019s important to be able to quantify \u201ceventual.\u201d \nSloppy Quorums and Hinted Handoff\nDatabases with appropriately configured quorums can tolerate the failure of individ\u2010\nual nodes without the need for failover. They can also tolerate individual nodes going\nslow, because requests don\u2019t have to wait for all n nodes to respond\u2014they can return\nwhen w or r nodes have responded. These characteristics make databases with leader\u2010\nless replication appealing for use cases that require high availability and low latency,\nand that can tolerate occasional stale reads.\nHowever, quorums (as described so far) are not as fault-tolerant as they could be. A\nnetwork interruption can easily cut off a client from a large number of database\nnodes. Although those nodes are alive, and other clients may be able to connect to\nthem, to a client that is cut off from the database nodes, they might as well be dead. In\nthis situation, it\u2019s likely that fewer than w or r reachable nodes remain, so the client\ncan no longer reach a quorum.\nIn a large cluster (with significantly more than n nodes) it\u2019s likely that the client can\nconnect to some database nodes during the network interruption, just not to the\nnodes that it needs to assemble a quorum for a particular value. In that case, database\ndesigners face a trade-off:\n\u2022 Is it better to return errors to all requests for which we cannot reach a quorum of\nw or r nodes?\n\u2022 Or should we accept writes anyway, and write them to some nodes that are\nreachable but aren\u2019t among the n nodes on which the value usually lives?\nThe latter is known as a sloppy quorum [37]: writes and reads still require w and r\nsuccessful responses, but those may include nodes that are not among the designated\nn \u201chome\u201d nodes for a value. By analogy, if you lock yourself out of your house, you\nmay knock on the neighbor\u2019s door and ask whether you may stay on their couch tem\u2010\nporarily.\nOnce the network interruption is fixed, any writes that one node temporarily\naccepted on behalf of another node are sent to the appropriate \u201chome\u201d nodes. This is\nLeaderless Replication | 183", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "36e511cf-918e-4171-af39-9dbe13e45955": {"__data__": {"id_": "36e511cf-918e-4171-af39-9dbe13e45955", "embedding": null, "metadata": {"page_label": "184", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0dbcf08e-010c-4586-abb4-bc3f8e1b7972", "node_type": "4", "metadata": {"page_label": "184", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a372d865a8b62120826add4baa166a1a8420616803bc18ca4bf836fc7976d548", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "called hinted handoff . (Once you find the keys to your house again, your neighbor\npolitely asks you to get off their couch and go home.)\nSloppy quorums are particularly useful for increasing write availability: as long as any\nw nodes are available, the database can accept writes. However, this means that even\nwhen w + r > n, you cannot be sure to read the latest value for a key, because the\nlatest value may have been temporarily written to some nodes outside of n [47].\nThus, a sloppy quorum actually isn\u2019t a quorum at all in the traditional sense. It\u2019s only\nan assurance of durability, namely that the data is stored on w nodes somewhere.\nThere is no guarantee that a read of r nodes will see it until the hinted handoff has\ncompleted.\nSloppy quorums are optional in all common Dynamo implementations. In Riak they\nare enabled by default, and in Cassandra and Voldemort they are disabled by default\n[46, 49, 50].\nMulti-datacenter operation\nWe previously discussed cross-datacenter replication as a use case for multi-leader\nreplication (see \u201cMulti-Leader Replication\u201d on page 168). Leaderless replication is\nalso suitable for multi-datacenter operation, since it is designed to tolerate conflicting\nconcurrent writes, network interruptions, and latency spikes.\nCassandra and Voldemort implement their multi-datacenter support within the nor\u2010\nmal leaderless model: the number of replicas n includes nodes in all datacenters, and\nin the configuration you can specify how many of the n replicas you want to have in\neach datacenter. Each write from a client is sent to all replicas, regardless of datacen\u2010\nter, but the client usually only waits for acknowledgment from a quorum of nodes\nwithin its local datacenter so that it is unaffected by delays and interruptions on the\ncross-datacenter link. The higher-latency writes to other datacenters are often config\u2010\nured to happen asynchronously, although there is some flexibility in the configura\u2010\ntion [50, 51].\nRiak keeps all communication between clients and database nodes local to one data\u2010\ncenter, so n describes the number of replicas within one datacenter. Cross-datacenter\nreplication between database clusters happens asynchronously in the background, in\na style that is similar to multi-leader replication [52].\nDetecting Concurrent Writes\nDynamo-style databases allow several clients to concurrently write to the same key,\nwhich means that conflicts will occur even if strict quorums are used. The situation is\nsimilar to multi-leader replication (see \u201cHandling Write Conflicts\u201d on page 171),\nalthough in Dynamo-style databases conflicts can also arise during read repair or\nhinted handoff.\n184 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df2aaf5c-c017-4f00-809a-350b95f2122e": {"__data__": {"id_": "df2aaf5c-c017-4f00-809a-350b95f2122e", "embedding": null, "metadata": {"page_label": "185", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26bcc9c5-b2c5-439a-bcd5-59f0d7b6314e", "node_type": "4", "metadata": {"page_label": "185", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5242dfc8eb3ad7be1a4bfcd248bfe7d4e43ae1adc661b70ec07775c18825b023", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The problem is that events may arrive in a different order at different nodes, due to\nvariable network delays and partial failures. For example, Figure 5-12 shows two cli\u2010\nents, A and B, simultaneously writing to a key X in a three-node datastore:\n\u2022 Node 1 receives the write from A, but never receives the write from B due to a\ntransient outage.\n\u2022 Node 2 first receives the write from A, then the write from B.\n\u2022 Node 3 first receives the write from B, then the write from A.\nFigure 5-12. Concurrent writes in a Dynamo-style datastore: there is no well-defined\nordering.\nIf each node simply overwrote the value for a key whenever it received a write request\nfrom a client, the nodes would become permanently inconsistent, as shown by the\nfinal get request in Figure 5-12: node 2 thinks that the final value of X is B, whereas\nthe other nodes think that the value is A.\nIn order to become eventually consistent, the replicas should converge toward the\nsame value. How do they do that? One might hope that replicated databases would\nhandle this automatically, but unfortunately most implementations are quite poor: if\nyou want to avoid losing data, you\u2014the application developer\u2014need to know a lot\nabout the internals of your database\u2019s conflict handling.\nWe briefly touched on some techniques for conflict resolution in \u201cHandling Write\nConflicts\u201d on page 171. Before we wrap up this chapter, let\u2019s explore the issue in a bit\nmore detail.\nLeaderless Replication | 185", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1466, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0cdd17b2-cdf0-4c0f-b8af-8d185635be8f": {"__data__": {"id_": "0cdd17b2-cdf0-4c0f-b8af-8d185635be8f", "embedding": null, "metadata": {"page_label": "186", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dadd8d88-008b-4a35-b3ce-9c1d6a089e5c", "node_type": "4", "metadata": {"page_label": "186", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c43f073f9e9b2ef95daa3111ba495318768389499546f33f7383b4ac65417eb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Last write wins (discarding concurrent writes)\nOne approach for achieving eventual convergence is to declare that each replica need\nonly store the most \u201crecent\u201d value and allow \u201colder\u201d values to be overwritten and dis\u2010\ncarded. Then, as long as we have some way of unambiguously determining which\nwrite is more \u201crecent,\u201d and every write is eventually copied to every replica, the repli\u2010\ncas will eventually converge to the same value.\nAs indicated by the quotes around \u201crecent,\u201d this idea is actually quite misleading. In\nthe example of Figure 5-12, neither client knew about the other one when it sent its\nwrite requests to the database nodes, so it\u2019s not clear which one happened first. In\nfact, it doesn\u2019t really make sense to say that either happened \u201cfirst\u201d: we say the writes\nare concurrent, so their order is undefined.\nEven though the writes don\u2019t have a natural ordering, we can force an arbitrary order\non them. For example, we can attach a timestamp to each write, pick the biggest\ntimestamp as the most \u201crecent,\u201d and discard any writes with an earlier timestamp.\nThis conflict resolution algorithm, called last write wins  (LWW), is the only sup\u2010\nported conflict resolution method in Cassandra [ 53], and an optional feature in Riak\n[35].\nLWW achieves the goal of eventual convergence, but at the cost of durability: if there\nare several concurrent writes to the same key, even if they were all reported as suc\u2010\ncessful to the client (because they were written to w replicas), only one of the writes\nwill survive and the others will be silently discarded. Moreover, LWW may even drop\nwrites that are not concurrent, as we shall discuss in \u201cTimestamps for ordering\nevents\u201d on page 291.\nThere are some situations, such as caching, in which lost writes are perhaps accepta\u2010\nble. If losing data is not acceptable, LWW is a poor choice for conflict resolution.\nThe only safe way of using a database with LWW is to ensure that a key is only writ\u2010\nten once and thereafter treated as immutable, thus avoiding any concurrent updates\nto the same key. For example, a recommended way of using Cassandra is to use a\nUUID as the key, thus giving each write operation a unique key [53].\nThe \u201chappens-before\u201d relationship and concurrency\nHow do we decide whether two operations are concurrent or not? To develop an\nintuition, let\u2019s look at some examples:\n\u2022 In Figure 5-9 , the two writes are not concurrent: A\u2019s insert happens before  B\u2019s\nincrement, because the value incremented by B is the value inserted by A. In\nother words, B\u2019s operation builds upon A\u2019s operation, so B\u2019s operation must have\nhappened later. We also say that B is causally dependent on A.\n186 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d6de103-3582-4e1a-9d46-94ab401ddaf4": {"__data__": {"id_": "8d6de103-3582-4e1a-9d46-94ab401ddaf4", "embedding": null, "metadata": {"page_label": "187", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e23216a8-c2b3-421d-a992-d9008946d8e6", "node_type": "4", "metadata": {"page_label": "187", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d3c61f89eded8ae5500573f7953132818df2b9b9d5a49a6e8a38ab9202056035", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 On the other hand, the two writes in Figure 5-12 are concurrent: when each cli\u2010\nent starts the operation, it does not know that another client is also performing\nan operation on the same key. Thus, there is no causal dependency between the\noperations.\nAn operation A happens before another operation B if B knows about A, or depends\non A, or builds upon A in some way. Whether one operation happens before another\noperation is the key to defining what concurrency means. In fact, we can simply say\nthat two operations are concurrent if neither happens before the other (i.e., neither\nknows about the other) [54].\nThus, whenever you have two operations A and B, there are three possibilities: either\nA happened before B, or B happened before A, or A and B are concurrent. What we\nneed is an algorithm to tell us whether two operations are concurrent or not. If one\noperation happened before another, the later operation should overwrite the earlier\noperation, but if the operations are concurrent, we have a conflict that needs to be\nresolved.\nConcurrency, Time, and Relativity\nIt may seem that two operations should be called concurrent if they occur \u201cat the\nsame time\u201d\u2014but in fact, it is not important whether they literally overlap in time.\nBecause of problems with clocks in distributed systems, it is actually quite difficult to\ntell whether two things happened at exactly the same time\u2014an issue we will discuss\nin more detail in Chapter 8.\nFor defining concurrency, exact time doesn\u2019t matter: we simply call two operations\nconcurrent if they are both unaware of each other, regardless of the physical time at\nwhich they occurred. People sometimes make a connection between this principle\nand the special theory of relativity in physics [ 54], which introduced the idea that\ninformation cannot travel faster than the speed of light. Consequently, two events\nthat occur some distance apart cannot possibly affect each other if the time between\nthe events is shorter than the time it takes light to travel the distance between them.\nIn computer systems, two operations might be concurrent even though the speed of\nlight would in principle have allowed one operation to affect the other. For example,\nif the network was slow or interrupted at the time, two operations can occur some\ntime apart and still be concurrent, because the network problems prevented one\noperation from being able to know about the other.\nCapturing the happens-before relationship\nLet\u2019s look at an algorithm that determines whether two operations are concurrent, or\nwhether one happened before another. To keep things simple, let\u2019s start with a data\u2010\nLeaderless Replication | 187", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ffee36e-827f-4d1d-af4b-9cfeb18d25ff": {"__data__": {"id_": "3ffee36e-827f-4d1d-af4b-9cfeb18d25ff", "embedding": null, "metadata": {"page_label": "188", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f549920-ed69-436a-9b02-bef620480b9d", "node_type": "4", "metadata": {"page_label": "188", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9a68a4ea4f049d1eb761b07af2d88b5df74e00ed55d054c2fe9a3255ad89e8ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "base that has only one replica. Once we have worked out how to do this on a single\nreplica, we can generalize the approach to a leaderless database with multiple replicas.\nFigure 5-13 shows two clients concurrently adding items to the same shopping cart.\n(If that example strikes you as too inane, imagine instead two air traffic controllers\nconcurrently adding aircraft to the sector they are tracking.) Initially, the cart is\nempty. Between them, the clients make five writes to the database:\n1. Client 1 adds milk to the cart. This is the first write to that key, so the server suc\u2010\ncessfully stores it and assigns it version 1. The server also echoes the value back\nto the client, along with the version number.\n2. Client 2 adds eggs to the cart, not knowing that client 1 concurrently added milk\n(client 2 thought that its eggs were the only item in the cart). The server assigns\nversion 2 to this write, and stores eggs and milk as two separate values. It then\nreturns both values to the client, along with the version number of 2.\n3. Client 1, oblivious to client 2\u2019s write, wants to add flour to the cart, so it thinks\nthe current cart contents should be [milk, flour]. It sends this value to the\nserver, along with the version number 1 that the server gave client 1 previously.\nThe server can tell from the version number that the write of [milk, flour]\nsupersedes the prior value of [milk] but that it is concurrent with [eggs]. Thus,\nthe server assigns version 3 to [milk, flour], overwrites the version 1 value\n[milk], but keeps the version 2 value [eggs] and returns both remaining values\nto the client.\n4. Meanwhile, client 2 wants to add ham to the cart, unaware that client 1 just added\nflour. Client 2 received the two values [milk] and [eggs] from the server in the\nlast response, so the client now merges those values and adds ham to form a new\nvalue, [eggs, milk, ham]. It sends that value to the server, along with the previ\u2010\nous version number 2. The server detects that version 2 overwrites [eggs] but is\nconcurrent with [milk, flour], so the two remaining values are [milk, flour]\nwith version 3, and [eggs, milk, ham] with version 4.\n5. Finally, client 1 wants to add bacon. It previously received [milk, flour] and\n[eggs] from the server at version 3, so it merges those, adds bacon, and sends the\nfinal value [milk, flour, eggs, bacon]  to the server, along with the version\nnumber 3. This overwrites [milk, flour] (note that [eggs] was already over\u2010\nwritten in the last step) but is concurrent with [eggs, milk, ham], so the server\nkeeps those two concurrent values.\n188 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "764c594e-5898-4204-8d91-d03a863677c9": {"__data__": {"id_": "764c594e-5898-4204-8d91-d03a863677c9", "embedding": null, "metadata": {"page_label": "189", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "388cb89f-2a82-46df-a496-441d6df6e64c", "node_type": "4", "metadata": {"page_label": "189", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "15589700308a3436a771d05f98b57925d834e1482177b5c0466d889762008706", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 5-13. Capturing causal dependencies between two clients concurrently editing a\nshopping cart.\nThe dataflow between the operations in Figure 5-13  is illustrated graphically in\nFigure 5-14. The arrows indicate which operation happened before which other oper\u2010\nation, in the sense that the later operation knew about or depended on the earlier one.\nIn this example, the clients are never fully up to date with the data on the server, since\nthere is always another operation going on concurrently. But old versions of the value\ndo get overwritten eventually, and no writes are lost.\nFigure 5-14. Graph of causal dependencies in Figure 5-13.\nNote that the server can determine whether two operations are concurrent by looking\nat the version numbers\u2014it does not need to interpret the value itself (so the value\ncould be any data structure). The algorithm works as follows:\nLeaderless Replication | 189", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 903, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "147f5959-c780-4186-94c9-afa4d07738cb": {"__data__": {"id_": "147f5959-c780-4186-94c9-afa4d07738cb", "embedding": null, "metadata": {"page_label": "190", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c683c1c-e3e0-4220-932a-51301c1b07e4", "node_type": "4", "metadata": {"page_label": "190", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9d996668bddf88f0e355286a3cb998f92c19ace0c1c3405c62b588089f23f0ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The server maintains a version number for every key, increments the version\nnumber every time that key is written, and stores the new version number along\nwith the value written.\n\u2022 When a client reads a key, the server returns all values that have not been over\u2010\nwritten, as well as the latest version number. A client must read a key before\nwriting.\n\u2022 When a client writes a key, it must include the version number from the prior\nread, and it must merge together all values that it received in the prior read. (The\nresponse from a write request can be like a read, returning all current values,\nwhich allows us to chain several writes like in the shopping cart example.)\n\u2022 When the server receives a write with a particular version number, it can over\u2010\nwrite all values with that version number or below (since it knows that they have\nbeen merged into the new value), but it must keep all values with a higher ver\u2010\nsion number (because those values are concurrent with the incoming write).\nWhen a write includes the version number from a prior read, that tells us which pre\u2010\nvious state the write is based on. If you make a write without including a version\nnumber, it is concurrent with all other writes, so it will not overwrite anything\u2014it\nwill just be returned as one of the values on subsequent reads.\nMerging concurrently written values\nThis algorithm ensures that no data is silently dropped, but it unfortunately requires\nthat the clients do some extra work: if several operations happen concurrently, clients\nhave to clean up afterward by merging the concurrently written values. Riak calls\nthese concurrent values siblings.\nMerging sibling values is essentially the same problem as conflict resolution in multi-\nleader replication, which we discussed previously (see \u201cHandling Write Conflicts\u201d on\npage 171). A simple approach is to just pick one of the values based on a version\nnumber or timestamp (last write wins), but that implies losing data. So, you may\nneed to do something more intelligent in application code.\nWith the example of a shopping cart, a reasonable approach to merging siblings is to\njust take the union. In Figure 5-14, the two final siblings are [milk, flour, eggs,\nbacon] and [eggs, milk, ham] ; note that milk and eggs appear in both, even\nthough they were each only written once. The merged value might be something like\n[milk, flour, eggs, bacon, ham], without duplicates.\nHowever, if you want to allow people to also remove things from their carts, and not\njust add things, then taking the union of siblings may not yield the right result: if you\nmerge two sibling carts and an item has been removed in only one of them, then the\nremoved item will reappear in the union of the siblings [ 37]. To prevent this prob\u2010\n190 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c912a3d4-3a4e-4b91-b3d5-701f6386ebed": {"__data__": {"id_": "c912a3d4-3a4e-4b91-b3d5-701f6386ebed", "embedding": null, "metadata": {"page_label": "191", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5561d24-1aca-4b2b-b322-dd5ea7226001", "node_type": "4", "metadata": {"page_label": "191", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d4e2b364723164ef636ff7f79f452321d5e215724ccf72c0edf676860efa3d55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "lem, an item cannot simply be deleted from the database when it is removed; instead,\nthe system must leave a marker with an appropriate version number to indicate that\nthe item has been removed when merging siblings. Such a deletion marker is known\nas a tombstone. (We previously saw tombstones in the context of log compaction in\n\u201cHash Indexes\u201d on page 72.)\nAs merging siblings in application code is complex and error-prone, there are some\nefforts to design data structures that can perform this merging automatically, as dis\u2010\ncussed in \u201cAutomatic Conflict Resolution\u201d on page 174. For example, Riak\u2019s datatype\nsupport uses a family of data structures called CRDTs [ 38, 39, 55] that can automati\u2010\ncally merge siblings in sensible ways, including preserving deletions.\nVersion vectors\nThe example in Figure 5-13  used only a single replica. How does the algorithm\nchange when there are multiple replicas, but no leader?\nFigure 5-13  uses a single version number to capture dependencies between opera\u2010\ntions, but that is not sufficient when there are multiple replicas accepting writes con\u2010\ncurrently. Instead, we need to use a version number per replica  as well as per key.\nEach replica increments its own version number when processing a write, and also\nkeeps track of the version numbers it has seen from each of the other replicas. This\ninformation indicates which values to overwrite and which values to keep as siblings.\nThe collection of version numbers from all the replicas is called a version vector [56].\nA few variants of this idea are in use, but the most interesting is probably the dotted\nversion vector [57], which is used in Riak 2.0 [ 58, 59]. We won\u2019t go into the details,\nbut the way it works is quite similar to what we saw in our cart example.\nLike the version numbers in Figure 5-13, version vectors are sent from the database\nreplicas to clients when values are read, and need to be sent back to the database\nwhen a value is subsequently written. (Riak encodes the version vector as a string that\nit calls causal context.) The version vector allows the database to distinguish between\noverwrites and concurrent writes.\nAlso, like in the single-replica example, the application may need to merge siblings.\nThe version vector structure ensures that it is safe to read from one replica and subse\u2010\nquently write back to another replica. Doing so may result in siblings being created,\nbut no data is lost as long as siblings are merged correctly.\nVersion vectors and vector clocks\nA version vector is sometimes also called a vector clock, even though\nthey are not quite the same. The difference is subtle\u2014please see the\nreferences for details [ 57, 60, 61]. In brief, when comparing the\nstate of replicas, version vectors are the right data structure to use. \nLeaderless Replication | 191", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e32ae60-6dbc-4500-a4cb-838898df0327": {"__data__": {"id_": "0e32ae60-6dbc-4500-a4cb-838898df0327", "embedding": null, "metadata": {"page_label": "192", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79809051-c57e-4978-bfee-b8fc6ca852b1", "node_type": "4", "metadata": {"page_label": "192", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "063cdbd150822b50b57cb3c3887d9f901f50111e0a09e21d881a91af483a552f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Summary\nIn this chapter we looked at the issue of replication. Replication can serve several\npurposes:\nHigh availability\nKeeping the system running, even when one machine (or several machines, or an\nentire datacenter) goes down\nDisconnected operation\nAllowing an application to continue working when there is a network interrup\u2010\ntion\nLatency\nPlacing data geographically close to users, so that users can interact with it faster\nScalability\nBeing able to handle a higher volume of reads than a single machine could han\u2010\ndle, by performing reads on replicas\nDespite being a simple goal\u2014keeping a copy of the same data on several machines\u2014\nreplication turns out to be a remarkably tricky problem. It requires carefully thinking\nabout concurrency and about all the things that can go wrong, and dealing with the\nconsequences of those faults. At a minimum, we need to deal with unavailable nodes\nand network interruptions (and that\u2019s not even considering the more insidious kinds\nof fault, such as silent data corruption due to software bugs).\nWe discussed three main approaches to replication:\nSingle-leader replication\nClients send all writes to a single node (the leader), which sends a stream of data\nchange events to the other replicas (followers). Reads can be performed on any\nreplica, but reads from followers might be stale.\nMulti-leader replication\nClients send each write to one of several leader nodes, any of which can accept\nwrites. The leaders send streams of data change events to each other and to any\nfollower nodes.\nLeaderless replication\nClients send each write to several nodes, and read from several nodes in parallel\nin order to detect and correct nodes with stale data.\nEach approach has advantages and disadvantages. Single-leader replication is popular\nbecause it is fairly easy to understand and there is no conflict resolution to worry\nabout. Multi-leader and leaderless replication can be more robust in the presence of\n192 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1972, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5392ead7-bccc-443c-8969-14c5599cda9d": {"__data__": {"id_": "5392ead7-bccc-443c-8969-14c5599cda9d", "embedding": null, "metadata": {"page_label": "193", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d237f0d-f526-4cdd-a73f-52644e70bd0b", "node_type": "4", "metadata": {"page_label": "193", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ed448cb595690fb587d9e8ed5c0565a5e5e274a4b6b96079298d1b248a8649ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "faulty nodes, network interruptions, and latency spikes\u2014at the cost of being harder\nto reason about and providing only very weak consistency guarantees.\nReplication can be synchronous or asynchronous, which has a profound effect on the\nsystem behavior when there is a fault. Although asynchronous replication can be fast\nwhen the system is running smoothly, it\u2019s important to figure out what happens\nwhen replication lag increases and servers fail. If a leader fails and you promote an\nasynchronously updated follower to be the new leader, recently committed data may\nbe lost.\nWe looked at some strange effects that can be caused by replication lag, and we dis\u2010\ncussed a few consistency models which are helpful for deciding how an application\nshould behave under replication lag:\nRead-after-write consistency\nUsers should always see data that they submitted themselves.\nMonotonic reads\nAfter users have seen the data at one point in time, they shouldn\u2019t later see the\ndata from some earlier point in time.\nConsistent prefix reads\nUsers should see the data in a state that makes causal sense: for example, seeing a\nquestion and its reply in the correct order.\nFinally, we discussed the concurrency issues that are inherent in multi-leader and\nleaderless replication approaches: because they allow multiple writes to happen con\u2010\ncurrently, conflicts may occur. We examined an algorithm that a database might use\nto determine whether one operation happened before another, or whether they hap\u2010\npened concurrently. We also touched on methods for resolving conflicts by merging\ntogether concurrent updates.\nIn the next chapter we will continue looking at data that is distributed across multiple\nmachines, through the counterpart of replication: splitting a large dataset into parti\u2010\ntions. \nReferences\n[1] Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.: \u201c Notes on Dis\u2010\ntributed Databases,\u201d IBM Research, Research Report RJ2571(33471), July 1979.\n[2] \u201cOracle Active Data Guard Real-Time Data Protection and Availability ,\u201d Oracle\nWhite Paper, June 2013.\n[3] \u201cAlwaysOn Availability Groups,\u201d in SQL Server Books Online, Microsoft, 2012.\nSummary | 193", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2165, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f95ce866-380c-4f98-be2f-abb011bcf3e7": {"__data__": {"id_": "f95ce866-380c-4f98-be2f-abb011bcf3e7", "embedding": null, "metadata": {"page_label": "194", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2bd46fd-96a9-4435-bd28-c07e274925c1", "node_type": "4", "metadata": {"page_label": "194", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cecb67aa825768c28bfaa3224a8b027b476a0e8843d10ad94c6a0f57981ed2ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[4] Lin Qiao, Kapil Surlaker, Shirshanka Das, et al.: \u201c On Brewing Fresh Espresso:\nLinkedIn\u2019s Distributed Data Serving Platform ,\u201d at ACM International Conference on\nManagement of Data (SIGMOD), June 2013.\n[5] Jun Rao: \u201c Intra-Cluster Replication for Apache Kafka ,\u201d at ApacheCon North\nAmerica, February 2013.\n[6] \u201cHighly Available Queues,\u201d in RabbitMQ Server Documentation, Pivotal Software,\nInc., 2014.\n[7] Yoshinori Matsunobu: \u201c Semi-Synchronous Replication at Facebook ,\u201d yoshinori\u2010\nmatsunobu.blogspot.co.uk, April 1, 2014.\n[8] Robbert van Renesse and Fred B. Schneider: \u201c Chain Replication for Supporting\nHigh Throughput and Availability ,\u201d at 6th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), December 2004.\n[9] Jeff Terrace and Michael J. Freedman: \u201c Object Storage on CRAQ: High-\nThroughput Chain Replication for Read-Mostly Workloads ,\u201d at USENIX Annual\nTechnical Conference (ATC), June 2009.\n[10] Brad Calder, Ju Wang, Aaron Ogus, et al.: \u201c Windows Azure Storage: A Highly\nAvailable Cloud Storage Service with Strong Consistency ,\u201d at 23rd ACM Symposium\non Operating Systems Principles (SOSP), October 2011.\n[11] Andrew Wang: \u201cWindows Azure Storage,\u201d umbrant.com, February 4, 2016.\n[12] \u201cPercona Xtrabackup - Documentation,\u201d Percona LLC, 2014.\n[13] Jesse Newland: \u201c GitHub Availability This Week ,\u201d github.com, September 14,\n2012.\n[14] Mark Imbriaco: \u201cDowntime Last Saturday,\u201d github.com, December 26, 2012.\n[15] John Hugg: \u201c \u2018All in\u2019 with Determinism for Performance and Testing in Dis\u2010\ntributed Systems,\u201d at Strange Loop, September 2015.\n[16] Amit Kapila: \u201c WAL Internals of PostgreSQL ,\u201d at PostgreSQL Conference\n(PGCon), May 2012.\n[17] MySQL Internals Manual. Oracle, 2014.\n[18] Yogeshwer Sharma, Philippe Ajoux, Petchean Ang, et al.: \u201c Wormhole: Reliable\nPub-Sub to Support Geo-Replicated Internet Services ,\u201d at 12th USENIX Symposium\non Networked Systems Design and Implementation (NSDI), May 2015.\n[19] \u201cOracle GoldenGate 12c: Real-Time Access to Real-Time Information ,\u201d Oracle\nWhite Paper, October 2013.\n[20] Shirshanka Das, Chavdar Botev, Kapil Surlaker, et al.: \u201c All Aboard the Data\u2010\nbus!,\u201d at ACM Symposium on Cloud Computing (SoCC), October 2012.\n194 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "88d8b002-84a7-4dc6-82ab-73c1837b6d86": {"__data__": {"id_": "88d8b002-84a7-4dc6-82ab-73c1837b6d86", "embedding": null, "metadata": {"page_label": "195", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "017d6d9c-ed45-4169-95a5-59ccd6a35de0", "node_type": "4", "metadata": {"page_label": "195", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0c9cd7c1a03c37aead3c1932d6d6bc036790e70e474b7b0915e0aefeb6ee92c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[21] Greg Sabino Mullane: \u201c Version 5 of Bucardo Database Replication System ,\u201d\nblog.endpoint.com, June 23, 2014.\n[22] Werner Vogels: \u201c Eventually Consistent ,\u201d ACM Queue , volume 6, number 6,\npages 14\u201319, October 2008. doi:10.1145/1466443.1466448\n[23] Douglas B. Terry: \u201c Replicated Data Consistency Explained Through Baseball ,\u201d\nMicrosoft Research, Technical Report MSR-TR-2011-137, October 2011.\n[24] Douglas B. Terry, Alan J. Demers, Karin Petersen, et al.: \u201c Session Guarantees for\nWeakly Consistent Replicated Data ,\u201d at 3rd International Conference on Parallel and\nDistributed Information Systems  (PDIS), September 1994. doi:10.1109/PDIS.\n1994.331722\n[25] Terry Pratchett: Reaper Man: A Discworld Novel . Victor Gollancz, 1991. ISBN:\n978-0-575-04979-6\n[26] \u201cTungsten Replicator,\u201d Continuent, Inc., 2014.\n[27] \u201c BDR 0.10.0 Documentation ,\u201d The PostgreSQL Global Development Group,\nbdr-project.org, 2015.\n[28] Robert Hodges: \u201c If You *Must* Deploy Multi-Master Replication, Read This\nFirst,\u201d scale-out-blog.blogspot.co.uk, March 30, 2012.\n[29] J. Chris Anderson, Jan Lehnardt, and Noah Slater: CouchDB: The Definitive\nGuide. O\u2019Reilly Media, 2010. ISBN: 978-0-596-15589-6\n[30] AppJet, Inc.: \u201cEtherpad and EasySync Technical Manual,\u201d github.com, March 26,\n2011.\n[31] John Day-Richter: \u201c What\u2019s Different About the New Google Docs: Making Col\u2010\nlaboration Fast,\u201d googledrive.blogspot.com, 23 September 2010.\n[32] Martin Kleppmann and Alastair R. Beresford: \u201c A Conflict-Free Replicated JSON\nDatatype,\u201d arXiv:1608.03960, August 13, 2016.\n[33] Frazer Clement: \u201c Eventual Consistency \u2013 Detecting Conflicts ,\u201d messagepass\u2010\ning.blogspot.co.uk, October 20, 2011.\n[34] Robert Hodges: \u201c State of the Art for MySQL Multi-Master Replication ,\u201d at Per\u2010\ncona Live: MySQL Conference & Expo, April 2013.\n[35] John Daily: \u201c Clocks Are Bad, or, Welcome to the Wonderful World of Dis\u2010\ntributed Systems,\u201d basho.com, November 12, 2013.\n[36] Riley Berton: \u201c Is Bi-Directional Replication (BDR) in Postgres Transactional? ,\u201d\nsdf.org, January 4, 2016.\nSummary | 195", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7dcc5b5f-efa4-4281-b101-f3f4b0f9c4d3": {"__data__": {"id_": "7dcc5b5f-efa4-4281-b101-f3f4b0f9c4d3", "embedding": null, "metadata": {"page_label": "196", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1aff4c98-de25-4051-800d-7b57d2ae66ab", "node_type": "4", "metadata": {"page_label": "196", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2176bb1e5609d2d0c6954845f44c87d4475c40dbff467cf6eb0ad99705d7e1ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[37] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: \u201c Dynamo: Ama\u2010\nzon\u2019s Highly Available Key-Value Store ,\u201d at 21st ACM Symposium on Operating Sys\u2010\ntems Principles (SOSP), October 2007.\n[38] Marc Shapiro, Nuno Pregui\u00e7a, Carlos Baquero, and Marek Zawirski: \u201c A Com\u2010\nprehensive Study of Convergent and Commutative Replicated Data Types ,\u201d INRIA\nResearch Report no. 7506, January 2011.\n[39] Sam Elliott: \u201c CRDTs: An UPDATE (or Maybe Just a PUT) ,\u201d at RICON West ,\nOctober 2013.\n[40] Russell Brown: \u201c A Bluffers Guide to CRDTs in Riak ,\u201d gist.github.com, October\n28, 2013.\n[41] Benjamin Farinier, Thomas Gazagnaire, and Anil Madhavapeddy: \u201c Mergeable\nPersistent Data Structures ,\u201d at 26es Journ\u00e9es Francophones des Langages Applicatifs\n(JFLA), January 2015.\n[42] Chengzheng Sun and Clarence Ellis: \u201c Operational Transformation in Real-Time\nGroup Editors: Issues, Algorithms, and Achievements ,\u201d at ACM Conference on Com\u2010\nputer Supported Cooperative Work (CSCW), November 1998.\n[43] Lars Hofhansl: \u201c HBASE-7709: Infinite Loop Possible in Master/Master Replica\u2010\ntion,\u201d issues.apache.org, January 29, 2013.\n[44] David K. Gifford: \u201c Weighted Voting for Replicated Data ,\u201d at 7th ACM Sympo\u2010\nsium on Operating Systems Principles  (SOSP), December 1979. doi:\n10.1145/800215.806583\n[45] Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman: \u201cFlexible Paxos: Quo\u2010\nrum Intersection Revisited,\u201d arXiv:1608.06696, August 24, 2016.\n[46] Joseph Blomstedt: \u201c Re: Absolute Consistency ,\u201d email to riak-users mailing list,\nlists.basho.com, January 11, 2012.\n[47] Joseph Blomstedt: \u201c Bringing Consistency to Riak ,\u201d at RICON West , October\n2012.\n[48] Peter Bailis, Shivaram Venkataraman, Michael J. Franklin, et al.: \u201c Quantifying\nEventual Consistency with PBS,\u201d Communications of the ACM, volume 57, number 8,\npages 93\u2013102, August 2014. doi:10.1145/2632792\n[49] Jonathan Ellis: \u201cModern Hinted Handoff,\u201d datastax.com, December 11, 2012.\n[50] \u201cProject Voldemort Wiki,\u201d github.com, 2013.\n[51] \u201cApache Cassandra 2.0 Documentation,\u201d DataStax, Inc., 2014.\n[52] \u201c Riak Enterprise: Multi-Datacenter Replication .\u201d Technical whitepaper, Basho\nTechnologies, Inc., September 2014.\n196 | Chapter 5: Replication", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "006f11ba-5d3e-4943-8360-5998e1e44070": {"__data__": {"id_": "006f11ba-5d3e-4943-8360-5998e1e44070", "embedding": null, "metadata": {"page_label": "197", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e514d2da-7c09-41be-b0a7-46492578fed4", "node_type": "4", "metadata": {"page_label": "197", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "57f57f4cd35d6b3d10094525e1ce5220aee92eedd80a03415b82e027544c266e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[53] Jonathan Ellis: \u201c Why Cassandra Doesn\u2019t Need Vector Clocks ,\u201d datastax.com,\nSeptember 2, 2013.\n[54] Leslie Lamport: \u201cTime, Clocks, and the Ordering of Events in a Distributed Sys\u2010\ntem,\u201d Communications of the ACM , volume 21, number 7, pages 558\u2013565, July 1978.\ndoi:10.1145/359545.359563\n[55] Joel Jacobson: \u201cRiak 2.0: Data Types,\u201d blog.joeljacobson.com, March 23, 2014.\n[56] D. Stott Parker Jr., Gerald J. Popek, Gerard Rudisin, et al.: \u201c Detection of Mutual\nInconsistency in Distributed Systems ,\u201d IEEE Transactions on Software Engineering ,\nvolume 9, number 3, pages 240\u2013247, May 1983. doi:10.1109/TSE.1983.236733\n[57] Nuno Pregui\u00e7a, Carlos Baquero, Paulo S\u00e9rgio Almeida, et al.: \u201c Dotted Version\nVectors: Logical Clocks for Optimistic Replication ,\u201d arXiv:1011.5808, November 26,\n2010.\n[58] Sean Cribbs: \u201cA Brief History of Time in Riak,\u201d at RICON, October 2014.\n[59] Russell Brown: \u201c Vector Clocks Revisited Part 2: Dotted Version Vectors ,\u201d\nbasho.com, November 10, 2015.\n[60] Carlos Baquero: \u201c Version Vectors Are Not Vector Clocks ,\u201d haslab.word\u2010\npress.com, July 8, 2011.\n[61] Reinhard Schwarz and Friedemann Mattern: \u201c Detecting Causal Relationships in\nDistributed Computations: In Search of the Holy Grail ,\u201d Distributed Computing, vol\u2010\nume 7, number 3, pages 149\u2013174, March 1994. doi:10.1007/BF02277859\nSummary | 197", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4696ebd6-50ec-4f02-9aa4-bfac0f275a0e": {"__data__": {"id_": "4696ebd6-50ec-4f02-9aa4-bfac0f275a0e", "embedding": null, "metadata": {"page_label": "198", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c6aad94f-5348-48cd-b55f-99dca572b39e", "node_type": "4", "metadata": {"page_label": "198", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55d98d56-baf8-4f8a-a8e7-fa5399ad62fd": {"__data__": {"id_": "55d98d56-baf8-4f8a-a8e7-fa5399ad62fd", "embedding": null, "metadata": {"page_label": "199", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3ec671b-22ae-47e2-bb4b-fda3fc2fcfe0", "node_type": "4", "metadata": {"page_label": "199", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "45cd4ab69aeb1e081f473c305a3c706b2549c57930868cf3affb7ea73caa74c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. Partitioning, as discussed in this chapter, is a way of intentionally breaking a large database down into\nsmaller ones. It has nothing to do with network partitions (netsplits), a type of fault in the network between\nnodes. We will discuss such faults in Chapter 8.\nCHAPTER 6\nPartitioning\nClearly, we must break away from the sequential and not limit the computers. We must\nstate definitions and provide for priorities and descriptions of data. We must state relation\u2010\nships, not procedures.\n\u2014Grace Murray Hopper, Management and the Computer of the Future (1962)\nIn Chapter 5  we discussed replication\u2014that is, having multiple copies of the same\ndata on different nodes. For very large datasets, or very high query throughput, that is\nnot sufficient: we need to break the data up into partitions, also known as sharding.i\nTerminological confusion\nWhat we call a partition here is called a shard in MongoDB, Elas\u2010\nticsearch, and SolrCloud; it\u2019s known as a region in HBase, a tablet\nin Bigtable, a vnode in Cassandra and Riak, and a vBucket in\nCouchbase. However, partitioning is the most established term, so\nwe\u2019ll stick with that.\nNormally, partitions are defined in such a way that each piece of data (each record,\nrow, or document) belongs to exactly one partition. There are various ways of achiev\u2010\ning this, which we discuss in depth in this chapter. In effect, each partition is a small\ndatabase of its own, although the database may support operations that touch multi\u2010\nple partitions at the same time.\nThe main reason for wanting to partition data is scalability. Different partitions can\nbe placed on different nodes in a shared-nothing cluster (see the introduction to\n199", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3edc449f-3b74-4248-9c91-942de2c19985": {"__data__": {"id_": "3edc449f-3b74-4248-9c91-942de2c19985", "embedding": null, "metadata": {"page_label": "200", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d55a1f2-1d5b-467e-92ec-332f0adc5247", "node_type": "4", "metadata": {"page_label": "200", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e41e4c5cce07e8f64b49d994573c80151699e57fe01e631f48a72546ec757439", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Part II  for a definition of shared nothing ). Thus, a large dataset can be distributed\nacross many disks, and the query load can be distributed across many processors.\nFor queries that operate on a single partition, each node can independently execute\nthe queries for its own partition, so query throughput can be scaled by adding more\nnodes. Large, complex queries can potentially be parallelized across many nodes,\nalthough this gets significantly harder.\nPartitioned databases were pioneered in the 1980s by products such as Teradata and\nTandem NonStop SQL [1], and more recently rediscovered by NoSQL databases and\nHadoop-based data warehouses. Some systems are designed for transactional work\u2010\nloads, and others for analytics (see \u201cTransaction Processing or Analytics?\u201d on page\n90): this difference affects how the system is tuned, but the fundamentals of partition\u2010\ning apply to both kinds of workloads.\nIn this chapter we will first look at different approaches for partitioning large datasets\nand observe how the indexing of data interacts with partitioning. We\u2019ll then talk\nabout rebalancing, which is necessary if you want to add or remove nodes in your\ncluster. Finally, we\u2019ll get an overview of how databases route requests to the right par\u2010\ntitions and execute queries.\nPartitioning and Replication\nPartitioning is usually combined with replication so that copies of each partition are\nstored on multiple nodes. This means that, even though each record belongs to\nexactly one partition, it may still be stored on several different nodes for fault toler\u2010\nance.\nA node may store more than one partition. If a leader\u2013follower replication model is\nused, the combination of partitioning and replication can look like Figure 6-1. Each\npartition\u2019s leader is assigned to one node, and its followers are assigned to other\nnodes. Each node may be the leader for some partitions and a follower for other par\u2010\ntitions.\nEverything we discussed in Chapter 5 about replication of databases applies equally\nto replication of partitions. The choice of partitioning scheme is mostly independent\nof the choice of replication scheme, so we will keep things simple and ignore replica\u2010\ntion in this chapter.\n200 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65cf5177-e10b-4eef-b04a-e8b698530545": {"__data__": {"id_": "65cf5177-e10b-4eef-b04a-e8b698530545", "embedding": null, "metadata": {"page_label": "201", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f372b5ec-85ea-449c-9e78-4f9d9ed6f908", "node_type": "4", "metadata": {"page_label": "201", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "16210ec63078e56e0a54daf62fa393f2581fbe4df9496e5376f76dc64809787a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6-1. Combining replication and partitioning: each node acts as leader for some\npartitions and follower for other partitions.\nPartitioning of Key-Value Data\nSay you have a large amount of data, and you want to partition it. How do you decide\nwhich records to store on which nodes?\nOur goal with partitioning is to spread the data and the query load evenly across\nnodes. If every node takes a fair share, then\u2014in theory\u201410 nodes should be able to\nhandle 10 times as much data and 10 times the read and write throughput of a single\nnode (ignoring replication for now).\nIf the partitioning is unfair, so that some partitions have more data or queries than\nothers, we call it skewed. The presence of skew makes partitioning much less effective.\nIn an extreme case, all the load could end up on one partition, so 9 out of 10 nodes\nare idle and your bottleneck is the single busy node. A partition with disproportion\u2010\nately high load is called a hot spot.\nThe simplest approach for avoiding hot spots would be to assign records to nodes\nrandomly. That would distribute the data quite evenly across the nodes, but it has a\nbig disadvantage: when you\u2019re trying to read a particular item, you have no way of\nknowing which node it is on, so you have to query all nodes in parallel.\nWe can do better. Let\u2019s assume for now that you have a simple key-value data model,\nin which you always access a record by its primary key. For example, in an old-\nfashioned paper encyclopedia, you look up an entry by its title; since all the entries\nare alphabetically sorted by title, you can quickly find the one you\u2019re looking for.\nPartitioning of Key-Value Data | 201", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2113153b-ccac-4a32-a2b6-1ab608e64121": {"__data__": {"id_": "2113153b-ccac-4a32-a2b6-1ab608e64121", "embedding": null, "metadata": {"page_label": "202", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bce9237-fefe-40d4-a719-c3f2e335cbf8", "node_type": "4", "metadata": {"page_label": "202", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4551b13d24c9cd1cd92c8e4cf561417ea64795a20ddd70ce54c20a62db9d2ef6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Partitioning by Key Range\nOne way of partitioning is to assign a continuous range of keys (from some mini\u2010\nmum to some maximum) to each partition, like the volumes of a paper encyclopedia\n(Figure 6-2). If you know the boundaries between the ranges, you can easily deter\u2010\nmine which partition contains a given key. If you also know which partition is\nassigned to which node, then you can make your request directly to the appropriate\nnode (or, in the case of the encyclopedia, pick the correct book off the shelf).\nFigure 6-2. A print encyclopedia is partitioned by key range.\nThe ranges of keys are not necessarily evenly spaced, because your data may not be\nevenly distributed. For example, in Figure 6-2, volume 1 contains words starting with\nA and B, but volume 12 contains words starting with T, U, V, X, Y, and Z. Simply\nhaving one volume per two letters of the alphabet would lead to some volumes being\nmuch bigger than others. In order to distribute the data evenly, the partition bound\u2010\naries need to adapt to the data.\nThe partition boundaries might be chosen manually by an administrator, or the data\u2010\nbase can choose them automatically (we will discuss choices of partition boundaries\nin more detail in \u201cRebalancing Partitions\u201d on page 209). This partitioning strategy is\nused by Bigtable, its open source equivalent HBase [ 2, 3], RethinkDB, and MongoDB\nbefore version 2.4 [4].\nWithin each partition, we can keep keys in sorted order (see \u201cSSTables and LSM-\nTrees\u201d on page 76). This has the advantage that range scans are easy, and you can\ntreat the key as a concatenated index in order to fetch several related records in one\nquery (see \u201cMulti-column indexes\u201d on page 87). For example, consider an application\nthat stores data from a network of sensors, where the key is the timestamp of the\nmeasurement (year-month-day-hour-minute-second). Range scans are very useful in\nthis case, because they let you easily fetch, say, all the readings from a particular\nmonth.\n202 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83e9b5ba-d9ee-416e-9033-823f6e7f1772": {"__data__": {"id_": "83e9b5ba-d9ee-416e-9033-823f6e7f1772", "embedding": null, "metadata": {"page_label": "203", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1818347e-aa20-4baa-a8b6-c64d667cc55e", "node_type": "4", "metadata": {"page_label": "203", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "81f99ca9b03c1fc8e62e8ef92ec9ea36b0bc7ba6320402bb0eaecb4e2f331794", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the downside of key range partitioning is that certain access patterns can\nlead to hot spots. If the key is a timestamp, then the partitions correspond to ranges\nof time\u2014e.g., one partition per day. Unfortunately, because we write data from the\nsensors to the database as the measurements happen, all the writes end up going to\nthe same partition (the one for today), so that partition can be overloaded with writes\nwhile others sit idle [5].\nTo avoid this problem in the sensor database, you need to use something other than\nthe timestamp as the first element of the key. For example, you could prefix each\ntimestamp with the sensor name so that the partitioning is first by sensor name and\nthen by time. Assuming you have many sensors active at the same time, the write\nload will end up more evenly spread across the partitions. Now, when you want to\nfetch the values of multiple sensors within a time range, you need to perform a sepa\u2010\nrate range query for each sensor name.\nPartitioning by Hash of Key\nBecause of this risk of skew and hot spots, many distributed datastores use a hash\nfunction to determine the partition for a given key.\nA good hash function takes skewed data and makes it uniformly distributed. Say you\nhave a 32-bit hash function that takes a string. Whenever you give it a new string, it\nreturns a seemingly random number between 0 and 2 32 \u2212 1. Even if the input strings\nare very similar, their hashes are evenly distributed across that range of numbers.\nFor partitioning purposes, the hash function need not be cryptographically strong:\nfor example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler\u2013\nNoll\u2013Vo function. Many programming languages have simple hash functions built in\n(as they are used for hash tables), but they may not be suitable for partitioning: for\nexample, in Java\u2019s Object.hashCode() and Ruby\u2019s Object#hash, the same key may\nhave a different hash value in different processes [6].\nOnce you have a suitable hash function for keys, you can assign each partition a\nrange of hashes (rather than a range of keys), and every key whose hash falls within a\npartition\u2019s range will be stored in that partition. This is illustrated in Figure 6-3.\nPartitioning of Key-Value Data | 203", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dc37a75-3b4a-4252-ae20-bafaae4d5922": {"__data__": {"id_": "3dc37a75-3b4a-4252-ae20-bafaae4d5922", "embedding": null, "metadata": {"page_label": "204", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3cf5a39-ad8a-4105-abd2-261a9d97db68", "node_type": "4", "metadata": {"page_label": "204", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3903dd68455b00644b3e1a8b96d218a18f0f4384ccf92babec6a820eed6f73f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6-3. Partitioning by hash of key.\nThis technique is good at distributing keys fairly among the partitions. The partition\nboundaries can be evenly spaced, or they can be chosen pseudorandomly (in which\ncase the technique is sometimes known as consistent hashing).\nConsistent Hashing\nConsistent hashing, as defined by Karger et al. [7], is a way of evenly distributing load\nacross an internet-wide system of caches such as a content delivery network (CDN).\nIt uses randomly chosen partition boundaries to avoid the need for central control or\ndistributed consensus. Note that consistent here has nothing to do with replica consis\u2010\ntency (see Chapter 5 ) or ACID consistency (see Chapter 7 ), but rather describes a\nparticular approach to rebalancing.\nAs we shall see in \u201cRebalancing Partitions\u201d on page 209, this particular approach\nactually doesn\u2019t work very well for databases [ 8], so it is rarely used in practice (the\ndocumentation of some databases still refers to consistent hashing, but it is often\ninaccurate). Because this is so confusing, it\u2019s best to avoid the term consistent hashing\nand just call it hash partitioning instead.\nUnfortunately however, by using the hash of the key for partitioning we lose a nice\nproperty of key-range partitioning: the ability to do efficient range queries. Keys that\nwere once adjacent are now scattered across all the partitions, so their sort order is\nlost. In MongoDB, if you have enabled hash-based sharding mode, any range query\nhas to be sent to all partitions [ 4]. Range queries on the primary key are not sup\u2010\nported by Riak [9], Couchbase [10], or Voldemort. \nCassandra achieves a compromise between the two partitioning strategies [ 11, 12,\n13]. A table in Cassandra can be declared with a compound primary key consisting of\nseveral columns. Only the first part of that key is hashed to determine the partition,\nbut the other columns are used as a concatenated index for sorting the data in Cas\u2010\nsandra\u2019s SSTables. A query therefore cannot search for a range of values within the\n204 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5186f191-47f2-472d-94ef-6795f19b7e3c": {"__data__": {"id_": "5186f191-47f2-472d-94ef-6795f19b7e3c", "embedding": null, "metadata": {"page_label": "205", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c384dbd-602b-40bb-b715-e2917af954e6", "node_type": "4", "metadata": {"page_label": "205", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2e588dd6d40494cc125feae25112c59ca83f9c52faf38ad96d0e32687ecce139", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "first column of a compound key, but if it specifies a fixed value for the first column, it\ncan perform an efficient range scan over the other columns of the key. \nThe concatenated index approach enables an elegant data model for one-to-many\nrelationships. For example, on a social media site, one user may post many updates. If\nthe primary key for updates is chosen to be (user_id, update_timestamp), then you\ncan efficiently retrieve all updates made by a particular user within some time inter\u2010\nval, sorted by timestamp. Different users may be stored on different partitions, but\nwithin each user, the updates are stored ordered by timestamp on a single partition. \nSkewed Workloads and Relieving Hot Spots\nAs discussed, hashing a key to determine its partition can help reduce hot spots.\nHowever, it can\u2019t avoid them entirely: in the extreme case where all reads and writes\nare for the same key, you still end up with all requests being routed to the same parti\u2010\ntion.\nThis kind of workload is perhaps unusual, but not unheard of: for example, on a\nsocial media site, a celebrity user with millions of followers may cause a storm of\nactivity when they do something [ 14]. This event can result in a large volume of\nwrites to the same key (where the key is perhaps the user ID of the celebrity, or the ID\nof the action that people are commenting on). Hashing the key doesn\u2019t help, as the\nhash of two identical IDs is still the same.\nToday, most data systems are not able to automatically compensate for such a highly\nskewed workload, so it\u2019s the responsibility of the application to reduce the skew. For\nexample, if one key is known to be very hot, a simple technique is to add a random\nnumber to the beginning or end of the key. Just a two-digit decimal random number\nwould split the writes to the key evenly across 100 different keys, allowing those keys\nto be distributed to different partitions.\nHowever, having split the writes across different keys, any reads now have to do addi\u2010\ntional work, as they have to read the data from all 100 keys and combine it. This tech\u2010\nnique also requires additional bookkeeping: it only makes sense to append the\nrandom number for the small number of hot keys; for the vast majority of keys with\nlow write throughput this would be unnecessary overhead. Thus, you also need some\nway of keeping track of which keys are being split.\nPerhaps in the future, data systems will be able to automatically detect and compen\u2010\nsate for skewed workloads; but for now, you need to think through the trade-offs for\nyour own application. \nPartitioning of Key-Value Data | 205", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15bd0a88-87eb-4559-8c40-ef0c0f6b540d": {"__data__": {"id_": "15bd0a88-87eb-4559-8c40-ef0c0f6b540d", "embedding": null, "metadata": {"page_label": "206", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d641463c-1bef-4129-a1d2-fcca7062f840", "node_type": "4", "metadata": {"page_label": "206", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "40bd9cd110ad4fcfda91d449f8fb11e677d505159a16ae8b4723377a743b2081", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. If your database only supports a key-value model, you might be tempted to implement a secondary index\nyourself by creating a mapping from values to document IDs in application code. If you go down this route,\nyou need to take great care to ensure your indexes remain consistent with the underlying data. Race condi\u2010\ntions and intermittent write failures (where some changes were saved but others weren\u2019t) can very easily cause\nthe data to go out of sync\u2014see \u201cThe need for multi-object transactions\u201d on page 231.\nPartitioning and Secondary Indexes\nThe partitioning schemes we have discussed so far rely on a key-value data model. If\nrecords are only ever accessed via their primary key, we can determine the partition\nfrom that key and use it to route read and write requests to the partition responsible\nfor that key.\nThe situation becomes more complicated if secondary indexes are involved (see also\n\u201cOther Indexing Structures\u201d on page 85). A secondary index usually doesn\u2019t identify\na record uniquely but rather is a way of searching for occurrences of a particular\nvalue: find all actions by user 123, find all articles containing the word hogwash, find\nall cars whose color is red, and so on.\nSecondary indexes are the bread and butter of relational databases, and they are com\u2010\nmon in document databases too. Many key-value stores (such as HBase and Volde\u2010\nmort) have avoided secondary indexes because of their added implementation\ncomplexity, but some (such as Riak) have started adding them because they are so\nuseful for data modeling. And finally, secondary indexes are the raison d\u2019\u00eatre  of\nsearch servers such as Solr and Elasticsearch.\nThe problem with secondary indexes is that they don\u2019t map neatly to partitions.\nThere are two main approaches to partitioning a database with secondary indexes:\ndocument-based partitioning and term-based partitioning.\nPartitioning Secondary Indexes by Document\nFor example, imagine you are operating a website for selling used cars (illustrated in\nFigure 6-4). Each listing has a unique ID\u2014call it the document ID\u2014and you partition\nthe database by the document ID (for example, IDs 0 to 499 in partition 0, IDs 500 to\n999 in partition 1, etc.).\nYou want to let users search for cars, allowing them to filter by color and by make, so\nyou need a secondary index on color and make (in a document database these would\nbe fields; in a relational database they would be columns). If you have declared the\nindex, the database can perform the indexing automatically. ii For example, whenever\na red car is added to the database, the database partition automatically adds it to the\nlist of document IDs for the index entry color:red.\n206 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9105c188-1b32-4f20-ad2c-c9c006c8bc54": {"__data__": {"id_": "9105c188-1b32-4f20-ad2c-c9c006c8bc54", "embedding": null, "metadata": {"page_label": "207", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3d6b3e9-d57e-47e8-b6ca-d763851c7509", "node_type": "4", "metadata": {"page_label": "207", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3241c79f50d953773629bbb8c130a8bcf094ce59a728f49f80f970c906c03bac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6-4. Partitioning secondary indexes by document.\nIn this indexing approach, each partition is completely separate: each partition main\u2010\ntains its own secondary indexes, covering only the documents in that partition. It\ndoesn\u2019t care what data is stored in other partitions. Whenever you need to write to\nthe database\u2014to add, remove, or update a document\u2014you only need to deal with the\npartition that contains the document ID that you are writing. For that reason, a\ndocument-partitioned index is also known as a local index  (as opposed to a global\nindex, described in the next section).\nHowever, reading from a document-partitioned index requires care: unless you have\ndone something special with the document IDs, there is no reason why all the cars\nwith a particular color or a particular make would be in the same partition. In\nFigure 6-4, red cars appear in both partition 0 and partition 1. Thus, if you want to\nsearch for red cars, you need to send the query to all partitions, and combine all the\nresults you get back.\nThis approach to querying a partitioned database is sometimes known as scatter/\ngather, and it can make read queries on secondary indexes quite expensive. Even if\nyou query the partitions in parallel, scatter/gather is prone to tail latency amplifica\u2010\ntion (see \u201cPercentiles in Practice\u201d on page 16). Nevertheless, it is widely used: Mon\u2010\ngoDB, Riak [15], Cassandra [16], Elasticsearch [17], SolrCloud [18], and VoltDB [19]\nall use document-partitioned secondary indexes. Most database vendors recommend\nthat you structure your partitioning scheme so that secondary index queries can be\nserved from a single partition, but that is not always possible, especially when you\u2019re\nusing multiple secondary indexes in a single query (such as filtering cars by color and\nby make at the same time).\nPartitioning and Secondary Indexes | 207", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "affee9d9-3086-4424-a1f8-29050ac1aec7": {"__data__": {"id_": "affee9d9-3086-4424-a1f8-29050ac1aec7", "embedding": null, "metadata": {"page_label": "208", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ae2a488-63eb-424d-a1bb-1bb5fc66abb8", "node_type": "4", "metadata": {"page_label": "208", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7451add99048f19c335e6a1b0be53c26b27bfd6fc99ac63b1f9caefa2db389f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6-5. Partitioning secondary indexes by term.\nPartitioning Secondary Indexes by Term\nRather than each partition having its own secondary index (a local index ), we can\nconstruct a global index that covers data in all partitions. However, we can\u2019t just store\nthat index on one node, since it would likely become a bottleneck and defeat the pur\u2010\npose of partitioning. A global index must also be partitioned, but it can be partitioned\ndifferently from the primary key index.\nFigure 6-5  illustrates what this could look like: red cars from all partitions appear\nunder color:red in the index, but the index is partitioned so that colors starting with\nthe letters a to r appear in partition 0 and colors starting with s to z appear in parti\u2010\ntion 1. The index on the make of car is partitioned similarly (with the partition\nboundary being between f and h).\nWe call this kind of index term-partitioned, because the term we\u2019re looking for deter\u2010\nmines the partition of the index. Here, a term would be color:red, for example. The\nname term comes from full-text indexes (a particular kind of secondary index), where\nthe terms are all the words that occur in a document.\nAs before, we can partition the index by the term itself, or using a hash of the term.\nPartitioning by the term itself can be useful for range scans (e.g., on a numeric prop\u2010\nerty, such as the asking price of the car), whereas partitioning on a hash of the term\ngives a more even distribution of load.\nThe advantage of a global (term-partitioned) index over a document-partitioned\nindex is that it can make reads more efficient: rather than doing scatter/gather over\nall partitions, a client only needs to make a request to the partition containing the\nterm that it wants. However, the downside of a global index is that writes are slower\nand more complicated, because a write to a single document may now affect multiple\n208 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22070ba0-eb69-4e64-897e-a5d04a38f9df": {"__data__": {"id_": "22070ba0-eb69-4e64-897e-a5d04a38f9df", "embedding": null, "metadata": {"page_label": "209", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae70fef7-8300-4626-9199-205f444fe720", "node_type": "4", "metadata": {"page_label": "209", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ca2052478dc8a2fa40ed69a909f7413d32c3710b2804b1ed6448a54f9ae9e9fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "partitions of the index (every term in the document might be on a different partition,\non a different node).\nIn an ideal world, the index would always be up to date, and every document written\nto the database would immediately be reflected in the index. However, in a term-\npartitioned index, that would require a distributed transaction across all partitions\naffected by a write, which is not supported in all databases (see Chapter 7 and Chap\u2010\nter 9).\nIn practice, updates to global secondary indexes are often asynchronous (that is, if\nyou read the index shortly after a write, the change you just made may not yet be\nreflected in the index). For example, Amazon DynamoDB states that its global secon\u2010\ndary indexes are updated within a fraction of a second in normal circumstances, but\nmay experience longer propagation delays in cases of faults in the infrastructure [20].\nOther uses of global term-partitioned indexes include Riak\u2019s search feature [ 21] and\nthe Oracle data warehouse, which lets you choose between local and global indexing\n[22]. We will return to the topic of implementing term-partitioned secondary indexes\nin Chapter 12. \nRebalancing Partitions\nOver time, things change in a database:\n\u2022 The query throughput increases, so you want to add more CPUs to handle the\nload.\n\u2022 The dataset size increases, so you want to add more disks and RAM to store it.\n\u2022 A machine fails, and other machines need to take over the failed machine\u2019s\nresponsibilities.\nAll of these changes call for data and requests to be moved from one node to another.\nThe process of moving load from one node in the cluster to another is called reba\u2010\nlancing.\nNo matter which partitioning scheme is used, rebalancing is usually expected to meet\nsome minimum requirements:\n\u2022 After rebalancing, the load (data storage, read and write requests) should be\nshared fairly between the nodes in the cluster.\n\u2022 While rebalancing is happening, the database should continue accepting reads\nand writes.\n\u2022 No more data than necessary should be moved between nodes, to make rebalanc\u2010\ning fast and to minimize the network and disk I/O load.\nRebalancing Partitions | 209", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "faae5c7d-8997-4203-9de1-90d49e1e96f4": {"__data__": {"id_": "faae5c7d-8997-4203-9de1-90d49e1e96f4", "embedding": null, "metadata": {"page_label": "210", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a63d4b5a-cd72-4508-bd4e-3bab18a77731", "node_type": "4", "metadata": {"page_label": "210", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8cec8a1ae644b105f3fbfce7bfca31db314edefc245a0c118cfb84d59094d068", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Strategies for Rebalancing\nThere are a few different ways of assigning partitions to nodes [ 23]. Let\u2019s briefly dis\u2010\ncuss each in turn.\nHow not to do it: hash mod N\nWhen partitioning by the hash of a key, we said earlier ( Figure 6-3) that it\u2019s best to\ndivide the possible hashes into ranges and assign each range to a partition (e.g., assign\nkey to partition 0 if 0 \u2264 hash(key) < b0, to partition 1 if b0 \u2264 hash(key) < b1, etc.).\nPerhaps you wondered why we don\u2019t just use mod (the % operator in many program\u2010\nming languages). For example, hash(key) mod 10 would return a number between 0\nand 9 (if we write the hash as a decimal number, the hash mod 10 would be the last\ndigit). If we have 10 nodes, numbered 0 to 9, that seems like an easy way of assigning\neach key to a node.\nThe problem with the mod N  approach is that if the number of nodes N changes,\nmost of the keys will need to be moved from one node to another. For example, say\nhash(key) = 123456. If you initially have 10 nodes, that key starts out on node 6\n(because 123456 mod 10 = 6). When you grow to 11 nodes, the key needs to move to\nnode 3 (123456 mod 11 = 3), and when you grow to 12 nodes, it needs to move to\nnode 0 (123456 mod 12 = 0). Such frequent moves make rebalancing excessively\nexpensive.\nWe need an approach that doesn\u2019t move data around more than necessary.\nFixed number of partitions\nFortunately, there is a fairly simple solution: create many more partitions than there\nare nodes, and assign several partitions to each node. For example, a database run\u2010\nning on a cluster of 10 nodes may be split into 1,000 partitions from the outset so that\napproximately 100 partitions are assigned to each node.\nNow, if a node is added to the cluster, the new node can steal a few partitions from\nevery existing node until partitions are fairly distributed once again. This process is\nillustrated in Figure 6-6. If a node is removed from the cluster, the same happens in\nreverse.\nOnly entire partitions are moved between nodes. The number of partitions does not\nchange, nor does the assignment of keys to partitions. The only thing that changes is\nthe assignment of partitions to nodes. This change of assignment is not immediate\u2014\nit takes some time to transfer a large amount of data over the network\u2014so the old\nassignment of partitions is used for any reads and writes that happen while the trans\u2010\nfer is in progress.\n210 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2422, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3f78a50-c1e4-4c71-bdb5-fa0b9317d810": {"__data__": {"id_": "b3f78a50-c1e4-4c71-bdb5-fa0b9317d810", "embedding": null, "metadata": {"page_label": "211", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f282b10-aaf4-4c9e-a7ea-ba2fbe971e76", "node_type": "4", "metadata": {"page_label": "211", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6cf4f1a1fa910e3c4dd6d7e6e935b96c2b2e517c6c6e4800a9558844160cd68f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6-6. Adding a new node to a database cluster with multiple partitions per node.\nIn principle, you can even account for mismatched hardware in your cluster: by\nassigning more partitions to nodes that are more powerful, you can force those nodes\nto take a greater share of the load.\nThis approach to rebalancing is used in Riak [15], Elasticsearch [24], Couchbase [10],\nand Voldemort [25].\nIn this configuration, the number of partitions is usually fixed when the database is\nfirst set up and not changed afterward. Although in principle it\u2019s possible to split and\nmerge partitions (see the next section), a fixed number of partitions is operationally\nsimpler, and so many fixed-partition databases choose not to implement partition\nsplitting. Thus, the number of partitions configured at the outset is the maximum\nnumber of nodes you can have, so you need to choose it high enough to accommo\u2010\ndate future growth. However, each partition also has management overhead, so it\u2019s\ncounterproductive to choose too high a number.\nChoosing the right number of partitions is difficult if the total size of the dataset is\nhighly variable (for example, if it starts small but may grow much larger over time).\nSince each partition contains a fixed fraction of the total data, the size of each parti\u2010\ntion grows proportionally to the total amount of data in the cluster. If partitions are\nvery large, rebalancing and recovery from node failures become expensive. But if par\u2010\ntitions are too small, they incur too much overhead. The best performance is\nachieved when the size of partitions is \u201cjust right,\u201d neither too big nor too small,\nwhich can be hard to achieve if the number of partitions is fixed but the dataset size\nvaries.\nRebalancing Partitions | 211", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2531531-5bb9-43ad-8619-b2f88c53f3ee": {"__data__": {"id_": "f2531531-5bb9-43ad-8619-b2f88c53f3ee", "embedding": null, "metadata": {"page_label": "212", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b73212b3-5dbc-4053-8fd5-6ec06b7ea30f", "node_type": "4", "metadata": {"page_label": "212", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "61ab35af0d6b89c6773b99d7900065b387deaf5369edd36dccb5c6b33a1b6b7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dynamic partitioning\nFor databases that use key range partitioning (see \u201cPartitioning by Key Range\u201d on\npage 202), a fixed number of partitions with fixed boundaries would be very incon\u2010\nvenient: if you got the boundaries wrong, you could end up with all of the data in one\npartition and all of the other partitions empty. Reconfiguring the partition bound\u2010\naries manually would be very tedious.\nFor that reason, key range\u2013partitioned databases such as HBase and RethinkDB cre\u2010\nate partitions dynamically. When a partition grows to exceed a configured size (on\nHBase, the default is 10 GB), it is split into two partitions so that approximately half\nof the data ends up on each side of the split [ 26]. Conversely, if lots of data is deleted\nand a partition shrinks below some threshold, it can be merged with an adjacent par\u2010\ntition. This process is similar to what happens at the top level of a B-tree (see \u201cB-\nTrees\u201d on page 79).\nEach partition is assigned to one node, and each node can handle multiple partitions,\nlike in the case of a fixed number of partitions. After a large partition has been split,\none of its two halves can be transferred to another node in order to balance the load.\nIn the case of HBase, the transfer of partition files happens through HDFS, the\nunderlying distributed filesystem [3].\nAn advantage of dynamic partitioning is that the number of partitions adapts to the\ntotal data volume. If there is only a small amount of data, a small number of parti\u2010\ntions is sufficient, so overheads are small; if there is a huge amount of data, the size of\neach individual partition is limited to a configurable maximum [23].\nHowever, a caveat is that an empty database starts off with a single partition, since\nthere is no a priori information about where to draw the partition boundaries. While\nthe dataset is small\u2014until it hits the point at which the first partition is split\u2014all\nwrites have to be processed by a single node while the other nodes sit idle. To miti\u2010\ngate this issue, HBase and MongoDB allow an initial set of partitions to be configured\non an empty database (this is called pre-splitting). In the case of key-range partition\u2010\ning, pre-splitting requires that you already know what the key distribution is going to\nlook like [4, 26].\nDynamic partitioning is not only suitable for key range\u2013partitioned data, but can\nequally well be used with hash-partitioned data. MongoDB since version 2.4 supports\nboth key-range and hash partitioning, and it splits partitions dynamically in either\ncase.\nPartitioning proportionally to nodes\nWith dynamic partitioning, the number of partitions is proportional to the size of the\ndataset, since the splitting and merging processes keep the size of each partition\nbetween some fixed minimum and maximum. On the other hand, with a fixed num\u2010\n212 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2841, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecd41e61-2c04-40db-942a-8dd73f3e5da7": {"__data__": {"id_": "ecd41e61-2c04-40db-942a-8dd73f3e5da7", "embedding": null, "metadata": {"page_label": "213", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5fe18cb-8e22-4ebc-b789-315eb7098b4a", "node_type": "4", "metadata": {"page_label": "213", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "aaeda53c7035187d65e89e7d9227d297ccce3f42d446f3213cd887a7317843a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ber of partitions, the size of each partition is proportional to the size of the dataset. In\nboth of these cases, the number of partitions is independent of the number of nodes.\nA third option, used by Cassandra and Ketama, is to make the number of partitions\nproportional to the number of nodes\u2014in other words, to have a fixed number of par\u2010\ntitions per node [23, 27, 28]. In this case, the size of each partition grows proportion\u2010\nally to the dataset size while the number of nodes remains unchanged, but when you\nincrease the number of nodes, the partitions become smaller again. Since a larger\ndata volume generally requires a larger number of nodes to store, this approach also\nkeeps the size of each partition fairly stable.\nWhen a new node joins the cluster, it randomly chooses a fixed number of existing\npartitions to split, and then takes ownership of one half of each of those split parti\u2010\ntions while leaving the other half of each partition in place. The randomization can\nproduce unfair splits, but when averaged over a larger number of partitions (in Cas\u2010\nsandra, 256 partitions per node by default), the new node ends up taking a fair share\nof the load from the existing nodes. Cassandra 3.0 introduced an alternative rebalanc\u2010\ning algorithm that avoids unfair splits [29].\nPicking partition boundaries randomly requires that hash-based partitioning is used\n(so the boundaries can be picked from the range of numbers produced by the hash\nfunction). Indeed, this approach corresponds most closely to the original definition\nof consistent hashing [ 7] (see \u201cConsistent Hashing\u201d on page 204). Newer hash func\u2010\ntions can achieve a similar effect with lower metadata overhead [8].\nOperations: Automatic or Manual Rebalancing\nThere is one important question with regard to rebalancing that we have glossed\nover: does the rebalancing happen automatically or manually?\nThere is a gradient between fully automatic rebalancing (the system decides automat\u2010\nically when to move partitions from one node to another, without any administrator\ninteraction) and fully manual (the assignment of partitions to nodes is explicitly con\u2010\nfigured by an administrator, and only changes when the administrator explicitly\nreconfigures it). For example, Couchbase, Riak, and Voldemort generate a suggested\npartition assignment automatically, but require an administrator to commit it before\nit takes effect.\nFully automated rebalancing can be convenient, because there is less operational\nwork to do for normal maintenance. However, it can be unpredictable. Rebalancing\nis an expensive operation, because it requires rerouting requests and moving a large\namount of data from one node to another. If it is not done carefully, this process can\noverload the network or the nodes and harm the performance of other requests while\nthe rebalancing is in progress.\nRebalancing Partitions | 213", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c737e3c2-b6a6-4b8c-9a28-98c814539daa": {"__data__": {"id_": "c737e3c2-b6a6-4b8c-9a28-98c814539daa", "embedding": null, "metadata": {"page_label": "214", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f8184cb-8aaa-4941-9835-ca2235fc1473", "node_type": "4", "metadata": {"page_label": "214", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b18f579c054151a30012e0fcc3b7a394079ee6b239ee92c6d0fc53dcc92f6cc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such automation can be dangerous in combination with automatic failure detection.\nFor example, say one node is overloaded and is temporarily slow to respond to\nrequests. The other nodes conclude that the overloaded node is dead, and automati\u2010\ncally rebalance the cluster to move load away from it. This puts additional load on the\noverloaded node, other nodes, and the network\u2014making the situation worse and\npotentially causing a cascading failure.\nFor that reason, it can be a good thing to have a human in the loop for rebalancing.\nIt\u2019s slower than a fully automatic process, but it can help prevent operational\nsurprises. \nRequest Routing\nWe have now partitioned our dataset across multiple nodes running on multiple\nmachines. But there remains an open question: when a client wants to make a\nrequest, how does it know which node to connect to? As partitions are rebalanced,\nthe assignment of partitions to nodes changes. Somebody needs to stay on top of\nthose changes in order to answer the question: if I want to read or write the key \u201cfoo\u201d,\nwhich IP address and port number do I need to connect to?\nThis is an instance of a more general problem called service discovery , which isn\u2019t\nlimited to just databases. Any piece of software that is accessible over a network has\nthis problem, especially if it is aiming for high availability (running in a redundant\nconfiguration on multiple machines). Many companies have written their own in-\nhouse service discovery tools, and many of these have been released as open source\n[30].\nOn a high level, there are a few different approaches to this problem (illustrated in\nFigure 6-7):\n1. Allow clients to contact any node (e.g., via a round-robin load balancer). If that\nnode coincidentally owns the partition to which the request applies, it can handle\nthe request directly; otherwise, it forwards the request to the appropriate node,\nreceives the reply, and passes the reply along to the client.\n2. Send all requests from clients to a routing tier first, which determines the node\nthat should handle each request and forwards it accordingly. This routing tier\ndoes not itself handle any requests; it only acts as a partition-aware load balancer.\n3. Require that clients be aware of the partitioning and the assignment of partitions\nto nodes. In this case, a client can connect directly to the appropriate node,\nwithout any intermediary.\nIn all cases, the key problem is: how does the component making the routing decision\n(which may be one of the nodes, or the routing tier, or the client) learn about changes\nin the assignment of partitions to nodes?\n214 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fd34988-116e-47c8-a946-ec2634a1b4a4": {"__data__": {"id_": "9fd34988-116e-47c8-a946-ec2634a1b4a4", "embedding": null, "metadata": {"page_label": "215", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55e64ba1-3626-4432-9553-4a725df12809", "node_type": "4", "metadata": {"page_label": "215", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f3214769b44fd383a1aa83d1648b375e4362d53f87f28b84be0fe723dd74bc07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6-7. Three different ways of routing a request to the right node.\nThis is a challenging problem, because it is important that all participants agree\u2014\notherwise requests would be sent to the wrong nodes and not handled correctly.\nThere are protocols for achieving consensus in a distributed system, but they are hard\nto implement correctly (see Chapter 9).\nMany distributed data systems rely on a separate coordination service such as Zoo\u2010\nKeeper to keep track of this cluster metadata, as illustrated in Figure 6-8. Each node\nregisters itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of\npartitions to nodes. Other actors, such as the routing tier or the partitioning-aware\nclient, can subscribe to this information in ZooKeeper. Whenever a partition changes\nownership, or a node is added or removed, ZooKeeper notifies the routing tier so that\nit can keep its routing information up to date.\nFigure 6-8. Using ZooKeeper to keep track of assignment of partitions to nodes.\nRequest Routing | 215", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9487f11c-0654-459d-81fe-c5d8df12c1d7": {"__data__": {"id_": "9487f11c-0654-459d-81fe-c5d8df12c1d7", "embedding": null, "metadata": {"page_label": "216", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4d55bfe-7b55-41c1-b30b-0a02e339b3e9", "node_type": "4", "metadata": {"page_label": "216", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7db9a6e22accf11a073a8a49f090287e5cddfe046d77eda2147a8c8658e79073", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, LinkedIn\u2019s Espresso uses Helix [ 31] for cluster management (which in\nturn relies on ZooKeeper), implementing a routing tier as shown in Figure 6-8 .\nHBase, SolrCloud, and Kafka also use ZooKeeper to track partition assignment.\nMongoDB has a similar architecture, but it relies on its own config server implemen\u2010\ntation and mongos daemons as the routing tier.\nCassandra and Riak take a different approach: they use a gossip protocol among the\nnodes to disseminate any changes in cluster state. Requests can be sent to any node,\nand that node forwards them to the appropriate node for the requested partition\n(approach 1 in Figure 6-7). This model puts more complexity in the database nodes\nbut avoids the dependency on an external coordination service such as ZooKeeper.\nCouchbase does not rebalance automatically, which simplifies the design. Normally it\nis configured with a routing tier called moxi, which learns about routing changes\nfrom the cluster nodes [32].\nWhen using a routing tier or when sending requests to a random node, clients still\nneed to find the IP addresses to connect to. These are not as fast-changing as the\nassignment of partitions to nodes, so it is often sufficient to use DNS for this purpose.\nParallel Query Execution\nSo far we have focused on very simple queries that read or write a single key (plus\nscatter/gather queries in the case of document-partitioned secondary indexes). This is\nabout the level of access supported by most NoSQL distributed datastores.\nHowever, massively parallel processing  (MPP) relational database products, often\nused for analytics, are much more sophisticated in the types of queries they support.\nA typical data warehouse query contains several join, filtering, grouping, and aggre\u2010\ngation operations. The MPP query optimizer breaks this complex query into a num\u2010\nber of execution stages and partitions, many of which can be executed in parallel on\ndifferent nodes of the database cluster. Queries that involve scanning over large parts\nof the dataset particularly benefit from such parallel execution.\nFast parallel execution of data warehouse queries is a specialized topic, and given the\nbusiness importance of analytics, it receives a lot of commercial interest. We will dis\u2010\ncuss some techniques for parallel query execution in Chapter 10. For a more detailed\noverview of techniques used in parallel databases, please see the references [1, 33]. \nSummary\nIn this chapter we explored different ways of partitioning a large dataset into smaller\nsubsets. Partitioning is necessary when you have so much data that storing and pro\u2010\ncessing it on a single machine is no longer feasible.\n216 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f88ee966-7ef3-43f0-bd85-05f3ad109188": {"__data__": {"id_": "f88ee966-7ef3-43f0-bd85-05f3ad109188", "embedding": null, "metadata": {"page_label": "217", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddea5aad-c97b-4e50-823c-791b6faea3a6", "node_type": "4", "metadata": {"page_label": "217", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7de67693665f34099e401fbb97b6a353aa0992c0a5fea454b9365d46944962b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The goal of partitioning is to spread the data and query load evenly across multiple\nmachines, avoiding hot spots (nodes with disproportionately high load). This\nrequires choosing a partitioning scheme that is appropriate to your data, and reba\u2010\nlancing the partitions when nodes are added to or removed from the cluster.\nWe discussed two main approaches to partitioning:\n\u2022 Key range partitioning , where keys are sorted, and a partition owns all the keys\nfrom some minimum up to some maximum. Sorting has the advantage that effi\u2010\ncient range queries are possible, but there is a risk of hot spots if the application\noften accesses keys that are close together in the sorted order.\nIn this approach, partitions are typically rebalanced dynamically by splitting the\nrange into two subranges when a partition gets too big.\n\u2022 Hash partitioning, where a hash function is applied to each key, and a partition\nowns a range of hashes. This method destroys the ordering of keys, making range\nqueries inefficient, but may distribute load more evenly.\nWhen partitioning by hash, it is common to create a fixed number of partitions\nin advance, to assign several partitions to each node, and to move entire parti\u2010\ntions from one node to another when nodes are added or removed. Dynamic\npartitioning can also be used.\nHybrid approaches are also possible, for example with a compound key: using one\npart of the key to identify the partition and another part for the sort order.\nWe also discussed the interaction between partitioning and secondary indexes. A sec\u2010\nondary index also needs to be partitioned, and there are two methods:\n\u2022 Document-partitioned indexes  (local indexes), where the secondary indexes are\nstored in the same partition as the primary key and value. This means that only a\nsingle partition needs to be updated on write, but a read of the secondary index\nrequires a scatter/gather across all partitions.\n\u2022 Term-partitioned indexes (global indexes), where the secondary indexes are parti\u2010\ntioned separately, using the indexed values. An entry in the secondary index may\ninclude records from all partitions of the primary key. When a document is writ\u2010\nten, several partitions of the secondary index need to be updated; however, a read\ncan be served from a single partition.\nFinally, we discussed techniques for routing queries to the appropriate partition,\nwhich range from simple partition-aware load balancing to sophisticated parallel\nquery execution engines.\nBy design, every partition operates mostly independently\u2014that\u2019s what allows a parti\u2010\ntioned database to scale to multiple machines. However, operations that need to write\nSummary | 217", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6d648ac-d93b-42dc-9a73-e0ee9a0db712": {"__data__": {"id_": "f6d648ac-d93b-42dc-9a73-e0ee9a0db712", "embedding": null, "metadata": {"page_label": "218", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "807f5503-36e6-4f89-a513-2bbfbeaba8c5", "node_type": "4", "metadata": {"page_label": "218", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "82a953543fd441c41f173ef05a7ecdf55d2fbd0237f39a75edc57194a222b78a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to several partitions can be difficult to reason about: for example, what happens if the\nwrite to one partition succeeds, but another fails? We will address that question in the\nfollowing chapters. \nReferences\n[1] David J. DeWitt and Jim N. Gray: \u201c Parallel Database Systems: The Future of High\nPerformance Database Systems,\u201d Communications of the ACM, volume 35, number 6,\npages 85\u201398, June 1992. doi:10.1145/129888.129894\n[2] Lars George: \u201cHBase vs. BigTable Comparison,\u201d larsgeorge.com, November 2009.\n[3] \u201c The Apache HBase Reference Guide ,\u201d Apache Software Foundation,\nhbase.apache.org, 2014.\n[4] MongoDB, Inc.: \u201cNew Hash-Based Sharding Feature in MongoDB 2.4,\u201d blog.mon\u2010\ngodb.org, April 10, 2013.\n[5] Ikai Lan: \u201cApp Engine Datastore Tip: Monotonically Increasing Values Are Bad ,\u201d\nikaisays.com, January 25, 2011.\n[6] Martin Kleppmann: \u201c Java\u2019s hashCode Is Not Safe for Distributed Systems ,\u201d mar\u2010\ntin.kleppmann.com, June 18, 2012.\n[7] David Karger, Eric Lehman, Tom Leighton, et al.: \u201c Consistent Hashing and Ran\u2010\ndom Trees: Distributed Caching Protocols for Relieving Hot Spots on the World\nWide Web ,\u201d at 29th Annual ACM Symposium on Theory of Computing  (STOC),\npages 654\u2013663, 1997. doi:10.1145/258533.258660\n[8] John Lamping and Eric Veach: \u201cA Fast, Minimal Memory, Consistent Hash Algo\u2010\nrithm,\u201d arxiv.org, June 2014.\n[9] Eric Redmond: \u201cA Little Riak Book,\u201d Version 1.4.0, Basho Technologies, Septem\u2010\nber 2013.\n[10] \u201cCouchbase 2.5 Administrator Guide,\u201d Couchbase, Inc., 2014.\n[11] Avinash Lakshman and Prashant Malik: \u201c Cassandra \u2013 A Decentralized Struc\u2010\ntured Storage System ,\u201d at 3rd ACM SIGOPS International Workshop on Large Scale\nDistributed Systems and Middleware (LADIS), October 2009.\n[12] Jonathan Ellis: \u201c Facebook\u2019s Cassandra Paper, Annotated and Compared to\nApache Cassandra 2.0,\u201d datastax.com, September 12, 2013.\n[13] \u201cIntroduction to Cassandra Query Language,\u201d DataStax, Inc., 2014.\n[14] Samuel Axon: \u201c 3% of Twitter\u2019s Servers Dedicated to Justin Bieber ,\u201d masha\u2010\nble.com, September 7, 2010.\n[15] \u201cRiak 1.4.8 Docs,\u201d Basho Technologies, Inc., 2014.\n218 | Chapter 6: Partitioning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a49adf62-2366-47cf-a697-10914acbedb3": {"__data__": {"id_": "a49adf62-2366-47cf-a697-10914acbedb3", "embedding": null, "metadata": {"page_label": "219", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12d93309-5e41-4d01-9f3f-025abf8714aa", "node_type": "4", "metadata": {"page_label": "219", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b67240a82c6a2149e1ac9037ec6cf67233149c94f346653da14c46c87fabee6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] Richard Low: \u201cThe Sweet Spot for Cassandra Secondary Indexing,\u201d wentnet.com,\nOctober 21, 2013.\n[17] Zachary Tong: \u201c Customizing Your Document Routing ,\u201d elasticsearch.org, June\n3, 2013.\n[18] \u201cApache Solr Reference Guide,\u201d Apache Software Foundation, 2014.\n[19] Andrew Pavlo: \u201c H-Store Frequently Asked Questions ,\u201d hstore.cs.brown.edu,\nOctober 2013.\n[20] \u201cAmazon DynamoDB Developer Guide,\u201d Amazon Web Services, Inc., 2014.\n[21] Rusty Klophaus: \u201cDifference Between 2I and Search,\u201d email to riak-users mailing\nlist, lists.basho.com, October 25, 2011.\n[22] Donald K. Burleson: \u201c Object Partitioning in Oracle,\u201d dba-oracle.com, November\n8, 2000.\n[23] Eric Evans: \u201cRethinking Topology in Cassandra,\u201d at ApacheCon Europe, Novem\u2010\nber 2012.\n[24] Rafa\u0142 Ku\u0107: \u201c Reroute API Explained,\u201d elasticsearchserverbook.com, September 30,\n2013.\n[25] \u201cProject Voldemort Documentation,\u201d project-voldemort.com.\n[26] Enis Soztutar: \u201cApache HBase Region Splitting and Merging ,\u201d hortonworks.com,\nFebruary 1, 2013.\n[27] Brandon Williams: \u201c Virtual Nodes in Cassandra 1.2 ,\u201d datastax.com, December\n4, 2012.\n[28] Richard Jones: \u201clibketama: Consistent Hashing Library for Memcached Clients ,\u201d\nmetabrew.com, April 10, 2007.\n[29] Branimir Lambov: \u201c New Token Allocation Algorithm in Cassandra 3.0 ,\u201d data\u2010\nstax.com, January 28, 2016.\n[30] Jason Wilder: \u201c Open-Source Service Discovery ,\u201d jasonwilder.com, February\n2014.\n[31] Kishore Gopalakrishna, Shi Lu, Zhen Zhang, et al.: \u201cUntangling Cluster Manage\u2010\nment with Helix ,\u201d at ACM Symposium on Cloud Computing  (SoCC), October 2012.\ndoi:10.1145/2391229.2391248\n[32] \u201cMoxi 1.8 Manual,\u201d Couchbase, Inc., 2014.\n[33] Shivnath Babu and Herodotos Herodotou: \u201c Massively Parallel Databases and\nMapReduce Systems ,\u201d Foundations and Trends in Databases , volume 5, number 1,\npages 1\u2013104, November 2013. doi:10.1561/1900000036\nSummary | 219", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18613643-c614-40ff-a3a8-83e90f90f38f": {"__data__": {"id_": "18613643-c614-40ff-a3a8-83e90f90f38f", "embedding": null, "metadata": {"page_label": "220", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e02434b-e831-4ec2-807a-1f1eee14ba9f", "node_type": "4", "metadata": {"page_label": "220", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56904f13-9976-4cc6-a96e-af0458a62978": {"__data__": {"id_": "56904f13-9976-4cc6-a96e-af0458a62978", "embedding": null, "metadata": {"page_label": "221", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b60672d-8eb8-43c9-b9de-d82902f8df0f", "node_type": "4", "metadata": {"page_label": "221", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "36c459f952c16ab58fffce6b33b6ad5b952122834aa5cf59c19c5b158751cb07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 7\nTransactions\nSome authors have claimed that general two-phase commit is too expensive to support,\nbecause of the performance or availability problems that it brings. We believe it is better to\nhave application programmers deal with performance problems due to overuse of transac\u2010\ntions as bottlenecks arise, rather than always coding around the lack of transactions.\n\u2014James Corbett et al., Spanner: Google\u2019s Globally-Distributed Database (2012)\nIn the harsh reality of data systems, many things can go wrong:\n\u2022 The database software or hardware may fail at any time (including in the middle\nof a write operation).\n\u2022 The application may crash at any time (including halfway through a series of\noperations).\n\u2022 Interruptions in the network can unexpectedly cut off the application from the\ndatabase, or one database node from another.\n\u2022 Several clients may write to the database at the same time, overwriting each\nother\u2019s changes.\n\u2022 A client may read data that doesn\u2019t make sense because it has only partially been\nupdated.\n\u2022 Race conditions between clients can cause surprising bugs.\nIn order to be reliable, a system has to deal with these faults and ensure that they\ndon\u2019t cause catastrophic failure of the entire system. However, implementing fault-\ntolerance mechanisms is a lot of work. It requires a lot of careful thinking about all\nthe things that can go wrong, and a lot of testing to ensure that the solution actually\nworks.\n221", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1446, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86b31c19-7569-4742-a08a-7a1c8d3ab429": {"__data__": {"id_": "86b31c19-7569-4742-a08a-7a1c8d3ab429", "embedding": null, "metadata": {"page_label": "222", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfebe986-0279-4307-aa33-6308ee7cd67f", "node_type": "4", "metadata": {"page_label": "222", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f5efcf3d309bc12255b70493fd901282ba53846f76de17f50fc9c633a0812701", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For decades, transactions have been the mechanism of choice for simplifying these\nissues. A transaction is a way for an application to group several reads and writes\ntogether into a logical unit. Conceptually, all the reads and writes in a transaction are\nexecuted as one operation: either the entire transaction succeeds ( commit) or it fails\n(abort, rollback). If it fails, the application can safely retry. With transactions, error\nhandling becomes much simpler for an application, because it doesn\u2019t need to worry\nabout partial failure\u2014i.e., the case where some operations succeed and some fail (for\nwhatever reason).\nIf you have spent years working with transactions, they may seem obvious, but we\nshouldn\u2019t take them for granted. Transactions are not a law of nature; they were cre\u2010\nated with a purpose, namely to simplify the programming model  for applications\naccessing a database. By using transactions, the application is free to ignore certain\npotential error scenarios and concurrency issues, because the database takes care of\nthem instead (we call these safety guarantees).\nNot every application needs transactions, and sometimes there are advantages to\nweakening transactional guarantees or abandoning them entirely (for example, to\nachieve higher performance or higher availability). Some safety properties can be\nachieved without transactions.\nHow do you figure out whether you need transactions? In order to answer that ques\u2010\ntion, we first need to understand exactly what safety guarantees transactions can pro\u2010\nvide, and what costs are associated with them. Although transactions seem\nstraightforward at first glance, there are actually many subtle but important details\nthat come into play.\nIn this chapter, we will examine many examples of things that can go wrong, and\nexplore the algorithms that databases use to guard against those issues. We will go\nespecially deep in the area of concurrency control, discussing various kinds of race\nconditions that can occur and how databases implement isolation levels such as read\ncommitted, snapshot isolation, and serializability.\nThis chapter applies to both single-node and distributed databases; in Chapter 8 we\nwill focus the discussion on the particular challenges that arise only in distributed\nsystems.\nThe Slippery Concept of a Transaction\nAlmost all relational databases today, and some nonrelational databases, support\ntransactions. Most of them follow the style that was introduced in 1975 by IBM Sys\u2010\ntem R, the first SQL database [ 1, 2, 3]. Although some implementation details have\nchanged, the general idea has remained virtually the same for 40 years: the transac\u2010\ntion support in MySQL, PostgreSQL, Oracle, SQL Server, etc., is uncannily similar to\nthat of System R.\n222 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb4c5da9-5bc6-4962-ac8f-39ed2346b3e7": {"__data__": {"id_": "fb4c5da9-5bc6-4962-ac8f-39ed2346b3e7", "embedding": null, "metadata": {"page_label": "223", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86ffb901-20ea-4f43-a051-a25a91325180", "node_type": "4", "metadata": {"page_label": "223", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "09987a852bff3c9a4e942380c80ac1a4b9de892bd437dcc79e7bfe309d2453bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the late 2000s, nonrelational (NoSQL) databases started gaining popularity. They\naimed to improve upon the relational status quo by offering a choice of new data\nmodels (see Chapter 2 ), and by including replication ( Chapter 5 ) and partitioning\n(Chapter 6) by default. Transactions were the main casualty of this movement: many\nof this new generation of databases abandoned transactions entirely, or redefined the\nword to describe a much weaker set of guarantees than had previously been under\u2010\nstood [4].\nWith the hype around this new crop of distributed databases, there emerged a popu\u2010\nlar belief that transactions were the antithesis of scalability, and that any large-scale\nsystem would have to abandon transactions in order to maintain good performance\nand high availability [ 5, 6]. On the other hand, transactional guarantees are some\u2010\ntimes presented by database vendors as an essential requirement for \u201cserious applica\u2010\ntions\u201d with \u201cvaluable data.\u201d Both viewpoints are pure hyperbole.\nThe truth is not that simple: like every other technical design choice, transactions\nhave advantages and limitations. In order to understand those trade-offs, let\u2019s go into\nthe details of the guarantees that transactions can provide\u2014both in normal operation\nand in various extreme (but realistic) circumstances.\nThe Meaning of ACID\nThe safety guarantees provided by transactions are often described by the well-\nknown acronym ACID, which stands for Atomicity, Consistency, Isolation, and Dura\u2010\nbility. It was coined in 1983 by Theo H\u00e4rder and Andreas Reuter [ 7] in an effort to\nestablish precise terminology for fault-tolerance mechanisms in databases.\nHowever, in practice, one database\u2019s implementation of ACID does not equal\nanother\u2019s implementation. For example, as we shall see, there is a lot of ambiguity\naround the meaning of isolation [8]. The high-level idea is sound, but the devil is in\nthe details. Today, when a system claims to be \u201cACID compliant,\u201d it\u2019s unclear what\nguarantees you can actually expect. ACID has unfortunately become mostly a mar\u2010\nketing term.\n(Systems that do not meet the ACID criteria are sometimes called BASE, which\nstands for Basically Available , Soft state , and Eventual consistency  [9]. This is even\nmore vague than the definition of ACID. It seems that the only sensible definition of\nBASE is \u201cnot ACID\u201d; i.e., it can mean almost anything you want.)\nLet\u2019s dig into the definitions of atomicity, consistency, isolation, and durability, as\nthis will let us refine our idea of transactions.\nAtomicity\nIn general, atomic refers to something that cannot be broken down into smaller parts.\nThe word means similar but subtly different things in different branches of comput\u2010\nThe Slippery Concept of a Transaction | 223", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9255cc5-dffa-4b58-91c7-cf72f0e70c6e": {"__data__": {"id_": "a9255cc5-dffa-4b58-91c7-cf72f0e70c6e", "embedding": null, "metadata": {"page_label": "224", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91cc06b7-7f6d-4bc6-92e5-e9ec0d649a14", "node_type": "4", "metadata": {"page_label": "224", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5a71834a056fcc2a40930fe82de0fa9f46706777d74614c4ae232ef1571ba8b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ing. For example, in multi-threaded programming, if one thread executes an atomic\noperation, that means there is no way that another thread could see the half-finished\nresult of the operation. The system can only be in the state it was before the operation\nor after the operation, not something in between.\nBy contrast, in the context of ACID, atomicity is not about concurrency. It does not\ndescribe what happens if several processes try to access the same data at the same\ntime, because that is covered under the letter I, for isolation (see \u201cIsolation\u201d on page\n225).\nRather, ACID atomicity describes what happens if a client wants to make several\nwrites, but a fault occurs after some of the writes have been processed\u2014for example,\na process crashes, a network connection is interrupted, a disk becomes full, or some\nintegrity constraint is violated. If the writes are grouped together into an atomic\ntransaction, and the transaction cannot be completed (committed) due to a fault, then\nthe transaction is aborted and the database must discard or undo any writes it has\nmade so far in that transaction.\nWithout atomicity, if an error occurs partway through making multiple changes, it\u2019s\ndifficult to know which changes have taken effect and which haven\u2019t. The application\ncould try again, but that risks making the same change twice, leading to duplicate or\nincorrect data. Atomicity simplifies this problem: if a transaction was aborted, the\napplication can be sure that it didn\u2019t change anything, so it can safely be retried.\nThe ability to abort a transaction on error and have all writes from that transaction\ndiscarded is the defining feature of ACID atomicity. Perhaps abortability would have\nbeen a better term than atomicity, but we will stick with atomicity since that\u2019s the\nusual word.\nConsistency\nThe word consistency is terribly overloaded:\n\u2022 In Chapter 5  we discussed replica consistency  and the issue of eventual consis\u2010\ntency that arises in asynchronously replicated systems (see \u201cProblems with Repli\u2010\ncation Lag\u201d on page 161).\n\u2022 Consistent hashing is an approach to partitioning that some systems use for reba\u2010\nlancing (see \u201cConsistent Hashing\u201d on page 204).\n\u2022 In the CAP theorem (see Chapter 9), the word consistency is used to mean linear\u2010\nizability (see \u201cLinearizability\u201d on page 324).\n\u2022 In the context of ACID, consistency refers to an application-specific notion of the\ndatabase being in a \u201cgood state.\u201d\nIt\u2019s unfortunate that the same word is used with at least four different meanings.\n224 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb120704-295b-4610-84b0-b95237f733b0": {"__data__": {"id_": "bb120704-295b-4610-84b0-b95237f733b0", "embedding": null, "metadata": {"page_label": "225", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d3fd03e-4fd8-44d8-b7ef-393253e96071", "node_type": "4", "metadata": {"page_label": "225", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "565e496d19d1673f14cfd2addaeca395e8e231dfa01e220544b7393d226d78c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. Joe Hellerstein has remarked that the C in ACID was \u201ctossed in to make the acronym work\u201d in H\u00e4rder and\nReuter\u2019s paper [7], and that it wasn\u2019t considered important at the time.\nThe idea of ACID consistency is that you have certain statements about your data\n(invariants) that must always be true\u2014for example, in an accounting system, credits\nand debits across all accounts must always be balanced. If a transaction starts with a\ndatabase that is valid according to these invariants, and any writes during the transac\u2010\ntion preserve the validity, then you can be sure that the invariants are always satisfied.\nHowever, this idea of consistency depends on the application\u2019s notion of invariants,\nand it\u2019s the application\u2019s responsibility to define its transactions correctly so that they\npreserve consistency. This is not something that the database can guarantee: if you\nwrite bad data that violates your invariants, the database can\u2019t stop you. (Some spe\u2010\ncific kinds of invariants can be checked by the database, for example using foreign\nkey constraints or uniqueness constraints. However, in general, the application\ndefines what data is valid or invalid\u2014the database only stores it.)\nAtomicity, isolation, and durability are properties of the database, whereas consis\u2010\ntency (in the ACID sense) is a property of the application. The application may rely\non the database\u2019s atomicity and isolation properties in order to achieve consistency,\nbut it\u2019s not up to the database alone. Thus, the letter C doesn\u2019t really belong in ACID.i\nIsolation\nMost databases are accessed by several clients at the same time. That is no problem if\nthey are reading and writing different parts of the database, but if they are accessing\nthe same database records, you can run into concurrency problems (race conditions).\nFigure 7-1  is a simple example of this kind of problem. Say you have two clients\nsimultaneously incrementing a counter that is stored in a database. Each client needs\nto read the current value, add 1, and write the new value back (assuming there is no\nincrement operation built into the database). In Figure 7-1 the counter should have\nincreased from 42 to 44, because two increments happened, but it actually only went\nto 43 because of the race condition.\nIsolation in the sense of ACID means that concurrently executing transactions are\nisolated from each other: they cannot step on each other\u2019s toes. The classic database\ntextbooks formalize isolation as serializability, which means that each transaction can\npretend that it is the only transaction running on the entire database. The database\nensures that when the transactions have committed, the result is the same as if they\nhad run serially (one after another), even though in reality they may have run con\u2010\ncurrently [10].\nThe Slippery Concept of a Transaction | 225", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4249d453-86c0-4995-a9f4-e129492ca7a3": {"__data__": {"id_": "4249d453-86c0-4995-a9f4-e129492ca7a3", "embedding": null, "metadata": {"page_label": "226", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9438b325-5078-4032-ab75-23aee677f786", "node_type": "4", "metadata": {"page_label": "226", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2e63eb3eb38c4550fa6a875f669156f7c61840d0656b4429db779800334787d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 7-1. A race condition between two clients concurrently incrementing a counter.\nHowever, in practice, serializable isolation is rarely used, because it carries a perfor\u2010\nmance penalty. Some popular databases, such as Oracle 11g, don\u2019t even implement it.\nIn Oracle there is an isolation level called \u201cserializable,\u201d but it actually implements\nsomething called snapshot isolation, which is a weaker guarantee than serializability\n[8, 11]. We will explore snapshot isolation and other forms of isolation in \u201cWeak Iso\u2010\nlation Levels\u201d on page 233.\nDurability\nThe purpose of a database system is to provide a safe place where data can be stored\nwithout fear of losing it. Durability is the promise that once a transaction has com\u2010\nmitted successfully, any data it has written will not be forgotten, even if there is a\nhardware fault or the database crashes.\nIn a single-node database, durability typically means that the data has been written to\nnonvolatile storage such as a hard drive or SSD. It usually also involves a write-ahead\nlog or similar (see \u201cMaking B-trees reliable\u201d on page 82), which allows recovery in the\nevent that the data structures on disk are corrupted. In a replicated database, durabil\u2010\nity may mean that the data has been successfully copied to some number of nodes. In\norder to provide a durability guarantee, a database must wait until these writes or\nreplications are complete before reporting a transaction as successfully committed.\nAs discussed in \u201cReliability\u201d on page 6, perfect durability does not exist: if all your\nhard disks and all your backups are destroyed at the same time, there\u2019s obviously\nnothing your database can do to save you.\n226 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c56b5adb-907c-4ff7-8828-346b3d673581": {"__data__": {"id_": "c56b5adb-907c-4ff7-8828-346b3d673581", "embedding": null, "metadata": {"page_label": "227", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9b1a814-2bd1-467e-a2f8-90eea72933a2", "node_type": "4", "metadata": {"page_label": "227", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ab16142c4301c99a1da2b7a0b16c9d9ffb5d7988241ef999a1c500446ccac951", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Replication and Durability\nHistorically, durability meant writing to an archive tape. Then it was understood as\nwriting to a disk or SSD. More recently, it has been adapted to mean replication.\nWhich implementation is better?\nThe truth is, nothing is perfect:\n\u2022 If you write to disk and the machine dies, even though your data isn\u2019t lost, it is\ninaccessible until you either fix the machine or transfer the disk to another\nmachine. Replicated systems can remain available.\n\u2022 A correlated fault\u2014a power outage or a bug that crashes every node on a particu\u2010\nlar input\u2014can knock out all replicas at once (see \u201cReliability\u201d on page 6), losing\nany data that is only in memory. Writing to disk is therefore still relevant for in-\nmemory databases.\n\u2022 In an asynchronously replicated system, recent writes may be lost when the\nleader becomes unavailable (see \u201cHandling Node Outages\u201d on page 156).\n\u2022 When the power is suddenly cut, SSDs in particular have been shown to some\u2010\ntimes violate the guarantees they are supposed to provide: even fsync isn\u2019t guar\u2010\nanteed to work correctly [ 12]. Disk firmware can have bugs, just like any other\nkind of software [13, 14].\n\u2022 Subtle interactions between the storage engine and the filesystem implementa\u2010\ntion can lead to bugs that are hard to track down, and may cause files on disk to\nbe corrupted after a crash [15, 16].\n\u2022 Data on disk can gradually become corrupted without this being detected [ 17]. If\ndata has been corrupted for some time, replicas and recent backups may also be\ncorrupted. In this case, you will need to try to restore the data from a historical\nbackup.\n\u2022 One study of SSDs found that between 30% and 80% of drives develop at least\none bad block during the first four years of operation [ 18]. Magnetic hard drives\nhave a lower rate of bad sectors, but a higher rate of complete failure than SSDs.\n\u2022 If an SSD is disconnected from power, it can start losing data within a few weeks,\ndepending on the temperature [19].\nIn practice, there is no one technique that can provide absolute guarantees. There are\nonly various risk-reduction techniques, including writing to disk, replicating to\nremote machines, and backups\u2014and they can and should be used together. As\nalways, it\u2019s wise to take any theoretical \u201cguarantees\u201d with a healthy grain of salt.\nThe Slippery Concept of a Transaction | 227", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05c1489e-26a6-4244-8dc5-b4ca413a5eee": {"__data__": {"id_": "05c1489e-26a6-4244-8dc5-b4ca413a5eee", "embedding": null, "metadata": {"page_label": "228", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db1a1697-d138-4b24-9e51-4abbeb69f6e5", "node_type": "4", "metadata": {"page_label": "228", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "690c58eae1667c01e0126f1887b47db85ef6ed0860884a785fc60e2df36a1605", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. Arguably, an incorrect counter in an email application is not a particularly critical problem. Alternatively,\nthink of a customer account balance instead of an unread counter, and a payment transaction instead of an\nemail.\nSingle-Object and Multi-Object Operations\nTo recap, in ACID, atomicity and isolation describe what the database should do if a\nclient makes several writes within the same transaction:\nAtomicity\nIf an error occurs halfway through a sequence of writes, the transaction should\nbe aborted, and the writes made up to that point should be discarded. In other\nwords, the database saves you from having to worry about partial failure, by giv\u2010\ning an all-or-nothing guarantee.\nIsolation\nConcurrently running transactions shouldn\u2019t interfere with each other. For\nexample, if one transaction makes several writes, then another transaction should\nsee either all or none of those writes, but not some subset.\nThese definitions assume that you want to modify several objects (rows, documents,\nrecords) at once. Such multi-object transactions are often needed if several pieces of\ndata need to be kept in sync. Figure 7-2 shows an example from an email application.\nTo display the number of unread messages for a user, you could query something\nlike:\nSELECT COUNT(*) FROM emails WHERE recipient_id = 2 AND unread_flag = true\nHowever, you might find this query to be too slow if there are many emails, and\ndecide to store the number of unread messages in a separate field (a kind of denorm\u2010\nalization). Now, whenever a new message comes in, you have to increment the\nunread counter as well, and whenever a message is marked as read, you also have to\ndecrement the unread counter.\nIn Figure 7-2 , user 2 experiences an anomaly: the mailbox listing shows an unread\nmessage, but the counter shows zero unread messages because the counter increment\nhas not yet happened. ii Isolation would have prevented this issue by ensuring that\nuser 2 sees either both the inserted email and the updated counter, or neither, but not\nan inconsistent halfway point.\n228 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ee830aa-719a-4435-a416-0aefc36799de": {"__data__": {"id_": "1ee830aa-719a-4435-a416-0aefc36799de", "embedding": null, "metadata": {"page_label": "229", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "078ecd0c-4fe1-44ee-ad86-5bb08b4acd9e", "node_type": "4", "metadata": {"page_label": "229", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "80f64707fd2cdfc776e01bc34db38a8290f39ebc0e7534580c4af3d43248a378", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. This is not ideal. If the TCP connection is interrupted, the transaction must be aborted. If the interruption\nhappens after the client has requested a commit but before the server acknowledges that the commit hap\u2010\npened, the client doesn\u2019t know whether the transaction was committed or not. To solve this issue, a transac\u2010\ntion manager can group operations by a unique transaction identifier that is not bound to a particular TCP\nconnection. We will return to this topic in \u201cThe End-to-End Argument for Databases\u201d on page 516.\nFigure 7-2. Violating isolation: one transaction reads another transaction\u2019s uncommit\u2010\nted writes (a \u201cdirty read\u201d).\nFigure 7-3 illustrates the need for atomicity: if an error occurs somewhere over the\ncourse of the transaction, the contents of the mailbox and the unread counter might\nbecome out of sync. In an atomic transaction, if the update to the counter fails, the\ntransaction is aborted and the inserted email is rolled back.\nFigure 7-3. Atomicity ensures that if an error occurs any prior writes from that transac\u2010\ntion are undone, to avoid an inconsistent state.\nMulti-object transactions require some way of determining which read and write\noperations belong to the same transaction. In relational databases, that is typically\ndone based on the client\u2019s TCP connection to the database server: on any particular\nconnection, everything between a BEGIN TRANSACTION and a COMMIT statement is\nconsidered to be part of the same transaction.iii\nThe Slippery Concept of a Transaction | 229", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1aa05958-52f8-4180-84ad-d857fb4dd308": {"__data__": {"id_": "1aa05958-52f8-4180-84ad-d857fb4dd308", "embedding": null, "metadata": {"page_label": "230", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a7c549f-524d-4575-9fcc-ef049f75fbe7", "node_type": "4", "metadata": {"page_label": "230", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f7be7416dc6dc76de9671653ec8deb0b01d47b088d717d2a7bb5e7051875a203", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. Strictly speaking, the term atomic increment uses the word atomic in the sense of multi-threaded pro\u2010\ngramming. In the context of ACID, it should actually be called isolated or serializable increment. But that\u2019s\ngetting nitpicky.\nOn the other hand, many nonrelational databases don\u2019t have such a way of grouping\noperations together. Even if there is a multi-object API (for example, a key-value\nstore may have a multi-put operation that updates several keys in one operation), that\ndoesn\u2019t necessarily mean it has transaction semantics: the command may succeed for\nsome keys and fail for others, leaving the database in a partially updated state.\nSingle-object writes\nAtomicity and isolation also apply when a single object is being changed. For exam\u2010\nple, imagine you are writing a 20 KB JSON document to a database:\n\u2022 If the network connection is interrupted after the first 10 KB have been sent, does\nthe database store that unparseable 10 KB fragment of JSON?\n\u2022 If the power fails while the database is in the middle of overwriting the previous\nvalue on disk, do you end up with the old and new values spliced together?\n\u2022 If another client reads that document while the write is in progress, will it see a\npartially updated value?\nThose issues would be incredibly confusing, so storage engines almost universally\naim to provide atomicity and isolation on the level of a single object (such as a key-\nvalue pair) on one node. Atomicity can be implemented using a log for crash recov\u2010\nery (see \u201cMaking B-trees reliable\u201d on page 82), and isolation can be implemented\nusing a lock on each object (allowing only one thread to access an object at any one\ntime).\nSome databases also provide more complex atomic operations,iv such as an increment\noperation, which removes the need for a read-modify-write cycle like that in\nFigure 7-1. Similarly popular is a compare-and-set operation, which allows a write to\nhappen only if the value has not been concurrently changed by someone else (see\n\u201cCompare-and-set\u201d on page 245).\nThese single-object operations are useful, as they can prevent lost updates when sev\u2010\neral clients try to write to the same object concurrently (see \u201cPreventing Lost\nUpdates\u201d on page 242). However, they are not transactions in the usual sense of the\nword. Compare-and-set and other single-object operations have been dubbed \u201clight\u2010\nweight transactions\u201d or even \u201cACID\u201d for marketing purposes [ 20, 21, 22], but that\nterminology is misleading. A transaction is usually understood as a mechanism for\ngrouping multiple operations on multiple objects into one unit of execution.\n230 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2625, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d276dec9-25b1-410f-8b3c-4ebadbd2bfcd": {"__data__": {"id_": "d276dec9-25b1-410f-8b3c-4ebadbd2bfcd", "embedding": null, "metadata": {"page_label": "231", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc153b65-8fe2-4645-88a1-dcd4f44b9695", "node_type": "4", "metadata": {"page_label": "231", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7f120d32fdbf9ed7a7705d35338402dd8007870514cdd56819b7a9d321da1fee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The need for multi-object transactions\nMany distributed datastores have abandoned multi-object transactions because they\nare difficult to implement across partitions, and they can get in the way in some sce\u2010\nnarios where very high availability or performance is required. However, there is\nnothing that fundamentally prevents transactions in a distributed database, and we\nwill discuss implementations of distributed transactions in Chapter 9.\nBut do we need multi-object transactions at all? Would it be possible to implement\nany application with only a key-value data model and single-object operations?\nThere are some use cases in which single-object inserts, updates, and deletes are suffi\u2010\ncient. However, in many other cases writes to several different objects need to be\ncoordinated:\n\u2022 In a relational data model, a row in one table often has a foreign key reference to\na row in another table. (Similarly, in a graph-like data model, a vertex has edges\nto other vertices.) Multi-object transactions allow you to ensure that these refer\u2010\nences remain valid: when inserting several records that refer to one another, the\nforeign keys have to be correct and up to date, or the data becomes nonsensical.\n\u2022 In a document data model, the fields that need to be updated together are often\nwithin the same document, which is treated as a single object\u2014no multi-object\ntransactions are needed when updating a single document. However, document\ndatabases lacking join functionality also encourage denormalization (see \u201cRela\u2010\ntional Versus Document Databases Today\u201d on page 38). When denormalized\ninformation needs to be updated, like in the example of Figure 7-2, you need to\nupdate several documents in one go. Transactions are very useful in this situation\nto prevent denormalized data from going out of sync.\n\u2022 In databases with secondary indexes (almost everything except pure key-value\nstores), the indexes also need to be updated every time you change a value. These\nindexes are different database objects from a transaction point of view: for exam\u2010\nple, without transaction isolation, it\u2019s possible for a record to appear in one index\nbut not another, because the update to the second index hasn\u2019t happened yet.\nSuch applications can still be implemented without transactions. However, error han\u2010\ndling becomes much more complicated without atomicity, and the lack of isolation\ncan cause concurrency problems. We will discuss those in \u201cWeak Isolation Levels\u201d on\npage 233, and explore alternative approaches in Chapter 12.\nHandling errors and aborts\nA key feature of a transaction is that it can be aborted and safely retried if an error\noccurred. ACID databases are based on this philosophy: if the database is in danger\nThe Slippery Concept of a Transaction | 231", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1acd896-ee09-476d-b84c-cf7a59921b16": {"__data__": {"id_": "e1acd896-ee09-476d-b84c-cf7a59921b16", "embedding": null, "metadata": {"page_label": "232", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "439bd428-700d-4a3c-b961-ea2246b1e6b3", "node_type": "4", "metadata": {"page_label": "232", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ceabd31b6405626a6c3b74cbcc5108479ffeeceb3bf230ab6b1a9f9a2e1c7ba2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of violating its guarantee of atomicity, isolation, or durability, it would rather aban\u2010\ndon the transaction entirely than allow it to remain half-finished.\nNot all systems follow that philosophy, though. In particular, datastores with leader\u2010\nless replication (see \u201cLeaderless Replication\u201d on page 177) work much more on a\n\u201cbest effort\u201d basis, which could be summarized as \u201cthe database will do as much as it\ncan, and if it runs into an error, it won\u2019t undo something it has already done\u201d\u2014so it\u2019s\nthe application\u2019s responsibility to recover from errors.\nErrors will inevitably happen, but many software developers prefer to think only\nabout the happy path rather than the intricacies of error handling. For example, pop\u2010\nular object-relational mapping (ORM) frameworks such as Rails\u2019s ActiveRecord and\nDjango don\u2019t retry aborted transactions\u2014the error usually results in an exception\nbubbling up the stack, so any user input is thrown away and the user gets an error\nmessage. This is a shame, because the whole point of aborts is to enable safe retries.\nAlthough retrying an aborted transaction is a simple and effective error handling\nmechanism, it isn\u2019t perfect:\n\u2022 If the transaction actually succeeded, but the network failed while the server tried\nto acknowledge the successful commit to the client (so the client thinks it failed),\nthen retrying the transaction causes it to be performed twice\u2014unless you have an\nadditional application-level deduplication mechanism in place.\n\u2022 If the error is due to overload, retrying the transaction will make the problem\nworse, not better. To avoid such feedback cycles, you can limit the number of\nretries, use exponential backoff, and handle overload-related errors differently\nfrom other errors (if possible).\n\u2022 It is only worth retrying after transient errors (for example due to deadlock, iso\u2010\nlation violation, temporary network interruptions, and failover); after a perma\u2010\nnent error (e.g., constraint violation) a retry would be pointless.\n\u2022 If the transaction also has side effects outside of the database, those side effects\nmay happen even if the transaction is aborted. For example, if you\u2019re sending an\nemail, you wouldn\u2019t want to send the email again every time you retry the trans\u2010\naction. If you want to make sure that several different systems either commit or\nabort together, two-phase commit can help (we will discuss this in \u201cAtomic\nCommit and Two-Phase Commit (2PC)\u201d on page 354).\n\u2022 If the client process fails while retrying, any data it was trying to write to the\ndatabase is lost. \n232 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae9dbe8f-7a9e-426c-aa60-042e77642367": {"__data__": {"id_": "ae9dbe8f-7a9e-426c-aa60-042e77642367", "embedding": null, "metadata": {"page_label": "233", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "609dc07e-9863-456e-8249-5e6d274ad36c", "node_type": "4", "metadata": {"page_label": "233", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "83cdc4162f2a60f73f52c06671c36ca05c16b49b7c8e8b08a7b6418be570e15a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Weak Isolation Levels\nIf two transactions don\u2019t touch the same data, they can safely be run in parallel,\nbecause neither depends on the other. Concurrency issues (race conditions) only\ncome into play when one transaction reads data that is concurrently modified by\nanother transaction, or when two transactions try to simultaneously modify the same\ndata.\nConcurrency bugs are hard to find by testing, because such bugs are only triggered\nwhen you get unlucky with the timing. Such timing issues might occur very rarely,\nand are usually difficult to reproduce. Concurrency is also very difficult to reason\nabout, especially in a large application where you don\u2019t necessarily know which other\npieces of code are accessing the database. Application development is difficult\nenough if you just have one user at a time; having many concurrent users makes it\nmuch harder still, because any piece of data could unexpectedly change at any time.\nFor that reason, databases have long tried to hide concurrency issues from applica\u2010\ntion developers by providing transaction isolation. In theory, isolation should make\nyour life easier by letting you pretend that no concurrency is happening: serializable\nisolation means that the database guarantees that transactions have the same effect as\nif they ran serially (i.e., one at a time, without any concurrency).\nIn practice, isolation is unfortunately not that simple. Serializable isolation has a per\u2010\nformance cost, and many databases don\u2019t want to pay that price [ 8]. It\u2019s therefore\ncommon for systems to use weaker levels of isolation, which protect against some\nconcurrency issues, but not all. Those levels of isolation are much harder to under\u2010\nstand, and they can lead to subtle bugs, but they are nevertheless used in practice\n[23].\nConcurrency bugs caused by weak transaction isolation are not just a theoretical\nproblem. They have caused substantial loss of money [ 24, 25], led to investigation by\nfinancial auditors [ 26], and caused customer data to be corrupted [ 27]. A popular\ncomment on revelations of such problems is \u201cUse an ACID database if you\u2019re han\u2010\ndling financial data!\u201d\u2014but that misses the point. Even many popular relational data\u2010\nbase systems (which are usually considered \u201cACID\u201d) use weak isolation, so they\nwouldn\u2019t necessarily have prevented these bugs from occurring.\nRather than blindly relying on tools, we need to develop a good understanding of the\nkinds of concurrency problems that exist, and how to prevent them. Then we can\nbuild applications that are reliable and correct, using the tools at our disposal.\nIn this section we will look at several weak (nonserializable) isolation levels that are\nused in practice, and discuss in detail what kinds of race conditions can and cannot\noccur, so that you can decide what level is appropriate to your application. Once\nwe\u2019ve done that, we will discuss serializability in detail (see \u201cSerializability\u201d on page\nWeak Isolation Levels | 233", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbc44fc3-af1b-41f3-bf69-f7a4f75e8d04": {"__data__": {"id_": "dbc44fc3-af1b-41f3-bf69-f7a4f75e8d04", "embedding": null, "metadata": {"page_label": "234", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bb75d5e-fa94-47d1-b1fc-6778db6f4dcd", "node_type": "4", "metadata": {"page_label": "234", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9beaf4a107b1132cad7f83fd883aee0138dfd34141d2324027f40b6572f35a12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "v. Some databases support an even weaker isolation level called read uncommitted. It prevents dirty writes,\nbut does not prevent dirty reads.\n251). Our discussion of isolation levels will be informal, using examples. If you want\nrigorous definitions and analyses of their properties, you can find them in the aca\u2010\ndemic literature [28, 29, 30].\nRead Committed\nThe most basic level of transaction isolation is read committed.v It makes two guaran\u2010\ntees:\n1. When reading from the database, you will only see data that has been committed\n(no dirty reads).\n2. When writing to the database, you will only overwrite data that has been com\u2010\nmitted (no dirty writes).\nLet\u2019s discuss these two guarantees in more detail.\nNo dirty reads\nImagine a transaction has written some data to the database, but the transaction has\nnot yet committed or aborted. Can another transaction see that uncommitted data? If\nyes, that is called a dirty read [2].\nTransactions running at the read committed isolation level must prevent dirty reads.\nThis means that any writes by a transaction only become visible to others when that\ntransaction commits (and then all of its writes become visible at once). This is illus\u2010\ntrated in Figure 7-4, where user 1 has set x = 3, but user 2\u2019s get x still returns the old\nvalue, 2, while user 1 has not yet committed.\nFigure 7-4. No dirty reads: user 2 sees the new value for x only after user 1\u2019s transaction\nhas committed.\n234 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a47d7bf-0fba-4daa-beab-5f5ebfe5e4b1": {"__data__": {"id_": "4a47d7bf-0fba-4daa-beab-5f5ebfe5e4b1", "embedding": null, "metadata": {"page_label": "235", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc8b4ab0-8511-4f24-a921-7cbf29a03d9f", "node_type": "4", "metadata": {"page_label": "235", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4e1bfd0cb880189e16b4f30bc49b2e712b9bcd6611627101195fd43b85eb4f71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are a few reasons why it\u2019s useful to prevent dirty reads:\n\u2022 If a transaction needs to update several objects, a dirty read means that another\ntransaction may see some of the updates but not others. For example, in\nFigure 7-2, the user sees the new unread email but not the updated counter. This\nis a dirty read of the email. Seeing the database in a partially updated state is con\u2010\nfusing to users and may cause other transactions to take incorrect decisions.\n\u2022 If a transaction aborts, any writes it has made need to be rolled back (like in\nFigure 7-3). If the database allows dirty reads, that means a transaction may see\ndata that is later rolled back\u2014i.e., which is never actually committed to the data\u2010\nbase. Reasoning about the consequences quickly becomes mind-bending.\nNo dirty writes\nWhat happens if two transactions concurrently try to update the same object in a\ndatabase? We don\u2019t know in which order the writes will happen, but we normally\nassume that the later write overwrites the earlier write.\nHowever, what happens if the earlier write is part of a transaction that has not yet\ncommitted, so the later write overwrites an uncommitted value? This is called a dirty\nwrite [28]. Transactions running at the read committed isolation level must prevent\ndirty writes, usually by delaying the second write until the first write\u2019s transaction has\ncommitted or aborted.\nBy preventing dirty writes, this isolation level avoids some kinds of concurrency\nproblems:\n\u2022 If transactions update multiple objects, dirty writes can lead to a bad outcome.\nFor example, consider Figure 7-5 , which illustrates a used car sales website on\nwhich two people, Alice and Bob, are simultaneously trying to buy the same car.\nBuying a car requires two database writes: the listing on the website needs to be\nupdated to reflect the buyer, and the sales invoice needs to be sent to the buyer.\nIn the case of Figure 7-5 , the sale is awarded to Bob (because he performs the\nwinning update to the listings table), but the invoice is sent to Alice (because\nshe performs the winning update to the invoices table). Read committed pre\u2010\nvents such mishaps.\n\u2022 However, read committed does not prevent the race condition between two\ncounter increments in Figure 7-1. In this case, the second write happens after the\nfirst transaction has committed, so it\u2019s not a dirty write. It\u2019s still incorrect, but for\na different reason\u2014in \u201cPreventing Lost Updates\u201d on page 242 we will discuss how\nto make such counter increments safe.\nWeak Isolation Levels | 235", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e098c751-b2f4-4cf2-a488-789e07a1c972": {"__data__": {"id_": "e098c751-b2f4-4cf2-a488-789e07a1c972", "embedding": null, "metadata": {"page_label": "236", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4ab655f-0313-44ca-ad7c-a45e7c2cf1e0", "node_type": "4", "metadata": {"page_label": "236", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "98b6d9c7c4030d63858f3eda6efa286a81b9f9c25cbddaf13b03249cf6091e01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 7-5. With dirty writes, conflicting writes from different transactions can be\nmixed up.\nImplementing read committed\nRead committed is a very popular isolation level. It is the default setting in Oracle\n11g, PostgreSQL, SQL Server 2012, MemSQL, and many other databases [8].\nMost commonly, databases prevent dirty writes by using row-level locks: when a\ntransaction wants to modify a particular object (row or document), it must first\nacquire a lock on that object. It must then hold that lock until the transaction is com\u2010\nmitted or aborted. Only one transaction can hold the lock for any given object; if\nanother transaction wants to write to the same object, it must wait until the first\ntransaction is committed or aborted before it can acquire the lock and continue. This\nlocking is done automatically by databases in read committed mode (or stronger iso\u2010\nlation levels).\nHow do we prevent dirty reads? One option would be to use the same lock, and to\nrequire any transaction that wants to read an object to briefly acquire the lock and\nthen release it again immediately after reading. This would ensure that a read\ncouldn\u2019t happen while an object has a dirty, uncommitted value (because during that\ntime the lock would be held by the transaction that has made the write).\nHowever, the approach of requiring read locks does not work well in practice,\nbecause one long-running write transaction can force many read-only transactions to\nwait until the long-running transaction has completed. This harms the response time\nof read-only transactions and is bad for operability: a slowdown in one part of an\napplication can have a knock-on effect in a completely different part of the applica\u2010\ntion, due to waiting for locks.\n236 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95858d7e-43f8-4e2d-954c-18862952ba72": {"__data__": {"id_": "95858d7e-43f8-4e2d-954c-18862952ba72", "embedding": null, "metadata": {"page_label": "237", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e8d4c61-96a6-4108-b38e-c57469a5e0d2", "node_type": "4", "metadata": {"page_label": "237", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "eaf182657d6bb575ae53aac74166bc20fb07f6e7887f3f2f104a12070433956d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vi. At the time of writing, the only mainstream databases that use locks for read committed isolation are IBM\nDB2 and Microsoft SQL Server in the read_committed_snapshot=off configuration [23, 36].\nFor that reason, most databasesvi prevent dirty reads using the approach illustrated in\nFigure 7-4: for every object that is written, the database remembers both the old com\u2010\nmitted value and the new value set by the transaction that currently holds the write\nlock. While the transaction is ongoing, any other transactions that read the object are\nsimply given the old value. Only when the new value is committed do transactions\nswitch over to reading the new value. \nSnapshot Isolation and Repeatable Read\nIf you look superficially at read committed isolation, you could be forgiven for think\u2010\ning that it does everything that a transaction needs to do: it allows aborts (required\nfor atomicity), it prevents reading the incomplete results of transactions, and it pre\u2010\nvents concurrent writes from getting intermingled. Indeed, those are useful features,\nand much stronger guarantees than you can get from a system that has no transac\u2010\ntions.\nHowever, there are still plenty of ways in which you can have concurrency bugs when\nusing this isolation level. For example, Figure 7-6 illustrates a problem that can occur\nwith read committed.\nFigure 7-6. Read skew: Alice observes the database in an inconsistent state.\nSay Alice has $1,000 of savings at a bank, split across two accounts with $500 each.\nNow a transaction transfers $100 from one of her accounts to the other. If she is\nunlucky enough to look at her list of account balances in the same moment as that\ntransaction is being processed, she may see one account balance at a time before the\nWeak Isolation Levels | 237", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2b0bbdc-8a96-4162-8786-a4591581bac0": {"__data__": {"id_": "f2b0bbdc-8a96-4162-8786-a4591581bac0", "embedding": null, "metadata": {"page_label": "238", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "344fb342-dcd1-41fc-8635-c0b301550fa2", "node_type": "4", "metadata": {"page_label": "238", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5dc36f501f646001fb88784aa476f54566ed82fa4d92b25f5a6331e3939a7132", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "incoming payment has arrived (with a balance of $500), and the other account after\nthe outgoing transfer has been made (the new balance being $400). To Alice it now\nappears as though she only has a total of $900 in her accounts\u2014it seems that $100 has\nvanished into thin air.\nThis anomaly is called a nonrepeatable read  or read skew : if Alice were to read the\nbalance of account 1 again at the end of the transaction, she would see a different\nvalue ($600) than she saw in her previous query. Read skew is considered acceptable\nunder read committed isolation: the account balances that Alice saw were indeed\ncommitted at the time when she read them.\nThe term skew is unfortunately overloaded: we previously used it in\nthe sense of an unbalanced workload with hot spots  (see \u201cSkewed\nWorkloads and Relieving Hot Spots\u201d on page 205), whereas here it\nmeans timing anomaly.\nIn Alice\u2019s case, this is not a lasting problem, because she will most likely see consis\u2010\ntent account balances if she reloads the online banking website a few seconds later.\nHowever, some situations cannot tolerate such temporary inconsistency:\nBackups\nTaking a backup requires making a copy of the entire database, which may take\nhours on a large database. During the time that the backup process is running,\nwrites will continue to be made to the database. Thus, you could end up with\nsome parts of the backup containing an older version of the data, and other parts\ncontaining a newer version. If you need to restore from such a backup, the\ninconsistencies (such as disappearing money) become permanent.\nAnalytic queries and integrity checks\nSometimes, you may want to run a query that scans over large parts of the data\u2010\nbase. Such queries are common in analytics (see \u201cTransaction Processing or Ana\u2010\nlytics?\u201d on page 90), or may be part of a periodic integrity check that everything\nis in order (monitoring for data corruption). These queries are likely to return\nnonsensical results if they observe parts of the database at different points in\ntime. \nSnapshot isolation [28] is the most common solution to this problem. The idea is that\neach transaction reads from a consistent snapshot of the database\u2014that is, the trans\u2010\naction sees all the data that was committed in the database at the start of the transac\u2010\ntion. Even if the data is subsequently changed by another transaction, each\ntransaction sees only the old data from that particular point in time.\nSnapshot isolation is a boon for long-running, read-only queries such as backups and\nanalytics. It is very hard to reason about the meaning of a query if the data on which\n238 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d022fd9-104a-440c-ad86-b091a596086e": {"__data__": {"id_": "8d022fd9-104a-440c-ad86-b091a596086e", "embedding": null, "metadata": {"page_label": "239", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "faff5807-f23f-4e13-ab66-53e25c1bb2df", "node_type": "4", "metadata": {"page_label": "239", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bb615fced979830a99ab90c4f46cd3445beeebe1d8f85ce0d64b2bf89471f0df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vii. To be precise, transaction IDs are 32-bit integers, so they overflow after approximately 4 billion transac\u2010\ntions. PostgreSQL\u2019s vacuum process performs cleanup which ensures that overflow does not affect the data.\nit operates is changing at the same time as the query is executing. When a transaction\ncan see a consistent snapshot of the database, frozen at a particular point in time, it is\nmuch easier to understand.\nSnapshot isolation is a popular feature: it is supported by PostgreSQL, MySQL with\nthe InnoDB storage engine, Oracle, SQL Server, and others [23, 31, 32].\nImplementing snapshot isolation\nLike read committed isolation, implementations of snapshot isolation typically use\nwrite locks to prevent dirty writes (see \u201cImplementing read committed\u201d on page 236),\nwhich means that a transaction that makes a write can block the progress of another\ntransaction that writes to the same object. However, reads do not require any locks.\nFrom a performance point of view, a key principle of snapshot isolation is readers\nnever block writers, and writers never block readers . This allows a database to handle\nlong-running read queries on a consistent snapshot at the same time as processing\nwrites normally, without any lock contention between the two.\nTo implement snapshot isolation, databases use a generalization of the mechanism\nwe saw for preventing dirty reads in Figure 7-4. The database must potentially keep\nseveral different committed versions of an object, because various in-progress trans\u2010\nactions may need to see the state of the database at different points in time. Because it\nmaintains several versions of an object side by side, this technique is known as multi-\nversion concurrency control (MVCC).\nIf a database only needed to provide read committed isolation, but not snapshot iso\u2010\nlation, it would be sufficient to keep two versions of an object: the committed version\nand the overwritten-but-not-yet-committed version. However, storage engines that\nsupport snapshot isolation typically use MVCC for their read committed isolation\nlevel as well. A typical approach is that read committed uses a separate snapshot for\neach query, while snapshot isolation uses the same snapshot for an entire transaction.\nFigure 7-7 illustrates how MVCC-based snapshot isolation is implemented in Post\u2010\ngreSQL [31] (other implementations are similar). When a transaction is started, it is\ngiven a unique, always-increasing vii transaction ID ( txid). Whenever a transaction\nwrites anything to the database, the data it writes is tagged with the transaction ID of\nthe writer.\nWeak Isolation Levels | 239", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b76295c3-ef69-4aa7-a5d8-a2c2e30941f6": {"__data__": {"id_": "b76295c3-ef69-4aa7-a5d8-a2c2e30941f6", "embedding": null, "metadata": {"page_label": "240", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "faa21e24-2335-4321-8210-7f4d58b666b1", "node_type": "4", "metadata": {"page_label": "240", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4ef34af096aff8ac61fb8e09d8cf0eda3db07b0598b07c511081b15ed98c781b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 7-7. Implementing snapshot isolation using multi-version objects.\nEach row in a table has a created_by field, containing the ID of the transaction that\ninserted this row into the table. Moreover, each row has a deleted_by field, which is\ninitially empty. If a transaction deletes a row, the row isn\u2019t actually deleted from the\ndatabase, but it is marked for deletion by setting the deleted_by field to the ID of the\ntransaction that requested the deletion. At some later time, when it is certain that no\ntransaction can any longer access the deleted data, a garbage collection process in the\ndatabase removes any rows marked for deletion and frees their space.\nAn update is internally translated into a delete and a create. For example, in\nFigure 7-7, transaction 13 deducts $100 from account 2, changing the balance from\n$500 to $400. The accounts table now actually contains two rows for account 2: a\nrow with a balance of $500 which was marked as deleted by transaction 13, and a row\nwith a balance of $400 which was created by transaction 13.\nVisibility rules for observing a consistent snapshot\nWhen a transaction reads from the database, transaction IDs are used to decide\nwhich objects it can see and which are invisible. By carefully defining visibility rules,\n240 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1305, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00452fac-97c6-4f8e-8310-76792f535037": {"__data__": {"id_": "00452fac-97c6-4f8e-8310-76792f535037", "embedding": null, "metadata": {"page_label": "241", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c0350b0-7685-43e6-bcd0-765f0a95c059", "node_type": "4", "metadata": {"page_label": "241", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8d7725d76427a0956ad6772c4872dcc809255a60262f68bf5b0730b6c7012dbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the database can present a consistent snapshot of the database to the application. This\nworks as follows:\n1. At the start of each transaction, the database makes a list of all the other transac\u2010\ntions that are in progress (not yet committed or aborted) at that time. Any writes\nthat those transactions have made are ignored, even if the transactions subse\u2010\nquently commit.\n2. Any writes made by aborted transactions are ignored.\n3. Any writes made by transactions with a later transaction ID (i.e., which started\nafter the current transaction started) are ignored, regardless of whether those\ntransactions have committed.\n4. All other writes are visible to the application\u2019s queries.\nThese rules apply to both creation and deletion of objects. In Figure 7-7, when trans\u2010\naction 12 reads from account 2, it sees a balance of $500 because the deletion of the\n$500 balance was made by transaction 13 (according to rule 3, transaction 12 cannot\nsee a deletion made by transaction 13), and the creation of the $400 balance is not yet\nvisible (by the same rule).\nPut another way, an object is visible if both of the following conditions are true:\n\u2022 At the time when the reader\u2019s transaction started, the transaction that created the\nobject had already committed.\n\u2022 The object is not marked for deletion, or if it is, the transaction that requested\ndeletion had not yet committed at the time when the reader\u2019s transaction started.\nA long-running transaction may continue using a snapshot for a long time, continu\u2010\ning to read values that (from other transactions\u2019 point of view) have long been over\u2010\nwritten or deleted. By never updating values in place but instead creating a new\nversion every time a value is changed, the database can provide a consistent snapshot\nwhile incurring only a small overhead.\nIndexes and snapshot isolation\nHow do indexes work in a multi-version database? One option is to have the index\nsimply point to all versions of an object and require an index query to filter out any\nobject versions that are not visible to the current transaction. When garbage collec\u2010\ntion removes old object versions that are no longer visible to any transaction, the cor\u2010\nresponding index entries can also be removed.\nIn practice, many implementation details determine the performance of multi-\nversion concurrency control. For example, PostgreSQL has optimizations for avoid\u2010\ning index updates if different versions of the same object can fit on the same page\n[31].\nWeak Isolation Levels | 241", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d94fde1a-35ef-423a-88a9-6ca42a583763": {"__data__": {"id_": "d94fde1a-35ef-423a-88a9-6ca42a583763", "embedding": null, "metadata": {"page_label": "242", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54c98d2f-258a-4d13-a0cd-a7f9fb29d5e7", "node_type": "4", "metadata": {"page_label": "242", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b03b583859eea4e9e0d15121925344684cd181afab42bfc6a70827cbaae67680", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another approach is used in CouchDB, Datomic, and LMDB. Although they also use\nB-trees (see \u201cB-Trees\u201d on page 79), they use an append-only/copy-on-write variant\nthat does not overwrite pages of the tree when they are updated, but instead creates a\nnew copy of each modified page. Parent pages, up to the root of the tree, are copied\nand updated to point to the new versions of their child pages. Any pages that are not\naffected by a write do not need to be copied, and remain immutable [33, 34, 35].\nWith append-only B-trees, every write transaction (or batch of transactions) creates a\nnew B-tree root, and a particular root is a consistent snapshot of the database at the\npoint in time when it was created. There is no need to filter out objects based on\ntransaction IDs because subsequent writes cannot modify an existing B-tree; they can\nonly create new tree roots. However, this approach also requires a background pro\u2010\ncess for compaction and garbage collection.\nRepeatable read and naming confusion\nSnapshot isolation is a useful isolation level, especially for read-only transactions.\nHowever, many databases that implement it call it by different names. In Oracle it is\ncalled serializable, and in PostgreSQL and MySQL it is called repeatable read [23].\nThe reason for this naming confusion is that the SQL standard doesn\u2019t have the con\u2010\ncept of snapshot isolation, because the standard is based on System R\u2019s 1975 defini\u2010\ntion of isolation levels [ 2] and snapshot isolation hadn\u2019t yet been invented then.\nInstead, it defines repeatable read, which looks superficially similar to snapshot isola\u2010\ntion. PostgreSQL and MySQL call their snapshot isolation level repeatable read\nbecause it meets the requirements of the standard, and so they can claim standards\ncompliance.\nUnfortunately, the SQL standard\u2019s definition of isolation levels is flawed\u2014it is ambig\u2010\nuous, imprecise, and not as implementation-independent as a standard should be\n[28]. Even though several databases implement repeatable read, there are big differ\u2010\nences in the guarantees they actually provide, despite being ostensibly standardized\n[23]. There has been a formal definition of repeatable read in the research literature\n[29, 30], but most implementations don\u2019t satisfy that formal definition. And to top it\noff, IBM DB2 uses \u201crepeatable read\u201d to refer to serializability [8].\nAs a result, nobody really knows what repeatable read means. \nPreventing Lost Updates\nThe read committed and snapshot isolation levels we\u2019ve discussed so far have been\nprimarily about the guarantees of what a read-only transaction can see in the pres\u2010\nence of concurrent writes. We have mostly ignored the issue of two transactions writ\u2010\ning concurrently\u2014we have only discussed dirty writes (see \u201cNo dirty writes\u201d on page\n235), one particular type of write-write conflict that can occur.\n242 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe2931b8-7128-456f-91d3-13dac568976c": {"__data__": {"id_": "fe2931b8-7128-456f-91d3-13dac568976c", "embedding": null, "metadata": {"page_label": "243", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17c8aff2-c905-4674-98cd-f21304fe72a4", "node_type": "4", "metadata": {"page_label": "243", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9038c941f8b7faeff86d318994de6e277cdc75f5ee4f5b65782bf0c75619c414", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "viii. It is possible, albeit fairly complicated, to express the editing of a text document as a stream of atomic\nmutations. See \u201cAutomatic Conflict Resolution\u201d on page 174 for some pointers.\nThere are several other interesting kinds of conflicts that can occur between concur\u2010\nrently writing transactions. The best known of these is the lost update problem, illus\u2010\ntrated in Figure 7-1 with the example of two concurrent counter increments.\nThe lost update problem can occur if an application reads some value from the data\u2010\nbase, modifies it, and writes back the modified value (a read-modify-write cycle ). If\ntwo transactions do this concurrently, one of the modifications can be lost, because\nthe second write does not include the first modification. (We sometimes say that the\nlater write clobbers the earlier write.) This pattern occurs in various different\nscenarios:\n\u2022 Incrementing a counter or updating an account balance (requires reading the\ncurrent value, calculating the new value, and writing back the updated value)\n\u2022 Making a local change to a complex value, e.g., adding an element to a list within\na JSON document (requires parsing the document, making the change, and writ\u2010\ning back the modified document)\n\u2022 Two users editing a wiki page at the same time, where each user saves their\nchanges by sending the entire page contents to the server, overwriting whatever\nis currently in the database\nBecause this is such a common problem, a variety of solutions have been developed.\nAtomic write operations\nMany databases provide atomic update operations, which remove the need to imple\u2010\nment read-modify-write cycles in application code. They are usually the best solution\nif your code can be expressed in terms of those operations. For example, the follow\u2010\ning instruction is concurrency-safe in most relational databases:\nUPDATE counters SET value = value + 1 WHERE key = 'foo';\nSimilarly, document databases such as MongoDB provide atomic operations for\nmaking local modifications to a part of a JSON document, and Redis provides atomic\noperations for modifying data structures such as priority queues. Not all writes can\neasily be expressed in terms of atomic operations\u2014for example, updates to a wiki\npage involve arbitrary text editing viii\u2014but in situations where atomic operations can\nbe used, they are usually the best choice.\nAtomic operations are usually implemented by taking an exclusive lock on the object\nwhen it is read so that no other transaction can read it until the update has been\nWeak Isolation Levels | 243", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2543, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4829a8b8-3d2a-4292-a4bb-c5724289c1a3": {"__data__": {"id_": "4829a8b8-3d2a-4292-a4bb-c5724289c1a3", "embedding": null, "metadata": {"page_label": "244", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37142b41-f8f0-4d73-beff-2c98ebb4e9b2", "node_type": "4", "metadata": {"page_label": "244", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "819c11314edd7d2788306d3538c28e57eb4623220f463b31ceb07bf7076a3572", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "applied. This technique is sometimes known as cursor stability  [36, 37]. Another\noption is to simply force all atomic operations to be executed on a single thread.\nUnfortunately, object-relational mapping frameworks make it easy to accidentally\nwrite code that performs unsafe read-modify-write cycles instead of using atomic\noperations provided by the database [38]. That\u2019s not a problem if you know what you\nare doing, but it is potentially a source of subtle bugs that are difficult to find by\ntesting.\nExplicit locking\nAnother option for preventing lost updates, if the database\u2019s built-in atomic opera\u2010\ntions don\u2019t provide the necessary functionality, is for the application to explicitly lock\nobjects that are going to be updated. Then the application can perform a read-\nmodify-write cycle, and if any other transaction tries to concurrently read the same\nobject, it is forced to wait until the first read-modify-write cycle has completed.\nFor example, consider a multiplayer game in which several players can move the\nsame figure concurrently. In this case, an atomic operation may not be sufficient,\nbecause the application also needs to ensure that a player\u2019s move abides by the rules\nof the game, which involves some logic that you cannot sensibly implement as a data\u2010\nbase query. Instead, you may use a lock to prevent two players from concurrently\nmoving the same piece, as illustrated in Example 7-1.\nExample 7-1. Explicitly locking rows to prevent lost updates\nBEGIN TRANSACTION;\nSELECT * FROM figures\n  WHERE name = 'robot' AND game_id = 222\n  FOR UPDATE; \n-- Check whether move is valid, then update the position\n-- of the piece that was returned by the previous SELECT.\nUPDATE figures SET position = 'c4' WHERE id = 1234;\nCOMMIT;\nThe FOR UPDATE clause indicates that the database should take a lock on all rows\nreturned by this query.\nThis works, but to get it right, you need to carefully think about your application\nlogic. It\u2019s easy to forget to add a necessary lock somewhere in the code, and thus\nintroduce a race condition.\n244 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2078, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21a90c2d-d3e8-4a24-adf1-4b8469c01d68": {"__data__": {"id_": "21a90c2d-d3e8-4a24-adf1-4b8469c01d68", "embedding": null, "metadata": {"page_label": "245", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f64c40cb-e6a8-4648-b6ec-2747dffc2df9", "node_type": "4", "metadata": {"page_label": "245", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f7d2e5d5aba91aebbf1f88e961822e072052ca05079f5c680873f5184cd1cca1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Automatically detecting lost updates\nAtomic operations and locks are ways of preventing lost updates by forcing the read-\nmodify-write cycles to happen sequentially. An alternative is to allow them to execute\nin parallel and, if the transaction manager detects a lost update, abort the transaction\nand force it to retry its read-modify-write cycle.\nAn advantage of this approach is that databases can perform this check efficiently in\nconjunction with snapshot isolation. Indeed, PostgreSQL\u2019s repeatable read, Oracle\u2019s\nserializable, and SQL Server\u2019s snapshot isolation levels automatically detect when a\nlost update has occurred and abort the offending transaction. However, MySQL/\nInnoDB\u2019s repeatable read does not detect lost updates [ 23]. Some authors [ 28, 30]\nargue that a database must prevent lost updates in order to qualify as providing snap\u2010\nshot isolation, so MySQL does not provide snapshot isolation under this definition.\nLost update detection is a great feature, because it doesn\u2019t require application code to\nuse any special database features\u2014you may forget to use a lock or an atomic opera\u2010\ntion and thus introduce a bug, but lost update detection happens automatically and is\nthus less error-prone.\nCompare-and-set\nIn databases that don\u2019t provide transactions, you sometimes find an atomic compare-\nand-set operation (previously mentioned in \u201cSingle-object writes\u201d on page 230). The\npurpose of this operation is to avoid lost updates by allowing an update to happen\nonly if the value has not changed since you last read it. If the current value does not\nmatch what you previously read, the update has no effect, and the read-modify-write\ncycle must be retried.\nFor example, to prevent two users concurrently updating the same wiki page, you\nmight try something like this, expecting the update to occur only if the content of the\npage hasn\u2019t changed since the user started editing it:\n-- This may or may not be safe, depending on the database implementation\nUPDATE wiki_pages SET content = 'new content'\n  WHERE id = 1234 AND content = 'old content';\nIf the content has changed and no longer matches 'old content', this update will\nhave no effect, so you need to check whether the update took effect and retry if neces\u2010\nsary. However, if the database allows the WHERE clause to read from an old snapshot,\nthis statement may not prevent lost updates, because the condition may be true even\nthough another concurrent write is occurring. Check whether your database\u2019s\ncompare-and-set operation is safe before relying on it.\nWeak Isolation Levels | 245", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f68c1d7-2890-4f89-a558-cc651cf5ffc4": {"__data__": {"id_": "3f68c1d7-2890-4f89-a558-cc651cf5ffc4", "embedding": null, "metadata": {"page_label": "246", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e97586c-d232-4cec-831c-c5e0892d54a4", "node_type": "4", "metadata": {"page_label": "246", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "08cd4a12b63e3e334e90b994ec53540d45236396b2b9f7d3b2c0f8b49c0317be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conflict resolution and replication\nIn replicated databases (see Chapter 5 ), preventing lost updates takes on another\ndimension: since they have copies of the data on multiple nodes, and the data can\npotentially be modified concurrently on different nodes, some additional steps need\nto be taken to prevent lost updates.\nLocks and compare-and-set operations assume that there is a single up-to-date copy\nof the data. However, databases with multi-leader or leaderless replication usually\nallow several writes to happen concurrently and replicate them asynchronously, so\nthey cannot guarantee that there is a single up-to-date copy of the data. Thus, techni\u2010\nques based on locks or compare-and-set do not apply in this context. (We will revisit\nthis issue in more detail in \u201cLinearizability\u201d on page 324.)\nInstead, as discussed in \u201cDetecting Concurrent Writes\u201d on page 184, a common\napproach in such replicated databases is to allow concurrent writes to create several\nconflicting versions of a value (also known as siblings), and to use application code or\nspecial data structures to resolve and merge these versions after the fact.\nAtomic operations can work well in a replicated context, especially if they are com\u2010\nmutative (i.e., you can apply them in a different order on different replicas, and still\nget the same result). For example, incrementing a counter or adding an element to a\nset are commutative operations. That is the idea behind Riak 2.0 datatypes, which\nprevent lost updates across replicas. When a value is concurrently updated by differ\u2010\nent clients, Riak automatically merges together the updates in such a way that no\nupdates are lost [39].\nOn the other hand, the last write wins (LWW) conflict resolution method is prone to\nlost updates, as discussed in \u201cLast write wins (discarding concurrent writes)\u201d  on page\n186. Unfortunately, LWW is the default in many replicated databases. \nWrite Skew and Phantoms\nIn the previous sections we saw dirty writes and lost updates, two kinds of race condi\u2010\ntions that can occur when different transactions concurrently try to write to the same\nobjects. In order to avoid data corruption, those race conditions need to be prevented\n\u2014either automatically by the database, or by manual safeguards such as using locks\nor atomic write operations.\nHowever, that is not the end of the list of potential race conditions that can occur\nbetween concurrent writes. In this section we will see some subtler examples of\nconflicts.\nTo begin, imagine this example: you are writing an application for doctors to manage\ntheir on-call shifts at a hospital. The hospital usually tries to have several doctors on\ncall at any one time, but it absolutely must have at least one doctor on call. Doctors\n246 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bee69d8-a2ff-44d7-9cae-e951fa0522c3": {"__data__": {"id_": "0bee69d8-a2ff-44d7-9cae-e951fa0522c3", "embedding": null, "metadata": {"page_label": "247", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0868aa4e-7f15-4586-bbca-f7524768c0bd", "node_type": "4", "metadata": {"page_label": "247", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "211a93cff0fd7ed1595fdbb6336d628df78b17af89df29f22b8ca84487d64aa7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "can give up their shifts (e.g., if they are sick themselves), provided that at least one\ncolleague remains on call in that shift [40, 41].\nNow imagine that Alice and Bob are the two on-call doctors for a particular shift.\nBoth are feeling unwell, so they both decide to request leave. Unfortunately, they\nhappen to click the button to go off call at approximately the same time. What hap\u2010\npens next is illustrated in Figure 7-8.\nFigure 7-8. Example of write skew causing an application bug.\nIn each transaction, your application first checks that two or more doctors are cur\u2010\nrently on call; if yes, it assumes it\u2019s safe for one doctor to go off call. Since the data\u2010\nbase is using snapshot isolation, both checks return 2, so both transactions proceed to\nthe next stage. Alice updates her own record to take herself off call, and Bob updates\nhis own record likewise. Both transactions commit, and now no doctor is on call.\nYour requirement of having at least one doctor on call has been violated.\nCharacterizing write skew\nThis anomaly is called write skew [28]. It is neither a dirty write nor a lost update,\nbecause the two transactions are updating two different objects (Alice\u2019s and Bob\u2019s on-\ncall records, respectively). It is less obvious that a conflict occurred here, but it\u2019s defi\u2010\nnitely a race condition: if the two transactions had run one after another, the second\nWeak Isolation Levels | 247", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f59a7c48-6005-4c00-947c-c86209f8c0e6": {"__data__": {"id_": "f59a7c48-6005-4c00-947c-c86209f8c0e6", "embedding": null, "metadata": {"page_label": "248", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5425a85e-b5ad-4dd6-953e-464a22a789ad", "node_type": "4", "metadata": {"page_label": "248", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "952a05a3012a26577742574126bbc933732bb82a56b99697ac9de1f624b36062", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "doctor would have been prevented from going off call. The anomalous behavior was\nonly possible because the transactions ran concurrently.\nYou can think of write skew as a generalization of the lost update problem. Write\nskew can occur if two transactions read the same objects, and then update some of\nthose objects (different transactions may update different objects). In the special case\nwhere different transactions update the same object, you get a dirty write or lost\nupdate anomaly (depending on the timing).\nWe saw that there are various different ways of preventing lost updates. With write\nskew, our options are more restricted:\n\u2022 Atomic single-object operations don\u2019t help, as multiple objects are involved.\n\u2022 The automatic detection of lost updates that you find in some implementations\nof snapshot isolation unfortunately doesn\u2019t help either: write skew is not auto\u2010\nmatically detected in PostgreSQL\u2019s repeatable read, MySQL/InnoDB\u2019s repeatable\nread, Oracle\u2019s serializable, or SQL Server\u2019s snapshot isolation level [ 23]. Auto\u2010\nmatically preventing write skew requires true serializable isolation (see \u201cSerializa\u2010\nbility\u201d on page 251).\n\u2022 Some databases allow you to configure constraints, which are then enforced by\nthe database (e.g., uniqueness, foreign key constraints, or restrictions on a partic\u2010\nular value). However, in order to specify that at least one doctor must be on call,\nyou would need a constraint that involves multiple objects. Most databases do\nnot have built-in support for such constraints, but you may be able to implement\nthem with triggers or materialized views, depending on the database [42].\n\u2022 If you can\u2019t use a serializable isolation level, the second-best option in this case is\nprobably to explicitly lock the rows that the transaction depends on. In the doc\u2010\ntors example, you could write something like the following:\nBEGIN TRANSACTION;\nSELECT * FROM doctors\n  WHERE on_call = true\n  AND shift_id = 1234 FOR UPDATE; \nUPDATE doctors\n  SET on_call = false\n  WHERE name = 'Alice'\n  AND shift_id = 1234;\nCOMMIT;\nAs before, FOR UPDATE tells the database to lock all rows returned by this\nquery.\n248 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2165, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54c3160d-3a0d-4b48-af46-83cddb434a3c": {"__data__": {"id_": "54c3160d-3a0d-4b48-af46-83cddb434a3c", "embedding": null, "metadata": {"page_label": "249", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d505de-fcd3-4b28-b6c5-c61c3059ccf7", "node_type": "4", "metadata": {"page_label": "249", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f81389549eba4a73975c51c65453aef42ea8d2bdbee5a524d44facd68b484cec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ix. In PostgreSQL you can do this more elegantly using range types, but they are not widely supported in\nother databases.\nMore examples of write skew\nWrite skew may seem like an esoteric issue at first, but once you\u2019re aware of it, you\nmay notice more situations in which it can occur. Here are some more examples:\nMeeting room booking system\nSay you want to enforce that there cannot be two bookings for the same meeting\nroom at the same time [ 43]. When someone wants to make a booking, you first\ncheck for any conflicting bookings (i.e., bookings for the same room with an\noverlapping time range), and if none are found, you create the meeting (see\nExample 7-2).ix\nExample 7-2. A meeting room booking system tries to avoid double-booking (not\nsafe under snapshot isolation)\nBEGIN TRANSACTION;\n-- Check for any existing bookings that overlap with the period of noon-1pm\nSELECT COUNT(*) FROM bookings\n  WHERE room_id = 123 AND\n    end_time > '2015-01-01 12:00' AND start_time < '2015-01-01 13:00';\n-- If the previous query returned zero:\nINSERT INTO bookings\n  (room_id, start_time, end_time, user_id)\n  VALUES (123, '2015-01-01 12:00', '2015-01-01 13:00', 666);\nCOMMIT;\nUnfortunately, snapshot isolation does not prevent another user from concur\u2010\nrently inserting a conflicting meeting. In order to guarantee you won\u2019t get sched\u2010\nuling conflicts, you once again need serializable isolation.\nMultiplayer game\nIn Example 7-1, we used a lock to prevent lost updates (that is, making sure that\ntwo players can\u2019t move the same figure at the same time). However, the lock\ndoesn\u2019t prevent players from moving two different figures to the same position\non the board or potentially making some other move that violates the rules of the\ngame. Depending on the kind of rule you are enforcing, you might be able to use\na unique constraint, but otherwise you\u2019re vulnerable to write skew.\nWeak Isolation Levels | 249", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df0c5bbe-7b64-42a7-a671-efcd977d61af": {"__data__": {"id_": "df0c5bbe-7b64-42a7-a671-efcd977d61af", "embedding": null, "metadata": {"page_label": "250", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f4c1cd9-a48c-49d5-a4cd-9da444550806", "node_type": "4", "metadata": {"page_label": "250", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f11895b255cc6a47f1f56b954e14e01b90c70cf9acaf50cb05e966da6bb45579", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Claiming a username\nOn a website where each user has a unique username, two users may try to create\naccounts with the same username at the same time. You may use a transaction to\ncheck whether a name is taken and, if not, create an account with that name.\nHowever, like in the previous examples, that is not safe under snapshot isolation.\nFortunately, a unique constraint is a simple solution here (the second transaction\nthat tries to register the username will be aborted due to violating the constraint).\nPreventing double-spending\nA service that allows users to spend money or points needs to check that a user\ndoesn\u2019t spend more than they have. You might implement this by inserting a ten\u2010\ntative spending item into a user\u2019s account, listing all the items in the account, and\nchecking that the sum is positive [ 44]. With write skew, it could happen that two\nspending items are inserted concurrently that together cause the balance to go\nnegative, but that neither transaction notices the other.\nPhantoms causing write skew\nAll of these examples follow a similar pattern:\n1. A SELECT query checks whether some requirement is satisfied by searching for\nrows that match some search condition (there are at least two doctors on call,\nthere are no existing bookings for that room at that time, the position on the\nboard doesn\u2019t already have another figure on it, the username isn\u2019t already taken,\nthere is still money in the account).\n2. Depending on the result of the first query, the application code decides how to\ncontinue (perhaps to go ahead with the operation, or perhaps to report an error\nto the user and abort).\n3. If the application decides to go ahead, it makes a write ( INSERT, UPDATE, or\nDELETE) to the database and commits the transaction.\nThe effect of this write changes the precondition of the decision of step 2. In\nother words, if you were to repeat the SELECT query from step 1 after commiting\nthe write, you would get a different result, because the write changed the set of\nrows matching the search condition (there is now one fewer doctor on call, the\nmeeting room is now booked for that time, the position on the board is now\ntaken by the figure that was moved, the username is now taken, there is now less\nmoney in the account).\nThe steps may occur in a different order. For example, you could first make the write,\nthen the SELECT query, and finally decide whether to abort or commit based on the\nresult of the query.\n250 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35627666-0443-4c6a-8fdb-924f311fc955": {"__data__": {"id_": "35627666-0443-4c6a-8fdb-924f311fc955", "embedding": null, "metadata": {"page_label": "251", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "692e4d1a-9030-4f99-97c5-d881fdd6131f", "node_type": "4", "metadata": {"page_label": "251", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "eb3e25b4deda47f0e5cc23a2b7a92194b48062929163292d80eaab609ce71adb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of the doctor on call example, the row being modified in step 3 was one of\nthe rows returned in step 1, so we could make the transaction safe and avoid write\nskew by locking the rows in step 1 ( SELECT FOR UPDATE). However, the other four\nexamples are different: they check for the absence of rows matching some search con\u2010\ndition, and the write adds a row matching the same condition. If the query in step 1\ndoesn\u2019t return any rows, SELECT FOR UPDATE can\u2019t attach locks to anything.\nThis effect, where a write in one transaction changes the result of a search query in\nanother transaction, is called a phantom [3]. Snapshot isolation avoids phantoms in\nread-only queries, but in read-write transactions like the examples we discussed,\nphantoms can lead to particularly tricky cases of write skew. \nMaterializing conflicts\nIf the problem of phantoms is that there is no object to which we can attach the locks,\nperhaps we can artificially introduce a lock object into the database?\nFor example, in the meeting room booking case you could imagine creating a table of\ntime slots and rooms. Each row in this table corresponds to a particular room for a\nparticular time period (say, 15 minutes). You create rows for all possible combina\u2010\ntions of rooms and time periods ahead of time, e.g. for the next six months.\nNow a transaction that wants to create a booking can lock ( SELECT FOR UPDATE) the\nrows in the table that correspond to the desired room and time period. After it has\nacquired the locks, it can check for overlapping bookings and insert a new booking as\nbefore. Note that the additional table isn\u2019t used to store information about the book\u2010\ning\u2014it\u2019s purely a collection of locks which is used to prevent bookings on the same\nroom and time range from being modified concurrently.\nThis approach is called materializing conflicts, because it takes a phantom and turns it\ninto a lock conflict on a concrete set of rows that exist in the database [ 11]. Unfortu\u2010\nnately, it can be hard and error-prone to figure out how to materialize conflicts, and\nit\u2019s ugly to let a concurrency control mechanism leak into the application data model.\nFor those reasons, materializing conflicts should be considered a last resort if no\nalternative is possible. A serializable isolation level is much preferable in most cases. \nSerializability\nIn this chapter we have seen several examples of transactions that are prone to race\nconditions. Some race conditions are prevented by the read committed and snapshot\nisolation levels, but others are not. We encountered some particularly tricky exam\u2010\nples with write skew and phantoms. It\u2019s a sad situation:\n\u2022 Isolation levels are hard to understand, and inconsistently implemented in differ\u2010\nent databases (e.g., the meaning of \u201crepeatable read\u201d varies significantly).\nSerializability | 251", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "938fcc53-d73f-46a3-96b1-4853fd35995f": {"__data__": {"id_": "938fcc53-d73f-46a3-96b1-4853fd35995f", "embedding": null, "metadata": {"page_label": "252", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "efd61919-c942-4d87-9e00-f944444079ef", "node_type": "4", "metadata": {"page_label": "252", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "241b502f6e36f7c869f751ec9aa4481c01f3b62a0af929aa67d5e4a0f8c30aaa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If you look at your application code, it\u2019s difficult to tell whether it is safe to run at\na particular isolation level\u2014especially in a large application, where you might not\nbe aware of all the things that may be happening concurrently.\n\u2022 There are no good tools to help us detect race conditions. In principle, static\nanalysis may help [26], but research techniques have not yet found their way into\npractical use. Testing for concurrency issues is hard, because they are usually\nnondeterministic\u2014problems only occur if you get unlucky with the timing.\nThis is not a new problem\u2014it has been like this since the 1970s, when weak isolation\nlevels were first introduced [ 2]. All along, the answer from researchers has been sim\u2010\nple: use serializable isolation!\nSerializable isolation is usually regarded as the strongest isolation level. It guarantees\nthat even though transactions may execute in parallel, the end result is the same as if\nthey had executed one at a time, serially, without any concurrency. Thus, the database\nguarantees that if the transactions behave correctly when run individually, they con\u2010\ntinue to be correct when run concurrently\u2014in other words, the database prevents all\npossible race conditions.\nBut if serializable isolation is so much better than the mess of weak isolation levels,\nthen why isn\u2019t everyone using it? To answer this question, we need to look at the\noptions for implementing serializability, and how they perform. Most databases that\nprovide serializability today use one of three techniques, which we will explore in the\nrest of this chapter:\n\u2022 Literally executing transactions in a serial order (see \u201cActual Serial Execution\u201d on\npage 252)\n\u2022 Two-phase locking (see \u201cTwo-Phase Locking (2PL)\u201d on page 257), which for sev\u2010\neral decades was the only viable option\n\u2022 Optimistic concurrency control techniques such as serializable snapshot isolation\n(see \u201cSerializable Snapshot Isolation (SSI)\u201d on page 261)\nFor now, we will discuss these techniques primarily in the context of single-node\ndatabases; in Chapter 9 we will examine how they can be generalized to transactions\nthat involve multiple nodes in a distributed system.\nActual Serial Execution\nThe simplest way of avoiding concurrency problems is to remove the concurrency\nentirely: to execute only one transaction at a time, in serial order, on a single thread.\nBy doing so, we completely sidestep the problem of detecting and preventing con\u2010\nflicts between transactions: the resulting isolation is by definition serializable.\n252 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e467cecd-0de3-44b5-8349-db9ded548315": {"__data__": {"id_": "e467cecd-0de3-44b5-8349-db9ded548315", "embedding": null, "metadata": {"page_label": "253", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26e77fce-5744-440a-839e-633e524ad6b9", "node_type": "4", "metadata": {"page_label": "253", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c036edfd3e7eb89eef4683b33415afc4b7b0be52f6111336f377717b40b4eeb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even though this seems like an obvious idea, database designers only fairly recently\u2014\naround 2007\u2014decided that a single-threaded loop for executing transactions was fea\u2010\nsible [ 45]. If multi-threaded concurrency was considered essential for getting good\nperformance during the previous 30 years, what changed to make single-threaded\nexecution possible?\nTwo developments caused this rethink:\n\u2022 RAM became cheap enough that for many use cases is now feasible to keep the\nentire active dataset in memory (see \u201cKeeping everything in memory\u201d on page\n88). When all data that a transaction needs to access is in memory, transactions\ncan execute much faster than if they have to wait for data to be loaded from disk.\n\u2022 Database designers realized that OLTP transactions are usually short and only\nmake a small number of reads and writes (see \u201cTransaction Processing or Ana\u2010\nlytics?\u201d on page 90). By contrast, long-running analytic queries are typically read-\nonly, so they can be run on a consistent snapshot (using snapshot isolation)\noutside of the serial execution loop.\nThe approach of executing transactions serially is implemented in VoltDB/H-Store,\nRedis, and Datomic [46, 47, 48]. A system designed for single-threaded execution can\nsometimes perform better than a system that supports concurrency, because it can\navoid the coordination overhead of locking. However, its throughput is limited to\nthat of a single CPU core. In order to make the most of that single thread, transac\u2010\ntions need to be structured differently from their traditional form.\nEncapsulating transactions in stored procedures\nIn the early days of databases, the intention was that a database transaction could\nencompass an entire flow of user activity. For example, booking an airline ticket is a\nmulti-stage process (searching for routes, fares, and available seats; deciding on an\nitinerary; booking seats on each of the flights of the itinerary; entering passenger\ndetails; making payment). Database designers thought that it would be neat if that\nentire process was one transaction so that it could be committed atomically.\nUnfortunately, humans are very slow to make up their minds and respond. If a data\u2010\nbase transaction needs to wait for input from a user, the database needs to support a\npotentially huge number of concurrent transactions, most of them idle. Most data\u2010\nbases cannot do that efficiently, and so almost all OLTP applications keep transac\u2010\ntions short by avoiding interactively waiting for a user within a transaction. On the\nweb, this means that a transaction is committed within the same HTTP request\u2014a\ntransaction does not span multiple requests. A new HTTP request starts a new trans\u2010\naction.\nEven though the human has been taken out of the critical path, transactions have\ncontinued to be executed in an interactive client/server style, one statement at a time.\nSerializability | 253", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f11fa67-2ca5-457d-bea1-fc701c92c0bc": {"__data__": {"id_": "1f11fa67-2ca5-457d-bea1-fc701c92c0bc", "embedding": null, "metadata": {"page_label": "254", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86643081-82a7-408c-ba32-a5e5badd9400", "node_type": "4", "metadata": {"page_label": "254", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ce6d6e0eb44bec283416bfb891901d53f50487925f2a6630caee286129d92bb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An application makes a query, reads the result, perhaps makes another query\ndepending on the result of the first query, and so on. The queries and results are sent\nback and forth between the application code (running on one machine) and the data\u2010\nbase server (on another machine).\nIn this interactive style of transaction, a lot of time is spent in network communica\u2010\ntion between the application and the database. If you were to disallow concurrency in\nthe database and only process one transaction at a time, the throughput would be\ndreadful because the database would spend most of its time waiting for the applica\u2010\ntion to issue the next query for the current transaction. In this kind of database, it\u2019s\nnecessary to process multiple transactions concurrently in order to get reasonable\nperformance.\nFor this reason, systems with single-threaded serial transaction processing don\u2019t\nallow interactive multi-statement transactions. Instead, the application must submit\nthe entire transaction code to the database ahead of time, as a stored procedure. The\ndifferences between these approaches is illustrated in Figure 7-9 . Provided that all\ndata required by a transaction is in memory, the stored procedure can execute very\nfast, without waiting for any network or disk I/O.\nFigure 7-9. The difference between an interactive transaction and a stored procedure\n(using the example transaction of Figure 7-8).\n254 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5165da8-806d-46f6-bdca-65328e624bf5": {"__data__": {"id_": "c5165da8-806d-46f6-bdca-65328e624bf5", "embedding": null, "metadata": {"page_label": "255", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aafe8ec4-cf51-4792-b20c-6d0042ba4a13", "node_type": "4", "metadata": {"page_label": "255", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1708513345821b13e99c673e275424ce4a9def31d3aaf02c9d0794ff3bb5e7b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pros and cons of stored procedures\nStored procedures have existed for some time in relational databases, and they have\nbeen part of the SQL standard (SQL/PSM) since 1999. They have gained a somewhat\nbad reputation, for various reasons:\n\u2022 Each database vendor has its own language for stored procedures (Oracle has PL/\nSQL, SQL Server has T-SQL, PostgreSQL has PL/pgSQL, etc.). These languages\nhaven\u2019t kept up with developments in general-purpose programming languages,\nso they look quite ugly and archaic from today\u2019s point of view, and they lack the\necosystem of libraries that you find with most programming languages.\n\u2022 Code running in a database is difficult to manage: compared to an application\nserver, it\u2019s harder to debug, more awkward to keep in version control and deploy,\ntrickier to test, and difficult to integrate with a metrics collection system for\nmonitoring.\n\u2022 A database is often much more performance-sensitive than an application server,\nbecause a single database instance is often shared by many application servers. A\nbadly written stored procedure (e.g., using a lot of memory or CPU time) in a\ndatabase can cause much more trouble than equivalent badly written code in an\napplication server.\nHowever, those issues can be overcome. Modern implementations of stored proce\u2010\ndures have abandoned PL/SQL and use existing general-purpose programming lan\u2010\nguages instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis\nuses Lua.\nWith stored procedures and in-memory data, executing all transactions on a single\nthread becomes feasible. As they don\u2019t need to wait for I/O and they avoid the over\u2010\nhead of other concurrency control mechanisms, they can achieve quite good\nthroughput on a single thread.\nVoltDB also uses stored procedures for replication: instead of copying a transaction\u2019s\nwrites from one node to another, it executes the same stored procedure on each rep\u2010\nlica. VoltDB therefore requires that stored procedures are deterministic (when run on\ndifferent nodes, they must produce the same result). If a transaction needs to use the\ncurrent date and time, for example, it must do so through special deterministic APIs. \nPartitioning\nExecuting all transactions serially makes concurrency control much simpler, but lim\u2010\nits the transaction throughput of the database to the speed of a single CPU core on a\nsingle machine. Read-only transactions may execute elsewhere, using snapshot isola\u2010\ntion, but for applications with high write throughput, the single-threaded transaction\nprocessor can become a serious bottleneck.\nSerializability | 255", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba431e41-e6cf-4b36-a38c-50b0c043c7fc": {"__data__": {"id_": "ba431e41-e6cf-4b36-a38c-50b0c043c7fc", "embedding": null, "metadata": {"page_label": "256", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffd9175d-2101-4323-a1cf-1487f936a32a", "node_type": "4", "metadata": {"page_label": "256", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a013368c9448e56feb897aa8b7ccc5b04409544bdd6bd9b01d50f62dec9de823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "x. If a transaction needs to access data that\u2019s not in memory, the best solution may be to abort the transac\u2010\ntion, asynchronously fetch the data into memory while continuing to process other transactions, and then\nrestart the transaction when the data has been loaded. This approach is known as anti-caching, as previously\nmentioned in \u201cKeeping everything in memory\u201d on page 88.\nIn order to scale to multiple CPU cores, and multiple nodes, you can potentially par\u2010\ntition your data (see Chapter 6), which is supported in VoltDB. If you can find a way\nof partitioning your dataset so that each transaction only needs to read and write data\nwithin a single partition, then each partition can have its own transaction processing\nthread running independently from the others. In this case, you can give each CPU\ncore its own partition, which allows your transaction throughput to scale linearly\nwith the number of CPU cores [47].\nHowever, for any transaction that needs to access multiple partitions, the database\nmust coordinate the transaction across all the partitions that it touches. The stored\nprocedure needs to be performed in lock-step across all partitions to ensure serializa\u2010\nbility across the whole system.\nSince cross-partition transactions have additional coordination overhead, they are\nvastly slower than single-partition transactions. VoltDB reports a throughput of\nabout 1,000 cross-partition writes per second, which is orders of magnitude below its\nsingle-partition throughput and cannot be increased by adding more machines [49].\nWhether transactions can be single-partition depends very much on the structure of\nthe data used by the application. Simple key-value data can often be partitioned very\neasily, but data with multiple secondary indexes is likely to require a lot of cross-\npartition coordination (see \u201cPartitioning and Secondary Indexes\u201d on page 206).\nSummary of serial execution\nSerial execution of transactions has become a viable way of achieving serializable iso\u2010\nlation within certain constraints:\n\u2022 Every transaction must be small and fast, because it takes only one slow transac\u2010\ntion to stall all transaction processing.\n\u2022 It is limited to use cases where the active dataset can fit in memory. Rarely\naccessed data could potentially be moved to disk, but if it needed to be accessed\nin a single-threaded transaction, the system would get very slow.x\n\u2022 Write throughput must be low enough to be handled on a single CPU core, or\nelse transactions need to be partitioned without requiring cross-partition coordi\u2010\nnation.\n\u2022 Cross-partition transactions are possible, but there is a hard limit to the extent to\nwhich they can be used. \n256 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2699, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82f2bae5-5b11-47b2-bbb4-41c14a938318": {"__data__": {"id_": "82f2bae5-5b11-47b2-bbb4-41c14a938318", "embedding": null, "metadata": {"page_label": "257", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43d79507-4bfd-4089-b9da-cfca60c33aa1", "node_type": "4", "metadata": {"page_label": "257", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3aa8b1a64952947fd2b0f5f6ffc196baee5a83ed6f4bd49283ce2a4efc8a404", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "xi. Sometimes called strong strict two-phase locking (SS2PL) to distinguish it from other variants of 2PL.\nTwo-Phase Locking (2PL)\nFor around 30 years, there was only one widely used algorithm for serializability in\ndatabases: two-phase locking (2PL).xi\n2PL is not 2PC\nNote that while two-phase locking (2PL) sounds very similar to\ntwo-phase commit (2PC), they are completely different things. We\nwill discuss 2PC in Chapter 9.\nWe saw previously that locks are often used to prevent dirty writes (see \u201cNo dirty\nwrites\u201d on page 235): if two transactions concurrently try to write to the same object,\nthe lock ensures that the second writer must wait until the first one has finished its\ntransaction (aborted or committed) before it may continue.\nTwo-phase locking is similar, but makes the lock requirements much stronger. Sev\u2010\neral transactions are allowed to concurrently read the same object as long as nobody\nis writing to it. But as soon as anyone wants to write (modify or delete) an object,\nexclusive access is required:\n\u2022 If transaction A has read an object and transaction B wants to write to that\nobject, B must wait until A commits or aborts before it can continue. (This\nensures that B can\u2019t change the object unexpectedly behind A\u2019s back.)\n\u2022 If transaction A has written an object and transaction B wants to read that object,\nB must wait until A commits or aborts before it can continue. (Reading an old\nversion of the object, like in Figure 7-1, is not acceptable under 2PL.)\nIn 2PL, writers don\u2019t just block other writers; they also block readers and vice versa.\nSnapshot isolation has the mantra readers never block writers, and writers never block\nreaders (see \u201cImplementing snapshot isolation\u201d on page 239), which captures this key\ndifference between snapshot isolation and two-phase locking. On the other hand,\nbecause 2PL provides serializability, it protects against all the race conditions dis\u2010\ncussed earlier, including lost updates and write skew.\nImplementation of two-phase locking\n2PL is used by the serializable isolation level in MySQL (InnoDB) and SQL Server,\nand the repeatable read isolation level in DB2 [23, 36].\nSerializability | 257", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2167, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71adcf2a-c930-43cf-8fbc-d9a834f0f6fb": {"__data__": {"id_": "71adcf2a-c930-43cf-8fbc-d9a834f0f6fb", "embedding": null, "metadata": {"page_label": "258", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72f75733-9d8e-4758-95ad-ef2bc7cfcab0", "node_type": "4", "metadata": {"page_label": "258", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8c7f76ba88486c2eae5143f083551d66f68364ff3069781b3b08a86453d1dfc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blocking of readers and writers is implemented by a having a lock on each object\nin the database. The lock can either be in shared mode or in exclusive mode. The lock\nis used as follows:\n\u2022 If a transaction wants to read an object, it must first acquire the lock in shared\nmode. Several transactions are allowed to hold the lock in shared mode simulta\u2010\nneously, but if another transaction already has an exclusive lock on the object,\nthese transactions must wait.\n\u2022 If a transaction wants to write to an object, it must first acquire the lock in exclu\u2010\nsive mode. No other transaction may hold the lock at the same time (either in\nshared or in exclusive mode), so if there is any existing lock on the object, the\ntransaction must wait.\n\u2022 If a transaction first reads and then writes an object, it may upgrade its shared\nlock to an exclusive lock. The upgrade works the same as getting an exclusive\nlock directly.\n\u2022 After a transaction has acquired the lock, it must continue to hold the lock until\nthe end of the transaction (commit or abort). This is where the name \u201ctwo-\nphase\u201d comes from: the first phase (while the transaction is executing) is when\nthe locks are acquired, and the second phase (at the end of the transaction) is\nwhen all the locks are released.\nSince so many locks are in use, it can happen quite easily that transaction A is stuck\nwaiting for transaction B to release its lock, and vice versa. This situation is called\ndeadlock. The database automatically detects deadlocks between transactions and\naborts one of them so that the others can make progress. The aborted transaction\nneeds to be retried by the application.\nPerformance of two-phase locking\nThe big downside of two-phase locking, and the reason why it hasn\u2019t been used by\neverybody since the 1970s, is performance: transaction throughput and response\ntimes of queries are significantly worse under two-phase locking than under weak\nisolation.\nThis is partly due to the overhead of acquiring and releasing all those locks, but more\nimportantly due to reduced concurrency. By design, if two concurrent transactions\ntry to do anything that may in any way result in a race condition, one has to wait for\nthe other to complete.\nTraditional relational databases don\u2019t limit the duration of a transaction, because\nthey are designed for interactive applications that wait for human input. Conse\u2010\nquently, when one transaction has to wait on another, there is no limit on how long it\nmay have to wait. Even if you make sure that you keep all your transactions short, a\n258 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82acb866-e5f8-4f75-988b-3b880b9630ff": {"__data__": {"id_": "82acb866-e5f8-4f75-988b-3b880b9630ff", "embedding": null, "metadata": {"page_label": "259", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "297a91aa-162b-478d-bd1b-0e14265aa459", "node_type": "4", "metadata": {"page_label": "259", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "78d786d43d1d65cdb5fe6597030c36e61c68c79f518f98eb7b040ef2753be9be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "queue may form if several transactions want to access the same object, so a transac\u2010\ntion may have to wait for several others to complete before it can do anything.\nFor this reason, databases running 2PL can have quite unstable latencies, and they\ncan be very slow at high percentiles (see \u201cDescribing Performance\u201d on page 13) if\nthere is contention in the workload. It may take just one slow transaction, or one\ntransaction that accesses a lot of data and acquires many locks, to cause the rest of the\nsystem to grind to a halt. This instability is problematic when robust operation is\nrequired.\nAlthough deadlocks can happen with the lock-based read committed isolation level,\nthey occur much more frequently under 2PL serializable isolation (depending on the\naccess patterns of your transaction). This can be an additional performance problem:\nwhen a transaction is aborted due to deadlock and is retried, it needs to do its work\nall over again. If deadlocks are frequent, this can mean significant wasted effort.\nPredicate locks\nIn the preceding description of locks, we glossed over a subtle but important detail.\nIn \u201cPhantoms causing write skew\u201d on page 250 we discussed the problem of phan\u2010\ntoms\u2014that is, one transaction changing the results of another transaction\u2019s search\nquery. A database with serializable isolation must prevent phantoms.\nIn the meeting room booking example this means that if one transaction has\nsearched for existing bookings for a room within a certain time window (see\nExample 7-2 ), another transaction is not allowed to concurrently insert or update\nanother booking for the same room and time range. (It\u2019s okay to concurrently insert\nbookings for other rooms, or for the same room at a different time that doesn\u2019t affect\nthe proposed booking.)\nHow do we implement this? Conceptually, we need a predicate lock [3]. It works sim\u2010\nilarly to the shared/exclusive lock described earlier, but rather than belonging to a\nparticular object (e.g., one row in a table), it belongs to all objects that match some\nsearch condition, such as:\nSELECT * FROM bookings\n  WHERE room_id = 123 AND\n    end_time   > '2018-01-01 12:00' AND\n    start_time < '2018-01-01 13:00';\nA predicate lock restricts access as follows:\n\u2022 If transaction A wants to read objects matching some condition, like in that\nSELECT query, it must acquire a shared-mode predicate lock on the conditions of\nthe query. If another transaction B currently has an exclusive lock on any object\nmatching those conditions, A must wait until B releases its lock before it is\nallowed to make its query.\nSerializability | 259", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d90d14d-3730-42a5-86d6-e3b8702dab5c": {"__data__": {"id_": "1d90d14d-3730-42a5-86d6-e3b8702dab5c", "embedding": null, "metadata": {"page_label": "260", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a458f7be-3ba2-4ac7-a214-36d7e556600f", "node_type": "4", "metadata": {"page_label": "260", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "de4e506cabc52a705922cd996b9c06583591be2e1fa3f3b1f117146a37007806", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If transaction A wants to insert, update, or delete any object, it must first check\nwhether either the old or the new value matches any existing predicate lock. If\nthere is a matching predicate lock held by transaction B, then A must wait until B\nhas committed or aborted before it can continue.\nThe key idea here is that a predicate lock applies even to objects that do not yet exist\nin the database, but which might be added in the future (phantoms). If two-phase\nlocking includes predicate locks, the database prevents all forms of write skew and\nother race conditions, and so its isolation becomes serializable.\nIndex-range locks\nUnfortunately, predicate locks do not perform well: if there are many locks by active\ntransactions, checking for matching locks becomes time-consuming. For that reason,\nmost databases with 2PL actually implement index-range locking (also known as next-\nkey locking), which is a simplified approximation of predicate locking [41, 50].\nIt\u2019s safe to simplify a predicate by making it match a greater set of objects. For exam\u2010\nple, if you have a predicate lock for bookings of room 123 between noon and 1 p.m.,\nyou can approximate it by locking bookings for room 123 at any time, or you can\napproximate it by locking all rooms (not just room 123) between noon and 1 p.m.\nThis is safe, because any write that matches the original predicate will definitely also\nmatch the approximations.\nIn the room bookings database you would probably have an index on the room_id\ncolumn, and/or indexes on start_time and end_time (otherwise the preceding query\nwould be very slow on a large database):\n\u2022 Say your index is on room_id, and the database uses this index to find existing\nbookings for room 123. Now the database can simply attach a shared lock to this\nindex entry, indicating that a transaction has searched for bookings of room 123.\n\u2022 Alternatively, if the database uses a time-based index to find existing bookings, it\ncan attach a shared lock to a range of values in that index, indicating that a trans\u2010\naction has searched for bookings that overlap with the time period of noon to 1\np.m. on January 1, 2018.\nEither way, an approximation of the search condition is attached to one of the\nindexes. Now, if another transaction wants to insert, update, or delete a booking for\nthe same room and/or an overlapping time period, it will have to update the same\npart of the index. In the process of doing so, it will encounter the shared lock, and it\nwill be forced to wait until the lock is released.\nThis provides effective protection against phantoms and write skew. Index-range\nlocks are not as precise as predicate locks would be (they may lock a bigger range of\n260 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72510c96-12a9-430c-87c4-9e59db09b427": {"__data__": {"id_": "72510c96-12a9-430c-87c4-9e59db09b427", "embedding": null, "metadata": {"page_label": "261", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cc3edc6-2603-4fdf-934a-f264119a3e9b", "node_type": "4", "metadata": {"page_label": "261", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0d03087d1979e538f0db495ebab91e851a5f54b254f16ecd913a691fcd2979c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "objects than is strictly necessary to maintain serializability), but since they have much\nlower overheads, they are a good compromise.\nIf there is no suitable index where a range lock can be attached, the database can fall\nback to a shared lock on the entire table. This will not be good for performance, since\nit will stop all other transactions writing to the table, but it\u2019s a safe fallback position. \nSerializable Snapshot Isolation (SSI)\nThis chapter has painted a bleak picture of concurrency control in databases. On the\none hand, we have implementations of serializability that don\u2019t perform well (two-\nphase locking) or don\u2019t scale well (serial execution). On the other hand, we have weak\nisolation levels that have good performance, but are prone to various race conditions\n(lost updates, write skew, phantoms, etc.). Are serializable isolation and good perfor\u2010\nmance fundamentally at odds with each other?\nPerhaps not: an algorithm called serializable snapshot isolation  (SSI) is very promis\u2010\ning. It provides full serializability, but has only a small performance penalty com\u2010\npared to snapshot isolation. SSI is fairly new: it was first described in 2008 [ 40] and is\nthe subject of Michael Cahill\u2019s PhD thesis [51].\nToday SSI is used both in single-node databases (the serializable isolation level in\nPostgreSQL since version 9.1 [ 41]) and distributed databases (FoundationDB uses a\nsimilar algorithm). As SSI is so young compared to other concurrency control mech\u2010\nanisms, it is still proving its performance in practice, but it has the possibility of being\nfast enough to become the new default in the future.\nPessimistic versus optimistic concurrency control\nTwo-phase locking is a so-called pessimistic concurrency control mechanism: it is\nbased on the principle that if anything might possibly go wrong (as indicated by a\nlock held by another transaction), it\u2019s better to wait until the situation is safe again\nbefore doing anything. It is like mutual exclusion, which is used to protect data struc\u2010\ntures in multi-threaded programming.\nSerial execution is, in a sense, pessimistic to the extreme: it is essentially equivalent to\neach transaction having an exclusive lock on the entire database (or one partition of\nthe database) for the duration of the transaction. We compensate for the pessimism\nby making each transaction very fast to execute, so it only needs to hold the \u201clock\u201d for\na short time.\nBy contrast, serializable snapshot isolation is an optimistic concurrency control tech\u2010\nnique. Optimistic in this context means that instead of blocking if something poten\u2010\ntially dangerous happens, transactions continue anyway, in the hope that everything\nwill turn out all right. When a transaction wants to commit, the database checks\nwhether anything bad happened (i.e., whether isolation was violated); if so, the trans\u2010\nSerializability | 261", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2870, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ed4b873-65a2-48dc-b542-f3045d6830a4": {"__data__": {"id_": "6ed4b873-65a2-48dc-b542-f3045d6830a4", "embedding": null, "metadata": {"page_label": "262", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90c905e1-5ea6-446c-b280-4061f7830663", "node_type": "4", "metadata": {"page_label": "262", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cbd2af0edcf8be19b5ff95477b088bd5eb00db9ea825b7a4a63ac03e6ec243ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "action is aborted and has to be retried. Only transactions that executed serializably\nare allowed to commit.\nOptimistic concurrency control is an old idea [ 52], and its advantages and disadvan\u2010\ntages have been debated for a long time [ 53]. It performs badly if there is high con\u2010\ntention (many transactions trying to access the same objects), as this leads to a high\nproportion of transactions needing to abort. If the system is already close to its maxi\u2010\nmum throughput, the additional transaction load from retried transactions can make\nperformance worse.\nHowever, if there is enough spare capacity, and if contention between transactions is\nnot too high, optimistic concurrency control techniques tend to perform better than\npessimistic ones. Contention can be reduced with commutative atomic operations:\nfor example, if several transactions concurrently want to increment a counter, it\ndoesn\u2019t matter in which order the increments are applied (as long as the counter isn\u2019t\nread in the same transaction), so the concurrent increments can all be applied\nwithout conflicting.\nAs the name suggests, SSI is based on snapshot isolation\u2014that is, all reads within a\ntransaction are made from a consistent snapshot of the database (see \u201cSnapshot Isola\u2010\ntion and Repeatable Read\u201d on page 237). This is the main difference compared to ear\u2010\nlier optimistic concurrency control techniques. On top of snapshot isolation, SSI adds\nan algorithm for detecting serialization conflicts among writes and determining\nwhich transactions to abort.\nDecisions based on an outdated premise\nWhen we previously discussed write skew in snapshot isolation (see \u201cWrite Skew and\nPhantoms\u201d on page 246), we observed a recurring pattern: a transaction reads some\ndata from the database, examines the result of the query, and decides to take some\naction (write to the database) based on the result that it saw. However, under snap\u2010\nshot isolation, the result from the original query may no longer be up-to-date by the\ntime the transaction commits, because the data may have been modified in the mean\u2010\ntime.\nPut another way, the transaction is taking an action based on a premise (a fact that\nwas true at the beginning of the transaction, e.g., \u201cThere are currently two doctors on\ncall\u201d). Later, when the transaction wants to commit, the original data may have\nchanged\u2014the premise may no longer be true.\nWhen the application makes a query (e.g., \u201cHow many doctors are currently on\ncall?\u201d), the database doesn\u2019t know how the application logic uses the result of that\nquery. To be safe, the database needs to assume that any change in the query result\n(the premise) means that writes in that transaction may be invalid. In other words,\nthere may be a causal dependency between the queries and the writes in the transac\u2010\ntion. In order to provide serializable isolation, the database must detect situations in\n262 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2906, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9a8f1fe-216d-44b2-a28d-3d14c6337ace": {"__data__": {"id_": "b9a8f1fe-216d-44b2-a28d-3d14c6337ace", "embedding": null, "metadata": {"page_label": "263", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6794b52d-e16b-472e-ac32-00d8f30f0870", "node_type": "4", "metadata": {"page_label": "263", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9bce37080b7d17c9d8e2073c73cfef5cde187bda70063575bda98a3a2af3768b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which a transaction may have acted on an outdated premise and abort the transac\u2010\ntion in that case.\nHow does the database know if a query result might have changed? There are two\ncases to consider:\n\u2022 Detecting reads of a stale MVCC object version (uncommitted write occurred\nbefore the read)\n\u2022 Detecting writes that affect prior reads (the write occurs after the read)\nDetecting stale MVCC reads\nRecall that snapshot isolation is usually implemented by multi-version concurrency\ncontrol (MVCC; see Figure 7-10). When a transaction reads from a consistent snap\u2010\nshot in an MVCC database, it ignores writes that were made by any other transac\u2010\ntions that hadn\u2019t yet committed at the time when the snapshot was taken. In\nFigure 7-10, transaction 43 sees Alice as having on_call = true, because transaction\n42 (which modified Alice\u2019s on-call status) is uncommitted. However, by the time\ntransaction 43 wants to commit, transaction 42 has already committed. This means\nthat the write that was ignored when reading from the consistent snapshot has now\ntaken effect, and transaction 43\u2019s premise is no longer true.\nFigure 7-10. Detecting when a transaction reads outdated values from an MVCC\nsnapshot.\nSerializability | 263", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1216, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ee5a8d66-02f1-41fa-941d-cf0c7ed87e44": {"__data__": {"id_": "ee5a8d66-02f1-41fa-941d-cf0c7ed87e44", "embedding": null, "metadata": {"page_label": "264", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b65208a1-da2e-44d9-b943-449420968989", "node_type": "4", "metadata": {"page_label": "264", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "eed0004dca7972881acd57b06f5f596a11bb84069654d7266ec668a3a94bd72e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to prevent this anomaly, the database needs to track when a transaction\nignores another transaction\u2019s writes due to MVCC visibility rules. When the transac\u2010\ntion wants to commit, the database checks whether any of the ignored writes have\nnow been committed. If so, the transaction must be aborted.\nWhy wait until committing? Why not abort transaction 43 immediately when the\nstale read is detected? Well, if transaction 43 was a read-only transaction, it wouldn\u2019t\nneed to be aborted, because there is no risk of write skew. At the time when transac\u2010\ntion 43 makes its read, the database doesn\u2019t yet know whether that transaction is\ngoing to later perform a write. Moreover, transaction 42 may yet abort or may still be\nuncommitted at the time when transaction 43 is committed, and so the read may\nturn out not to have been stale after all. By avoiding unnecessary aborts, SSI preserves\nsnapshot isolation\u2019s support for long-running reads from a consistent snapshot.\nDetecting writes that affect prior reads\nThe second case to consider is when another transaction modifies data after it has\nbeen read. This case is illustrated in Figure 7-11.\nFigure 7-11. In serializable snapshot isolation, detecting when one transaction modifies\nanother transaction\u2019s reads.\nIn the context of two-phase locking we discussed index-range locks (see \u201cIndex-range\nlocks\u201d on page 260), which allow the database to lock access to all rows matching\nsome search query, such as WHERE shift_id = 1234. We can use a similar technique\nhere, except that SSI locks don\u2019t block other transactions.\n264 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2579d4d-3b93-4e6b-9e49-30f33ba36e68": {"__data__": {"id_": "c2579d4d-3b93-4e6b-9e49-30f33ba36e68", "embedding": null, "metadata": {"page_label": "265", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17433cef-5ad8-4e43-b827-07d57be06b58", "node_type": "4", "metadata": {"page_label": "265", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a61be2f27c8002fe0b61061728c6439b5dd8f23532323a296482040fc5d95cbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Figure 7-11 , transactions 42 and 43 both search for on-call doctors during shift\n1234. If there is an index on shift_id, the database can use the index entry 1234 to\nrecord the fact that transactions 42 and 43 read this data. (If there is no index, this\ninformation can be tracked at the table level.) This information only needs to be kept\nfor a while: after a transaction has finished (committed or aborted), and all concur\u2010\nrent transactions have finished, the database can forget what data it read.\nWhen a transaction writes to the database, it must look in the indexes for any other\ntransactions that have recently read the affected data. This process is similar to\nacquiring a write lock on the affected key range, but rather than blocking until the\nreaders have committed, the lock acts as a tripwire: it simply notifies the transactions\nthat the data they read may no longer be up to date.\nIn Figure 7-11, transaction 43 notifies transaction 42 that its prior read is outdated,\nand vice versa. Transaction 42 is first to commit, and it is successful: although trans\u2010\naction 43\u2019s write affected 42, 43 hasn\u2019t yet committed, so the write has not yet taken\neffect. However, when transaction 43 wants to commit, the conflicting write from 42\nhas already been committed, so 43 must abort. \nPerformance of serializable snapshot isolation\nAs always, many engineering details affect how well an algorithm works in practice.\nFor example, one trade-off is the granularity at which transactions\u2019 reads and writes\nare tracked. If the database keeps track of each transaction\u2019s activity in great detail, it\ncan be precise about which transactions need to abort, but the bookkeeping overhead\ncan become significant. Less detailed tracking is faster, but may lead to more transac\u2010\ntions being aborted than strictly necessary.\nIn some cases, it\u2019s okay for a transaction to read information that was overwritten by\nanother transaction: depending on what else happened, it\u2019s sometimes possible to\nprove that the result of the execution is nevertheless serializable. PostgreSQL uses this\ntheory to reduce the number of unnecessary aborts [11, 41].\nCompared to two-phase locking, the big advantage of serializable snapshot isolation\nis that one transaction doesn\u2019t need to block waiting for locks held by another trans\u2010\naction. Like under snapshot isolation, writers don\u2019t block readers, and vice versa. This\ndesign principle makes query latency much more predictable and less variable. In\nparticular, read-only queries can run on a consistent snapshot without requiring any\nlocks, which is very appealing for read-heavy workloads.\nCompared to serial execution, serializable snapshot isolation is not limited to the\nthroughput of a single CPU core: FoundationDB distributes the detection of seriali\u2010\nzation conflicts across multiple machines, allowing it to scale to very high through\u2010\nput. Even though data may be partitioned across multiple machines, transactions can\nread and write data in multiple partitions while ensuring serializable isolation [54].\nSerializability | 265", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f76d9530-6870-449a-8783-cfa87f7b6128": {"__data__": {"id_": "f76d9530-6870-449a-8783-cfa87f7b6128", "embedding": null, "metadata": {"page_label": "266", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f084b401-eb4c-4664-968c-ecff59af92d2", "node_type": "4", "metadata": {"page_label": "266", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "732c85be8a05d1a999326de655a00767b09573181225ed509211114dcf70f527", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rate of aborts significantly affects the overall performance of SSI. For example, a\ntransaction that reads and writes data over a long period of time is likely to run into\nconflicts and abort, so SSI requires that read-write transactions be fairly short (long-\nrunning read-only transactions may be okay). However, SSI is probably less sensitive\nto slow transactions than two-phase locking or serial execution. \nSummary\nTransactions are an abstraction layer that allows an application to pretend that cer\u2010\ntain concurrency problems and certain kinds of hardware and software faults don\u2019t\nexist. A large class of errors is reduced down to a simple transaction abort, and the\napplication just needs to try again.\nIn this chapter we saw many examples of problems that transactions help prevent.\nNot all applications are susceptible to all those problems: an application with very\nsimple access patterns, such as reading and writing only a single record, can probably\nmanage without transactions. However, for more complex access patterns, transac\u2010\ntions can hugely reduce the number of potential error cases you need to think about.\nWithout transactions, various error scenarios (processes crashing, network interrup\u2010\ntions, power outages, disk full, unexpected concurrency, etc.) mean that data can\nbecome inconsistent in various ways. For example, denormalized data can easily go\nout of sync with the source data. Without transactions, it becomes very difficult to\nreason about the effects that complex interacting accesses can have on the database.\nIn this chapter, we went particularly deep into the topic of concurrency control. We\ndiscussed several widely used isolation levels, in particular read committed, snapshot\nisolation (sometimes called repeatable read), and serializable. We characterized those\nisolation levels by discussing various examples of race conditions:\nDirty reads\nOne client reads another client\u2019s writes before they have been committed. The\nread committed isolation level and stronger levels prevent dirty reads.\nDirty writes\nOne client overwrites data that another client has written, but not yet committed.\nAlmost all transaction implementations prevent dirty writes.\nRead skew (nonrepeatable reads)\nA client sees different parts of the database at different points in time. This issue\nis most commonly prevented with snapshot isolation, which allows a transaction\nto read from a consistent snapshot at one point in time. It is usually implemented\nwith multi-version concurrency control (MVCC).\n266 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41c1494d-0a71-453a-8e2b-abea48e25e7e": {"__data__": {"id_": "41c1494d-0a71-453a-8e2b-abea48e25e7e", "embedding": null, "metadata": {"page_label": "267", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1984c0f-7d75-46a0-b135-b7d25cebc7cc", "node_type": "4", "metadata": {"page_label": "267", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b4ec68d07b68bd03a276f31f04236257ef969edc6b0b3aff1a2a3b285ace6b29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost updates\nTwo clients concurrently perform a read-modify-write cycle. One overwrites the\nother\u2019s write without incorporating its changes, so data is lost. Some implemen\u2010\ntations of snapshot isolation prevent this anomaly automatically, while others\nrequire a manual lock (SELECT FOR UPDATE).\nWrite skew\nA transaction reads something, makes a decision based on the value it saw, and\nwrites the decision to the database. However, by the time the write is made, the\npremise of the decision is no longer true. Only serializable isolation prevents this\nanomaly.\nPhantom reads\nA transaction reads objects that match some search condition. Another client\nmakes a write that affects the results of that search. Snapshot isolation prevents\nstraightforward phantom reads, but phantoms in the context of write skew\nrequire special treatment, such as index-range locks.\nWeak isolation levels protect against some of those anomalies but leave you, the\napplication developer, to handle others manually (e.g., using explicit locking). Only\nserializable isolation protects against all of these issues. We discussed three different\napproaches to implementing serializable transactions:\nLiterally executing transactions in a serial order\nIf you can make each transaction very fast to execute, and the transaction\nthroughput is low enough to process on a single CPU core, this is a simple and\neffective option.\nTwo-phase locking\nFor decades this has been the standard way of implementing serializability, but\nmany applications avoid using it because of its performance characteristics.\nSerializable snapshot isolation (SSI)\nA fairly new algorithm that avoids most of the downsides of the previous\napproaches. It uses an optimistic approach, allowing transactions to proceed\nwithout blocking. When a transaction wants to commit, it is checked, and it is\naborted if the execution was not serializable.\nThe examples in this chapter used a relational data model. However, as discussed in\n\u201cThe need for multi-object transactions\u201d on page 231, transactions are a valuable\ndatabase feature, no matter which data model is used.\nIn this chapter, we explored ideas and algorithms mostly in the context of a database\nrunning on a single machine. Transactions in distributed databases open a new set of\ndifficult challenges, which we\u2019ll discuss in the next two chapters. \nSummary | 267", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2357, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbcb5c31-13fd-47e3-bd4a-9012d36ae7ef": {"__data__": {"id_": "dbcb5c31-13fd-47e3-bd4a-9012d36ae7ef", "embedding": null, "metadata": {"page_label": "268", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24f49a3e-a823-43a1-8e11-4eb53efcd002", "node_type": "4", "metadata": {"page_label": "268", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "da797f0963e8556ddebedc059b41737d653d24e1ad6c402fac3a1533be1fe72e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\n[1] Donald D. Chamberlin, Morton M. Astrahan, Michael W. Blasgen, et al.: \u201c A His\u2010\ntory and Evaluation of System R ,\u201d Communications of the ACM , volume 24, number\n10, pages 632\u2013646, October 1981. doi:10.1145/358769.358784\n[2] Jim N. Gray, Raymond A. Lorie, Gianfranco R. Putzolu, and Irving L. Traiger:\n\u201cGranularity of Locks and Degrees of Consistency in a Shared Data Base ,\u201d in Model\u2010\nling in Data Base Management Systems: Proceedings of the IFIP Working Conference\non Modelling in Data Base Management Systems , edited by G. M. Nijssen, pages 364\u2013\n394, Elsevier/North Holland Publishing, 1976. Also in Readings in Database Systems ,\n4th edition, edited by Joseph M. Hellerstein and Michael Stonebraker, MIT Press,\n2005. ISBN: 978-0-262-69314-1\n[3] Kapali P. Eswaran, Jim N. Gray, Raymond A. Lorie, and Irving L. Traiger: \u201c The\nNotions of Consistency and Predicate Locks in a Database System ,\u201d Communications\nof the ACM, volume 19, number 11, pages 624\u2013633, November 1976.\n[4] \u201cACID Transactions Are Incredibly Helpful,\u201d FoundationDB, LLC, 2013.\n[5] John D. Cook: \u201c ACID Versus BASE for Database Transactions ,\u201d johndcook.com,\nJuly 6, 2009.\n[6] Gavin Clarke: \u201cNoSQL\u2019s CAP Theorem Busters: We Don\u2019t Drop ACID ,\u201d theregis\u2010\nter.co.uk, November 22, 2012.\n[7] Theo H\u00e4rder and Andreas Reuter: \u201c Principles of Transaction-Oriented Database\nRecovery,\u201d ACM Computing Surveys , volume 15, number 4, pages 287\u2013317, Decem\u2010\nber 1983. doi:10.1145/289.291\n[8] Peter Bailis, Alan Fekete, Ali Ghodsi, et al.: \u201c HAT, not CAP: Towards Highly\nAvailable Transactions,\u201d at 14th USENIX Workshop on Hot Topics in Operating Sys\u2010\ntems (HotOS), May 2013.\n[9] Armando Fox, Steven D. Gribble, Yatin Chawathe, et al.: \u201c Cluster-Based Scalable\nNetwork Services,\u201d at 16th ACM Symposium on Operating Systems Principles  (SOSP),\nOctober 1997.\n[10] Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: Concurrency\nControl and Recovery in Database Systems . Addison-Wesley, 1987. ISBN:\n978-0-201-10715-9, available online at research.microsoft.com.\n[11] Alan Fekete, Dimitrios Liarokapis, Elizabeth O\u2019Neil, et al.: \u201c Making Snapshot\nIsolation Serializable,\u201d ACM Transactions on Database Systems , volume 30, number\n2, pages 492\u2013528, June 2005. doi:10.1145/1071610.1071615\n268 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3353eae1-aad4-426c-ad6b-e6d47a175c73": {"__data__": {"id_": "3353eae1-aad4-426c-ad6b-e6d47a175c73", "embedding": null, "metadata": {"page_label": "269", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b0bd982-d606-4186-9c30-2700f52e4f78", "node_type": "4", "metadata": {"page_label": "269", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0c91f5361f99ed002c486a7abc1dc7f2b947d855b55def86aa21f711998d8b78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] Mai Zheng, Joseph Tucek, Feng Qin, and Mark Lillibridge: \u201c Understanding the\nRobustness of SSDs Under Power Fault ,\u201d at 11th USENIX Conference on File and\nStorage Technologies (FAST), February 2013.\n[13] Laurie Denness: \u201cSSDs: A Gift and a Curse,\u201d laur.ie, June 2, 2015.\n[14] Adam Surak: \u201c When Solid State Drives Are Not That Solid ,\u201d blog.algolia.com,\nJune 15, 2015.\n[15] Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram, Ramnatthan Ala\u2010\ngappan, et al.: \u201c All File Systems Are Not Created Equal: On the Complexity of Craft\u2010\ning Crash-Consistent Applications ,\u201d at 11th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI), October 2014.\n[16] Chris Siebenmann: \u201c Unix\u2019s File Durability Problem ,\u201d utcc.utoronto.ca, April 14,\n2016.\n[17] Lakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder, et al.: \u201c An\nAnalysis of Data Corruption in the Storage Stack ,\u201d at 6th USENIX Conference on File\nand Storage Technologies (FAST), February 2008.\n[18] Bianca Schroeder, Raghav Lagisetty, and Arif Merchant: \u201c Flash Reliability in\nProduction: The Expected and the Unexpected ,\u201d at 14th USENIX Conference on File\nand Storage Technologies (FAST), February 2016.\n[19] Don Allison: \u201c SSD Storage \u2013 Ignorance of Technology Is No Excuse ,\u201d blog.kore\u2010\nlogic.com, March 24, 2015.\n[20] Dave Scherer: \u201c Those Are Not Transactions (Cassandra 2.0) ,\u201d blog.founda\u2010\ntiondb.com, September 6, 2013.\n[21] Kyle Kingsbury: \u201cCall Me Maybe: Cassandra,\u201d aphyr.com, September 24, 2013.\n[22] \u201cACID Support in Aerospike,\u201d Aerospike, Inc., June 2014.\n[23] Martin Kleppmann: \u201c Hermitage: Testing the \u2018I\u2019 in ACID ,\u201d martin.klepp\u2010\nmann.com, November 25, 2014.\n[24] Tristan D\u2019Agosta: \u201cBTC Stolen from Poloniex,\u201d bitcointalk.org, March 4, 2014.\n[25] bitcointhief2: \u201c How I Stole Roughly 100 BTC from an Exchange and How I\nCould Have Stolen More!,\u201d reddit.com, February 2, 2014.\n[26] Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan: \u201c Auto\u2010\nmating the Detection of Snapshot Isolation Anomalies,\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007.\n[27] Michael Melanson: \u201c Transactions: The Limits of Isolation ,\u201d michaelmelan\u2010\nson.net, March 20, 2014.\nSummary | 269", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27678ab8-265a-4ec6-bf1f-447e3fbf91d8": {"__data__": {"id_": "27678ab8-265a-4ec6-bf1f-447e3fbf91d8", "embedding": null, "metadata": {"page_label": "270", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5062fc32-8fcc-4657-8dd9-32aa692b11a0", "node_type": "4", "metadata": {"page_label": "270", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "70a7c123f7a8026e5745af48e8b79518cb60f0a607dff45482db7f25d853c98a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[28] Hal Berenson, Philip A. Bernstein, Jim N. Gray, et al.: \u201c A Critique of ANSI SQL\nIsolation Levels ,\u201d at ACM International Conference on Management of Data  (SIG\u2010\nMOD), May 1995.\n[29] Atul Adya: \u201c Weak Consistency: A Generalized Theory and Optimistic Imple\u2010\nmentations for Distributed Transactions ,\u201d PhD Thesis, Massachusetts Institute of\nTechnology, March 1999.\n[30] Peter Bailis, Aaron Davidson, Alan Fekete, et al.: \u201cHighly Available Transactions:\nVirtues and Limitations (Extended Version) ,\u201d at 40th International Conference on\nVery Large Data Bases (VLDB), September 2014.\n[31] Bruce Momjian: \u201cMVCC Unmasked,\u201d momjian.us, July 2014.\n[32] Annamalai Gurusami: \u201cRepeatable Read Isolation Level in InnoDB \u2013 How Con\u2010\nsistent Read View Works,\u201d blogs.oracle.com, January 15, 2013.\n[33] Nikita Prokopov: \u201c Unofficial Guide to Datomic Internals ,\u201d tonsky.me, May 6,\n2014.\n[34] Baron Schwartz: \u201c Immutability, MVCC, and Garbage Collection ,\u201d xaprb.com,\nDecember 28, 2013.\n[35] J. Chris Anderson, Jan Lehnardt, and Noah Slater: CouchDB: The Definitive\nGuide. O\u2019Reilly Media, 2010. ISBN: 978-0-596-15589-6\n[36] Rikdeb Mukherjee: \u201c Isolation in DB2 (Repeatable Read, Read Stability, Cursor\nStability, Uncommitted Read) with Examples,\u201d mframes.blogspot.co.uk, July 4, 2013.\n[37] Steve Hilker: \u201c Cursor Stability (CS) \u2013 IBM DB2 Community ,\u201d toadworld.com,\nMarch 14, 2013.\n[38] Nate Wiger: \u201cAn Atomic Rant,\u201d nateware.com, February 18, 2010.\n[39] Joel Jacobson: \u201cRiak 2.0: Data Types,\u201d blog.joeljacobson.com, March 23, 2014.\n[40] Michael J. Cahill, Uwe R\u00f6hm, and Alan Fekete: \u201c Serializable Isolation for Snap\u2010\nshot Databases ,\u201d at ACM International Conference on Management of Data  (SIG\u2010\nMOD), June 2008. doi:10.1145/1376616.1376690\n[41] Dan R. K. Ports and Kevin Grittner: \u201c Serializable Snapshot Isolation in Post\u2010\ngreSQL,\u201d at 38th International Conference on Very Large Databases  (VLDB), August\n2012.\n[42] Tony Andrews: \u201c Enforcing Complex Constraints in Oracle ,\u201d tonyandrews.blog\u2010\nspot.co.uk, October 15, 2004.\n[43] Douglas B. Terry, Marvin M. Theimer, Karin Petersen, et al.: \u201c Managing Update\nConflicts in Bayou, a Weakly Connected Replicated Storage System ,\u201d at 15th ACM\n270 | Chapter 7: Transactions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "808f8330-1680-4df8-9a21-d6b448342624": {"__data__": {"id_": "808f8330-1680-4df8-9a21-d6b448342624", "embedding": null, "metadata": {"page_label": "271", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d50e18f-ab9b-496e-b029-a462597b901e", "node_type": "4", "metadata": {"page_label": "271", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "509ea37fa24e558758bfa840b1010b26203ac4b2b28ae8a10a6fde1b4b419927", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Symposium on Operating Systems Principles  (SOSP), December 1995. doi:\n10.1145/224056.224070\n[44] Gary Fredericks: \u201cPostgres Serializability Bug,\u201d github.com, September 2015.\n[45] Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.: \u201c The End of an\nArchitectural Era (It\u2019s Time for a Complete Rewrite) ,\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007.\n[46] John Hugg: \u201c H-Store/VoltDB Architecture vs. CEP Systems and Newer Stream\u2010\ning Architectures,\u201d at Data @Scale Boston, November 2014.\n[47] Robert Kallman, Hideaki Kimura, Jonathan Natkins, et al.: \u201c H-Store: A High-\nPerformance, Distributed Main Memory Transaction Processing System ,\u201d Proceed\u2010\nings of the VLDB Endowment, volume 1, number 2, pages 1496\u20131499, August 2008.\n[48] Rich Hickey: \u201cThe Architecture of Datomic,\u201d infoq.com, November 2, 2012.\n[49] John Hugg: \u201c Debunking Myths About the VoltDB In-Memory Database ,\u201d\nvoltdb.com, May 12, 2014.\n[50] Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton: \u201c Architecture\nof a Database System ,\u201d Foundations and Trends in Databases , volume 1, number 2,\npages 141\u2013259, November 2007. doi:10.1561/1900000002\n[51] Michael J. Cahill: \u201c Serializable Isolation for Snapshot Databases ,\u201d PhD Thesis,\nUniversity of Sydney, July 2009.\n[52] D. Z. Badal: \u201c Correctness of Concurrency Control and Implications in Dis\u2010\ntributed Databases ,\u201d at 3rd International IEEE Computer Software and Applications\nConference (COMPSAC), November 1979.\n[53] Rakesh Agrawal, Michael J. Carey, and Miron Livny: \u201c Concurrency Control Per\u2010\nformance Modeling: Alternatives and Implications ,\u201d ACM Transactions on Database\nSystems (TODS), volume 12, number 4, pages 609\u2013654, December 1987. doi:\n10.1145/32204.32220\n[54] Dave Rosenthal: \u201c Databases at 14.4MHz,\u201d blog.foundationdb.com, December 10,\n2014.\nSummary | 271", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a276039-1ee1-4a89-96a1-a62f2d63774a": {"__data__": {"id_": "1a276039-1ee1-4a89-96a1-a62f2d63774a", "embedding": null, "metadata": {"page_label": "272", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f0c8856-047f-44ad-b6a1-c854209e5789", "node_type": "4", "metadata": {"page_label": "272", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be61191d-5935-42e9-9ee6-d26361fef5b8": {"__data__": {"id_": "be61191d-5935-42e9-9ee6-d26361fef5b8", "embedding": null, "metadata": {"page_label": "273", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a901a4b1-b8a5-4ae0-8c4c-2a48d5d7b51d", "node_type": "4", "metadata": {"page_label": "273", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9a77239b4918f2e38c51c89da1312356da8c0358368d9d376ec5947a8a4a6871", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. With one exception: we will assume that faults are non-Byzantine (see \u201cByzantine Faults\u201d on page 304).\nCHAPTER 8\nThe Trouble with Distributed Systems\nHey I just met you\nThe network\u2019s laggy\nBut here\u2019s my data\nSo store it maybe\n\u2014Kyle Kingsbury, Carly Rae Jepsen and the Perils of Network Partitions (2013)\nA recurring theme in the last few chapters has been how systems handle things going\nwrong. For example, we discussed replica failover ( \u201cHandling Node Outages\u201d on\npage 156), replication lag ( \u201cProblems with Replication Lag\u201d on page 161), and con\u2010\ncurrency control for transactions (\u201cWeak Isolation Levels\u201d on page 233). As we come\nto understand various edge cases that can occur in real systems, we get better at han\u2010\ndling them.\nHowever, even though we have talked a lot about faults, the last few chapters have\nstill been too optimistic. The reality is even darker. We will now turn our pessimism\nto the maximum and assume that anything that can go wrong will go wrong.i (Expe\u2010\nrienced systems operators will tell you that is a reasonable assumption. If you ask\nnicely, they might tell you some frightening stories while nursing their scars of past\nbattles.)\nWorking with distributed systems is fundamentally different from writing software\non a single computer\u2014and the main difference is that there are lots of new and excit\u2010\ning ways for things to go wrong [ 1, 2]. In this chapter, we will get a taste of the prob\u2010\nlems that arise in practice, and an understanding of the things we can and cannot rely\non.\n273", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dbfb414-0379-46dd-9df6-7dc965d13c1d": {"__data__": {"id_": "5dbfb414-0379-46dd-9df6-7dc965d13c1d", "embedding": null, "metadata": {"page_label": "274", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3cf8bd25-df63-4537-bd80-9fafe18103fa", "node_type": "4", "metadata": {"page_label": "274", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9d997206ee7c65a14b1ddf6d9dc38166df3f13af84f1886e68f9bb239c4c7493", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the end, our task as engineers is to build systems that do their job (i.e., meet the\nguarantees that users are expecting), in spite of everything going wrong. In Chapter 9,\nwe will look at some examples of algorithms that can provide such guarantees in a\ndistributed system. But first, in this chapter, we must understand what challenges we\nare up against.\nThis chapter is a thoroughly pessimistic and depressing overview of things that may\ngo wrong in a distributed system. We will look into problems with networks ( \u201cUnre\u2010\nliable Networks\u201d on page 277); clocks and timing issues (\u201cUnreliable Clocks\u201d on page\n287); and we\u2019ll discuss to what degree they are avoidable. The consequences of all\nthese issues are disorienting, so we\u2019ll explore how to think about the state of a dis\u2010\ntributed system and how to reason about things that have happened ( \u201cKnowledge,\nTruth, and Lies\u201d on page 300).\nFaults and Partial Failures\nWhen you are writing a program on a single computer, it normally behaves in a fairly\npredictable way: either it works or it doesn\u2019t. Buggy software may give the appearance\nthat the computer is sometimes \u201chaving a bad day\u201d (a problem that is often fixed by a\nreboot), but that is mostly just a consequence of badly written software.\nThere is no fundamental reason why software on a single computer should be flaky:\nwhen the hardware is working correctly, the same operation always produces the\nsame result (it is deterministic). If there is a hardware problem (e.g., memory corrup\u2010\ntion or a loose connector), the consequence is usually a total system failure (e.g., ker\u2010\nnel panic, \u201cblue screen of death,\u201d failure to start up). An individual computer with\ngood software is usually either fully functional or entirely broken, but not something\nin between.\nThis is a deliberate choice in the design of computers: if an internal fault occurs, we\nprefer a computer to crash completely rather than returning a wrong result, because\nwrong results are difficult and confusing to deal with. Thus, computers hide the fuzzy\nphysical reality on which they are implemented and present an idealized system\nmodel that operates with mathematical perfection. A CPU instruction always does\nthe same thing; if you write some data to memory or disk, that data remains intact\nand doesn\u2019t get randomly corrupted. This design goal of always-correct computation\ngoes all the way back to the very first digital computer [3].\nWhen you are writing software that runs on several computers, connected by a net\u2010\nwork, the situation is fundamentally different. In distributed systems, we are no\nlonger operating in an idealized system model\u2014we have no choice but to confront\nthe messy reality of the physical world. And in the physical world, a remarkably wide\nrange of things can go wrong, as illustrated by this anecdote [4]:\n274 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f06cc0b9-76ca-4eb4-bb50-2cb06877f298": {"__data__": {"id_": "f06cc0b9-76ca-4eb4-bb50-2cb06877f298", "embedding": null, "metadata": {"page_label": "275", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7029599-2a8b-4350-aa5a-429aa86669be", "node_type": "4", "metadata": {"page_label": "275", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "700074d80ae1ab198892d9b526093da3259b0245f5bc780dd992ae895c40a0cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In my limited experience I\u2019ve dealt with long-lived network partitions in a single data\ncenter (DC), PDU [power distribution unit] failures, switch failures, accidental power\ncycles of whole racks, whole-DC backbone failures, whole-DC power failures, and a\nhypoglycemic driver smashing his Ford pickup truck into a DC\u2019s HVAC [heating, ven\u2010\ntilation, and air conditioning] system. And I\u2019m not even an ops guy.\n\u2014Coda Hale\nIn a distributed system, there may well be some parts of the system that are broken in\nsome unpredictable way, even though other parts of the system are working fine. This\nis known as a partial failure. The difficulty is that partial failures are nondeterministic:\nif you try to do anything involving multiple nodes and the network, it may sometimes\nwork and sometimes unpredictably fail. As we shall see, you may not even know\nwhether something succeeded or not, as the time it takes for a message to travel\nacross a network is also nondeterministic!\nThis nondeterminism and possibility of partial failures is what makes distributed sys\u2010\ntems hard to work with [5].\nCloud Computing and Supercomputing\nThere is a spectrum of philosophies on how to build large-scale computing systems:\n\u2022 At one end of the scale is the field of high-performance computing (HPC). Super\u2010\ncomputers with thousands of CPUs are typically used for computationally inten\u2010\nsive scientific computing tasks, such as weather forecasting or molecular\ndynamics (simulating the movement of atoms and molecules).\n\u2022 At the other extreme is cloud computing, which is not very well defined [ 6] but is\noften associated with multi-tenant datacenters, commodity computers connected\nwith an IP network (often Ethernet), elastic/on-demand resource allocation, and\nmetered billing.\n\u2022 Traditional enterprise datacenters lie somewhere between these extremes.\nWith these philosophies come very different approaches to handling faults. In a\nsupercomputer, a job typically checkpoints the state of its computation to durable\nstorage from time to time. If one node fails, a common solution is to simply stop the\nentire cluster workload. After the faulty node is repaired, the computation is restarted\nfrom the last checkpoint [ 7, 8]. Thus, a supercomputer is more like a single-node\ncomputer than a distributed system: it deals with partial failure by letting it escalate\ninto total failure\u2014if any part of the system fails, just let everything crash (like a kernel\npanic on a single machine).\nIn this book we focus on systems for implementing internet services, which usually\nlook very different from supercomputers:\nFaults and Partial Failures | 275", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9fde751-018b-4a76-bfdf-00a515b5869b": {"__data__": {"id_": "f9fde751-018b-4a76-bfdf-00a515b5869b", "embedding": null, "metadata": {"page_label": "276", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "afebad18-6729-4eb7-aa94-e1a408169132", "node_type": "4", "metadata": {"page_label": "276", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8555edf1d143884c66c9e58da079ada12f3cfaebb06233ece21402ff549f40f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Many internet-related applications are online, in the sense that they need to be\nable to serve users with low latency at any time. Making the service unavailable\u2014\nfor example, stopping the cluster for repair\u2014is not acceptable. In contrast, off\u2010\nline (batch) jobs like weather simulations can be stopped and restarted with fairly\nlow impact.\n\u2022 Supercomputers are typically built from specialized hardware, where each node\nis quite reliable, and nodes communicate through shared memory and remote\ndirect memory access (RDMA). On the other hand, nodes in cloud services are\nbuilt from commodity machines, which can provide equivalent performance at\nlower cost due to economies of scale, but also have higher failure rates.\n\u2022 Large datacenter networks are often based on IP and Ethernet, arranged in Clos\ntopologies to provide high bisection bandwidth [ 9]. Supercomputers often use\nspecialized network topologies, such as multi-dimensional meshes and toruses\n[10], which yield better performance for HPC workloads with known communi\u2010\ncation patterns.\n\u2022 The bigger a system gets, the more likely it is that one of its components is bro\u2010\nken. Over time, broken things get fixed and new things break, but in a system\nwith thousands of nodes, it is reasonable to assume that something is always bro\u2010\nken [7]. When the error handling strategy consists of simply giving up, a large\nsystem can end up spending a lot of its time recovering from faults rather than\ndoing useful work [8].\n\u2022 If the system can tolerate failed nodes and still keep working as a whole, that is a\nvery useful feature for operations and maintenance: for example, you can per\u2010\nform a rolling upgrade (see Chapter 4), restarting one node at a time, while the\nservice continues serving users without interruption. In cloud environments, if\none virtual machine is not performing well, you can just kill it and request a new\none (hoping that the new one will be faster).\n\u2022 In a geographically distributed deployment (keeping data geographically close to\nyour users to reduce access latency), communication most likely goes over the\ninternet, which is slow and unreliable compared to local networks. Supercom\u2010\nputers generally assume that all of their nodes are close together.\nIf we want to make distributed systems work, we must accept the possibility of partial\nfailure and build fault-tolerance mechanisms into the software. In other words, we\nneed to build a reliable system from unreliable components. (As discussed in \u201cRelia\u2010\nbility\u201d on page 6, there is no such thing as perfect reliability, so we\u2019ll need to under\u2010\nstand the limits of what we can realistically promise.)\nEven in smaller systems consisting of only a few nodes, it\u2019s important to think about\npartial failure. In a small system, it\u2019s quite likely that most of the components are\nworking correctly most of the time. However, sooner or later, some part of the system\n276 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87974162-00de-46bf-a9f8-68c1899f7827": {"__data__": {"id_": "87974162-00de-46bf-a9f8-68c1899f7827", "embedding": null, "metadata": {"page_label": "277", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1922822c-7f61-41e8-882c-d455ea7dd004", "node_type": "4", "metadata": {"page_label": "277", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f2d1e24788faca30983d397fa47ec3451be8cec9c8761f2eff528601b277a7bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "will become faulty, and the software will have to somehow handle it. The fault han\u2010\ndling must be part of the software design, and you (as operator of the software) need\nto know what behavior to expect from the software in the case of a fault.\nIt would be unwise to assume that faults are rare and simply hope for the best. It is\nimportant to consider a wide range of possible faults\u2014even fairly unlikely ones\u2014and\nto artificially create such situations in your testing environment to see what happens.\nIn distributed systems, suspicion, pessimism, and paranoia pay off.\nBuilding a Reliable System from Unreliable Components\nYou may wonder whether this makes any sense\u2014intuitively it may seem like a system\ncan only be as reliable as its least reliable component (its weakest link). This is not the\ncase: in fact, it is an old idea in computing to construct a more reliable system from a\nless reliable underlying base [11]. For example:\n\u2022 Error-correcting codes allow digital data to be transmitted accurately across a\ncommunication channel that occasionally gets some bits wrong, for example due\nto radio interference on a wireless network [12].\n\u2022 IP (the Internet Protocol) is unreliable: it may drop, delay, duplicate, or reorder\npackets. TCP (the Transmission Control Protocol) provides a more reliable\ntransport layer on top of IP: it ensures that missing packets are retransmitted,\nduplicates are eliminated, and packets are reassembled into the order in which\nthey were sent.\nAlthough the system can be more reliable than its underlying parts, there is always a\nlimit to how much more reliable it can be. For example, error-correcting codes can\ndeal with a small number of single-bit errors, but if your signal is swamped by inter\u2010\nference, there is a fundamental limit to how much data you can get through your\ncommunication channel [ 13]. TCP can hide packet loss, duplication, and reordering\nfrom you, but it cannot magically remove delays in the network.\nAlthough the more reliable higher-level system is not perfect, it\u2019s still useful because it\ntakes care of some of the tricky low-level faults, and so the remaining faults are usu\u2010\nally easier to reason about and deal with. We will explore this matter further in \u201cThe\nend-to-end argument\u201d on page 519. \nUnreliable Networks\nAs discussed in the introduction to Part II, the distributed systems we focus on in this\nbook are shared-nothing systems: i.e., a bunch of machines connected by a network.\nThe network is the only way those machines can communicate\u2014we assume that each\nUnreliable Networks | 277", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "590ec620-cc35-48f4-8cfb-f4bcfc8ccd50": {"__data__": {"id_": "590ec620-cc35-48f4-8cfb-f4bcfc8ccd50", "embedding": null, "metadata": {"page_label": "278", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06f08744-8672-4555-93ff-8b27740a7105", "node_type": "4", "metadata": {"page_label": "278", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ef7cd7fb2618ec194c4acb3e201ec1445ee016dd75f467c78c95f997f6d82e32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "machine has its own memory and disk, and one machine cannot access another\nmachine\u2019s memory or disk (except by making requests to a service over the network).\nShared-nothing is not the only way of building systems, but it has become the domi\u2010\nnant approach for building internet services, for several reasons: it\u2019s comparatively\ncheap because it requires no special hardware, it can make use of commoditized\ncloud computing services, and it can achieve high reliability through redundancy\nacross multiple geographically distributed datacenters.\nThe internet and most internal networks in datacenters (often Ethernet) are asyn\u2010\nchronous packet networks . In this kind of network, one node can send a message (a\npacket) to another node, but the network gives no guarantees as to when it will arrive,\nor whether it will arrive at all. If you send a request and expect a response, many\nthings could go wrong (some of which are illustrated in Figure 8-1):\n1. Your request may have been lost (perhaps someone unplugged a network cable).\n2. Your request may be waiting in a queue and will be delivered later (perhaps the\nnetwork or the recipient is overloaded).\n3. The remote node may have failed (perhaps it crashed or it was powered down).\n4. The remote node may have temporarily stopped responding (perhaps it is expe\u2010\nriencing a long garbage collection pause; see \u201cProcess Pauses\u201d on page 295), but it\nwill start responding again later.\n5. The remote node may have processed your request, but the response has been\nlost on the network (perhaps a network switch has been misconfigured).\n6. The remote node may have processed your request, but the response has been\ndelayed and will be delivered later (perhaps the network or your own machine is\noverloaded).\nFigure 8-1. If you send a request and don\u2019t get a response, it\u2019s not possible to distinguish\nwhether (a) the request was lost, (b) the remote node is down, or (c) the response was\nlost.\n278 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91b6c310-4a2e-4115-88e2-238b54ae9168": {"__data__": {"id_": "91b6c310-4a2e-4115-88e2-238b54ae9168", "embedding": null, "metadata": {"page_label": "279", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8369408-a7a0-46cb-9641-fe7b484dfb39", "node_type": "4", "metadata": {"page_label": "279", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "81ae3ce0e6f4ba8cd07eed98a8665803f2d1b416068a2a40e8af3b2c7beed2e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sender can\u2019t even tell whether the packet was delivered: the only option is for the\nrecipient to send a response message, which may in turn be lost or delayed. These\nissues are indistinguishable in an asynchronous network: the only information you\nhave is that you haven\u2019t received a response yet. If you send a request to another node\nand don\u2019t receive a response, it is impossible to tell why.\nThe usual way of handling this issue is a timeout: after some time you give up waiting\nand assume that the response is not going to arrive. However, when a timeout occurs,\nyou still don\u2019t know whether the remote node got your request or not (and if the\nrequest is still queued somewhere, it may still be delivered to the recipient, even if the\nsender has given up on it).\nNetwork Faults in Practice\nWe have been building computer networks for decades\u2014one might hope that by now\nwe would have figured out how to make them reliable. However, it seems that we\nhave not yet succeeded.\nThere are some systematic studies, and plenty of anecdotal evidence, showing that\nnetwork problems can be surprisingly common, even in controlled environments like\na datacenter operated by one company [14]. One study in a medium-sized datacenter\nfound about 12 network faults per month, of which half disconnected a single\nmachine, and half disconnected an entire rack [ 15]. Another study measured the fail\u2010\nure rates of components like top-of-rack switches, aggregation switches, and load bal\u2010\nancers [16]. It found that adding redundant networking gear doesn\u2019t reduce faults as\nmuch as you might hope, since it doesn\u2019t guard against human error (e.g., misconfig\u2010\nured switches), which is a major cause of outages.\nPublic cloud services such as EC2 are notorious for having frequent transient net\u2010\nwork glitches [ 14], and well-managed private datacenter networks can be stabler\nenvironments. Nevertheless, nobody is immune from network problems: for exam\u2010\nple, a problem during a software upgrade for a switch could trigger a network topol\u2010\nogy reconfiguration, during which network packets could be delayed for more than a\nminute [17]. Sharks might bite undersea cables and damage them [18]. Other surpris\u2010\ning faults include a network interface that sometimes drops all inbound packets but\nsends outbound packets successfully [ 19]: just because a network link works in one\ndirection doesn\u2019t guarantee it\u2019s also working in the opposite direction.\nNetwork partitions\nWhen one part of the network is cut off from the rest due to a net\u2010\nwork fault, that is sometimes called a network partition or netsplit.\nIn this book we\u2019ll generally stick with the more general term net\u2010\nwork fault, to avoid confusion with partitions (shards) of a storage\nsystem, as discussed in Chapter 6.\nUnreliable Networks | 279", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f9f1759-3369-4930-a4c4-7cfcd1be5491": {"__data__": {"id_": "5f9f1759-3369-4930-a4c4-7cfcd1be5491", "embedding": null, "metadata": {"page_label": "280", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6463ed32-d90e-4ae3-abde-fa5cfcfc1305", "node_type": "4", "metadata": {"page_label": "280", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d739ddd17fc912198956d2c6e1b9fe0ca9482b887fcd2094ca0badb5155d1a9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even if network faults are rare in your environment, the fact that faults can occur\nmeans that your software needs to be able to handle them. Whenever any communi\u2010\ncation happens over a network, it may fail\u2014there is no way around it.\nIf the error handling of network faults is not defined and tested, arbitrarily bad things\ncould happen: for example, the cluster could become deadlocked and permanently\nunable to serve requests, even when the network recovers [ 20], or it could even delete\nall of your data [ 21]. If software is put in an unanticipated situation, it may do arbi\u2010\ntrary unexpected things.\nHandling network faults doesn\u2019t necessarily mean tolerating them: if your network is\nnormally fairly reliable, a valid approach may be to simply show an error message to\nusers while your network is experiencing problems. However, you do need to know\nhow your software reacts to network problems and ensure that the system can\nrecover from them. It may make sense to deliberately trigger network problems and\ntest the system\u2019s response (this is the idea behind Chaos Monkey; see \u201cReliability\u201d on\npage 6).\nDetecting Faults\nMany systems need to automatically detect faulty nodes. For example:\n\u2022 A load balancer needs to stop sending requests to a node that is dead (i.e., take it\nout of rotation).\n\u2022 In a distributed database with single-leader replication, if the leader fails, one of\nthe followers needs to be promoted to be the new leader (see \u201cHandling Node\nOutages\u201d on page 156).\nUnfortunately, the uncertainty about the network makes it difficult to tell whether a\nnode is working or not. In some specific circumstances you might get some feedback\nto explicitly tell you that something is not working:\n\u2022 If you can reach the machine on which the node should be running, but no pro\u2010\ncess is listening on the destination port (e.g., because the process crashed), the\noperating system will helpfully close or refuse TCP connections by sending a RST\nor FIN packet in reply. However, if the node crashed while it was handling your\nrequest, you have no way of knowing how much data was actually processed by\nthe remote node [22].\n\u2022 If a node process crashed (or was killed by an administrator) but the node\u2019s oper\u2010\nating system is still running, a script can notify other nodes about the crash so\nthat another node can take over quickly without having to wait for a timeout to\nexpire. For example, HBase does this [23].\n280 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "193abb13-9ee6-4158-bd4c-fc39ad25fef6": {"__data__": {"id_": "193abb13-9ee6-4158-bd4c-fc39ad25fef6", "embedding": null, "metadata": {"page_label": "281", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08274d27-5b3c-4259-85d1-00e40ab9e6db", "node_type": "4", "metadata": {"page_label": "281", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "57ea403569849364af074f4b75809a148ab277f73d53125ac4adedf24b536b29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If you have access to the management interface of the network switches in your\ndatacenter, you can query them to detect link failures at a hardware level (e.g., if\nthe remote machine is powered down). This option is ruled out if you\u2019re con\u2010\nnecting via the internet, or if you\u2019re in a shared datacenter with no access to the\nswitches themselves, or if you can\u2019t reach the management interface due to a net\u2010\nwork problem.\n\u2022 If a router is sure that the IP address you\u2019re trying to connect to is unreachable, it\nmay reply to you with an ICMP Destination Unreachable packet. However, the\nrouter doesn\u2019t have a magic failure detection capability either\u2014it is subject to the\nsame limitations as other participants of the network.\nRapid feedback about a remote node being down is useful, but you can\u2019t count on it.\nEven if TCP acknowledges that a packet was delivered, the application may have\ncrashed before handling it. If you want to be sure that a request was successful, you\nneed a positive response from the application itself [24].\nConversely, if something has gone wrong, you may get an error response at some\nlevel of the stack, but in general you have to assume that you will get no response at\nall. You can retry a few times (TCP retries transparently, but you may also retry at the\napplication level), wait for a timeout to elapse, and eventually declare the node dead if\nyou don\u2019t hear back within the timeout. \nTimeouts and Unbounded Delays\nIf a timeout is the only sure way of detecting a fault, then how long should the time\u2010\nout be? There is unfortunately no simple answer.\nA long timeout means a long wait until a node is declared dead (and during this time,\nusers may have to wait or see error messages). A short timeout detects faults faster,\nbut carries a higher risk of incorrectly declaring a node dead when in fact it has only\nsuffered a temporary slowdown (e.g., due to a load spike on the node or the network).\nPrematurely declaring a node dead is problematic: if the node is actually alive and in\nthe middle of performing some action (for example, sending an email), and another\nnode takes over, the action may end up being performed twice. We will discuss this\nissue in more detail in \u201cKnowledge, Truth, and Lies\u201d on page 300, and in Chapters 9\nand 11.\nWhen a node is declared dead, its responsibilities need to be transferred to other\nnodes, which places additional load on other nodes and the network. If the system is\nalready struggling with high load, declaring nodes dead prematurely can make the\nproblem worse. In particular, it could happen that the node actually wasn\u2019t dead but\nonly slow to respond due to overload; transferring its load to other nodes can cause a\ncascading failure (in the extreme case, all nodes declare each other dead, and every\u2010\nthing stops working).\nUnreliable Networks | 281", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bca4f73-72cd-4e1b-ad78-0a9a2006c8ee": {"__data__": {"id_": "0bca4f73-72cd-4e1b-ad78-0a9a2006c8ee", "embedding": null, "metadata": {"page_label": "282", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c16c5a69-4a10-455c-8808-8cfdb8891d86", "node_type": "4", "metadata": {"page_label": "282", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "47128aeebe6220052c462524bd8a95b3efd725f10d2503d29cf156dac6617280", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Imagine a fictitious system with a network that guaranteed a maximum delay for\npackets\u2014every packet is either delivered within some time d, or it is lost, but delivery\nnever takes longer than d. Furthermore, assume that you can guarantee that a non-\nfailed node always handles a request within some time r. In this case, you could guar\u2010\nantee that every successful request receives a response within time 2 d + r\u2014and if you\ndon\u2019t receive a response within that time, you know that either the network or the\nremote node is not working. If this was true, 2 d + r would be a reasonable timeout to\nuse.\nUnfortunately, most systems we work with have neither of those guarantees: asyn\u2010\nchronous networks have unbounded delays  (that is, they try to deliver packets as\nquickly as possible, but there is no upper limit on the time it may take for a packet to\narrive), and most server implementations cannot guarantee that they can handle\nrequests within some maximum time (see \u201cResponse time guarantees\u201d on page 298).\nFor failure detection, it\u2019s not sufficient for the system to be fast most of the time: if\nyour timeout is low, it only takes a transient spike in round-trip times to throw the\nsystem off-balance.\nNetwork congestion and queueing\nWhen driving a car, travel times on road networks often vary most due to traffic con\u2010\ngestion. Similarly, the variability of packet delays on computer networks is most often\ndue to queueing [25]:\n\u2022 If several different nodes simultaneously try to send packets to the same destina\u2010\ntion, the network switch must queue them up and feed them into the destination\nnetwork link one by one (as illustrated in Figure 8-2). On a busy network link, a\npacket may have to wait a while until it can get a slot (this is called network con\u2010\ngestion). If there is so much incoming data that the switch queue fills up, the\npacket is dropped, so it needs to be resent\u2014even though the network is function\u2010\ning fine.\n\u2022 When a packet reaches the destination machine, if all CPU cores are currently\nbusy, the incoming request from the network is queued by the operating system\nuntil the application is ready to handle it. Depending on the load on the machine,\nthis may take an arbitrary length of time.\n\u2022 In virtualized environments, a running operating system is often paused for tens\nof milliseconds while another virtual machine uses a CPU core. During this time,\nthe VM cannot consume any data from the network, so the incoming data is\nqueued (buffered) by the virtual machine monitor [ 26], further increasing the\nvariability of network delays.\n\u2022 TCP performs flow control (also known as congestion avoidance or backpressure),\nin which a node limits its own rate of sending in order to avoid overloading a\n282 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5fbfd3be-e56a-4cad-9d14-6aa3f12caed7": {"__data__": {"id_": "5fbfd3be-e56a-4cad-9d14-6aa3f12caed7", "embedding": null, "metadata": {"page_label": "283", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bc6e1ea-2743-49a3-8414-fff1a177faeb", "node_type": "4", "metadata": {"page_label": "283", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ff067445212c9424d6b80bd003fb3535d4a2fecb88cb3104e4d3ac82353c0f7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "network link or the receiving node [ 27]. This means additional queueing at the\nsender before the data even enters the network.\nFigure 8-2. If several machines send network traffic to the same destination, its switch\nqueue can fill up. Here, ports 1, 2, and 4 are all trying to send packets to port 3.\nMoreover, TCP considers a packet to be lost if it is not acknowledged within some\ntimeout (which is calculated from observed round-trip times), and lost packets are\nautomatically retransmitted. Although the application does not see the packet loss\nand retransmission, it does see the resulting delay (waiting for the timeout to expire,\nand then waiting for the retransmitted packet to be acknowledged).\nTCP Versus UDP\nSome latency-sensitive applications, such as videoconferencing and Voice over IP\n(VoIP), use UDP rather than TCP. It\u2019s a trade-off between reliability and variability\nof delays: as UDP does not perform flow control and does not retransmit lost packets,\nit avoids some of the reasons for variable network delays (although it is still suscepti\u2010\nble to switch queues and scheduling delays).\nUDP is a good choice in situations where delayed data is worthless. For example, in a\nVoIP phone call, there probably isn\u2019t enough time to retransmit a lost packet before\nits data is due to be played over the loudspeakers. In this case, there\u2019s no point in\nretransmitting the packet\u2014the application must instead fill the missing packet\u2019s time\nslot with silence (causing a brief interruption in the sound) and move on in the\nstream. The retry happens at the human layer instead. (\u201cCould you repeat that please?\nThe sound just cut out for a moment.\u201d)\nAll of these factors contribute to the variability of network delays. Queueing delays\nhave an especially wide range when a system is close to its maximum capacity: a sys\u2010\nUnreliable Networks | 283", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1853, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12a8b523-f14d-44a7-8a7a-53b31a4ce63f": {"__data__": {"id_": "12a8b523-f14d-44a7-8a7a-53b31a4ce63f", "embedding": null, "metadata": {"page_label": "284", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76901f59-946a-4a22-ad06-71d826f3bdb7", "node_type": "4", "metadata": {"page_label": "284", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "03e21095aefb33efe073c2dfcbd16ca01264c86fe41a4b7331a0a39e626e921d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tem with plenty of spare capacity can easily drain queues, whereas in a highly utilized\nsystem, long queues can build up very quickly.\nIn public clouds and multi-tenant datacenters, resources are shared among many\ncustomers: the network links and switches, and even each machine\u2019s network inter\u2010\nface and CPUs (when running on virtual machines), are shared. Batch workloads\nsuch as MapReduce (see Chapter 10) can easily saturate network links. As you have\nno control over or insight into other customers\u2019 usage of the shared resources, net\u2010\nwork delays can be highly variable if someone near you (a noisy neighbor) is using a\nlot of resources [28, 29].\nIn such environments, you can only choose timeouts experimentally: measure the\ndistribution of network round-trip times over an extended period, and over many\nmachines, to determine the expected variability of delays. Then, taking into account\nyour application\u2019s characteristics, you can determine an appropriate trade-off\nbetween failure detection delay and risk of premature timeouts.\nEven better, rather than using configured constant timeouts, systems can continually\nmeasure response times and their variability ( jitter), and automatically adjust time\u2010\nouts according to the observed response time distribution. This can be done with a\nPhi Accrual failure detector [ 30], which is used for example in Akka and Cassandra\n[31]. TCP retransmission timeouts also work similarly [27].\nSynchronous Versus Asynchronous Networks\nDistributed systems would be a lot simpler if we could rely on the network to deliver\npackets with some fixed maximum delay, and not to drop packets. Why can\u2019t we\nsolve this at the hardware level and make the network reliable so that the software\ndoesn\u2019t need to worry about it?\nTo answer this question, it\u2019s interesting to compare datacenter networks to the tradi\u2010\ntional fixed-line telephone network (non-cellular, non-VoIP), which is extremely\nreliable: delayed audio frames and dropped calls are very rare. A phone call requires a\nconstantly low end-to-end latency and enough bandwidth to transfer the audio sam\u2010\nples of your voice. Wouldn\u2019t it be nice to have similar reliability and predictability in\ncomputer networks?\nWhen you make a call over the telephone network, it establishes a circuit: a fixed,\nguaranteed amount of bandwidth is allocated for the call, along the entire route\nbetween the two callers. This circuit remains in place until the call ends [ 32]. For\nexample, an ISDN network runs at a fixed rate of 4,000 frames per second. When a\ncall is established, it is allocated 16 bits of space within each frame (in each direction).\nThus, for the duration of the call, each side is guaranteed to be able to send exactly 16\nbits of audio data every 250 microseconds [33, 34].\n284 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03c3b789-8e17-4bcb-a94a-85322c763587": {"__data__": {"id_": "03c3b789-8e17-4bcb-a94a-85322c763587", "embedding": null, "metadata": {"page_label": "285", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "280c5167-53c5-4c84-ae47-536a354972ab", "node_type": "4", "metadata": {"page_label": "285", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "93a5072cea45771fa1bd9218980d499b959ecae667a9724e1cbb9a57f7f32863", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. Except perhaps for an occasional keepalive packet, if TCP keepalive is enabled.\niii. Asynchronous Transfer Mode (ATM) was a competitor to Ethernet in the 1980s [32], but it didn\u2019t gain\nmuch adoption outside of telephone network core switches. It has nothing to do with automatic teller\nmachines (also known as cash machines), despite sharing an acronym. Perhaps, in some parallel universe, the\ninternet is based on something like ATM\u2014in that universe, internet video calls are probably a lot more relia\u2010\nble than they are in ours, because they don\u2019t suffer from dropped and delayed packets.\nThis kind of network is synchronous: even as data passes through several routers, it\ndoes not suffer from queueing, because the 16 bits of space for the call have already\nbeen reserved in the next hop of the network. And because there is no queueing, the\nmaximum end-to-end latency of the network is fixed. We call this a bounded delay.\nCan we not simply make network delays predictable?\nNote that a circuit in a telephone network is very different from a TCP connection: a\ncircuit is a fixed amount of reserved bandwidth which nobody else can use while the\ncircuit is established, whereas the packets of a TCP connection opportunistically use\nwhatever network bandwidth is available. You can give TCP a variable-sized block of\ndata (e.g., an email or a web page), and it will try to transfer it in the shortest time\npossible. While a TCP connection is idle, it doesn\u2019t use any bandwidth.ii\nIf datacenter networks and the internet were circuit-switched networks, it would be\npossible to establish a guaranteed maximum round-trip time when a circuit was set\nup. However, they are not: Ethernet and IP are packet-switched protocols, which suf\u2010\nfer from queueing and thus unbounded delays in the network. These protocols do\nnot have the concept of a circuit.\nWhy do datacenter networks and the internet use packet switching? The answer is\nthat they are optimized for bursty traffic. A circuit is good for an audio or video call,\nwhich needs to transfer a fairly constant number of bits per second for the duration\nof the call. On the other hand, requesting a web page, sending an email, or transfer\u2010\nring a file doesn\u2019t have any particular bandwidth requirement\u2014we just want it to\ncomplete as quickly as possible.\nIf you wanted to transfer a file over a circuit, you would have to guess a bandwidth\nallocation. If you guess too low, the transfer is unnecessarily slow, leaving network\ncapacity unused. If you guess too high, the circuit cannot be set up (because the net\u2010\nwork cannot allow a circuit to be created if its bandwidth allocation cannot be guar\u2010\nanteed). Thus, using circuits for bursty data transfers wastes network capacity and\nmakes transfers unnecessarily slow. By contrast, TCP dynamically adapts the rate of\ndata transfer to the available network capacity.\nThere have been some attempts to build hybrid networks that support both circuit\nswitching and packet switching, such as ATM.iii InfiniBand has some similarities [35]:\nit implements end-to-end flow control at the link layer, which reduces the need for\nUnreliable Networks | 285", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a448399b-1559-461e-b941-b19d6b026eae": {"__data__": {"id_": "a448399b-1559-461e-b941-b19d6b026eae", "embedding": null, "metadata": {"page_label": "286", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6689f3b-3150-45b0-852c-663332516c6b", "node_type": "4", "metadata": {"page_label": "286", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8008d6f3a5a8a64b2a09f75c6b6c85751a7a66fe83fd14f56d519db38a64a1ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "queueing in the network, although it can still suffer from delays due to link conges\u2010\ntion [36]. With careful use of quality of service (QoS, prioritization and scheduling of\npackets) and admission control (rate-limiting senders), it is possible to emulate circuit\nswitching on packet networks, or provide statistically bounded delay [25, 32].\nLatency and Resource Utilization\nMore generally, you can think of variable delays as a consequence of dynamic\nresource partitioning.\nSay you have a wire between two telephone switches that can carry up to 10,000\nsimultaneous calls. Each circuit that is switched over this wire occupies one of those\ncall slots. Thus, you can think of the wire as a resource that can be shared by up to\n10,000 simultaneous users. The resource is divided up in a static way: even if you\u2019re\nthe only call on the wire right now, and all other 9,999 slots are unused, your circuit is\nstill allocated the same fixed amount of bandwidth as when the wire is fully utilized.\nBy contrast, the internet shares network bandwidth dynamically. Senders push and\njostle with each other to get their packets over the wire as quickly as possible, and the\nnetwork switches decide which packet to send (i.e., the bandwidth allocation) from\none moment to the next. This approach has the downside of queueing, but the advan\u2010\ntage is that it maximizes utilization of the wire. The wire has a fixed cost, so if you\nutilize it better, each byte you send over the wire is cheaper.\nA similar situation arises with CPUs: if you share each CPU core dynamically\nbetween several threads, one thread sometimes has to wait in the operating system\u2019s\nrun queue while another thread is running, so a thread can be paused for varying\nlengths of time. However, this utilizes the hardware better than if you allocated a\nstatic number of CPU cycles to each thread (see \u201cResponse time guarantees\u201d on page\n298). Better hardware utilization is also a significant motivation for using virtual\nmachines.\nLatency guarantees are achievable in certain environments, if resources are statically\npartitioned (e.g., dedicated hardware and exclusive bandwidth allocations). However,\nit comes at the cost of reduced utilization\u2014in other words, it is more expensive. On\nthe other hand, multi-tenancy with dynamic resource partitioning provides better\nutilization, so it is cheaper, but it has the downside of variable delays.\nVariable delays in networks are not a law of nature, but simply the result of a cost/\nbenefit trade-off. \n286 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "05b54e45-8cfe-4aff-b6e0-02a481436b76": {"__data__": {"id_": "05b54e45-8cfe-4aff-b6e0-02a481436b76", "embedding": null, "metadata": {"page_label": "287", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fe95aaa-bd99-4cad-bd5f-b5171fdf5793", "node_type": "4", "metadata": {"page_label": "287", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a1564dff255ec30d3ca4ee9414338dc606007d6828d2b22e88fb7c6a1b69bd70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. Peering agreements between internet service providers and the establishment of routes through the Bor\u2010\nder Gateway Protocol (BGP), bear closer resemblance to circuit switching than IP itself. At this level, it is pos\u2010\nsible to buy dedicated bandwidth. However, internet routing operates at the level of networks, not individual\nconnections between hosts, and at a much longer timescale.\nHowever, such quality of service is currently not enabled in multi-tenant datacenters\nand public clouds, or when communicating via the internet. iv Currently deployed\ntechnology does not allow us to make any guarantees about delays or reliability of the\nnetwork: we have to assume that network congestion, queueing, and unbounded\ndelays will happen. Consequently, there\u2019s no \u201ccorrect\u201d value for timeouts\u2014they need\nto be determined experimentally.\nUnreliable Clocks\nClocks and time are important. Applications depend on clocks in various ways to\nanswer questions like the following:\n1. Has this request timed out yet?\n2. What\u2019s the 99th percentile response time of this service?\n3. How many queries per second did this service handle on average in the last five\nminutes?\n4. How long did the user spend on our site?\n5. When was this article published?\n6. At what date and time should the reminder email be sent?\n7. When does this cache entry expire?\n8. What is the timestamp on this error message in the log file?\nExamples 1\u20134 measure durations (e.g., the time interval between a request being sent\nand a response being received), whereas examples 5\u20138 describe points in time (events\nthat occur on a particular date, at a particular time).\nIn a distributed system, time is a tricky business, because communication is not\ninstantaneous: it takes time for a message to travel across the network from one\nmachine to another. The time when a message is received is always later than the\ntime when it is sent, but due to variable delays in the network, we don\u2019t know how\nmuch later. This fact sometimes makes it difficult to determine the order in which\nthings happened when multiple machines are involved.\nMoreover, each machine on the network has its own clock, which is an actual hard\u2010\nware device: usually a quartz crystal oscillator. These devices are not perfectly accu\u2010\nrate, so each machine has its own notion of time, which may be slightly faster or\nUnreliable Clocks | 287", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce5d2775-f66e-4983-a866-f41d5dc1538e": {"__data__": {"id_": "ce5d2775-f66e-4983-a866-f41d5dc1538e", "embedding": null, "metadata": {"page_label": "288", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "285e2dbb-5bba-47eb-826a-aa493ce6035e", "node_type": "4", "metadata": {"page_label": "288", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f1e639a9943cf5a2a16dbd89700f65f956a3001118108a27545bcf966f4e8593", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "v. Although the clock is called real-time, it has nothing to do with real-time operating systems, as discussed\nin \u201cResponse time guarantees\u201d on page 298.\nslower than on other machines. It is possible to synchronize clocks to some degree:\nthe most commonly used mechanism is the Network Time Protocol (NTP), which\nallows the computer clock to be adjusted according to the time reported by a group of\nservers [ 37]. The servers in turn get their time from a more accurate time source,\nsuch as a GPS receiver.\nMonotonic Versus Time-of-Day Clocks\nModern computers have at least two different kinds of clocks: a time-of-day clock and\na monotonic clock. Although they both measure time, it is important to distinguish\nthe two, since they serve different purposes.\nTime-of-day clocks\nA time-of-day clock does what you intuitively expect of a clock: it returns the current\ndate and time according to some calendar (also known as wall-clock time). For exam\u2010\nple, clock_gettime(CLOCK_REALTIME) on Linux v and System.currentTimeMillis()\nin Java return the number of seconds (or milliseconds) since the epoch: midnight\nUTC on January 1, 1970, according to the Gregorian calendar, not counting leap sec\u2010\nonds. Some systems use other dates as their reference point.\nTime-of-day clocks are usually synchronized with NTP, which means that a time\u2010\nstamp from one machine (ideally) means the same as a timestamp on another\nmachine. However, time-of-day clocks also have various oddities, as described in the\nnext section. In particular, if the local clock is too far ahead of the NTP server, it may\nbe forcibly reset and appear to jump back to a previous point in time. These jumps, as\nwell as the fact that they often ignore leap seconds, make time-of-day clocks unsuita\u2010\nble for measuring elapsed time [38].\nTime-of-day clocks have also historically had quite a coarse-grained resolution, e.g.,\nmoving forward in steps of 10 ms on older Windows systems [ 39]. On recent sys\u2010\ntems, this is less of a problem.\nMonotonic clocks\nA monotonic clock is suitable for measuring a duration (time interval), such as a\ntimeout or a service\u2019s response time: clock_gettime(CLOCK_MONOTONIC) on Linux\nand System.nanoTime() in Java are monotonic clocks, for example. The name comes\nfrom the fact that they are guaranteed to always move forward (whereas a time-of-\nday clock may jump back in time).\n288 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2418, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d64137af-b224-46f8-8af9-6acdf6ab2acd": {"__data__": {"id_": "d64137af-b224-46f8-8af9-6acdf6ab2acd", "embedding": null, "metadata": {"page_label": "289", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fba866dc-60b3-4fad-b3de-64ec560e7a0e", "node_type": "4", "metadata": {"page_label": "289", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "677e81434cddd75821a4f81276e303082d7d87bd515d79419f5a205e6c113e3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can check the value of the monotonic clock at one point in time, do something,\nand then check the clock again at a later time. The difference between the two values\ntells you how much time elapsed between the two checks. However, the absolute\nvalue of the clock is meaningless: it might be the number of nanoseconds since the\ncomputer was started, or something similarly arbitrary. In particular, it makes no\nsense to compare monotonic clock values from two different computers, because they\ndon\u2019t mean the same thing.\nOn a server with multiple CPU sockets, there may be a separate timer per CPU,\nwhich is not necessarily synchronized with other CPUs. Operating systems compen\u2010\nsate for any discrepancy and try to present a monotonic view of the clock to applica\u2010\ntion threads, even as they are scheduled across different CPUs. However, it is wise to\ntake this guarantee of monotonicity with a pinch of salt [40].\nNTP may adjust the frequency at which the monotonic clock moves forward (this is\nknown as slewing the clock) if it detects that the computer\u2019s local quartz is moving\nfaster or slower than the NTP server. By default, NTP allows the clock rate to be spee\u2010\nded up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock\nto jump forward or backward. The resolution of monotonic clocks is usually quite\ngood: on most systems they can measure time intervals in microseconds or less.\nIn a distributed system, using a monotonic clock for measuring elapsed time (e.g.,\ntimeouts) is usually fine, because it doesn\u2019t assume any synchronization between dif\u2010\nferent nodes\u2019 clocks and is not sensitive to slight inaccuracies of measurement.\nClock Synchronization and Accuracy\nMonotonic clocks don\u2019t need synchronization, but time-of-day clocks need to be set\naccording to an NTP server or other external time source in order to be useful.\nUnfortunately, our methods for getting a clock to tell the correct time aren\u2019t nearly as\nreliable or accurate as you might hope\u2014hardware clocks and NTP can be fickle\nbeasts. To give just a few examples:\n\u2022 The quartz clock in a computer is not very accurate: it drifts (runs faster or\nslower than it should). Clock drift varies depending on the temperature of the\nmachine. Google assumes a clock drift of 200 ppm (parts per million) for its\nservers [41], which is equivalent to 6 ms drift for a clock that is resynchronized\nwith a server every 30 seconds, or 17 seconds drift for a clock that is resynchron\u2010\nized once a day. This drift limits the best possible accuracy you can achieve, even\nif everything is working correctly.\n\u2022 If a computer\u2019s clock differs too much from an NTP server, it may refuse to syn\u2010\nchronize, or the local clock will be forcibly reset [ 37]. Any applications observing\nthe time before and after this reset may see time go backward or suddenly jump\nforward.\nUnreliable Clocks | 289", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72b76ca3-008b-4646-a14b-ec8c164ca5ce": {"__data__": {"id_": "72b76ca3-008b-4646-a14b-ec8c164ca5ce", "embedding": null, "metadata": {"page_label": "290", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d919f57-9e0c-4cb2-ab1a-a404211b3290", "node_type": "4", "metadata": {"page_label": "290", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9b4871bb937a7d17ac2e5e6dbe3ee3fea8e55504ac9567a4e10b918bbf70d6d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If a node is accidentally firewalled off from NTP servers, the misconfiguration\nmay go unnoticed for some time. Anecdotal evidence suggests that this does hap\u2010\npen in practice.\n\u2022 NTP synchronization can only be as good as the network delay, so there is a limit\nto its accuracy when you\u2019re on a congested network with variable packet delays.\nOne experiment showed that a minimum error of 35 ms is achievable when syn\u2010\nchronizing over the internet [42], though occasional spikes in network delay lead\nto errors of around a second. Depending on the configuration, large network\ndelays can cause the NTP client to give up entirely.\n\u2022 Some NTP servers are wrong or misconfigured, reporting time that is off by\nhours [43, 44]. NTP clients are quite robust, because they query several servers\nand ignore outliers. Nevertheless, it\u2019s somewhat worrying to bet the correctness\nof your systems on the time that you were told by a stranger on the internet.\n\u2022 Leap seconds result in a minute that is 59 seconds or 61 seconds long, which\nmesses up timing assumptions in systems that are not designed with leap seconds\nin mind [ 45]. The fact that leap seconds have crashed many large systems [ 38,\n46] shows how easy it is for incorrect assumptions about clocks to sneak into a\nsystem. The best way of handling leap seconds may be to make NTP servers \u201clie,\u201d\nby performing the leap second adjustment gradually over the course of a day\n(this is known as smearing) [47, 48], although actual NTP server behavior varies\nin practice [49].\n\u2022 In virtual machines, the hardware clock is virtualized, which raises additional\nchallenges for applications that need accurate timekeeping [ 50]. When a CPU\ncore is shared between virtual machines, each VM is paused for tens of milli\u2010\nseconds while another VM is running. From an application\u2019s point of view, this\npause manifests itself as the clock suddenly jumping forward [26].\n\u2022 If you run software on devices that you don\u2019t fully control (e.g., mobile or\nembedded devices), you probably cannot trust the device\u2019s hardware clock at all.\nSome users deliberately set their hardware clock to an incorrect date and time,\nfor example to circumvent timing limitations in games. As a result, the clock\nmight be set to a time wildly in the past or the future.\nIt is possible to achieve very good clock accuracy if you care about it sufficiently to\ninvest significant resources. For example, the MiFID II draft European regulation for\nfinancial institutions requires all high-frequency trading funds to synchronize their\nclocks to within 100 microseconds of UTC, in order to help debug market anomalies\nsuch as \u201cflash crashes\u201d and to help detect market manipulation [51].\nSuch accuracy can be achieved using GPS receivers, the Precision Time Protocol\n(PTP) [52], and careful deployment and monitoring. However, it requires significant\neffort and expertise, and there are plenty of ways clock synchronization can go\n290 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b3675f7-9549-4639-81e6-7dd966756621": {"__data__": {"id_": "5b3675f7-9549-4639-81e6-7dd966756621", "embedding": null, "metadata": {"page_label": "291", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1a825b3-44fe-429f-848a-655dc6e4c2df", "node_type": "4", "metadata": {"page_label": "291", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6424547b2eb253b3c73bfdebf341431ec91f722a393fd13424a6da74ffff6ea0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "wrong. If your NTP daemon is misconfigured, or a firewall is blocking NTP traffic,\nthe clock error due to drift can quickly become large. \nRelying on Synchronized Clocks\nThe problem with clocks is that while they seem simple and easy to use, they have a\nsurprising number of pitfalls: a day may not have exactly 86,400 seconds, time-of-day\nclocks may move backward in time, and the time on one node may be quite different\nfrom the time on another node.\nEarlier in this chapter we discussed networks dropping and arbitrarily delaying pack\u2010\nets. Even though networks are well behaved most of the time, software must be\ndesigned on the assumption that the network will occasionally be faulty, and the soft\u2010\nware must handle such faults gracefully. The same is true with clocks: although they\nwork quite well most of the time, robust software needs to be prepared to deal with\nincorrect clocks.\nPart of the problem is that incorrect clocks easily go unnoticed. If a machine\u2019s CPU is\ndefective or its network is misconfigured, it most likely won\u2019t work at all, so it will\nquickly be noticed and fixed. On the other hand, if its quartz clock is defective or its\nNTP client is misconfigured, most things will seem to work fine, even though its\nclock gradually drifts further and further away from reality. If some piece of software\nis relying on an accurately synchronized clock, the result is more likely to be silent\nand subtle data loss than a dramatic crash [53, 54].\nThus, if you use software that requires synchronized clocks, it is essential that you\nalso carefully monitor the clock offsets between all the machines. Any node whose\nclock drifts too far from the others should be declared dead and removed from the\ncluster. Such monitoring ensures that you notice the broken clocks before they can\ncause too much damage.\nTimestamps for ordering events\nLet\u2019s consider one particular situation in which it is tempting, but dangerous, to rely\non clocks: ordering of events across multiple nodes. For example, if two clients write\nto a distributed database, who got there first? Which write is the more recent one?\nFigure 8-3 illustrates a dangerous use of time-of-day clocks in a database with multi-\nleader replication (the example is similar to Figure 5-9). Client A writes x = 1 on node\n1; the write is replicated to node 3; client B increments x on node 3 (we now have\nx = 2); and finally, both writes are replicated to node 2.\nUnreliable Clocks | 291", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3bdfa28f-bc01-46f4-9d17-1c6765b0f4bd": {"__data__": {"id_": "3bdfa28f-bc01-46f4-9d17-1c6765b0f4bd", "embedding": null, "metadata": {"page_label": "292", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "870e626b-c339-4276-beb3-5c1352e52297", "node_type": "4", "metadata": {"page_label": "292", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "180a1175842f159551034d6680dccd1d0e190601a1f97f2a06efba0262ed9926", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 8-3. The write by client B is causally later than the write by client A, but B\u2019s\nwrite has an earlier timestamp.\nIn Figure 8-3, when a write is replicated to other nodes, it is tagged with a timestamp\naccording to the time-of-day clock on the node where the write originated. The clock\nsynchronization is very good in this example: the skew between node 1 and node 3 is\nless than 3 ms, which is probably better than you can expect in practice.\nNevertheless, the timestamps in Figure 8-3 fail to order the events correctly: the write\nx = 1 has a timestamp of 42.004 seconds, but the write x = 2 has a timestamp of\n42.003 seconds, even though x = 2 occurred unambiguously later. When node 2\nreceives these two events, it will incorrectly conclude that x = 1 is the more recent\nvalue and drop the write x = 2. In effect, client B\u2019s increment operation will be lost.\nThis conflict resolution strategy is called last write wins (LWW), and it is widely used\nin both multi-leader replication and leaderless databases such as Cassandra [ 53] and\nRiak [54] (see \u201cLast write wins (discarding concurrent writes)\u201d  on page 186). Some\nimplementations generate timestamps on the client rather than the server, but this\ndoesn\u2019t change the fundamental problems with LWW:\n\u2022 Database writes can mysteriously disappear: a node with a lagging clock is unable\nto overwrite values previously written by a node with a fast clock until the clock\nskew between the nodes has elapsed [ 54, 55]. This scenario can cause arbitrary\namounts of data to be silently dropped without any error being reported to the\napplication.\n\u2022 LWW cannot distinguish between writes that occurred sequentially in quick suc\u2010\ncession (in Figure 8-3 , client B\u2019s increment definitely occurs after client A\u2019s\nwrite) and writes that were truly concurrent (neither writer was aware of the\nother). Additional causality tracking mechanisms, such as version vectors, are\n292 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efe245cc-3a9b-4cef-8a8f-1921c3b8b737": {"__data__": {"id_": "efe245cc-3a9b-4cef-8a8f-1921c3b8b737", "embedding": null, "metadata": {"page_label": "293", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ed1ed0e-1db8-4616-b55b-d5e6628152f4", "node_type": "4", "metadata": {"page_label": "293", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "feca62c1d242da38083b67696ab0deeb315c651ba445851b89a0f271954ede2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "needed in order to prevent violations of causality (see \u201cDetecting Concurrent\nWrites\u201d on page 184).\n\u2022 It is possible for two nodes to independently generate writes with the same time\u2010\nstamp, especially when the clock only has millisecond resolution. An additional\ntiebreaker value (which can simply be a large random number) is required to\nresolve such conflicts, but this approach can also lead to violations of causality\n[53].\nThus, even though it is tempting to resolve conflicts by keeping the most \u201crecent\u201d\nvalue and discarding others, it\u2019s important to be aware that the definition of \u201crecent\u201d\ndepends on a local time-of-day clock, which may well be incorrect. Even with tightly\nNTP-synchronized clocks, you could send a packet at timestamp 100 ms (according\nto the sender\u2019s clock) and have it arrive at timestamp 99 ms (according to the recipi\u2010\nent\u2019s clock)\u2014so it appears as though the packet arrived before it was sent, which is\nimpossible.\nCould NTP synchronization be made accurate enough that such incorrect orderings\ncannot occur? Probably not, because NTP\u2019s synchronization accuracy is itself limited\nby the network round-trip time, in addition to other sources of error such as quartz\ndrift. For correct ordering, you would need the clock source to be significantly more\naccurate than the thing you are measuring (namely network delay).\nSo-called logical clocks  [56, 57], which are based on incrementing counters rather\nthan an oscillating quartz crystal, are a safer alternative for ordering events (see\n\u201cDetecting Concurrent Writes\u201d on page 184). Logical clocks do not measure the time\nof day or the number of seconds elapsed, only the relative ordering of events\n(whether one event happened before or after another). In contrast, time-of-day and\nmonotonic clocks, which measure actual elapsed time, are also known as physical\nclocks. We\u2019ll look at ordering a bit more in \u201cOrdering Guarantees\u201d on page 339.\nClock readings have a confidence interval\nYou may be able to read a machine\u2019s time-of-day clock with microsecond or even\nnanosecond resolution. But even if you can get such a fine-grained measurement,\nthat doesn\u2019t mean the value is actually accurate to such precision. In fact, it most\nlikely is not\u2014as mentioned previously, the drift in an imprecise quartz clock can\neasily be several milliseconds, even if you synchronize with an NTP server on the\nlocal network every minute. With an NTP server on the public internet, the best pos\u2010\nsible accuracy is probably to the tens of milliseconds, and the error may easily spike\nto over 100 ms when there is network congestion [57].\nThus, it doesn\u2019t make sense to think of a clock reading as a point in time\u2014it is more\nlike a range of times, within a confidence interval: for example, a system may be 95%\nconfident that the time now is between 10.3 and 10.5 seconds past the minute, but it\nUnreliable Clocks | 293", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc44001f-1e98-4112-bc79-b1168a49692f": {"__data__": {"id_": "dc44001f-1e98-4112-bc79-b1168a49692f", "embedding": null, "metadata": {"page_label": "294", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4964c129-c94a-456e-916a-33e51414535b", "node_type": "4", "metadata": {"page_label": "294", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b13c7f5b82a284bb40e31c7dd37b8c07b348ada3bf5adaaaddb331dc64ce2e3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "doesn\u2019t know any more precisely than that [58]. If we only know the time +/\u2013 100 ms,\nthe microsecond digits in the timestamp are essentially meaningless.\nThe uncertainty bound can be calculated based on your time source. If you have a\nGPS receiver or atomic (caesium) clock directly attached to your computer, the\nexpected error range is reported by the manufacturer. If you\u2019re getting the time from\na server, the uncertainty is based on the expected quartz drift since your last sync\nwith the server, plus the NTP server\u2019s uncertainty, plus the network round-trip time\nto the server (to a first approximation, and assuming you trust the server).\nUnfortunately, most systems don\u2019t expose this uncertainty: for example, when you\ncall clock_gettime(), the return value doesn\u2019t tell you the expected error of the\ntimestamp, so you don\u2019t know if its confidence interval is five milliseconds or five\nyears.\nAn interesting exception is Google\u2019s TrueTime API in Spanner [ 41], which explicitly\nreports the confidence interval on the local clock. When you ask it for the current\ntime, you get back two values: [earliest, latest], which are the earliest possible\nand the latest possible  timestamp. Based on its uncertainty calculations, the clock\nknows that the actual current time is somewhere within that interval. The width of\nthe interval depends, among other things, on how long it has been since the local\nquartz clock was last synchronized with a more accurate clock source. \nSynchronized clocks for global snapshots\nIn \u201cSnapshot Isolation and Repeatable Read\u201d on page 237 we discussed snapshot iso\u2010\nlation, which is a very useful feature in databases that need to support both small, fast\nread-write transactions and large, long-running read-only transactions (e.g., for\nbackups or analytics). It allows read-only transactions to see the database in a consis\u2010\ntent state at a particular point in time, without locking and interfering with read-\nwrite transactions.\nThe most common implementation of snapshot isolation requires a monotonically\nincreasing transaction ID. If a write happened later than the snapshot (i.e., the write\nhas a greater transaction ID than the snapshot), that write is invisible to the snapshot\ntransaction. On a single-node database, a simple counter is sufficient for generating\ntransaction IDs.\nHowever, when a database is distributed across many machines, potentially in multi\u2010\nple datacenters, a global, monotonically increasing transaction ID (across all parti\u2010\ntions) is difficult to generate, because it requires coordination. The transaction ID\nmust reflect causality: if transaction B reads a value that was written by transaction A,\nthen B must have a higher transaction ID than A\u2014otherwise, the snapshot would not\n294 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61a19777-6ed6-4461-8c50-d307c1671ccb": {"__data__": {"id_": "61a19777-6ed6-4461-8c50-d307c1671ccb", "embedding": null, "metadata": {"page_label": "295", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c17705d-01dd-4013-b142-da1c13c79ed6", "node_type": "4", "metadata": {"page_label": "295", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1ef50bc16be737d41193e313ab0931358d7bb08742ee5a18b13daebc82a8293c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vi. There are distributed sequence number generators, such as Twitter\u2019s Snowflake, that generate approxi\u2010\nmately monotonically increasing unique IDs in a scalable way (e.g., by allocating blocks of the ID space to\ndifferent nodes). However, they typically cannot guarantee an ordering that is consistent with causality,\nbecause the timescale at which blocks of IDs are assigned is longer than the timescale of database reads and\nwrites. See also \u201cOrdering Guarantees\u201d on page 339.\nbe consistent. With lots of small, rapid transactions, creating transaction IDs in a dis\u2010\ntributed system becomes an untenable bottleneck.vi\nCan we use the timestamps from synchronized time-of-day clocks as transaction IDs?\nIf we could get the synchronization good enough, they would have the right proper\u2010\nties: later transactions have a higher timestamp. The problem, of course, is the uncer\u2010\ntainty about clock accuracy.\nSpanner implements snapshot isolation across datacenters in this way [59, 60]. It uses\nthe clock\u2019s confidence interval as reported by the TrueTime API, and is based on the\nfollowing observation: if you have two confidence intervals, each consisting of an ear\u2010\nliest and latest possible timestamp ( A = [ Aearliest, Alatest] and B = [ Bearliest, Blatest]), and\nthose two intervals do not overlap (i.e., Aearliest < Alatest < Bearliest < Blatest), then B defi\u2010\nnitely happened after A\u2014there can be no doubt. Only if the intervals overlap are we\nunsure in which order A and B happened.\nIn order to ensure that transaction timestamps reflect causality, Spanner deliberately\nwaits for the length of the confidence interval before committing a read-write trans\u2010\naction. By doing so, it ensures that any transaction that may read the data is at a suffi\u2010\nciently later time, so their confidence intervals do not overlap. In order to keep the\nwait time as short as possible, Spanner needs to keep the clock uncertainty as small as\npossible; for this purpose, Google deploys a GPS receiver or atomic clock in each\ndatacenter, allowing clocks to be synchronized to within about 7 ms [41].\nUsing clock synchronization for distributed transaction semantics is an area of active\nresearch [57, 61, 62]. These ideas are interesting, but they have not yet been imple\u2010\nmented in mainstream databases outside of Google. \nProcess Pauses\nLet\u2019s consider another example of dangerous clock use in a distributed system. Say\nyou have a database with a single leader per partition. Only the leader is allowed to\naccept writes. How does a node know that it is still leader (that it hasn\u2019t been declared\ndead by the others), and that it may safely accept writes?\nOne option is for the leader to obtain a lease from the other nodes, which is similar to\na lock with a timeout [ 63]. Only one node can hold the lease at any one time\u2014thus,\nwhen a node obtains a lease, it knows that it is the leader for some amount of time,\nuntil the lease expires. In order to remain leader, the node must periodically renew\nUnreliable Clocks | 295", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b915999c-4e19-4802-952f-e65440abe990": {"__data__": {"id_": "b915999c-4e19-4802-952f-e65440abe990", "embedding": null, "metadata": {"page_label": "296", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64fffc0d-b0ac-4198-a2b3-fc555b5d8eee", "node_type": "4", "metadata": {"page_label": "296", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "538db98c9261acdaad4bbfbff454e8f611e9a74a96ab948853f1468662c85bf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the lease before it expires. If the node fails, it stops renewing the lease, so another\nnode can take over when it expires.\nYou can imagine the request-handling loop looking something like this:\nwhile (true) {\n    request = getIncomingRequest();\n    // Ensure that the lease always has at least 10 seconds remaining\n    if (lease.expiryTimeMillis - System.currentTimeMillis() < 10000) {\n        lease = lease.renew();\n    }\n    if (lease.isValid()) {\n        process(request);\n    }\n}\nWhat\u2019s wrong with this code? Firstly, it\u2019s relying on synchronized clocks: the expiry\ntime on the lease is set by a different machine (where the expiry may be calculated as\nthe current time plus 30 seconds, for example), and it\u2019s being compared to the local\nsystem clock. If the clocks are out of sync by more than a few seconds, this code will\nstart doing strange things.\nSecondly, even if we change the protocol to only use the local monotonic clock, there\nis another problem: the code assumes that very little time passes between the point\nthat it checks the time ( System.currentTimeMillis()) and the time when the\nrequest is processed ( process(request)). Normally this code runs very quickly, so\nthe 10 second buffer is more than enough to ensure that the lease doesn\u2019t expire in\nthe middle of processing a request.\nHowever, what if there is an unexpected pause in the execution of the program? For\nexample, imagine the thread stops for 15 seconds around the line lease.isValid()\nbefore finally continuing. In that case, it\u2019s likely that the lease will have expired by the\ntime the request is processed, and another node has already taken over as leader.\nHowever, there is nothing to tell this thread that it was paused for so long, so this\ncode won\u2019t notice that the lease has expired until the next iteration of the loop\u2014by\nwhich time it may have already done something unsafe by processing the request.\nIs it crazy to assume that a thread might be paused for so long? Unfortunately not.\nThere are various reasons why this could happen:\n\u2022 Many programming language runtimes (such as the Java Virtual Machine) have\na garbage collector  (GC) that occasionally needs to stop all running threads.\nThese \u201cstop-the-world\u201d GC pauses have sometimes been known to last for several\nminutes [ 64]! Even so-called \u201cconcurrent\u201d garbage collectors like the HotSpot\nJVM\u2019s CMS cannot fully run in parallel with the application code\u2014even they\nneed to stop the world from time to time [ 65]. Although the pauses can often be\n296 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "baaaaa1c-c866-4041-96f1-1a997847bfff": {"__data__": {"id_": "baaaaa1c-c866-4041-96f1-1a997847bfff", "embedding": null, "metadata": {"page_label": "297", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ec1b229-ea8d-49e2-810f-43774a11dadc", "node_type": "4", "metadata": {"page_label": "297", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6fd71e7f4d78dd29465cfb99b71c7107f67b9ab325f85b845457dc4bbef8e987", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "reduced by changing allocation patterns or tuning GC settings [ 66], we must\nassume the worst if we want to offer robust guarantees.\n\u2022 In virtualized environments, a virtual machine can be suspended (pausing the\nexecution of all processes and saving the contents of memory to disk) and\nresumed (restoring the contents of memory and continuing execution). This\npause can occur at any time in a process\u2019s execution and can last for an arbitrary\nlength of time. This feature is sometimes used for live migration  of virtual\nmachines from one host to another without a reboot, in which case the length of\nthe pause depends on the rate at which processes are writing to memory [67].\n\u2022 On end-user devices such as laptops, execution may also be suspended and\nresumed arbitrarily, e.g., when the user closes the lid of their laptop.\n\u2022 When the operating system context-switches to another thread, or when the\nhypervisor switches to a different virtual machine (when running in a virtual\nmachine), the currently running thread can be paused at any arbitrary point in\nthe code. In the case of a virtual machine, the CPU time spent in other virtual\nmachines is known as steal time. If the machine is under heavy load\u2014i.e., if there\nis a long queue of threads waiting to run\u2014it may take some time before the\npaused thread gets to run again.\n\u2022 If the application performs synchronous disk access, a thread may be paused\nwaiting for a slow disk I/O operation to complete [ 68]. In many languages, disk\naccess can happen surprisingly, even if the code doesn\u2019t explicitly mention file\naccess\u2014for example, the Java classloader lazily loads class files when they are first\nused, which could happen at any time in the program execution. I/O pauses and\nGC pauses may even conspire to combine their delays [ 69]. If the disk is actually\na network filesystem or network block device (such as Amazon\u2019s EBS), the I/O\nlatency is further subject to the variability of network delays [29].\n\u2022 If the operating system is configured to allow swapping to disk (paging), a simple\nmemory access may result in a page fault that requires a page from disk to be\nloaded into memory. The thread is paused while this slow I/O operation takes\nplace. If memory pressure is high, this may in turn require a different page to be\nswapped out to disk. In extreme circumstances, the operating system may spend\nmost of its time swapping pages in and out of memory and getting little actual\nwork done (this is known as thrashing). To avoid this problem, paging is often\ndisabled on server machines (if you would rather kill a process to free up mem\u2010\nory than risk thrashing).\n\u2022 A Unix process can be paused by sending it the SIGSTOP signal, for example by\npressing Ctrl-Z in a shell. This signal immediately stops the process from getting\nany more CPU cycles until it is resumed with SIGCONT, at which point it contin\u2010\nues running where it left off. Even if your environment does not normally use\nSIGSTOP, it might be sent accidentally by an operations engineer.\nUnreliable Clocks | 297", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fab27331-15dc-44f1-af30-9157c91fdd2e": {"__data__": {"id_": "fab27331-15dc-44f1-af30-9157c91fdd2e", "embedding": null, "metadata": {"page_label": "298", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8326b6d6-b9da-4823-bb9a-6206d306e680", "node_type": "4", "metadata": {"page_label": "298", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8836d92fb65d665b2e2f68afa0f1f8726910647aa01c6541aeb483ac67118108", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All of these occurrences can preempt the running thread at any point and resume it at\nsome later time, without the thread even noticing. The problem is similar to making\nmulti-threaded code on a single machine thread-safe: you can\u2019t assume anything\nabout timing, because arbitrary context switches and parallelism may occur.\nWhen writing multi-threaded code on a single machine, we have fairly good tools for\nmaking it thread-safe: mutexes, semaphores, atomic counters, lock-free data struc\u2010\ntures, blocking queues, and so on. Unfortunately, these tools don\u2019t directly translate\nto distributed systems, because a distributed system has no shared memory\u2014only\nmessages sent over an unreliable network.\nA node in a distributed system must assume that its execution can be paused for a\nsignificant length of time at any point, even in the middle of a function. During the\npause, the rest of the world keeps moving and may even declare the paused node\ndead because it\u2019s not responding. Eventually, the paused node may continue running,\nwithout even noticing that it was asleep until it checks its clock sometime later. \nResponse time guarantees\nIn many programming languages and operating systems, threads and processes may\npause for an unbounded amount of time, as discussed. Those reasons for pausing can\nbe eliminated if you try hard enough.\nSome software runs in environments where a failure to respond within a specified\ntime can cause serious damage: computers that control aircraft, rockets, robots, cars,\nand other physical objects must respond quickly and predictably to their sensor\ninputs. In these systems, there is a specified deadline by which the software must\nrespond; if it doesn\u2019t meet the deadline, that may cause a failure of the entire system.\nThese are so-called hard real-time systems.\nIs real-time really real?\nIn embedded systems, real-time means that a system is carefully\ndesigned and tested to meet specified timing guarantees in all cir\u2010\ncumstances. This meaning is in contrast to the more vague use of\nthe term real-time on the web, where it describes servers pushing\ndata to clients and stream processing without hard response time\nconstraints (see Chapter 11).\nFor example, if your car\u2019s onboard sensors detect that you are currently experiencing\na crash, you wouldn\u2019t want the release of the airbag to be delayed due to an inoppor\u2010\ntune GC pause in the airbag release system.\nProviding real-time guarantees in a system requires support from all levels of the\nsoftware stack: a real-time operating system (RTOS) that allows processes to be sched\u2010\nuled with a guaranteed allocation of CPU time in specified intervals is needed; library\n298 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "736b478b-e213-46db-8762-8c59c6ca6f03": {"__data__": {"id_": "736b478b-e213-46db-8762-8c59c6ca6f03", "embedding": null, "metadata": {"page_label": "299", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3dd3086-f639-4b29-b85f-44b42d7311b8", "node_type": "4", "metadata": {"page_label": "299", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e46511e8864947be21c7453b645b03ed655ab922f643dcf8ae187d3fea920b05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "functions must document their worst-case execution times; dynamic memory alloca\u2010\ntion may be restricted or disallowed entirely (real-time garbage collectors exist, but\nthe application must still ensure that it doesn\u2019t give the GC too much work to do);\nand an enormous amount of testing and measurement must be done to ensure that\nguarantees are being met.\nAll of this requires a large amount of additional work and severely restricts the range\nof programming languages, libraries, and tools that can be used (since most lan\u2010\nguages and tools do not provide real-time guarantees). For these reasons, developing\nreal-time systems is very expensive, and they are most commonly used in safety-\ncritical embedded devices. Moreover, \u201creal-time\u201d is not the same as \u201chigh-\nperformance\u201d\u2014in fact, real-time systems may have lower throughput, since they\nhave to prioritize timely responses above all else (see also \u201cLatency and Resource Uti\u2010\nlization\u201d on page 286).\nFor most server-side data processing systems, real-time guarantees are simply not\neconomical or appropriate. Consequently, these systems must suffer the pauses and\nclock instability that come from operating in a non-real-time environment.\nLimiting the impact of garbage collection\nThe negative effects of process pauses can be mitigated without resorting to expen\u2010\nsive real-time scheduling guarantees. Language runtimes have some flexibility\naround when they schedule garbage collections, because they can track the rate of\nobject allocation and the remaining free memory over time.\nAn emerging idea is to treat GC pauses like brief planned outages of a node, and to\nlet other nodes handle requests from clients while one node is collecting its garbage.\nIf the runtime can warn the application that a node soon requires a GC pause, the\napplication can stop sending new requests to that node, wait for it to finish process\u2010\ning outstanding requests, and then perform the GC while no requests are in progress.\nThis trick hides GC pauses from clients and reduces the high percentiles of response\ntime [70, 71]. Some latency-sensitive financial trading systems [72] use this approach.\nA variant of this idea is to use the garbage collector only for short-lived objects\n(which are fast to collect) and to restart processes periodically, before they accumu\u2010\nlate enough long-lived objects to require a full GC of long-lived objects [ 65, 73]. One\nnode can be restarted at a time, and traffic can be shifted away from the node before\nthe planned restart, like in a rolling upgrade (see Chapter 4).\nThese measures cannot fully prevent garbage collection pauses, but they can usefully\nreduce their impact on the application. \nUnreliable Clocks | 299", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba643ffe-56d7-482b-a16b-4b2b5a64d3c1": {"__data__": {"id_": "ba643ffe-56d7-482b-a16b-4b2b5a64d3c1", "embedding": null, "metadata": {"page_label": "300", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53b432d7-598d-4280-bb3e-bd446d7cc82e", "node_type": "4", "metadata": {"page_label": "300", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b3368dce0001ce1c7f2407c254a6322d91198a7a4a37482b39176e2ed481a262", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowledge, Truth, and Lies\nSo far in this chapter we have explored the ways in which distributed systems are dif\u2010\nferent from programs running on a single computer: there is no shared memory, only\nmessage passing via an unreliable network with variable delays, and the systems may\nsuffer from partial failures, unreliable clocks, and processing pauses.\nThe consequences of these issues are profoundly disorienting if you\u2019re not used to\ndistributed systems. A node in the network cannot know anything for sure\u2014it can\nonly make guesses based on the messages it receives (or doesn\u2019t receive) via the net\u2010\nwork. A node can only find out what state another node is in (what data it has stored,\nwhether it is correctly functioning, etc.) by exchanging messages with it. If a remote\nnode doesn\u2019t respond, there is no way of knowing what state it is in, because prob\u2010\nlems in the network cannot reliably be distinguished from problems at a node.\nDiscussions of these systems border on the philosophical: What do we know to be\ntrue or false in our system? How sure can we be of that knowledge, if the mechanisms\nfor perception and measurement are unreliable? Should software systems obey the\nlaws that we expect of the physical world, such as cause and effect?\nFortunately, we don\u2019t need to go as far as figuring out the meaning of life. In a dis\u2010\ntributed system, we can state the assumptions we are making about the behavior (the\nsystem model) and design the actual system in such a way that it meets those assump\u2010\ntions. Algorithms can be proved to function correctly within a certain system model.\nThis means that reliable behavior is achievable, even if the underlying system model\nprovides very few guarantees.\nHowever, although it is possible to make software well behaved in an unreliable sys\u2010\ntem model, it is not straightforward to do so. In the rest of this chapter we will further\nexplore the notions of knowledge and truth in distributed systems, which will help us\nthink about the kinds of assumptions we can make and the guarantees we may want\nto provide. In Chapter 9 we will proceed to look at some examples of distributed sys\u2010\ntems, algorithms that provide particular guarantees under particular assumptions.\nThe Truth Is Defined by the Majority\nImagine a network with an asymmetric fault: a node is able to receive all messages\nsent to it, but any outgoing messages from that node are dropped or delayed [ 19].\nEven though that node is working perfectly well, and is receiving requests from other\nnodes, the other nodes cannot hear its responses. After some timeout, the other\nnodes declare it dead, because they haven\u2019t heard from the node. The situation\nunfolds like a nightmare: the semi-disconnected node is dragged to the graveyard,\nkicking and screaming \u201cI\u2019m not dead!\u201d\u2014but since nobody can hear its screaming, the\nfuneral procession continues with stoic determination.\n300 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "767f35b3-1c06-4823-97d6-44da033cfdd0": {"__data__": {"id_": "767f35b3-1c06-4823-97d6-44da033cfdd0", "embedding": null, "metadata": {"page_label": "301", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3337c29f-6428-4c88-bba8-26230533f988", "node_type": "4", "metadata": {"page_label": "301", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "220d93ae1b8557a23ed7e9cf2181f31b19cfaac7bcea1b5ff0bef45d256010a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In a slightly less nightmarish scenario, the semi-disconnected node may notice that\nthe messages it is sending are not being acknowledged by other nodes, and so realize\nthat there must be a fault in the network. Nevertheless, the node is wrongly declared\ndead by the other nodes, and the semi-disconnected node cannot do anything about\nit.\nAs a third scenario, imagine a node that experiences a long stop-the-world garbage\ncollection pause. All of the node\u2019s threads are preempted by the GC and paused for\none minute, and consequently, no requests are processed and no responses are sent.\nThe other nodes wait, retry, grow impatient, and eventually declare the node dead\nand load it onto the hearse. Finally, the GC finishes and the node\u2019s threads continue\nas if nothing had happened. The other nodes are surprised as the supposedly dead\nnode suddenly raises its head out of the coffin, in full health, and starts cheerfully\nchatting with bystanders. At first, the GCing node doesn\u2019t even realize that an entire\nminute has passed and that it was declared dead\u2014from its perspective, hardly any\ntime has passed since it was last talking to the other nodes.\nThe moral of these stories is that a node cannot necessarily trust its own judgment of\na situation. A distributed system cannot exclusively rely on a single node, because a\nnode may fail at any time, potentially leaving the system stuck and unable to recover.\nInstead, many distributed algorithms rely on a quorum, that is, voting among the\nnodes (see \u201cQuorums for reading and writing\u201d on page 179): decisions require some\nminimum number of votes from several nodes in order to reduce the dependence on\nany one particular node.\nThat includes decisions about declaring nodes dead. If a quorum of nodes declares\nanother node dead, then it must be considered dead, even if that node still very much\nfeels alive. The individual node must abide by the quorum decision and step down.\nMost commonly, the quorum is an absolute majority of more than half the nodes\n(although other kinds of quorums are possible). A majority quorum allows the sys\u2010\ntem to continue working if individual nodes have failed (with three nodes, one failure\ncan be tolerated; with five nodes, two failures can be tolerated). However, it is still\nsafe, because there can only be only one majority in the system\u2014there cannot be two\nmajorities with conflicting decisions at the same time. We will discuss the use of quo\u2010\nrums in more detail when we get to consensus algorithms in Chapter 9.\nThe leader and the lock\nFrequently, a system requires there to be only one of some thing. For example:\n\u2022 Only one node is allowed to be the leader for a database partition, to avoid split\nbrain (see \u201cHandling Node Outages\u201d on page 156).\n\u2022 Only one transaction or client is allowed to hold the lock for a particular resource\nor object, to prevent concurrently writing to it and corrupting it.\nKnowledge, Truth, and Lies | 301", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1706a084-50d7-4251-8447-5dbd68d7f39f": {"__data__": {"id_": "1706a084-50d7-4251-8447-5dbd68d7f39f", "embedding": null, "metadata": {"page_label": "302", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da4f8257-8431-4e24-8e8d-8189011b9d3e", "node_type": "4", "metadata": {"page_label": "302", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1f55ab77eebd2b8c4f1987a29c86172c1a218a2916149cfee93b765f23c6496b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Only one user is allowed to register a particular username, because a username\nmust uniquely identify a user.\nImplementing this in a distributed system requires care: even if a node believes that it\nis \u201cthe chosen one\u201d (the leader of the partition, the holder of the lock, the request\nhandler of the user who successfully grabbed the username), that doesn\u2019t necessarily\nmean a quorum of nodes agrees! A node may have formerly been the leader, but if\nthe other nodes declared it dead in the meantime (e.g., due to a network interruption\nor GC pause), it may have been demoted and another leader may have already been\nelected.\nIf a node continues acting as the chosen one, even though the majority of nodes have\ndeclared it dead, it could cause problems in a system that is not carefully designed.\nSuch a node could send messages to other nodes in its self-appointed capacity, and if\nother nodes believe it, the system as a whole may do something incorrect.\nFor example, Figure 8-4 shows a data corruption bug due to an incorrect implemen\u2010\ntation of locking. (The bug is not theoretical: HBase used to have this problem [ 74,\n75].) Say you want to ensure that a file in a storage service can only be accessed by\none client at a time, because if multiple clients tried to write to it, the file would\nbecome corrupted. You try to implement this by requiring a client to obtain a lease\nfrom a lock service before accessing the file.\nFigure 8-4. Incorrect implementation of a distributed lock: client 1 believes that it still\nhas a valid lease, even though it has expired, and thus corrupts a file in storage.\nThe problem is an example of what we discussed in \u201cProcess Pauses\u201d on page 295: if\nthe client holding the lease is paused for too long, its lease expires. Another client can\nobtain a lease for the same file, and start writing to the file. When the paused client\ncomes back, it believes (incorrectly) that it still has a valid lease and proceeds to also\nwrite to the file. As a result, the clients\u2019 writes clash and corrupt the file.\n302 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb03cd18-f380-4d13-bce7-64ee7c5004f9": {"__data__": {"id_": "cb03cd18-f380-4d13-bce7-64ee7c5004f9", "embedding": null, "metadata": {"page_label": "303", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "805aea54-bc7d-433e-90be-4d8e421a1b25", "node_type": "4", "metadata": {"page_label": "303", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "65face958dc719f14eb725b0f8775e6e43aeb11e8d52239382041dfc4069a9db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fencing tokens\nWhen using a lock or lease to protect access to some resource, such as the file storage\nin Figure 8-4, we need to ensure that a node that is under a false belief of being \u201cthe\nchosen one\u201d cannot disrupt the rest of the system. A fairly simple technique that ach\u2010\nieves this goal is called fencing, and is illustrated in Figure 8-5.\nFigure 8-5. Making access to storage safe by allowing writes only in the order of increas\u2010\ning fencing tokens.\nLet\u2019s assume that every time the lock server grants a lock or lease, it also returns a\nfencing token , which is a number that increases every time a lock is granted (e.g.,\nincremented by the lock service). We can then require that every time a client sends a\nwrite request to the storage service, it must include its current fencing token.\nIn Figure 8-5, client 1 acquires the lease with a token of 33, but then it goes into a\nlong pause and the lease expires. Client 2 acquires the lease with a token of 34 (the\nnumber always increases) and then sends its write request to the storage service,\nincluding the token of 34. Later, client 1 comes back to life and sends its write to the\nstorage service, including its token value 33. However, the storage server remembers\nthat it has already processed a write with a higher token number (34), and so it rejects\nthe request with token 33.\nIf ZooKeeper is used as lock service, the transaction ID zxid or the node version\ncversion can be used as fencing token. Since they are guaranteed to be monotoni\u2010\ncally increasing, they have the required properties [74].\nNote that this mechanism requires the resource itself to take an active role in check\u2010\ning tokens by rejecting any writes with an older token than one that has already been\nprocessed\u2014it is not sufficient to rely on clients checking their lock status themselves.\nFor resources that do not explicitly support fencing tokens, you might still be able\nwork around the limitation (for example, in the case of a file storage service you\ncould include the fencing token in the filename). However, some kind of check is\nnecessary to avoid processing requests outside of the lock\u2019s protection.\nKnowledge, Truth, and Lies | 303", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e4eccff-29f1-4199-ac74-481319a7caab": {"__data__": {"id_": "1e4eccff-29f1-4199-ac74-481319a7caab", "embedding": null, "metadata": {"page_label": "304", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd11957a-d0e9-46f1-a2de-544a1101cb70", "node_type": "4", "metadata": {"page_label": "304", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "327a97b3fb152ab7ae1455999dc8eac82b1ee2fc7e428576337272d1367bd005", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Checking a token on the server side may seem like a downside, but it is arguably a\ngood thing: it is unwise for a service to assume that its clients will always be well\nbehaved, because the clients are often run by people whose priorities are very differ\u2010\nent from the priorities of the people running the service [ 76]. Thus, it is a good idea\nfor any service to protect itself from accidentally abusive clients. \nByzantine Faults\nFencing tokens can detect and block a node that is inadvertently acting in error (e.g.,\nbecause it hasn\u2019t yet found out that its lease has expired). However, if the node delib\u2010\nerately wanted to subvert the system\u2019s guarantees, it could easily do so by sending\nmessages with a fake fencing token.\nIn this book we assume that nodes are unreliable but honest: they may be slow or\nnever respond (due to a fault), and their state may be outdated (due to a GC pause or\nnetwork delays), but we assume that if a node does respond, it is telling the \u201ctruth\u201d: to\nthe best of its knowledge, it is playing by the rules of the protocol.\nDistributed systems problems become much harder if there is a risk that nodes may\n\u201clie\u201d (send arbitrary faulty or corrupted responses)\u2014for example, if a node may claim\nto have received a particular message when in fact it didn\u2019t. Such behavior is known\nas a Byzantine fault, and the problem of reaching consensus in this untrusting envi\u2010\nronment is known as the Byzantine Generals Problem [77].\nThe Byzantine Generals Problem\nThe Byzantine Generals Problem is a generalization of the so-called Two Generals\nProblem [78], which imagines a situation in which two army generals need to agree\non a battle plan. As they have set up camp on two different sites, they can only com\u2010\nmunicate by messenger, and the messengers sometimes get delayed or lost (like pack\u2010\nets in a network). We will discuss this problem of consensus in Chapter 9.\nIn the Byzantine version of the problem, there are n generals who need to agree, and\ntheir endeavor is hampered by the fact that there are some traitors in their midst.\nMost of the generals are loyal, and thus send truthful messages, but the traitors may\ntry to deceive and confuse the others by sending fake or untrue messages (while try\u2010\ning to remain undiscovered). It is not known in advance who the traitors are.\nByzantium was an ancient Greek city that later became Constantinople, in the place\nwhich is now Istanbul in Turkey. There isn\u2019t any historic evidence that the generals of\nByzantium were any more prone to intrigue and conspiracy than those elsewhere.\nRather, the name is derived from Byzantine in the sense of excessively complicated,\nbureaucratic, devious, which was used in politics long before computers [ 79]. Lamp\u2010\nort wanted to choose a nationality that would not offend any readers, and he was\nadvised that calling it The Albanian Generals Problem was not such a good idea [80].\n304 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f71136d-ff0e-4237-b864-c311d4bb8d9d": {"__data__": {"id_": "8f71136d-ff0e-4237-b864-c311d4bb8d9d", "embedding": null, "metadata": {"page_label": "305", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e011e8d8-0f71-4918-b56e-0cf1f9df4e58", "node_type": "4", "metadata": {"page_label": "305", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5a3cdfc4efb8ab2fd111bc03a4f41355a4ffb64b9d2ed5cc144a144236259064", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A system is Byzantine fault-tolerant if it continues to operate correctly even if some\nof the nodes are malfunctioning and not obeying the protocol, or if malicious attack\u2010\ners are interfering with the network. This concern is relevant in certain specific cir\u2010\ncumstances. For example:\n\u2022 In aerospace environments, the data in a computer\u2019s memory or CPU register\ncould become corrupted by radiation, leading it to respond to other nodes in\narbitrarily unpredictable ways. Since a system failure would be very expensive\n(e.g., an aircraft crashing and killing everyone on board, or a rocket colliding\nwith the International Space Station), flight control systems must tolerate Byzan\u2010\ntine faults [81, 82].\n\u2022 In a system with multiple participating organizations, some participants may\nattempt to cheat or defraud others. In such circumstances, it is not safe for a\nnode to simply trust another node\u2019s messages, since they may be sent with mali\u2010\ncious intent. For example, peer-to-peer networks like Bitcoin and other block\u2010\nchains can be considered to be a way of getting mutually untrusting parties to\nagree whether a transaction happened or not, without relying on a central\nauthority [83].\nHowever, in the kinds of systems we discuss in this book, we can usually safely\nassume that there are no Byzantine faults. In your datacenter, all the nodes are con\u2010\ntrolled by your organization (so they can hopefully be trusted) and radiation levels\nare low enough that memory corruption is not a major problem. Protocols for mak\u2010\ning systems Byzantine fault-tolerant are quite complicated [ 84], and fault-tolerant\nembedded systems rely on support from the hardware level [ 81]. In most server-side\ndata systems, the cost of deploying Byzantine fault-tolerant solutions makes them\nimpractical.\nWeb applications do need to expect arbitrary and malicious behavior of clients that\nare under end-user control, such as web browsers. This is why input validation, sani\u2010\ntization, and output escaping are so important: to prevent SQL injection and cross-\nsite scripting, for example. However, we typically don\u2019t use Byzantine fault-tolerant\nprotocols here, but simply make the server the authority on deciding what client\nbehavior is and isn\u2019t allowed. In peer-to-peer networks, where there is no such cen\u2010\ntral authority, Byzantine fault tolerance is more relevant.\nA bug in the software could be regarded as a Byzantine fault, but if you deploy the\nsame software to all nodes, then a Byzantine fault-tolerant algorithm cannot save you.\nMost Byzantine fault-tolerant algorithms require a supermajority of more than two-\nthirds of the nodes to be functioning correctly (i.e., if you have four nodes, at most\none may malfunction). To use this approach against bugs, you would have to have\nfour independent implementations of the same software and hope that a bug only\nappears in one of the four implementations.\nKnowledge, Truth, and Lies | 305", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91bc95a3-761e-4105-813e-9e8b1c726b8a": {"__data__": {"id_": "91bc95a3-761e-4105-813e-9e8b1c726b8a", "embedding": null, "metadata": {"page_label": "306", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc5b80f8-5814-414b-8e4d-4a3056d6b9f3", "node_type": "4", "metadata": {"page_label": "306", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3923cc8733f3178a88fc802b8547991bcdbc445706cb50c559cc075cac9497b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly, it would be appealing if a protocol could protect us from vulnerabilities,\nsecurity compromises, and malicious attacks. Unfortunately, this is not realistic\neither: in most systems, if an attacker can compromise one node, they can probably\ncompromise all of them, because they are probably running the same software. Thus,\ntraditional mechanisms (authentication, access control, encryption, firewalls, and so\non) continue to be the main protection against attackers.\nWeak forms of lying\nAlthough we assume that nodes are generally honest, it can be worth adding mecha\u2010\nnisms to software that guard against weak forms of \u201clying\u201d\u2014for example, invalid\nmessages due to hardware issues, software bugs, and misconfiguration. Such protec\u2010\ntion mechanisms are not full-blown Byzantine fault tolerance, as they would not\nwithstand a determined adversary, but they are nevertheless simple and pragmatic\nsteps toward better reliability. For example:\n\u2022 Network packets do sometimes get corrupted due to hardware issues or bugs in\noperating systems, drivers, routers, etc. Usually, corrupted packets are caught by\nthe checksums built into TCP and UDP, but sometimes they evade detection [ 85,\n86, 87]. Simple measures are usually sufficient protection against such corrup\u2010\ntion, such as checksums in the application-level protocol.\n\u2022 A publicly accessible application must carefully sanitize any inputs from users,\nfor example checking that a value is within a reasonable range and limiting the\nsize of strings to prevent denial of service through large memory allocations. An\ninternal service behind a firewall may be able to get away with less strict checks\non inputs, but some basic sanity-checking of values (e.g., in protocol parsing\n[85]) is a good idea.\n\u2022 NTP clients can be configured with multiple server addresses. When synchroniz\u2010\ning, the client contacts all of them, estimates their errors, and checks that a\nmajority of servers agree on some time range. As long as most of the servers are\nokay, a misconfigured NTP server that is reporting an incorrect time is detected\nas an outlier and is excluded from synchronization [ 37]. The use of multiple\nservers makes NTP more robust than if it only uses a single server. \nSystem Model and Reality\nMany algorithms have been designed to solve distributed systems problems\u2014for\nexample, we will examine solutions for the consensus problem in Chapter 9. In order\nto be useful, these algorithms need to tolerate the various faults of distributed systems\nthat we discussed in this chapter.\nAlgorithms need to be written in a way that does not depend too heavily on the\ndetails of the hardware and software configuration on which they are run. This in\n306 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d134199f-e7f6-4340-abef-50cb140fa5e8": {"__data__": {"id_": "d134199f-e7f6-4340-abef-50cb140fa5e8", "embedding": null, "metadata": {"page_label": "307", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7185b27-654a-45aa-80e2-62b8cd9e817e", "node_type": "4", "metadata": {"page_label": "307", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9f46c94541d71dff3e282f12f87094f2c5e43d130c707035917668109eaf688f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "turn requires that we somehow formalize the kinds of faults that we expect to happen\nin a system. We do this by defining a system model , which is an abstraction that\ndescribes what things an algorithm may assume.\nWith regard to timing assumptions, three system models are in common use:\nSynchronous model\nThe synchronous model assumes bounded network delay, bounded process pau\u2010\nses, and bounded clock error. This does not imply exactly synchronized clocks or\nzero network delay; it just means you know that network delay, pauses, and clock\ndrift will never exceed some fixed upper bound [ 88]. The synchronous model is\nnot a realistic model of most practical systems, because (as discussed in this\nchapter) unbounded delays and pauses do occur.\nPartially synchronous model\nPartial synchrony means that a system behaves like a synchronous system most of\nthe time, but it sometimes exceeds the bounds for network delay, process pauses,\nand clock drift [ 88]. This is a realistic model of many systems: most of the time,\nnetworks and processes are quite well behaved\u2014otherwise we would never be\nable to get anything done\u2014but we have to reckon with the fact that any timing\nassumptions may be shattered occasionally. When this happens, network delay,\npauses, and clock error may become arbitrarily large.\nAsynchronous model\nIn this model, an algorithm is not allowed to make any timing assumptions\u2014in\nfact, it does not even have a clock (so it cannot use timeouts). Some algorithms\ncan be designed for the asynchronous model, but it is very restrictive.\nMoreover, besides timing issues, we have to consider node failures. The three most\ncommon system models for nodes are:\nCrash-stop faults\nIn the crash-stop model, an algorithm may assume that a node can fail in only\none way, namely by crashing. This means that the node may suddenly stop\nresponding at any moment, and thereafter that node is gone forever\u2014it never\ncomes back.\nCrash-recovery faults\nWe assume that nodes may crash at any moment, and perhaps start responding\nagain after some unknown time. In the crash-recovery model, nodes are assumed\nto have stable storage (i.e., nonvolatile disk storage) that is preserved across\ncrashes, while the in-memory state is assumed to be lost.\nByzantine (arbitrary) faults\nNodes may do absolutely anything, including trying to trick and deceive other\nnodes, as described in the last section.\nKnowledge, Truth, and Lies | 307", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f152fba-89e1-4850-b131-ff401ccedb7e": {"__data__": {"id_": "0f152fba-89e1-4850-b131-ff401ccedb7e", "embedding": null, "metadata": {"page_label": "308", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7da5b45b-7598-45d9-998d-2531afcfb62b", "node_type": "4", "metadata": {"page_label": "308", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1cb6c429010143712c12c932a3d6ea8f8ff5a83e9393ec40cf9ea627ef38807a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For modeling real systems, the partially synchronous model with crash-recovery\nfaults is generally the most useful model. But how do distributed algorithms cope\nwith that model?\nCorrectness of an algorithm\nTo define what it means for an algorithm to be correct, we can describe its properties.\nFor example, the output of a sorting algorithm has the property that for any two dis\u2010\ntinct elements of the output list, the element further to the left is smaller than the ele\u2010\nment further to the right. That is simply a formal way of defining what it means for a\nlist to be sorted.\nSimilarly, we can write down the properties we want of a distributed algorithm to\ndefine what it means to be correct. For example, if we are generating fencing tokens\nfor a lock (see \u201cFencing tokens\u201d on page 303), we may require the algorithm to have\nthe following properties:\nUniqueness\nNo two requests for a fencing token return the same value.\nMonotonic sequence\nIf request x returned token tx, and request y returned token ty, and x completed\nbefore y began, then tx < ty.\nAvailability\nA node that requests a fencing token and does not crash eventually receives a\nresponse.\nAn algorithm is correct in some system model if it always satisfies its properties in all\nsituations that we assume may occur in that system model. But how does this make\nsense? If all nodes crash, or all network delays suddenly become infinitely long, then\nno algorithm will be able to get anything done.\nSafety and liveness\nTo clarify the situation, it is worth distinguishing between two different kinds of\nproperties: safety and liveness properties. In the example just given, uniqueness and\nmonotonic sequence are safety properties, but availability is a liveness property.\nWhat distinguishes the two kinds of properties? A giveaway is that liveness properties\noften include the word \u201ceventually\u201d in their definition. (And yes, you guessed it\u2014\neventual consistency is a liveness property [89].)\nSafety is often informally defined as nothing bad happens , and liveness as something\ngood eventually happens. However, it\u2019s best to not read too much into those informal\ndefinitions, because the meaning of good and bad is subjective. The actual definitions\nof safety and liveness are precise and mathematical [90]:\n308 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c0a0407-70ed-4ed3-9599-749d72a747e6": {"__data__": {"id_": "2c0a0407-70ed-4ed3-9599-749d72a747e6", "embedding": null, "metadata": {"page_label": "309", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee31911-576c-44eb-a412-8552d0ffb9c9", "node_type": "4", "metadata": {"page_label": "309", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "06d7da3e818ee6a1ce9e455e5af978bf83e632b72b97610ebd7e959b460d713f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If a safety property is violated, we can point at a particular point in time at which\nit was broken (for example, if the uniqueness property was violated, we can iden\u2010\ntify the particular operation in which a duplicate fencing token was returned).\nAfter a safety property has been violated, the violation cannot be undone\u2014the\ndamage is already done.\n\u2022 A liveness property works the other way round: it may not hold at some point in\ntime (for example, a node may have sent a request but not yet received a\nresponse), but there is always hope that it may be satisfied in the future (namely\nby receiving a response).\nAn advantage of distinguishing between safety and liveness properties is that it helps\nus deal with difficult system models. For distributed algorithms, it is common to\nrequire that safety properties always hold, in all possible situations of a system model\n[88]. That is, even if all nodes crash, or the entire network fails, the algorithm must\nnevertheless ensure that it does not return a wrong result (i.e., that the safety proper\u2010\nties remain satisfied).\nHowever, with liveness properties we are allowed to make caveats: for example, we\ncould say that a request needs to receive a response only if a majority of nodes have\nnot crashed, and only if the network eventually recovers from an outage. The defini\u2010\ntion of the partially synchronous model requires that eventually the system returns to\na synchronous state\u2014that is, any period of network interruption lasts only for a finite\nduration and is then repaired.\nMapping system models to the real world\nSafety and liveness properties and system models are very useful for reasoning about\nthe correctness of a distributed algorithm. However, when implementing an algo\u2010\nrithm in practice, the messy facts of reality come back to bite you again, and it\nbecomes clear that the system model is a simplified abstraction of reality.\nFor example, algorithms in the crash-recovery model generally assume that data in\nstable storage survives crashes. However, what happens if the data on disk is corrup\u2010\nted, or the data is wiped out due to hardware error or misconfiguration [ 91]? What\nhappens if a server has a firmware bug and fails to recognize its hard drives on\nreboot, even though the drives are correctly attached to the server [92]?\nQuorum algorithms (see \u201cQuorums for reading and writing\u201d  on page 179) rely on a\nnode remembering the data that it claims to have stored. If a node may suffer from\namnesia and forget previously stored data, that breaks the quorum condition, and\nthus breaks the correctness of the algorithm. Perhaps a new system model is needed,\nin which we assume that stable storage mostly survives crashes, but may sometimes\nbe lost. But that model then becomes harder to reason about.\nKnowledge, Truth, and Lies | 309", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30a61c3b-d1ad-4c13-98f5-091a416ede63": {"__data__": {"id_": "30a61c3b-d1ad-4c13-98f5-091a416ede63", "embedding": null, "metadata": {"page_label": "310", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f512eb2-6e49-49e9-b014-22933af6692b", "node_type": "4", "metadata": {"page_label": "310", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7bab17b9dab309d1c51547cb1e9c4c8e45d3b65c7f63d1bb6f4232718b783489", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The theoretical description of an algorithm can declare that certain things are simply\nassumed not to happen\u2014and in non-Byzantine systems, we do have to make some\nassumptions about faults that can and cannot happen. However, a real implementa\u2010\ntion may still have to include code to handle the case where something happens that\nwas assumed to be impossible, even if that handling boils down to printf(\"Sucks to\nbe you\") and exit(666)\u2014i.e., letting a human operator clean up the mess [ 93].\n(This is arguably the difference between computer science and software engineering.)\nThat is not to say that theoretical, abstract system models are worthless\u2014quite the\nopposite. They are incredibly helpful for distilling down the complexity of real sys\u2010\ntems to a manageable set of faults that we can reason about, so that we can under\u2010\nstand the problem and try to solve it systematically. We can prove algorithms correct\nby showing that their properties always hold in some system model.\nProving an algorithm correct does not mean its implementation on a real system will\nnecessarily always behave correctly. But it\u2019s a very good first step, because the theo\u2010\nretical analysis can uncover problems in an algorithm that might remain hidden for a\nlong time in a real system, and that only come to bite you when your assumptions\n(e.g., about timing) are defeated due to unusual circumstances. Theoretical analysis\nand empirical testing are equally important. \nSummary\nIn this chapter we have discussed a wide range of problems that can occur in dis\u2010\ntributed systems, including:\n\u2022 Whenever you try to send a packet over the network, it may be lost or arbitrarily\ndelayed. Likewise, the reply may be lost or delayed, so if you don\u2019t get a reply,\nyou have no idea whether the message got through.\n\u2022 A node\u2019s clock may be significantly out of sync with other nodes (despite your\nbest efforts to set up NTP), it may suddenly jump forward or back in time, and\nrelying on it is dangerous because you most likely don\u2019t have a good measure of\nyour clock\u2019s error interval.\n\u2022 A process may pause for a substantial amount of time at any point in its execu\u2010\ntion (perhaps due to a stop-the-world garbage collector), be declared dead by\nother nodes, and then come back to life again without realizing that it was\npaused.\nThe fact that such partial failures  can occur is the defining characteristic of dis\u2010\ntributed systems. Whenever software tries to do anything involving other nodes,\nthere is the possibility that it may occasionally fail, or randomly go slow, or not\nrespond at all (and eventually time out). In distributed systems, we try to build toler\u2010\n310 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c603a64-d67d-4d8e-a0d2-d7f0f3e269d0": {"__data__": {"id_": "6c603a64-d67d-4d8e-a0d2-d7f0f3e269d0", "embedding": null, "metadata": {"page_label": "311", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "802d2266-c773-464d-b233-cec2ba0e113f", "node_type": "4", "metadata": {"page_label": "311", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ef6ef8d607a311ab5b62614be579e28bcc8d51106da1685dd47f72183856afba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ance of partial failures into software, so that the system as a whole may continue\nfunctioning even when some of its constituent parts are broken.\nTo tolerate faults, the first step is to detect them, but even that is hard. Most systems\ndon\u2019t have an accurate mechanism of detecting whether a node has failed, so most\ndistributed algorithms rely on timeouts to determine whether a remote node is still\navailable. However, timeouts can\u2019t distinguish between network and node failures,\nand variable network delay sometimes causes a node to be falsely suspected of crash\u2010\ning. Moreover, sometimes a node can be in a degraded state: for example, a Gigabit\nnetwork interface could suddenly drop to 1 Kb/s throughput due to a driver bug [ 94].\nSuch a node that is \u201climping\u201d but not dead can be even more difficult to deal with\nthan a cleanly failed node.\nOnce a fault is detected, making a system tolerate it is not easy either: there is no\nglobal variable, no shared memory, no common knowledge or any other kind of\nshared state between the machines. Nodes can\u2019t even agree on what time it is, let\nalone on anything more profound. The only way information can flow from one\nnode to another is by sending it over the unreliable network. Major decisions cannot\nbe safely made by a single node, so we require protocols that enlist help from other\nnodes and try to get a quorum to agree.\nIf you\u2019re used to writing software in the idealized mathematical perfection of a single\ncomputer, where the same operation always deterministically returns the same result,\nthen moving to the messy physical reality of distributed systems can be a bit of a\nshock. Conversely, distributed systems engineers will often regard a problem as triv\u2010\nial if it can be solved on a single computer [ 5], and indeed a single computer can do a\nlot nowadays [95]. If you can avoid opening Pandora\u2019s box and simply keep things on\na single machine, it is generally worth doing so.\nHowever, as discussed in the introduction to Part II, scalability is not the only reason\nfor wanting to use a distributed system. Fault tolerance and low latency (by placing\ndata geographically close to users) are equally important goals, and those things can\u2010\nnot be achieved with a single node.\nIn this chapter we also went on some tangents to explore whether the unreliability of\nnetworks, clocks, and processes is an inevitable law of nature. We saw that it isn\u2019t: it\nis possible to give hard real-time response guarantees and bounded delays in net\u2010\nworks, but doing so is very expensive and results in lower utilization of hardware\nresources. Most non-safety-critical systems choose cheap and unreliable over expen\u2010\nsive and reliable.\nWe also touched on supercomputers, which assume reliable components and thus\nhave to be stopped and restarted entirely when a component does fail. By contrast,\ndistributed systems can run forever without being interrupted at the service level,\nbecause all faults and maintenance can be handled at the node level\u2014at least in\nSummary | 311", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6d59a26-7c14-4944-add8-2246b00b3e88": {"__data__": {"id_": "c6d59a26-7c14-4944-add8-2246b00b3e88", "embedding": null, "metadata": {"page_label": "312", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa37e8b2-13a0-4eb4-98b4-c84698883a2c", "node_type": "4", "metadata": {"page_label": "312", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0fb6896c763c33c49d5d177c533ba64bb90e1baf84ef44643b2e1ceed740b175", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "theory. (In practice, if a bad configuration change is rolled out to all nodes, that will\nstill bring a distributed system to its knees.)\nThis chapter has been all about problems, and has given us a bleak outlook. In the\nnext chapter we will move on to solutions, and discuss some algorithms that have\nbeen designed to cope with all the problems in distributed systems. \nReferences\n[1] Mark Cavage: \u201c There\u2019s Just No Getting Around It: You\u2019re Building a Distributed\nSystem,\u201d ACM Queue , volume 11, number 4, pages 80-89, April 2013. doi:\n10.1145/2466486.2482856\n[2] Jay Kreps: \u201c Getting Real About Distributed System Reliability ,\u201d blog.empathy\u2010\nbox.com, March 19, 2012.\n[3] Sydney Padua: The Thrilling Adventures of Lovelace and Babbage: The (Mostly)\nTrue Story of the First Computer . Particular Books, April 2015. ISBN:\n978-0-141-98151-2\n[4] Coda Hale: \u201c You Can\u2019t Sacrifice Partition Tolerance ,\u201d codahale.com, October 7,\n2010.\n[5] Jeff Hodges: \u201c Notes on Distributed Systems for Young Bloods ,\u201d somethingsimi\u2010\nlar.com, January 14, 2013.\n[6] Antonio Regalado: \u201c Who Coined \u2018Cloud Computing\u2019? ,\u201d technologyreview.com,\nOctober 31, 2011.\n[7] Luiz Andr\u00e9 Barroso, Jimmy Clidaras, and Urs H\u00f6lzle: \u201c The Datacenter as a Com\u2010\nputer: An Introduction to the Design of Warehouse-Scale Machines, Second Edi\u2010\ntion,\u201d Synthesis Lectures on Computer Architecture , volume 8, number 3, Morgan &\nClaypool Publishers, July 2013. doi:10.2200/S00516ED2V01Y201306CAC024, ISBN:\n978-1-627-05010-4\n[8] David Fiala, Frank Mueller, Christian Engelmann, et al.: \u201c Detection and Correc\u2010\ntion of Silent Data Corruption for Large-Scale High-Performance Computing ,\u201d at\nInternational Conference for High Performance Computing, Networking, Storage and\nAnalysis (SC12), November 2012.\n[9] Arjun Singh, Joon Ong, Amit Agarwal, et al.: \u201c Jupiter Rising: A Decade of Clos\nTopologies and Centralized Control in Google\u2019s Datacenter Network ,\u201d at Annual\nConference of the ACM Special Interest Group on Data Communication (SIGCOMM),\nAugust 2015. doi:10.1145/2785956.2787508\n[10] Glenn K. Lockwood: \u201c Hadoop\u2019s Uncomfortable Fit in HPC ,\u201d glennklock\u2010\nwood.blogspot.co.uk, May 16, 2014.\n312 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2716131-b820-4f98-a85e-1927b0ed2e9f": {"__data__": {"id_": "a2716131-b820-4f98-a85e-1927b0ed2e9f", "embedding": null, "metadata": {"page_label": "313", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f9f5dc8-7542-499b-a32f-e9f3fc19cd02", "node_type": "4", "metadata": {"page_label": "313", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "efa2e0d149d6ab1df847ea84a8f0ac961b428874bff30c6969335fd9b2ccd4d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[11] John von Neumann: \u201c Probabilistic Logics and the Synthesis of Reliable Organ\u2010\nisms from Unreliable Components ,\u201d in Automata Studies (AM-34), edited by Claude\nE. Shannon and John McCarthy, Princeton University Press, 1956. ISBN:\n978-0-691-07916-5\n[12] Richard W. Hamming: The Art of Doing Science and Engineering. Taylor & Fran\u2010\ncis, 1997. ISBN: 978-9-056-99500-3\n[13] Claude E. Shannon: \u201c A Mathematical Theory of Communication ,\u201d The Bell Sys\u2010\ntem Technical Journal, volume 27, number 3, pages 379\u2013423 and 623\u2013656, July 1948.\n[14] Peter Bailis and Kyle Kingsbury: \u201c The Network Is Reliable ,\u201d ACM Queue, vol\u2010\nume 12, number 7, pages 48-55, July 2014. doi:10.1145/2639988.2639988\n[15] Joshua B. Leners, Trinabh Gupta, Marcos K. Aguilera, and Michael Walfish:\n\u201cTaming Uncertainty in Distributed Systems with Help from the Network ,\u201d at 10th\nEuropean Conference on Computer Systems  (EuroSys), April 2015. doi:\n10.1145/2741948.2741976\n[16] Phillipa Gill, Navendu Jain, and Nachiappan Nagappan: \u201c Understanding Net\u2010\nwork Failures in Data Centers: Measurement, Analysis, and Implications ,\u201d at ACM\nSIGCOMM Conference, August 2011. doi:10.1145/2018436.2018477\n[17] Mark Imbriaco: \u201cDowntime Last Saturday,\u201d github.com, December 26, 2012.\n[18] Will Oremus: \u201c The Global Internet Is Being Attacked by Sharks, Google Con\u2010\nfirms,\u201d slate.com, August 15, 2014.\n[19] Marc A. Donges: \u201c Re: bnx2 cards Intermittantly Going Offline ,\u201d Message to\nLinux netdev mailing list, spinics.net, September 13, 2012.\n[20] Kyle Kingsbury: \u201cCall Me Maybe: Elasticsearch,\u201d aphyr.com, June 15, 2014.\n[21] Salvatore Sanfilippo: \u201c A Few Arguments About Redis Sentinel Properties and\nFail Scenarios,\u201d antirez.com, October 21, 2014.\n[22] Bert Hubert: \u201c The Ultimate SO_LINGER Page, or: Why Is My TCP Not Relia\u2010\nble,\u201d blog.netherlabs.nl, January 18, 2009.\n[23] Nicolas Liochon: \u201c CAP: If All You Have Is a Timeout, Everything Looks Like a\nPartition,\u201d blog.thislongrun.com, May 25, 2015.\n[24] Jerome H. Saltzer, David P. Reed, and David D. Clark: \u201c End-To-End Arguments\nin System Design ,\u201d ACM Transactions on Computer Systems , volume 2, number 4,\npages 277\u2013288, November 1984. doi:10.1145/357401.357402\n[25] Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, et al.: \u201c Queues Don\u2019t\nMatter When You Can JUMP Them! ,\u201d at 12th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI), May 2015.\nSummary | 313", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5acfb37e-18a6-4b68-83c1-e3171489a851": {"__data__": {"id_": "5acfb37e-18a6-4b68-83c1-e3171489a851", "embedding": null, "metadata": {"page_label": "314", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e301a2a6-da2f-4aa7-b180-c9e47279cad3", "node_type": "4", "metadata": {"page_label": "314", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5543e5f88962336ec2c11969e02a1aece534230ad0372b2cab3a85ca3ea7bb33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[26] Guohui Wang and T. S. Eugene Ng: \u201c The Impact of Virtualization on Network\nPerformance of Amazon EC2 Data Center,\u201d at 29th IEEE International Conference on\nComputer Communications  (INFOCOM), March 2010. doi:10.1109/INFCOM.\n2010.5461931\n[27] Van Jacobson: \u201c Congestion Avoidance and Control ,\u201d at ACM Symposium on\nCommunications Architectures and Protocols  (SIGCOMM), August 1988. doi:\n10.1145/52324.52356\n[28] Brandon Philips: \u201c etcd: Distributed Locking and Service Discovery ,\u201d at Strange\nLoop, September 2014.\n[29] Steve Newman: \u201c A Systematic Look at EC2 I/O ,\u201d blog.scalyr.com, October 16,\n2012.\n[30] Naohiro Hayashibara, Xavier D\u00e9fago, Rami Yared, and Takuya Katayama: \u201c The\n\u03d5 Accrual Failure Detector ,\u201d Japan Advanced Institute of Science and Technology,\nSchool of Information Science, Technical Report IS-RR-2004-010, May 2004.\n[31] Jeffrey Wang: \u201c Phi Accrual Failure Detector ,\u201d ternarysearch.blogspot.co.uk,\nAugust 11, 2013.\n[32] Srinivasan Keshav: An Engineering Approach to Computer Networking: ATM\nNetworks, the Internet, and the Telephone Network . Addison-Wesley Professional,\nMay 1997. ISBN: 978-0-201-63442-6\n[33] Cisco, \u201cIntegrated Services Digital Network,\u201d docwiki.cisco.com.\n[34] Othmar Kyas: ATM Networks. International Thomson Publishing, 1995. ISBN:\n978-1-850-32128-6\n[35] \u201cInfiniBand FAQ,\u201d Mellanox Technologies, December 22, 2014.\n[36] Jose Renato Santos, Yoshio Turner, and G. (John) Janakiraman: \u201c End-to-End\nCongestion Control for InfiniBand ,\u201d at 22nd Annual Joint Conference of the IEEE\nComputer and Communications Societies (INFOCOM), April 2003. Also published by\nHP Laboratories Palo Alto, Tech Report HPL-2002-359. doi:10.1109/INFCOM.\n2003.1208949\n[37] Ulrich Windl, David Dalton, Marc Martinec, and Dale R. Worley: \u201c The NTP\nFAQ and HOWTO,\u201d ntp.org, November 2006.\n[38] John Graham-Cumming: \u201c How and why the leap second affected Cloudflare\nDNS,\u201d blog.cloudflare.com, January 1, 2017.\n[39] David Holmes: \u201c Inside the Hotspot VM: Clocks, Timers and Scheduling Events\n\u2013 Part I \u2013 Windows,\u201d blogs.oracle.com, October 2, 2006.\n[40] Steve Loughran: \u201c Time on Multi-Core, Multi-Socket Servers ,\u201d stevelough\u2010\nran.blogspot.co.uk, September 17, 2015.\n314 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d8c94c5-b90e-495e-b1d4-bd2e353e331b": {"__data__": {"id_": "5d8c94c5-b90e-495e-b1d4-bd2e353e331b", "embedding": null, "metadata": {"page_label": "315", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90da39fc-2439-4d31-958f-025eb82fee62", "node_type": "4", "metadata": {"page_label": "315", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "aaba2ae121464e9ef50359ca0640aeaa4a813fe12af15ba3765d4e403dc40f9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[41] James C. Corbett, Jeffrey Dean, Michael Epstein, et al.: \u201c Spanner: Google\u2019s\nGlobally-Distributed Database ,\u201d at 10th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), October 2012.\n[42] M. Caporaloni and R. Ambrosini: \u201c How Closely Can a Personal Computer\nClock Track the UTC Timescale Via the Internet? ,\u201d European Journal of Physics, vol\u2010\nume 23, number 4, pages L17\u2013L21, June 2012. doi:10.1088/0143-0807/23/4/103\n[43] Nelson Minar: \u201c A Survey of the NTP Network ,\u201d alumni.media.mit.edu, Decem\u2010\nber 1999.\n[44] Viliam Holub: \u201c Synchronizing Clocks in a Cassandra Cluster Pt. 1 \u2013 The Prob\u2010\nlem,\u201d blog.logentries.com, March 14, 2014.\n[45] Poul-Henning Kamp: \u201c The One-Second War (What Time Will You Die?) ,\u201d\nACM Queue , volume 9, number 4, pages 44\u201348, April 2011. doi:\n10.1145/1966989.1967009\n[46] Nelson Minar: \u201c Leap Second Crashes Half the Internet ,\u201d somebits.com, July 3,\n2012.\n[47] Christopher Pascoe: \u201c Time, Technology and Leaping Seconds ,\u201d googleblog.blog\u2010\nspot.co.uk, September 15, 2011.\n[48] Mingxue Zhao and Jeff Barr: \u201cLook Before You Leap \u2013 The Coming Leap Second\nand AWS,\u201d aws.amazon.com, May 18, 2015.\n[49] Darryl Veitch and Kanthaiah Vijayalayan: \u201c Network Timing and the 2015 Leap\nSecond,\u201d at 17th International Conference on Passive and Active Measurement\n(PAM), April 2016. doi:10.1007/978-3-319-30505-9_29\n[50] \u201cTimekeeping in VMware Virtual Machines,\u201d Information Guide, VMware, Inc.,\nDecember 2011.\n[51] \u201cMiFID II / MiFIR: Regulatory Technical and Implementing Standards \u2013 Annex\nI (Draft) ,\u201d European Securities and Markets Authority, Report ESMA/2015/1464,\nSeptember 2015.\n[52] Luke Bigum: \u201c Solving MiFID II Clock Synchronisation With Minimum Spend\n(Part 1),\u201d lmax.com, November 27, 2015.\n[53] Kyle Kingsbury: \u201cCall Me Maybe: Cassandra,\u201d aphyr.com, September 24, 2013.\n[54] John Daily: \u201c Clocks Are Bad, or, Welcome to the Wonderful World of Dis\u2010\ntributed Systems,\u201d basho.com, November 12, 2013.\n[55] Kyle Kingsbury: \u201cThe Trouble with Timestamps,\u201d aphyr.com, October 12, 2013.\nSummary | 315", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1330881a-3351-46b7-b477-b51928cd296b": {"__data__": {"id_": "1330881a-3351-46b7-b477-b51928cd296b", "embedding": null, "metadata": {"page_label": "316", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4294a845-4635-4233-9e19-aa7d389dce76", "node_type": "4", "metadata": {"page_label": "316", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "134bfd404e27d0bfb496d36f341149efdc8aaeca8adb707c575ef68bc4c53984", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[56] Leslie Lamport: \u201cTime, Clocks, and the Ordering of Events in a Distributed Sys\u2010\ntem,\u201d Communications of the ACM , volume 21, number 7, pages 558\u2013565, July 1978.\ndoi:10.1145/359545.359563\n[57] Sandeep Kulkarni, Murat Demirbas, Deepak Madeppa, et al.: \u201c Logical Physical\nClocks and Consistent Snapshots in Globally Distributed Databases,\u201d State University\nof New York at Buffalo, Computer Science and Engineering Technical Report\n2014-04, May 2014.\n[58] Justin Sheehy: \u201c There Is No Now: Problems With Simultaneity in Distributed\nSystems,\u201d ACM Queue , volume 13, number 3, pages 36\u201341, March 2015. doi:\n10.1145/2733108\n[59] Murat Demirbas: \u201cSpanner: Google\u2019s Globally-Distributed Database,\u201d muratbuf\u2010\nfalo.blogspot.co.uk, July 4, 2013.\n[60] Dahlia Malkhi and Jean-Philippe Martin: \u201c Spanner\u2019s Concurrency Control ,\u201d\nACM SIGACT News , volume 44, number 3, pages 73\u201377, September 2013. doi:\n10.1145/2527748.2527767\n[61] Manuel Bravo, Nuno Diegues, Jingna Zeng, et al.: \u201c On the Use of Clocks to\nEnforce Consistency in the Cloud,\u201d IEEE Data Engineering Bulletin, volume 38, num\u2010\nber 1, pages 18\u201331, March 2015.\n[62] Spencer Kimball: \u201cLiving Without Atomic Clocks,\u201d cockroachlabs.com, February\n17, 2016.\n[63] Cary G. Gray and David R. Cheriton: \u201cLeases: An Efficient Fault-Tolerant Mech\u2010\nanism for Distributed File Cache Consistency ,\u201d at 12th ACM Symposium on Operat\u2010\ning Systems Principles (SOSP), December 1989. doi:10.1145/74850.74870\n[64] Todd Lipcon: \u201c Avoiding Full GCs in Apache HBase with MemStore-Local Allo\u2010\ncation Buffers: Part 1,\u201d blog.cloudera.com, February 24, 2011.\n[65] Martin Thompson: \u201c Java Garbage Collection Distilled ,\u201d mechanical-\nsympathy.blogspot.co.uk, July 16, 2013.\n[66] Alexey Ragozin: \u201c How to Tame Java GC Pauses? Surviving 16GiB Heap and\nGreater,\u201d java.dzone.com, June 28, 2011.\n[67] Christopher Clark, Keir Fraser, Steven Hand, et al.: \u201c Live Migration of Virtual\nMachines,\u201d at 2nd USENIX Symposium on Symposium on Networked Systems Design\n& Implementation (NSDI), May 2005.\n[68] Mike Shaver: \u201cfsyncers and Curveballs,\u201d shaver.off.net, May 25, 2008.\n[69] Zhenyun Zhuang and Cuong Tran: \u201c Eliminating Large JVM GC Pauses Caused\nby Background IO Traffic,\u201d engineering.linkedin.com, February 10, 2016.\n316 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efa279e0-8fda-4507-8c15-cdc87bb6b7c5": {"__data__": {"id_": "efa279e0-8fda-4507-8c15-cdc87bb6b7c5", "embedding": null, "metadata": {"page_label": "317", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c45232a-fe8b-4edd-a0b7-20fadcd4f31f", "node_type": "4", "metadata": {"page_label": "317", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2aafda66a9ae43c62fd84aad3778c0a052742953bd51bf430d8c12dd7990dc99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[70] David Terei and Amit Levy: \u201c Blade: A Data Center Garbage Collector ,\u201d arXiv:\n1504.02578, April 13, 2015.\n[71] Martin Maas, Tim Harris, Krste Asanovi\u0107, and John Kubiatowicz: \u201c Trash Day:\nCoordinating Garbage Collection in Distributed Systems,\u201d at 15th USENIX Workshop\non Hot Topics in Operating Systems (HotOS), May 2015.\n[72] \u201c Predictable Low Latency ,\u201d Cinnober Financial Technology AB, cinnober.com,\nNovember 24, 2013.\n[73] Martin Fowler: \u201cThe LMAX Architecture,\u201d martinfowler.com, July 12, 2011.\n[74] Flavio P. Junqueira and Benjamin Reed: ZooKeeper: Distributed Process Coordi\u2010\nnation. O\u2019Reilly Media, 2013. ISBN: 978-1-449-36130-3\n[75] Enis S\u00f6ztutar: \u201cHBase and HDFS: Understanding Filesystem Usage in HBase,\u201d at\nHBaseCon, June 2013.\n[76] Caitie McCaffrey: \u201c Clients Are Jerks: AKA How Halo 4 DoSed the Services at\nLaunch & How We Survived,\u201d caitiem.com, June 23, 2015.\n[77] Leslie Lamport, Robert Shostak, and Marshall Pease: \u201c The Byzantine Generals\nProblem,\u201d ACM Transactions on Programming Languages and Systems  (TOPLAS),\nvolume 4, number 3, pages 382\u2013401, July 1982. doi:10.1145/357172.357176\n[78] Jim N. Gray: \u201cNotes on Data Base Operating Systems,\u201d in Operating Systems: An\nAdvanced Course, Lecture Notes in Computer Science, volume 60, edited by R. Bayer,\nR. M. Graham, and G. Seegm\u00fcller, pages 393\u2013481, Springer-Verlag, 1978. ISBN:\n978-3-540-08755-7\n[79] Brian Palmer: \u201cHow Complicated Was the Byzantine Empire? ,\u201d slate.com, Octo\u2010\nber 20, 2011.\n[80] Leslie Lamport: \u201cMy Writings,\u201d research.microsoft.com, December 16, 2014. This\npage can be found by searching the web for the 23-character string obtained by\nremoving the hyphens from the string allla-mport-spubso-ntheweb.\n[81] John Rushby: \u201c Bus Architectures for Safety-Critical Embedded Systems ,\u201d at 1st\nInternational Workshop on Embedded Software (EMSOFT), October 2001.\n[82] Jake Edge: \u201cELC: SpaceX Lessons Learned,\u201d lwn.net, March 6, 2013.\n[83] Andrew Miller and Joseph J. LaViola, Jr.: \u201c Anonymous Byzantine Consensus\nfrom Moderately-Hard Puzzles: A Model for Bitcoin ,\u201d University of Central Florida,\nTechnical Report CS-TR-14-01, April 2014.\n[84] James Mickens: \u201cThe Saddest Moment,\u201d USENIX ;login: logout, May 2013.\n[85] Evan Gilman: \u201c The Discovery of Apache ZooKeeper\u2019s Poison Packet ,\u201d pagerd\u2010\nuty.com, May 7, 2015.\nSummary | 317", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b21eba9d-611c-4381-9dee-81a4d4ff5698": {"__data__": {"id_": "b21eba9d-611c-4381-9dee-81a4d4ff5698", "embedding": null, "metadata": {"page_label": "318", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77802f9b-ccc2-44ef-9254-8700cc5a8c4b", "node_type": "4", "metadata": {"page_label": "318", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a6fdfb871ccb20a19031ee5d196deee1562b2468b105007c09b4bed737910e28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[86] Jonathan Stone and Craig Partridge: \u201c When the CRC and TCP Checksum Disa\u2010\ngree,\u201d at ACM Conference on Applications, Technologies, Architectures, and Protocols\nfor Computer Communication  (SIGCOMM), August 2000. doi:\n10.1145/347059.347561\n[87] Evan Jones: \u201cHow Both TCP and Ethernet Checksums Fail ,\u201d evanjones.ca, Octo\u2010\nber 5, 2015.\n[88] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: \u201c Consensus in the Pres\u2010\nence of Partial Synchrony ,\u201d Journal of the ACM , volume 35, number 2, pages 288\u2013\n323, April 1988. doi:10.1145/42282.42283\n[89] Peter Bailis and Ali Ghodsi: \u201c Eventual Consistency Today: Limitations, Exten\u2010\nsions, and Beyond ,\u201d ACM Queue , volume 11, number 3, pages 55-63, March 2013.\ndoi:10.1145/2460276.2462076\n[90] Bowen Alpern and Fred B. Schneider: \u201c Defining Liveness,\u201d Information Process\u2010\ning Letters , volume 21, number 4, pages 181\u2013185, October 1985. doi:\n10.1016/0020-0190(85)90056-0\n[91] Flavio P. Junqueira: \u201cDude, Where\u2019s My Metadata?,\u201d fpj.me, May 28, 2015.\n[92] Scott Sanders: \u201cJanuary 28th Incident Report,\u201d github.com, February 3, 2016.\n[93] Jay Kreps: \u201c A Few Notes on Kafka and Jepsen ,\u201d blog.empathybox.com, Septem\u2010\nber 25, 2013.\n[94] Thanh Do, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: \u201c Limplock:\nUnderstanding the Impact of Limpware on Scale-out Cloud Systems ,\u201d at 4th ACM\nSymposium on Cloud Computing  (SoCC), October 2013. doi:\n10.1145/2523616.2523627\n[95] Frank McSherry, Michael Isard, and Derek G. Murray: \u201c Scalability! But at What\nCOST?,\u201d at 15th USENIX Workshop on Hot Topics in Operating Systems  (HotOS),\nMay 2015.\n318 | Chapter 8: The Trouble with Distributed Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e7149c2a-4357-40fe-8c8b-c1da6ec5d906": {"__data__": {"id_": "e7149c2a-4357-40fe-8c8b-c1da6ec5d906", "embedding": null, "metadata": {"page_label": "319", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4abe58bf-6574-4b43-8a84-bd5af396f967", "node_type": "4", "metadata": {"page_label": "319", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fdae96b2-0a05-4c06-a0fa-b9a658a8d8af": {"__data__": {"id_": "fdae96b2-0a05-4c06-a0fa-b9a658a8d8af", "embedding": null, "metadata": {"page_label": "320", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "065d5d0c-83ba-429e-bb5f-98493f53da54", "node_type": "4", "metadata": {"page_label": "320", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6765d998-408f-49ad-be32-71a543e161de": {"__data__": {"id_": "6765d998-408f-49ad-be32-71a543e161de", "embedding": null, "metadata": {"page_label": "321", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "696d5cd1-317e-41a2-b635-958f4ac7a36c", "node_type": "4", "metadata": {"page_label": "321", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5d396f1c2dbd7ada27c9d3a331bda7daa0c5317c06cf871ef208ca4fdca308fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 9\nConsistency and Consensus\nIs it better to be alive and wrong or right and dead?\n\u2014Jay Kreps, A Few Notes on Kafka and Jepsen (2013)\nLots of things can go wrong in distributed systems, as discussed in Chapter 8 . The\nsimplest way of handling such faults is to simply let the entire service fail, and show\nthe user an error message. If that solution is unacceptable, we need to find ways of\ntolerating faults\u2014that is, of keeping the service functioning correctly, even if some\ninternal component is faulty.\nIn this chapter, we will talk about some examples of algorithms and protocols for\nbuilding fault-tolerant distributed systems. We will assume that all the problems\nfrom Chapter 8  can occur: packets can be lost, reordered, duplicated, or arbitrarily\ndelayed in the network; clocks are approximate at best; and nodes can pause (e.g., due\nto garbage collection) or crash at any time.\nThe best way of building fault-tolerant systems is to find some general-purpose\nabstractions with useful guarantees, implement them once, and then let applications\nrely on those guarantees. This is the same approach as we used with transactions in\nChapter 7 : by using a transaction, the application can pretend that there are no\ncrashes (atomicity), that nobody else is concurrently accessing the database (isola\u2010\ntion), and that storage devices are perfectly reliable (durability). Even though crashes,\nrace conditions, and disk failures do occur, the transaction abstraction hides those\nproblems so that the application doesn\u2019t need to worry about them.\nWe will now continue along the same lines, and seek abstractions that can allow an\napplication to ignore some of the problems with distributed systems. For example,\none of the most important abstractions for distributed systems is consensus: that is,\ngetting all of the nodes to agree on something. As we shall see in this chapter, reliably\n321", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8c06d128-0917-4be4-9a5d-6669b3b77e9a": {"__data__": {"id_": "8c06d128-0917-4be4-9a5d-6669b3b77e9a", "embedding": null, "metadata": {"page_label": "322", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0a95923-7449-46b7-8ecc-0066c504ec5e", "node_type": "4", "metadata": {"page_label": "322", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c3423c0cbc634acece1df49c993a76712debbfe2ea18c865db26c70a780aff04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "reaching consensus in spite of network faults and process failures is a surprisingly\ntricky problem.\nOnce you have an implementation of consensus, applications can use it for various\npurposes. For example, say you have a database with single-leader replication. If the\nleader dies and you need to fail over to another node, the remaining database nodes\ncan use consensus to elect a new leader. As discussed in \u201cHandling Node Outages\u201d on\npage 156, it\u2019s important that there is only one leader, and that all nodes agree who the\nleader is. If two nodes both believe that they are the leader, that situation is called split\nbrain, and it often leads to data loss. Correct implementations of consensus help\navoid such problems.\nLater in this chapter, in \u201cDistributed Transactions and Consensus\u201d on page 352, we\nwill look into algorithms to solve consensus and related problems. But first we first\nneed to explore the range of guarantees and abstractions that can be provided in a\ndistributed system.\nWe need to understand the scope of what can and cannot be done: in some situa\u2010\ntions, it\u2019s possible for the system to tolerate faults and continue working; in other sit\u2010\nuations, that is not possible. The limits of what is and isn\u2019t possible have been\nexplored in depth, both in theoretical proofs and in practical implementations. We\nwill get an overview of those fundamental limits in this chapter.\nResearchers in the field of distributed systems have been studying these topics for\ndecades, so there is a lot of material\u2014we\u2019ll only be able to scratch the surface. In this\nbook we don\u2019t have space to go into details of the formal models and proofs, so we\nwill stick with informal intuitions. The literature references offer plenty of additional\ndepth if you\u2019re interested.\nConsistency Guarantees\nIn \u201cProblems with Replication Lag\u201d on page 161 we looked at some timing issues that\noccur in a replicated database. If you look at two database nodes at the same moment\nin time, you\u2019re likely to see different data on the two nodes, because write requests\narrive on different nodes at different times. These inconsistencies occur no matter\nwhat replication method the database uses (single-leader, multi-leader, or leaderless\nreplication).\nMost replicated databases provide at least eventual consistency, which means that if\nyou stop writing to the database and wait for some unspecified length of time, then\neventually all read requests will return the same value [ 1]. In other words, the incon\u2010\nsistency is temporary, and it eventually resolves itself (assuming that any faults in the\nnetwork are also eventually repaired). A better name for eventual consistency may be\nconvergence, as we expect all replicas to eventually converge to the same value [2].\n322 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3068b87-d308-45dd-b31d-ee3cd7ca0df1": {"__data__": {"id_": "b3068b87-d308-45dd-b31d-ee3cd7ca0df1", "embedding": null, "metadata": {"page_label": "323", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ec43339-e699-40ca-ba21-a5b5428a097b", "node_type": "4", "metadata": {"page_label": "323", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "be18497103635fe22ef96d68bcef9bfa5a27345fff35d490b5c4854736f65d77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, this is a very weak guarantee\u2014it doesn\u2019t say anything about when the repli\u2010\ncas will converge. Until the time of convergence, reads could return anything or\nnothing [ 1]. For example, if you write a value and then immediately read it again,\nthere is no guarantee that you will see the value you just wrote, because the read may\nbe routed to a different replica (see \u201cReading Your Own Writes\u201d on page 162).\nEventual consistency is hard for application developers because it is so different from\nthe behavior of variables in a normal single-threaded program. If you assign a value\nto a variable and then read it shortly afterward, you don\u2019t expect to read back the old\nvalue, or for the read to fail. A database looks superficially like a variable that you can\nread and write, but in fact it has much more complicated semantics [3].\nWhen working with a database that provides only weak guarantees, you need to be\nconstantly aware of its limitations and not accidentally assume too much. Bugs are\noften subtle and hard to find by testing, because the application may work well most\nof the time. The edge cases of eventual consistency only become apparent when there\nis a fault in the system (e.g., a network interruption) or at high concurrency.\nIn this chapter we will explore stronger consistency models that data systems may\nchoose to provide. They don\u2019t come for free: systems with stronger guarantees may\nhave worse performance or be less fault-tolerant than systems with weaker guaran\u2010\ntees. Nevertheless, stronger guarantees can be appealing because they are easier to use\ncorrectly. Once you have seen a few different consistency models, you\u2019ll be in a better\nposition to decide which one best fits your needs.\nThere is some similarity between distributed consistency models and the hierarchy of\ntransaction isolation levels we discussed previously [ 4, 5] (see \u201cWeak Isolation Lev\u2010\nels\u201d on page 233). But while there is some overlap, they are mostly independent con\u2010\ncerns: transaction isolation is primarily about avoiding race conditions due to\nconcurrently executing transactions, whereas distributed consistency is mostly about\ncoordinating the state of replicas in the face of delays and faults.\nThis chapter covers a broad range of topics, but as we shall see, these areas are in fact\ndeeply linked:\n\u2022 We will start by looking at one of the strongest consistency models in common\nuse, linearizability, and examine its pros and cons.\n\u2022 We\u2019ll then examine the issue of ordering events in a distributed system ( \u201cOrder\u2010\ning Guarantees\u201d on page 339), particularly around causality and total ordering.\n\u2022 In the third section ( \u201cDistributed Transactions and Consensus\u201d on page 352) we\nwill explore how to atomically commit a distributed transaction, which will\nfinally lead us toward solutions for the consensus problem.\nConsistency Guarantees | 323", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1230c82-b0f9-4148-a2ee-966d9fed23d6": {"__data__": {"id_": "b1230c82-b0f9-4148-a2ee-966d9fed23d6", "embedding": null, "metadata": {"page_label": "324", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1a57b63-993f-4ce6-b6ec-86385572464f", "node_type": "4", "metadata": {"page_label": "324", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "be5aa74677e8fc62b97fd1421a1bbf3a9efda073985e913c762f5912bcacbc23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Linearizability\nIn an eventually consistent database, if you ask two different replicas the same ques\u2010\ntion at the same time, you may get two different answers. That\u2019s confusing. Wouldn\u2019t\nit be a lot simpler if the database could give the illusion that there is only one replica\n(i.e., only one copy of the data)? Then every client would have the same view of the\ndata, and you wouldn\u2019t have to worry about replication lag.\nThis is the idea behind linearizability [6] (also known as atomic consistency [7], strong\nconsistency, immediate consistency, or external consistency [8]). The exact definition\nof linearizability is quite subtle, and we will explore it in the rest of this section. But\nthe basic idea is to make a system appear as if there were only one copy of the data,\nand all operations on it are atomic. With this guarantee, even though there may be\nmultiple replicas in reality, the application does not need to worry about them.\nIn a linearizable system, as soon as one client successfully completes a write, all cli\u2010\nents reading from the database must be able to see the value just written. Maintaining\nthe illusion of a single copy of the data means guaranteeing that the value read is the\nmost recent, up-to-date value, and doesn\u2019t come from a stale cache or replica. In\nother words, linearizability is a recency guarantee. To clarify this idea, let\u2019s look at an\nexample of a system that is not linearizable.\nFigure 9-1. This system is not linearizable, causing football fans to be confused.\n324 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb4d2a41-4910-47ba-b5ba-19da953b6faa": {"__data__": {"id_": "cb4d2a41-4910-47ba-b5ba-19da953b6faa", "embedding": null, "metadata": {"page_label": "325", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29d45a3f-475e-4287-93cd-c7a0cf01754b", "node_type": "4", "metadata": {"page_label": "325", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dfb31a1b1ade6139b3abbdf32daff45e1792df5eefe99050c559f1236a98e548", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 9-1 shows an example of a nonlinearizable sports website [ 9]. Alice and Bob\nare sitting in the same room, both checking their phones to see the outcome of the\n2014 FIFA World Cup final. Just after the final score is announced, Alice refreshes\nthe page, sees the winner announced, and excitedly tells Bob about it. Bob incredu\u2010\nlously hits reload on his own phone, but his request goes to a database replica that is\nlagging, and so his phone shows that the game is still ongoing.\nIf Alice and Bob had hit reload at the same time, it would have been less surprising if\nthey had gotten two different query results, because they wouldn\u2019t know at exactly\nwhat time their respective requests were processed by the server. However, Bob\nknows that he hit the reload button (initiated his query) after he heard Alice exclaim\nthe final score, and therefore he expects his query result to be at least as recent as\nAlice\u2019s. The fact that his query returned a stale result is a violation of linearizability.\nWhat Makes a System Linearizable?\nThe basic idea behind linearizability is simple: to make a system appear as if there is\nonly a single copy of the data. However, nailing down precisely what that means\nactually requires some care. In order to understand linearizability better, let\u2019s look at\nsome more examples.\nFigure 9-2 shows three clients concurrently reading and writing the same key x in a\nlinearizable database. In the distributed systems literature, x is called a register\u2014in\npractice, it could be one key in a key-value store, one row in a relational database, or\none document in a document database, for example.\nFigure 9-2. If a read request is concurrent with a write request, it may return either the\nold or the new value.\nFor simplicity, Figure 9-2 shows only the requests from the clients\u2019 point of view, not\nthe internals of the database. Each bar is a request made by a client, where the start of\na bar is the time when the request was sent, and the end of a bar is when the response\nwas received by the client. Due to variable network delays, a client doesn\u2019t know\nLinearizability | 325", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f7379fd-fe10-4967-bcee-62283fb9710a": {"__data__": {"id_": "5f7379fd-fe10-4967-bcee-62283fb9710a", "embedding": null, "metadata": {"page_label": "326", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9f7158e-35f8-4d8e-bb54-e30a6ea0b366", "node_type": "4", "metadata": {"page_label": "326", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "344dfb68691219102fe80e64e837bdedd33ee187358a256e80c0134a16ee3ee8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. A subtle detail of this diagram is that it assumes the existence of a global clock, represented by the horizon\u2010\ntal axis. Even though real systems typically don\u2019t have accurate clocks (see \u201cUnreliable Clocks\u201d on page 287),\nthis assumption is okay: for the purposes of analyzing a distributed algorithm, we may pretend that an accu\u2010\nrate global clock exists, as long as the algorithm doesn\u2019t have access to it [47]. Instead, the algorithm can only\nsee a mangled approximation of real time, as produced by a quartz oscillator and NTP.\nii. A register in which reads may return either the old or the new value if they are concurrent with a write is\nknown as a regular register [7, 25].\nexactly when the database processed its request\u2014it only knows that it must have hap\u2010\npened sometime between the client sending the request and receiving the response.i\nIn this example, the register has two types of operations:\n\u2022 read(x) \u21d2 v means the client requested to read the value of register x, and the\ndatabase returned the value v.\n\u2022 write(x, v) \u21d2 r means the client requested to set the register x to value v, and the\ndatabase returned response r (which could be ok or error).\nIn Figure 9-2, the value of x is initially 0, and client C performs a write request to set\nit to 1. While this is happening, clients A and B are repeatedly polling the database to\nread the latest value. What are the possible responses that A and B might get for their\nread requests?\n\u2022 The first read operation by client A completes before the write begins, so it must\ndefinitely return the old value 0.\n\u2022 The last read by client A begins after the write has completed, so it must defi\u2010\nnitely return the new value 1 if the database is linearizable: we know that the\nwrite must have been processed sometime between the start and end of the write\noperation, and the read must have been processed sometime between the start\nand end of the read operation. If the read started after the write ended, then the\nread must have been processed after the write, and therefore it must see the new\nvalue that was written.\n\u2022 Any read operations that overlap in time with the write operation might return\neither 0 or 1, because we don\u2019t know whether or not the write has taken effect at\nthe time when the read operation is processed. These operations are concurrent\nwith the write.\nHowever, that is not yet sufficient to fully describe linearizability: if reads that are\nconcurrent with a write can return either the old or the new value, then readers could\nsee a value flip back and forth between the old and the new value several times while\na write is going on. That is not what we expect of a system that emulates a \u201csingle\ncopy of the data.\u201dii\n326 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2747, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b023ad1-0939-46ea-8ae5-a86560934aa6": {"__data__": {"id_": "0b023ad1-0939-46ea-8ae5-a86560934aa6", "embedding": null, "metadata": {"page_label": "327", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8de2432-1598-47fa-a541-d114fdaa0991", "node_type": "4", "metadata": {"page_label": "327", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "34a4e7477b2185186bd4a371b439c01507c0a1586fedc0955829b3ca00862327", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To make the system linearizable, we need to add another constraint, illustrated in\nFigure 9-3.\nFigure 9-3. After any one read has returned the new value, all following reads (on the\nsame or other clients) must also return the new value.\nIn a linearizable system we imagine that there must be some point in time (between\nthe start and end of the write operation) at which the value of x atomically flips from\n0 to 1. Thus, if one client\u2019s read returns the new value 1, all subsequent reads must\nalso return the new value, even if the write operation has not yet completed.\nThis timing dependency is illustrated with an arrow in Figure 9-3. Client A is the first\nto read the new value, 1. Just after A\u2019s read returns, B begins a new read. Since B\u2019s\nread occurs strictly after A\u2019s read, it must also return 1, even though the write by C is\nstill ongoing. (It\u2019s the same situation as with Alice and Bob in Figure 9-1: after Alice\nhas read the new value, Bob also expects to read the new value.)\nWe can further refine this timing diagram to visualize each operation taking effect\natomically at some point in time. A more complex example is shown in Figure 9-4\n[10].\nIn Figure 9-4 we add a third type of operation besides read and write:\n\u2022 cas(x, vold, vnew) \u21d2 r means the client requested an atomic compare-and-set oper\u2010\nation (see \u201cCompare-and-set\u201d on page 245). If the current value of the register x\nequals vold, it should be atomically set to vnew. If x \u2260 vold then the operation should\nleave the register unchanged and return an error. r is the database\u2019s response ( ok\nor error).\nEach operation in Figure 9-4 is marked with a vertical line (inside the bar for each\noperation) at the time when we think the operation was executed. Those markers are\njoined up in a sequential order, and the result must be a valid sequence of reads and\nwrites for a register (every read must return the value set by the most recent write).\nThe requirement of linearizability is that the lines joining up the operation markers\nalways move forward in time (from left to right), never backward. This requirement\nLinearizability | 327", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c289531-325b-4a8c-a56c-e87d089d6b48": {"__data__": {"id_": "2c289531-325b-4a8c-a56c-e87d089d6b48", "embedding": null, "metadata": {"page_label": "328", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cdb01ea5-7da9-47a0-998f-79c69f9f42c1", "node_type": "4", "metadata": {"page_label": "328", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cb5f0fffa844f5f3f8f27fcbfe8da4f2b56829d6482e787e3c2ea69f99c81c74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ensures the recency guarantee we discussed earlier: once a new value has been written\nor read, all subsequent reads see the value that was written, until it is overwritten\nagain.\nFigure 9-4. Visualizing the points in time at which the reads and writes appear to have\ntaken effect. The final read by B is not linearizable.\nThere are a few interesting details to point out in Figure 9-4:\n\u2022 First client B sent a request to read x, then client D sent a request to set x to 0,\nand then client A sent a request to set x to 1. Nevertheless, the value returned to\nB\u2019s read is 1 (the value written by A). This is okay: it means that the database first\nprocessed D\u2019s write, then A\u2019s write, and finally B\u2019s read. Although this is not the\norder in which the requests were sent, it\u2019s an acceptable order, because the three\nrequests are concurrent. Perhaps B\u2019s read request was slightly delayed in the net\u2010\nwork, so it only reached the database after the two writes.\n\u2022 Client B\u2019s read returned 1 before client A received its response from the database,\nsaying that the write of the value 1 was successful. This is also okay: it doesn\u2019t\nmean the value was read before it was written, it just means the ok response from\nthe database to client A was slightly delayed in the network.\n\u2022 This model doesn\u2019t assume any transaction isolation: another client may change\na value at any time. For example, C first reads 1 and then reads 2, because the\nvalue was changed by B between the two reads. An atomic compare-and-set ( cas)\noperation can be used to check the value hasn\u2019t been concurrently changed by\nanother client: B and C\u2019s cas requests succeed, but D\u2019s cas request fails (by the\ntime the database processes it, the value of x is no longer 0).\n\u2022 The final read by client B (in a shaded bar) is not linearizable. The operation is\nconcurrent with C\u2019s cas write, which updates x from 2 to 4. In the absence of\n328 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d193a44-3bd4-4d2b-b3f2-fa01f864d0e6": {"__data__": {"id_": "6d193a44-3bd4-4d2b-b3f2-fa01f864d0e6", "embedding": null, "metadata": {"page_label": "329", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca285005-75a7-4c7e-b2b5-46e62f3530de", "node_type": "4", "metadata": {"page_label": "329", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5c4ace2ab14b8865ae60507c73a6896aa4958bcb695eb9d4686def446fde8574", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "other requests, it would be okay for B\u2019s read to return 2. However, client A has\nalready read the new value 4 before B\u2019s read started, so B is not allowed to read\nan older value than A. Again, it\u2019s the same situation as with Alice and Bob in\nFigure 9-1.\nThat is the intuition behind linearizability; the formal definition [ 6] describes it more\nprecisely. It is possible (though computationally expensive) to test whether a system\u2019s\nbehavior is linearizable by recording the timings of all requests and responses, and\nchecking whether they can be arranged into a valid sequential order [11]. \nLinearizability Versus Serializability\nLinearizability is easily confused with serializability (see \u201cSerializability\u201d on page 251),\nas both words seem to mean something like \u201ccan be arranged in a sequential order.\u201d\nHowever, they are two quite different guarantees, and it is important to distinguish\nbetween them:\nSerializability\nSerializability is an isolation property of transactions, where every transaction\nmay read and write multiple objects (rows, documents, records)\u2014see \u201cSingle-\nObject and Multi-Object Operations\u201d on page 228. It guarantees that transac\u2010\ntions behave the same as if they had executed in some serial order (each\ntransaction running to completion before the next transaction starts). It is okay\nfor that serial order to be different from the order in which transactions were\nactually run [12].\nLinearizability\nLinearizability is a recency guarantee on reads and writes of a register (an indi\u2010\nvidual object). It doesn\u2019t group operations together into transactions, so it does\nnot prevent problems such as write skew (see \u201cWrite Skew and Phantoms\u201d on\npage 246), unless you take additional measures such as materializing conflicts\n(see \u201cMaterializing conflicts\u201d on page 251).\nA database may provide both serializability and linearizability, and this combination\nis known as strict serializability or strong one-copy serializability (strong-1SR) [4, 13].\nImplementations of serializability based on two-phase locking (see \u201cTwo-Phase Lock\u2010\ning (2PL)\u201d on page 257) or actual serial execution (see \u201cActual Serial Execution\u201d  on\npage 252) are typically linearizable.\nHowever, serializable snapshot isolation (see \u201cSerializable Snapshot Isolation (SSI)\u201d\non page 261) is not linearizable: by design, it makes reads from a consistent snapshot,\nto avoid lock contention between readers and writers. The whole point of a consistent\nsnapshot is that it does not include writes that are more recent than the snapshot, and\nthus reads from the snapshot are not linearizable.\nLinearizability | 329", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35ce4b15-151b-4e46-abf5-389ffea5e772": {"__data__": {"id_": "35ce4b15-151b-4e46-abf5-389ffea5e772", "embedding": null, "metadata": {"page_label": "330", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6be3961a-8656-4d71-86da-09c94d726dfc", "node_type": "4", "metadata": {"page_label": "330", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d702b8f1081b8596af725e478423a34b4e51a0e88558372edb6f4f8a6bab4216", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. Strictly speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default\nthey can be served by any one of the replicas. You can optionally request a linearizable read: etcd calls this a\nquorum read [16], and in ZooKeeper you need to call sync() before the read [15]; see \u201cImplementing linear\u2010\nizable storage using total order broadcast\u201d on page 350.\nRelying on Linearizability\nIn what circumstances is linearizability useful? Viewing the final score of a sporting\nmatch is perhaps a frivolous example: a result that is outdated by a few seconds is\nunlikely to cause any real harm in this situation. However, there a few areas in which\nlinearizability is an important requirement for making a system work correctly.\nLocking and leader election\nA system that uses single-leader replication needs to ensure that there is indeed only\none leader, not several (split brain). One way of electing a leader is to use a lock: every\nnode that starts up tries to acquire the lock, and the one that succeeds becomes the\nleader [14]. No matter how this lock is implemented, it must be linearizable: all nodes\nmust agree which node owns the lock; otherwise it is useless.\nCoordination services like Apache ZooKeeper [ 15] and etcd [ 16] are often used to\nimplement distributed locks and leader election. They use consensus algorithms to\nimplement linearizable operations in a fault-tolerant way (we discuss such algorithms\nlater in this chapter, in \u201cFault-Tolerant Consensus\u201d on page 364).iii There are still\nmany subtle details to implementing locks and leader election correctly (see for\nexample the fencing issue in \u201cThe leader and the lock\u201d on page 301), and libraries like\nApache Curator [ 17] help by providing higher-level recipes on top of ZooKeeper.\nHowever, a linearizable storage service is the basic foundation for these coordination\ntasks.\nDistributed locking is also used at a much more granular level in some distributed\ndatabases, such as Oracle Real Application Clusters (RAC) [ 18]. RAC uses a lock per\ndisk page, with multiple nodes sharing access to the same disk storage system. Since\nthese linearizable locks are on the critical path of transaction execution, RAC deploy\u2010\nments usually have a dedicated cluster interconnect network for communication\nbetween database nodes.\nConstraints and uniqueness guarantees\nUniqueness constraints are common in databases: for example, a username or email\naddress must uniquely identify one user, and in a file storage service there cannot be\ntwo files with the same path and filename. If you want to enforce this constraint as\nthe data is written (such that if two people try to concurrently create a user or a file\nwith the same name, one of them will be returned an error), you need linearizability.\n330 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2829, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b62a8b44-85e1-4e30-8642-2b024d45c432": {"__data__": {"id_": "b62a8b44-85e1-4e30-8642-2b024d45c432", "embedding": null, "metadata": {"page_label": "331", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99e4b054-2d92-4585-b778-a7ca930409c5", "node_type": "4", "metadata": {"page_label": "331", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c91c7e184c7bd1301860829deb7f921fa8fe0ee97e3e9b5abfd7bc6ceb1366b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This situation is actually similar to a lock: when a user registers for your service, you\ncan think of them acquiring a \u201clock\u201d on their chosen username. The operation is also\nvery similar to an atomic compare-and-set, setting the username to the ID of the user\nwho claimed it, provided that the username is not already taken.\nSimilar issues arise if you want to ensure that a bank account balance never goes neg\u2010\native, or that you don\u2019t sell more items than you have in stock in the warehouse, or\nthat two people don\u2019t concurrently book the same seat on a flight or in a theater.\nThese constraints all require there to be a single up-to-date value (the account bal\u2010\nance, the stock level, the seat occupancy) that all nodes agree on.\nIn real applications, it is sometimes acceptable to treat such constraints loosely (for\nexample, if a flight is overbooked, you can move customers to a different flight and\noffer them compensation for the inconvenience). In such cases, linearizability may\nnot be needed, and we will discuss such loosely interpreted constraints in \u201cTimeliness\nand Integrity\u201d on page 524.\nHowever, a hard uniqueness constraint, such as the one you typically find in rela\u2010\ntional databases, requires linearizability. Other kinds of constraints, such as foreign\nkey or attribute constraints, can be implemented without requiring linearizability\n[19].\nCross-channel timing dependencies\nNotice a detail in Figure 9-1: if Alice hadn\u2019t exclaimed the score, Bob wouldn\u2019t have\nknown that the result of his query was stale. He would have just refreshed the page\nagain a few seconds later, and eventually seen the final score. The linearizability viola\u2010\ntion was only noticed because there was an additional communication channel in the\nsystem (Alice\u2019s voice to Bob\u2019s ears).\nSimilar situations can arise in computer systems. For example, say you have a website\nwhere users can upload a photo, and a background process resizes the photos to\nlower resolution for faster download (thumbnails). The architecture and dataflow of\nthis system is illustrated in Figure 9-5.\nThe image resizer needs to be explicitly instructed to perform a resizing job, and this\ninstruction is sent from the web server to the resizer via a message queue (see Chap\u2010\nter 11). The web server doesn\u2019t place the entire photo on the queue, since most mes\u2010\nsage brokers are designed for small messages, and a photo may be several megabytes\nin size. Instead, the photo is first written to a file storage service, and once the write is\ncomplete, the instruction to the resizer is placed on the queue.\nLinearizability | 331", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29cd7e4f-9cc2-4f00-9ad3-3a29340e8820": {"__data__": {"id_": "29cd7e4f-9cc2-4f00-9ad3-3a29340e8820", "embedding": null, "metadata": {"page_label": "332", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d8a5ef6-a6bc-4498-a913-a6ae3d1c049a", "node_type": "4", "metadata": {"page_label": "332", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e7d79f49f9dcf5e8b522d2b2981101587bcbd2407fa9404c94b00fd0198a5ac5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 9-5. The web server and image resizer communicate both through file storage\nand a message queue, opening the potential for race conditions.\nIf the file storage service is linearizable, then this system should work fine. If it is not\nlinearizable, there is the risk of a race condition: the message queue (steps 3 and 4 in\nFigure 9-5) might be faster than the internal replication inside the storage service. In\nthis case, when the resizer fetches the image (step 5), it might see an old version of the\nimage, or nothing at all. If it processes an old version of the image, the full-size and\nresized images in the file storage become permanently inconsistent.\nThis problem arises because there are two different communication channels\nbetween the web server and the resizer: the file storage and the message queue.\nWithout the recency guarantee of linearizability, race conditions between these two\nchannels are possible. This situation is analogous to Figure 9-1, where there was also\na race condition between two communication channels: the database replication and\nthe real-life audio channel between Alice\u2019s mouth and Bob\u2019s ears.\nLinearizability is not the only way of avoiding this race condition, but it\u2019s the simplest\nto understand. If you control the additional communication channel (like in the case\nof the message queue, but not in the case of Alice and Bob), you can use alternative\napproaches similar to what we discussed in \u201cReading Your Own Writes\u201d on page 162,\nat the cost of additional complexity. \nImplementing Linearizable Systems\nNow that we\u2019ve looked at a few examples in which linearizability is useful, let\u2019s think\nabout how we might implement a system that offers linearizable semantics.\nSince linearizability essentially means \u201cbehave as though there is only a single copy of\nthe data, and all operations on it are atomic,\u201d the simplest answer would be to really\nonly use a single copy of the data. However, that approach would not be able to toler\u2010\nate faults: if the node holding that one copy failed, the data would be lost, or at least\ninaccessible until the node was brought up again.\n332 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ee6a0b2-e5ca-486a-821c-8190075c4704": {"__data__": {"id_": "6ee6a0b2-e5ca-486a-821c-8190075c4704", "embedding": null, "metadata": {"page_label": "333", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7683e053-388f-42b1-98b3-b6d1eb9d9636", "node_type": "4", "metadata": {"page_label": "333", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "33e578260cd29919703853bb2bdfb74dffd1ae2fa17b78dd2290b963f653842a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. Partitioning (sharding) a single-leader database, so that there is a separate leader per partition, does not\naffect linearizability, since it is only a single-object guarantee. Cross-partition transactions are a different mat\u2010\nter (see \u201cDistributed Transactions and Consensus\u201d on page 352).\nThe most common approach to making a system fault-tolerant is to use replication.\nLet\u2019s revisit the replication methods from Chapter 5, and compare whether they can\nbe made linearizable:\nSingle-leader replication (potentially linearizable)\nIn a system with single-leader replication (see \u201cLeaders and Followers\u201d on page\n152), the leader has the primary copy of the data that is used for writes, and the\nfollowers maintain backup copies of the data on other nodes. If you make reads\nfrom the leader, or from synchronously updated followers, they have the poten\u2010\ntial to be linearizable.iv However, not every single-leader database is actually line\u2010\narizable, either by design (e.g., because it uses snapshot isolation) or due to\nconcurrency bugs [10].\nUsing the leader for reads relies on the assumption that you know for sure who\nthe leader is. As discussed in \u201cThe Truth Is Defined by the Majority\u201d on page\n300, it is quite possible for a node to think that it is the leader, when in fact it is\nnot\u2014and if the delusional leader continues to serve requests, it is likely to violate\nlinearizability [20]. With asynchronous replication, failover may even lose com\u2010\nmitted writes (see \u201cHandling Node Outages\u201d on page 156), which violates both\ndurability and linearizability.\nConsensus algorithms (linearizable)\nSome consensus algorithms, which we will discuss later in this chapter, bear a\nresemblance to single-leader replication. However, consensus protocols contain\nmeasures to prevent split brain and stale replicas. Thanks to these details, con\u2010\nsensus algorithms can implement linearizable storage safely. This is how Zoo\u2010\nKeeper [21] and etcd [22] work, for example.\nMulti-leader replication (not linearizable)\nSystems with multi-leader replication are generally not linearizable, because they\nconcurrently process writes on multiple nodes and asynchronously replicate\nthem to other nodes. For this reason, they can produce conflicting writes that\nrequire resolution (see \u201cHandling Write Conflicts\u201d on page 171). Such conflicts\nare an artifact of the lack of a single copy of the data.\nLeaderless replication (probably not linearizable)\nFor systems with leaderless replication (Dynamo-style; see \u201cLeaderless Replica\u2010\ntion\u201d on page 177), people sometimes claim that you can obtain \u201cstrong consis\u2010\ntency\u201d by requiring quorum reads and writes (w + r > n). Depending on the exact\nLinearizability | 333", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5584b9d-f38f-4fad-9da1-5c77e9e93ff7": {"__data__": {"id_": "c5584b9d-f38f-4fad-9da1-5c77e9e93ff7", "embedding": null, "metadata": {"page_label": "334", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4048736-6fb3-4e67-8272-69948092b646", "node_type": "4", "metadata": {"page_label": "334", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a1ffe8c2d2c437849dbbb765156e803249c39d0423c8cd645a3e964af462932b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "configuration of the quorums, and depending on how you define strong consis\u2010\ntency, this is not quite true.\n\u201cLast write wins\u201d conflict resolution methods based on time-of-day clocks (e.g.,\nin Cassandra; see \u201cRelying on Synchronized Clocks\u201d on page 291) are almost cer\u2010\ntainly nonlinearizable, because clock timestamps cannot be guaranteed to be\nconsistent with actual event ordering due to clock skew. Sloppy quorums\n(\u201cSloppy Quorums and Hinted Handoff\u201d on page 183) also ruin any chance of\nlinearizability. Even with strict quorums, nonlinearizable behavior is possible, as\ndemonstrated in the next section.\nLinearizability and quorums\nIntuitively, it seems as though strict quorum reads and writes should be linearizable\nin a Dynamo-style model. However, when we have variable network delays, it is pos\u2010\nsible to have race conditions, as demonstrated in Figure 9-6.\nFigure 9-6. A nonlinearizable execution, despite using a strict quorum.\nIn Figure 9-6, the initial value of x is 0, and a writer client is updating x to 1 by send\u2010\ning the write to all three replicas ( n = 3, w = 3). Concurrently, client A reads from a\nquorum of two nodes (r = 2) and sees the new value 1 on one of the nodes. Also con\u2010\ncurrently with the write, client B reads from a different quorum of two nodes, and\ngets back the old value 0 from both.\nThe quorum condition is met ( w + r > n), but this execution is nevertheless not line\u2010\narizable: B\u2019s request begins after A\u2019s request completes, but B returns the old value\n334 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7920269-c0f8-4305-8d4a-369714e6dd2a": {"__data__": {"id_": "a7920269-c0f8-4305-8d4a-369714e6dd2a", "embedding": null, "metadata": {"page_label": "335", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c02a25a-494f-43c6-a565-446fb458179d", "node_type": "4", "metadata": {"page_label": "335", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "75d3e4e0ae488a8563d97062c990b28ea7556b2a73b18cda7455fb7807545043", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "while A returns the new value. (It\u2019s once again the Alice and Bob situation from\nFigure 9-1.)\nInterestingly, it is possible to make Dynamo-style quorums linearizable at the cost of\nreduced performance: a reader must perform read repair (see \u201cRead repair and anti-\nentropy\u201d on page 178) synchronously, before returning results to the application\n[23], and a writer must read the latest state of a quorum of nodes before sending its\nwrites [24, 25]. However, Riak does not perform synchronous read repair due to the\nperformance penalty [ 26]. Cassandra does wait for read repair to complete on quo\u2010\nrum reads [ 27], but it loses linearizability if there are multiple concurrent writes to\nthe same key, due to its use of last-write-wins conflict resolution.\nMoreover, only linearizable read and write operations can be implemented in this\nway; a linearizable compare-and-set operation cannot, because it requires a consen\u2010\nsus algorithm [28].\nIn summary, it is safest to assume that a leaderless system with Dynamo-style replica\u2010\ntion does not provide linearizability. \nThe Cost of Linearizability\nAs some replication methods can provide linearizability and others cannot, it is inter\u2010\nesting to explore the pros and cons of linearizability in more depth.\nWe already discussed some use cases for different replication methods in Chapter 5;\nfor example, we saw that multi-leader replication is often a good choice for multi-\ndatacenter replication (see \u201cMulti-datacenter operation\u201d on page 168). An example of\nsuch a deployment is illustrated in Figure 9-7.\nFigure 9-7. A network interruption forcing a choice between linearizability and availa\u2010\nbility.\nLinearizability | 335", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8606b33-d7dd-4791-b8b6-f45bd5d87a1a": {"__data__": {"id_": "a8606b33-d7dd-4791-b8b6-f45bd5d87a1a", "embedding": null, "metadata": {"page_label": "336", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bf757dc-44bc-414b-ba9b-2eb96f452623", "node_type": "4", "metadata": {"page_label": "336", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f2223543a921a2de4318af0dda329fc84fdadadcfb8e788b0ab12f20470698ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "v. These two choices are sometimes known as CP (consistent but not available under network partitions) and\nAP (available but not consistent under network partitions), respectively. However, this classification scheme\nhas several flaws [9], so it is best avoided.\nConsider what happens if there is a network interruption between the two datacen\u2010\nters. Let\u2019s assume that the network within each datacenter is working, and clients can\nreach the datacenters, but the datacenters cannot connect to each other.\nWith a multi-leader database, each datacenter can continue operating normally: since\nwrites from one datacenter are asynchronously replicated to the other, the writes are\nsimply queued up and exchanged when network connectivity is restored.\nOn the other hand, if single-leader replication is used, then the leader must be in one\nof the datacenters. Any writes and any linearizable reads must be sent to the leader\u2014\nthus, for any clients connected to a follower datacenter, those read and write requests\nmust be sent synchronously over the network to the leader datacenter.\nIf the network between datacenters is interrupted in a single-leader setup, clients con\u2010\nnected to follower datacenters cannot contact the leader, so they cannot make any\nwrites to the database, nor any linearizable reads. They can still make reads from the\nfollower, but they might be stale (nonlinearizable). If the application requires linear\u2010\nizable reads and writes, the network interruption causes the application to become\nunavailable in the datacenters that cannot contact the leader.\nIf clients can connect directly to the leader datacenter, this is not a problem, since the\napplication continues to work normally there. But clients that can only reach a fol\u2010\nlower datacenter will experience an outage until the network link is repaired.\nThe CAP theorem\nThis issue is not just a consequence of single-leader and multi-leader replication: any\nlinearizable database has this problem, no matter how it is implemented. The issue\nalso isn\u2019t specific to multi-datacenter deployments, but can occur on any unreliable\nnetwork, even within one datacenter. The trade-off is as follows:v\n\u2022 If your application requires linearizability, and some replicas are disconnected\nfrom the other replicas due to a network problem, then some replicas cannot\nprocess requests while they are disconnected: they must either wait until the net\u2010\nwork problem is fixed, or return an error (either way, they become unavailable).\n\u2022 If your application does not require linearizability, then it can be written in a way\nthat each replica can process requests independently, even if it is disconnected\nfrom other replicas (e.g., multi-leader). In this case, the application can remain\navailable in the face of a network problem, but its behavior is not linearizable.\n336 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77662751-1b92-4b0d-8337-091d61726928": {"__data__": {"id_": "77662751-1b92-4b0d-8337-091d61726928", "embedding": null, "metadata": {"page_label": "337", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6aa317e1-ae0b-4749-a6db-c673a5e7b5a4", "node_type": "4", "metadata": {"page_label": "337", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b6bc35fb488e684a9a7443533b5e8b58d8d431552b9294baebb3cbb5a0b40703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vi. As discussed in \u201cNetwork Faults in Practice\u201d on page 279, this book uses partitioning to refer to deliber\u2010\nately breaking down a large dataset into smaller ones (sharding; see Chapter 6). By contrast, a network parti\u2010\ntion is a particular type of network fault, which we normally don\u2019t consider separately from other kinds of\nfaults. However, since it\u2019s the P in CAP, we can\u2019t avoid the confusion in this case.\nThus, applications that don\u2019t require linearizability can be more tolerant of network\nproblems. This insight is popularly known as the CAP theorem  [29, 30, 31, 32],\nnamed by Eric Brewer in 2000, although the trade-off has been known to designers of\ndistributed databases since the 1970s [33, 34, 35, 36].\nCAP was originally proposed as a rule of thumb, without precise definitions, with the\ngoal of starting a discussion about trade-offs in databases. At the time, many dis\u2010\ntributed databases focused on providing linearizable semantics on a cluster of\nmachines with shared storage [ 18], and CAP encouraged database engineers to\nexplore a wider design space of distributed shared-nothing systems, which were more\nsuitable for implementing large-scale web services [ 37]. CAP deserves credit for this\nculture shift\u2014witness the explosion of new database technologies since the\nmid-2000s (known as NoSQL).\nThe Unhelpful CAP Theorem\nCAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2\nout of 3. Unfortunately, putting it this way is misleading [ 32] because network parti\u2010\ntions are a kind of fault, so they aren\u2019t something about which you have a choice: they\nwill happen whether you like it or not [38].\nAt times when the network is working correctly, a system can provide both consis\u2010\ntency (linearizability) and total availability. When a network fault occurs, you have to\nchoose between either linearizability or total availability. Thus, a better way of phras\u2010\ning CAP would be either Consistent or Available when Partitioned  [39]. A more relia\u2010\nble network needs to make this choice less often, but at some point the choice is\ninevitable.\nIn discussions of CAP there are several contradictory definitions of the term availa\u2010\nbility, and the formalization as a theorem [30] does not match its usual meaning [40].\nMany so-called \u201chighly available\u201d (fault-tolerant) systems actually do not meet CAP\u2019s\nidiosyncratic definition of availability. All in all, there is a lot of misunderstanding\nand confusion around CAP, and it does not help us understand systems better, so\nCAP is best avoided.\nThe CAP theorem as formally defined [ 30] is of very narrow scope: it only considers\none consistency model (namely linearizability) and one kind of fault ( network parti\u2010\ntions,vi or nodes that are alive but disconnected from each other). It doesn\u2019t say any\u2010\nLinearizability | 337", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb46fb0c-ac37-48aa-a599-7c648d41fcd0": {"__data__": {"id_": "cb46fb0c-ac37-48aa-a599-7c648d41fcd0", "embedding": null, "metadata": {"page_label": "338", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61c9d363-6a29-41fd-b772-95058130ac42", "node_type": "4", "metadata": {"page_label": "338", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0bf6bea63426b815bddfd4dfc3ee95eeb5d615a6b2fa9d4279385058b00ecc43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "thing about network delays, dead nodes, or other trade-offs. Thus, although CAP has\nbeen historically influential, it has little practical value for designing systems [9, 40].\nThere are many more interesting impossibility results in distributed systems [ 41],\nand CAP has now been superseded by more precise results [ 2, 42], so it is of mostly\nhistorical interest today. \nLinearizability and network delays\nAlthough linearizability is a useful guarantee, surprisingly few systems are actually\nlinearizable in practice. For example, even RAM on a modern multi-core CPU is not\nlinearizable [43]: if a thread running on one CPU core writes to a memory address,\nand a thread on another CPU core reads the same address shortly afterward, it is not\nguaranteed to read the value written by the first thread (unless a memory barrier or\nfence [44] is used).\nThe reason for this behavior is that every CPU core has its own memory cache and\nstore buffer. Memory access first goes to the cache by default, and any changes are\nasynchronously written out to main memory. Since accessing data in the cache is\nmuch faster than going to main memory [ 45], this feature is essential for good per\u2010\nformance on modern CPUs. However, there are now several copies of the data (one\nin main memory, and perhaps several more in various caches), and these copies are\nasynchronously updated, so linearizability is lost.\nWhy make this trade-off? It makes no sense to use the CAP theorem to justify the\nmulti-core memory consistency model: within one computer we usually assume reli\u2010\nable communication, and we don\u2019t expect one CPU core to be able to continue oper\u2010\nating normally if it is disconnected from the rest of the computer. The reason for\ndropping linearizability is performance, not fault tolerance.\nThe same is true of many distributed databases that choose not to provide lineariza\u2010\nble guarantees: they do so primarily to increase performance, not so much for fault\ntolerance [46]. Linearizability is slow\u2014and this is true all the time, not only during a\nnetwork fault.\nCan\u2019t we maybe find a more efficient implementation of linearizable storage? It\nseems the answer is no: Attiya and Welch [ 47] prove that if you want linearizability,\nthe response time of read and write requests is at least proportional to the uncertainty\nof delays in the network. In a network with highly variable delays, like most com\u2010\nputer networks (see \u201cTimeouts and Unbounded Delays\u201d on page 281), the response\ntime of linearizable reads and writes is inevitably going to be high. A faster algorithm\nfor linearizability does not exist, but weaker consistency models can be much faster,\nso this trade-off is important for latency-sensitive systems. In Chapter 12 we will dis\u2010\ncuss some approaches for avoiding linearizability without sacrificing correctness. \n338 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "94bcc9be-4361-420c-b54a-aa6e38ac0cf7": {"__data__": {"id_": "94bcc9be-4361-420c-b54a-aa6e38ac0cf7", "embedding": null, "metadata": {"page_label": "339", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3129500b-7b60-4971-9871-95973d3d53e0", "node_type": "4", "metadata": {"page_label": "339", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "75971e6cfea16607e73b6750841160a7978af373d8d96546bf32810afd6cc34e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ordering Guarantees\nWe said previously that a linearizable register behaves as if there is only a single copy\nof the data, and that every operation appears to take effect atomically at one point in\ntime. This definition implies that operations are executed in some well-defined order.\nWe illustrated the ordering in Figure 9-4 by joining up the operations in the order in\nwhich they seem to have executed.\nOrdering has been a recurring theme in this book, which suggests that it might be an\nimportant fundamental idea. Let\u2019s briefly recap some of the other contexts in which\nwe have discussed ordering:\n\u2022 In Chapter 5 we saw that the main purpose of the leader in single-leader replica\u2010\ntion is to determine the order of writes in the replication log\u2014that is, the order in\nwhich followers apply those writes. If there is no single leader, conflicts can occur\ndue to concurrent operations (see \u201cHandling Write Conflicts\u201d on page 171).\n\u2022 Serializability, which we discussed in Chapter 7, is about ensuring that transac\u2010\ntions behave as if they were executed in some sequential order. It can be achieved\nby literally executing transactions in that serial order, or by allowing concurrent\nexecution while preventing serialization conflicts (by locking or aborting).\n\u2022 The use of timestamps and clocks in distributed systems that we discussed in\nChapter 8  (see \u201cRelying on Synchronized Clocks\u201d on page 291) is another\nattempt to introduce order into a disorderly world, for example to determine\nwhich one of two writes happened later.\nIt turns out that there are deep connections between ordering, linearizability, and\nconsensus. Although this notion is a bit more theoretical and abstract than the rest of\nthis book, it is very helpful for clarifying our understanding of what systems can and\ncannot do. We will explore this topic in the next few sections.\nOrdering and Causality\nThere are several reasons why ordering keeps coming up, and one of the reasons is\nthat it helps preserve causality. We have already seen several examples over the\ncourse of this book where causality has been important:\n\u2022 In \u201cConsistent Prefix Reads\u201d on page 165 (Figure 5-5) we saw an example where\nthe observer of a conversation saw first the answer to a question, and then the\nquestion being answered. This is confusing because it violates our intuition of\ncause and effect: if a question is answered, then clearly the question had to be\nthere first, because the person giving the answer must have seen the question\n(assuming they are not psychic and cannot see into the future). We say that there\nis a causal dependency between the question and the answer.\nOrdering Guarantees | 339", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2662, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b75bab2f-74b5-4bc7-93f6-625322a8abca": {"__data__": {"id_": "b75bab2f-74b5-4bc7-93f6-625322a8abca", "embedding": null, "metadata": {"page_label": "340", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5544f9d2-9915-4226-abf0-c3ee6454199e", "node_type": "4", "metadata": {"page_label": "340", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a09fec2fd245aec8a56e83e703b4da0cdd8f1cbaff8f8cabba038b875e3a8871", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 A similar pattern appeared in Figure 5-9 , where we looked at the replication\nbetween three leaders and noticed that some writes could \u201covertake\u201d others due\nto network delays. From the perspective of one of the replicas it would look as\nthough there was an update to a row that did not exist. Causality here means that\na row must first be created before it can be updated.\n\u2022 In \u201cDetecting Concurrent Writes\u201d on page 184 we observed that if you have two\noperations A and B, there are three possibilities: either A happened before B, or B\nhappened before A, or A and B are concurrent. This happened before relationship\nis another expression of causality: if A happened before B, that means B might\nhave known about A, or built upon A, or depended on A. If A and B are concur\u2010\nrent, there is no causal link between them; in other words, we are sure that nei\u2010\nther knew about the other.\n\u2022 In the context of snapshot isolation for transactions ( \u201cSnapshot Isolation and\nRepeatable Read\u201d on page 237), we said that a transaction reads from a consistent\nsnapshot. But what does \u201cconsistent\u201d mean in this context? It means consistent\nwith causality: if the snapshot contains an answer, it must also contain the ques\u2010\ntion being answered [ 48]. Observing the entire database at a single point in time\nmakes it consistent with causality: the effects of all operations that happened cau\u2010\nsally before that point in time are visible, but no operations that happened cau\u2010\nsally afterward can be seen. Read skew (non-repeatable reads, as illustrated in\nFigure 7-6) means reading data in a state that violates causality.\n\u2022 Our examples of write skew between transactions (see \u201cWrite Skew and Phan\u2010\ntoms\u201d on page 246) also demonstrated causal dependencies: in Figure 7-8, Alice\nwas allowed to go off call because the transaction thought that Bob was still on\ncall, and vice versa. In this case, the action of going off call is causally dependent\non the observation of who is currently on call. Serializable snapshot isolation (see\n\u201cSerializable Snapshot Isolation (SSI)\u201d on page 261) detects write skew by track\u2010\ning the causal dependencies between transactions.\n\u2022 In the example of Alice and Bob watching football ( Figure 9-1), the fact that Bob\ngot a stale result from the server after hearing Alice exclaim the result is a causal\u2010\nity violation: Alice\u2019s exclamation is causally dependent on the announcement of\nthe score, so Bob should also be able to see the score after hearing Alice. The\nsame pattern appeared again in \u201cCross-channel timing dependencies\u201d on page\n331 in the guise of an image resizing service.\nCausality imposes an ordering on events: cause comes before effect; a message is sent\nbefore that message is received; the question comes before the answer. And, like in\nreal life, one thing leads to another: one node reads some data and then writes some\u2010\nthing as a result, another node reads the thing that was written and writes something\nelse in turn, and so on. These chains of causally dependent operations define the\ncausal order in the system\u2014i.e., what happened before what.\n340 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e610af4-060d-4aa2-b17a-7d7c2645c1ea": {"__data__": {"id_": "3e610af4-060d-4aa2-b17a-7d7c2645c1ea", "embedding": null, "metadata": {"page_label": "341", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "287fdc18-ac7c-488d-9a6b-63a934d73724", "node_type": "4", "metadata": {"page_label": "341", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "49b1e78ce6098737b0dd177f20e5c1a8f5d86deace8abb7af60cafb420df8ef7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If a system obeys the ordering imposed by causality, we say that it is causally consis\u2010\ntent. For example, snapshot isolation provides causal consistency: when you read\nfrom the database, and you see some piece of data, then you must also be able to see\nany data that causally precedes it (assuming it has not been deleted in the meantime).\nThe causal order is not a total order\nA total order allows any two elements to be compared, so if you have two elements,\nyou can always say which one is greater and which one is smaller. For example, natu\u2010\nral numbers are totally ordered: if I give you any two numbers, say 5 and 13, you can\ntell me that 13 is greater than 5.\nHowever, mathematical sets are not totally ordered: is { a, b} greater than {b, c}? Well,\nyou can\u2019t really compare them, because neither is a subset of the other. We say they\nare incomparable, and therefore mathematical sets are partially ordered: in some cases\none set is greater than another (if one set contains all the elements of another), but in\nother cases they are incomparable.\nThe difference between a total order and a partial order is reflected in different data\u2010\nbase consistency models:\nLinearizability\nIn a linearizable system, we have a total order of operations: if the system behaves\nas if there is only a single copy of the data, and every operation is atomic, this\nmeans that for any two operations we can always say which one happened first.\nThis total ordering is illustrated as a timeline in Figure 9-4.\nCausality\nWe said that two operations are concurrent if neither happened before the other\n(see \u201cThe \u201chappens-before\u201d relationship and concurrency\u201d on page 186). Put\nanother way, two events are ordered if they are causally related (one happened\nbefore the other), but they are incomparable if they are concurrent. This means\nthat causality defines a partial order , not a total order: some operations are\nordered with respect to each other, but some are incomparable.\nTherefore, according to this definition, there are no concurrent operations in a line\u2010\narizable datastore: there must be a single timeline along which all operations are\ntotally ordered. There might be several requests waiting to be handled, but the data\u2010\nstore ensures that every request is handled atomically at a single point in time, acting\non a single copy of the data, along a single timeline, without any concurrency.\nConcurrency would mean that the timeline branches and merges again\u2014and in this\ncase, operations on different branches are incomparable (i.e., concurrent). We saw\nthis phenomenon in Chapter 5: for example, Figure 5-14 is not a straight-line total\norder, but rather a jumble of different operations going on concurrently. The arrows\nin the diagram indicate causal dependencies\u2014the partial ordering of operations.\nOrdering Guarantees | 341", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bdee4a0-2a66-402f-bfe4-692ad4db56b7": {"__data__": {"id_": "2bdee4a0-2a66-402f-bfe4-692ad4db56b7", "embedding": null, "metadata": {"page_label": "342", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03d2d7df-6831-485d-bdcd-76b4ec0bfb08", "node_type": "4", "metadata": {"page_label": "342", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "67d9fb3291f8f5d7db959e4c492c8710d2293b6f3aac07c63ff5dfd5b9e10fce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you are familiar with distributed version control systems such as Git, their version\nhistories are very much like the graph of causal dependencies. Often one commit\nhappens after another, in a straight line, but sometimes you get branches (when sev\u2010\neral people concurrently work on a project), and merges are created when those con\u2010\ncurrently created commits are combined.\nLinearizability is stronger than causal consistency\nSo what is the relationship between the causal order and linearizability? The answer is\nthat linearizability implies causality: any system that is linearizable will preserve cau\u2010\nsality correctly [ 7]. In particular, if there are multiple communication channels in a\nsystem (such as the message queue and the file storage service in Figure 9-5), lineariz\u2010\nability ensures that causality is automatically preserved without the system having to\ndo anything special (such as passing around timestamps between different compo\u2010\nnents).\nThe fact that linearizability ensures causality is what makes linearizable systems sim\u2010\nple to understand and appealing. However, as discussed in \u201cThe Cost of Linearizabil\u2010\nity\u201d on page 335, making a system linearizable can harm its performance and\navailability, especially if the system has significant network delays (for example, if it\u2019s\ngeographically distributed). For this reason, some distributed data systems have\nabandoned linearizability, which allows them to achieve better performance but can\nmake them difficult to work with.\nThe good news is that a middle ground is possible. Linearizability is not the only way\nof preserving causality\u2014there are other ways too. A system can be causally consistent\nwithout incurring the performance hit of making it linearizable (in particular, the\nCAP theorem does not apply). In fact, causal consistency is the strongest possible\nconsistency model that does not slow down due to network delays, and remains\navailable in the face of network failures [2, 42].\nIn many cases, systems that appear to require linearizability in fact only really require\ncausal consistency, which can be implemented more efficiently. Based on this obser\u2010\nvation, researchers are exploring new kinds of databases that preserve causality, with\nperformance and availability characteristics that are similar to those of eventually\nconsistent systems [49, 50, 51].\nAs this research is quite recent, not much of it has yet made its way into production\nsystems, and there are still challenges to be overcome [ 52, 53]. However, it is a prom\u2010\nising direction for future systems.\nCapturing causal dependencies\nWe won\u2019t go into all the nitty-gritty details of how nonlinearizable systems can main\u2010\ntain causal consistency here, but just briefly explore some of the key ideas.\n342 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "58e4d1a4-7c18-4c48-81c5-643a9f102d6b": {"__data__": {"id_": "58e4d1a4-7c18-4c48-81c5-643a9f102d6b", "embedding": null, "metadata": {"page_label": "343", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2af1954a-ab08-44b4-8593-3b714352b5dd", "node_type": "4", "metadata": {"page_label": "343", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ddd090ee704ff054d09ae009fbcb147a12419badf560977f975a7e636cfd4792", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to maintain causality, you need to know which operation happened before\nwhich other operation. This is a partial order: concurrent operations may be pro\u2010\ncessed in any order, but if one operation happened before another, then they must be\nprocessed in that order on every replica. Thus, when a replica processes an operation,\nit must ensure that all causally preceding operations (all operations that happened\nbefore) have already been processed; if some preceding operation is missing, the later\noperation must wait until the preceding operation has been processed.\nIn order to determine causal dependencies, we need some way of describing the\n\u201cknowledge\u201d of a node in the system. If a node had already seen the value X when it\nissued the write Y, then X and Y may be causally related. The analysis uses the kinds\nof questions you would expect in a criminal investigation of fraud charges: did the\nCEO know about X at the time when they made decision Y?\nThe techniques for determining which operation happened before which other oper\u2010\nation are similar to what we discussed in \u201cDetecting Concurrent Writes\u201d on page 184.\nThat section discussed causality in a leaderless datastore, where we need to detect\nconcurrent writes to the same key in order to prevent lost updates. Causal consis\u2010\ntency goes further: it needs to track causal dependencies across the entire database,\nnot just for a single key. Version vectors can be generalized to do this [54].\nIn order to determine the causal ordering, the database needs to know which version\nof the data was read by the application. This is why, in Figure 5-13, the version num\u2010\nber from the prior operation is passed back to the database on a write. A similar idea\nappears in the conflict detection of SSI, as discussed in \u201cSerializable Snapshot Isola\u2010\ntion (SSI)\u201d on page 261: when a transaction wants to commit, the database checks\nwhether the version of the data that it read is still up to date. To this end, the database\nkeeps track of which data has been read by which transaction. \nSequence Number Ordering\nAlthough causality is an important theoretical concept, actually keeping track of all\ncausal dependencies can become impractical. In many applications, clients read lots\nof data before writing something, and then it is not clear whether the write is causally\ndependent on all or only some of those prior reads. Explicitly tracking all the data\nthat has been read would mean a large overhead.\nHowever, there is a better way: we can use sequence numbers or timestamps to order\nevents. A timestamp need not come from a time-of-day clock (or physical clock,\nwhich have many problems, as discussed in \u201cUnreliable Clocks\u201d on page 287). It can\ninstead come from a logical clock , which is an algorithm to generate a sequence of\nnumbers to identify operations, typically using counters that are incremented for\nevery operation.\nOrdering Guarantees | 343", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "008ecae1-2464-43ff-8fc7-a122a427c509": {"__data__": {"id_": "008ecae1-2464-43ff-8fc7-a122a427c509", "embedding": null, "metadata": {"page_label": "344", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf0d539a-a3bf-4b8f-b151-88cba71d43b6", "node_type": "4", "metadata": {"page_label": "344", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1ba41c65bd083c329e4b794d7ca14a4ee1d838097f956b4b1ced72badd43c52b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vii. A total order that is inconsistent with causality is easy to create, but not very useful. For example, you can\ngenerate a random UUID for each operation, and compare UUIDs lexicographically to define the total order\u2010\ning of operations. This is a valid total order, but the random UUIDs tell you nothing about which operation\nactually happened first, or whether the operations were concurrent.\nSuch sequence numbers or timestamps are compact (only a few bytes in size), and\nthey provide a total order: that is, every operation has a unique sequence number, and\nyou can always compare two sequence numbers to determine which is greater (i.e.,\nwhich operation happened later).\nIn particular, we can create sequence numbers in a total order that is consistent with\ncausality:vii we promise that if operation A causally happened before B, then A occurs\nbefore B in the total order (A has a lower sequence number than B). Concurrent\noperations may be ordered arbitrarily. Such a total order captures all the causality\ninformation, but also imposes more ordering than strictly required by causality.\nIn a database with single-leader replication (see \u201cLeaders and Followers\u201d on page\n152), the replication log defines a total order of write operations that is consistent\nwith causality. The leader can simply increment a counter for each operation, and\nthus assign a monotonically increasing sequence number to each operation in the\nreplication log. If a follower applies the writes in the order they appear in the replica\u2010\ntion log, the state of the follower is always causally consistent (even if it is lagging\nbehind the leader).\nNoncausal sequence number generators\nIf there is not a single leader (perhaps because you are using a multi-leader or leader\u2010\nless database, or because the database is partitioned), it is less clear how to generate\nsequence numbers for operations. Various methods are used in practice:\n\u2022 Each node can generate its own independent set of sequence numbers. For exam\u2010\nple, if you have two nodes, one node can generate only odd numbers and the\nother only even numbers. In general, you could reserve some bits in the binary\nrepresentation of the sequence number to contain a unique node identifier, and\nthis would ensure that two different nodes can never generate the same sequence\nnumber.\n\u2022 You can attach a timestamp from a time-of-day clock (physical clock) to each\noperation [55]. Such timestamps are not sequential, but if they have sufficiently\nhigh resolution, they might be sufficient to totally order operations. This fact is\nused in the last write wins conflict resolution method (see \u201cTimestamps for\nordering events\u201d on page 291).\n\u2022 You can preallocate blocks of sequence numbers. For example, node A might\nclaim the block of sequence numbers from 1 to 1,000, and node B might claim\n344 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef29320a-0e48-4d9c-adb9-2d1af822929a": {"__data__": {"id_": "ef29320a-0e48-4d9c-adb9-2d1af822929a", "embedding": null, "metadata": {"page_label": "345", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01073416-15d2-4ed3-9569-c3a7758c2fbf", "node_type": "4", "metadata": {"page_label": "345", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "08efdcf31660a6c897fa16ca00efbebb59aa7c5f8294cdf2be5a9ce5fb7f6cd6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "viii. It is possible to make physical clock timestamps consistent with causality: in \u201cSynchronized clocks for\nglobal snapshots\u201d on page 294 we discussed Google\u2019s Spanner, which estimates the expected clock skew and\nwaits out the uncertainty interval before committing a write. This method ensures that a causally later trans\u2010\naction is given a greater timestamp. However, most clocks cannot provide the required uncertainty metric.\nthe block from 1,001 to 2,000. Then each node can independently assign\nsequence numbers from its block, and allocate a new block when its supply of\nsequence numbers begins to run low.\nThese three options all perform better and are more scalable than pushing all opera\u2010\ntions through a single leader that increments a counter. They generate a unique,\napproximately increasing sequence number for each operation. However, they all\nhave a problem: the sequence numbers they generate are not consistent with causality.\nThe causality problems occur because these sequence number generators do not cor\u2010\nrectly capture the ordering of operations across different nodes:\n\u2022 Each node may process a different number of operations per second. Thus, if one\nnode generates even numbers and the other generates odd numbers, the counter\nfor even numbers may lag behind the counter for odd numbers, or vice versa. If\nyou have an odd-numbered operation and an even-numbered operation, you\ncannot accurately tell which one causally happened first.\n\u2022 Timestamps from physical clocks are subject to clock skew, which can make\nthem inconsistent with causality. For example, see Figure 8-3, which shows a sce\u2010\nnario in which an operation that happened causally later was actually assigned a\nlower timestamp.viii\n\u2022 In the case of the block allocator, one operation may be given a sequence number\nin the range from 1,001 to 2,000, and a causally later operation may be given a\nnumber in the range from 1 to 1,000. Here, again, the sequence number is incon\u2010\nsistent with causality.\nLamport timestamps\nAlthough the three sequence number generators just described are inconsistent with\ncausality, there is actually a simple method for generating sequence numbers that is\nconsistent with causality. It is called a Lamport timestamp, proposed in 1978 by Leslie\nLamport [56], in what is now one of the most-cited papers in the field of distributed\nsystems.\nThe use of Lamport timestamps is illustrated in Figure 9-8. Each node has a unique\nidentifier, and each node keeps a counter of the number of operations it has pro\u2010\ncessed. The Lamport timestamp is then simply a pair of ( counter, node ID ). Two\nOrdering Guarantees | 345", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80d4f7d8-cc79-40ec-8432-a39c92f93e90": {"__data__": {"id_": "80d4f7d8-cc79-40ec-8432-a39c92f93e90", "embedding": null, "metadata": {"page_label": "346", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "054e3658-ddc7-49ec-810d-a6de0f47add0", "node_type": "4", "metadata": {"page_label": "346", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1fc7a1337aedb62f8cd685c5f18b7977219fb2f0a6cde579ac4befa9e9312bac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "nodes may sometimes have the same counter value, but by including the node ID in\nthe timestamp, each timestamp is made unique.\nFigure 9-8. Lamport timestamps provide a total ordering consistent with causality.\nA Lamport timestamp bears no relationship to a physical time-of-day clock, but it\nprovides total ordering: if you have two timestamps, the one with a greater counter\nvalue is the greater timestamp; if the counter values are the same, the one with the\ngreater node ID is the greater timestamp.\nSo far this description is essentially the same as the even/odd counters described in\nthe last section. The key idea about Lamport timestamps, which makes them consis\u2010\ntent with causality, is the following: every node and every client keeps track of the\nmaximum counter value it has seen so far, and includes that maximum on every\nrequest. When a node receives a request or response with a maximum counter value\ngreater than its own counter value, it immediately increases its own counter to that\nmaximum.\nThis is shown in Figure 9-8, where client A receives a counter value of 5 from node 2,\nand then sends that maximum of 5 to node 1. At that time, node 1\u2019s counter was only\n1, but it was immediately moved forward to 5, so the next operation had an incre\u2010\nmented counter value of 6.\nAs long as the maximum counter value is carried along with every operation, this\nscheme ensures that the ordering from the Lamport timestamps is consistent with\ncausality, because every causal dependency results in an increased timestamp.\nLamport timestamps are sometimes confused with version vectors, which we saw in\n\u201cDetecting Concurrent Writes\u201d on page 184. Although there are some similarities,\nthey have a different purpose: version vectors can distinguish whether two operations\nare concurrent or whether one is causally dependent on the other, whereas Lamport\ntimestamps always enforce a total ordering. From the total ordering of Lamport time\u2010\n346 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1724835e-dc0b-417e-95f7-6ada304ee40f": {"__data__": {"id_": "1724835e-dc0b-417e-95f7-6ada304ee40f", "embedding": null, "metadata": {"page_label": "347", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd6e70eb-c1a9-4d6e-8a91-e9a0845d9ee7", "node_type": "4", "metadata": {"page_label": "347", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "af89bfaec3140e94c0dd65432ec01789f20c7f33ce5c6cae4963e6fcddecebf6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "stamps, you cannot tell whether two operations are concurrent or whether they are\ncausally dependent. The advantage of Lamport timestamps over version vectors is\nthat they are more compact. \nTimestamp ordering is not sufficient\nAlthough Lamport timestamps define a total order of operations that is consistent\nwith causality, they are not quite sufficient to solve many common problems in dis\u2010\ntributed systems.\nFor example, consider a system that needs to ensure that a username uniquely identi\u2010\nfies a user account. If two users concurrently try to create an account with the same\nusername, one of the two should succeed and the other should fail. (We touched on\nthis problem previously in \u201cThe leader and the lock\u201d on page 301.)\nAt first glance, it seems as though a total ordering of operations (e.g., using Lamport\ntimestamps) should be sufficient to solve this problem: if two accounts with the same\nusername are created, pick the one with the lower timestamp as the winner (the one\nwho grabbed the username first), and let the one with the greater timestamp fail.\nSince timestamps are totally ordered, this comparison is always valid.\nThis approach works for determining the winner after the fact: once you have collec\u2010\nted all the username creation operations in the system, you can compare their time\u2010\nstamps. However, it is not sufficient when a node has just received a request from a\nuser to create a username, and needs to decide right now whether the request should\nsucceed or fail. At that moment, the node does not know whether another node is\nconcurrently in the process of creating an account with the same username, and what\ntimestamp that other node may assign to the operation.\nIn order to be sure that no other node is in the process of concurrently creating an\naccount with the same username and a lower timestamp, you would have to check\nwith every other node to see what it is doing [ 56]. If one of the other nodes has failed\nor cannot be reached due to a network problem, this system would grind to a halt.\nThis is not the kind of fault-tolerant system that we need.\nThe problem here is that the total order of operations only emerges after you have\ncollected all of the operations. If another node has generated some operations, but\nyou don\u2019t yet know what they are, you cannot construct the final ordering of opera\u2010\ntions: the unknown operations from the other node may need to be inserted at vari\u2010\nous positions in the total order.\nTo conclude: in order to implement something like a uniqueness constraint for user\u2010\nnames, it\u2019s not sufficient to have a total ordering of operations\u2014you also need to\nknow when that order is finalized. If you have an operation to create a username, and\nyou are sure that no other node can insert a claim for the same username ahead of\nyour operation in the total order, then you can safely declare the operation successful.\nOrdering Guarantees | 347", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f941bcff-6544-4e72-b05f-35614512cf60": {"__data__": {"id_": "f941bcff-6544-4e72-b05f-35614512cf60", "embedding": null, "metadata": {"page_label": "348", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95dc23d6-cde0-4dba-8678-b0dec0c87ff1", "node_type": "4", "metadata": {"page_label": "348", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3be2ef50187b55c64aeb20e0ee3b80559993b767300ec95848a9717e08a756aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ix. The term atomic broadcast is traditional, but it is very confusing as it\u2019s inconsistent with other uses of the\nword atomic: it has nothing to do with atomicity in ACID transactions and is only indirectly related to atomic\noperations (in the sense of multi-threaded programming) or atomic registers (linearizable storage). The term\ntotal order multicast is another synonym.\nThis idea of knowing when your total order is finalized is captured in the topic of\ntotal order broadcast. \nTotal Order Broadcast\nIf your program runs only on a single CPU core, it is easy to define a total ordering of\noperations: it is simply the order in which they were executed by the CPU. However,\nin a distributed system, getting all nodes to agree on the same total ordering of opera\u2010\ntions is tricky. In the last section we discussed ordering by timestamps or sequence\nnumbers, but found that it is not as powerful as single-leader replication (if you use\ntimestamp ordering to implement a uniqueness constraint, you cannot tolerate any\nfaults).\nAs discussed, single-leader replication determines a total order of operations by\nchoosing one node as the leader and sequencing all operations on a single CPU core\non the leader. The challenge then is how to scale the system if the throughput is\ngreater than a single leader can handle, and also how to handle failover if the leader\nfails (see \u201cHandling Node Outages\u201d on page 156). In the distributed systems litera\u2010\nture, this problem is known as total order broadcast or atomic broadcast [25, 57, 58].ix\nScope of ordering guarantee\nPartitioned databases with a single leader per partition often main\u2010\ntain ordering only per partition, which means they cannot offer\nconsistency guarantees (e.g., consistent snapshots, foreign key ref\u2010\nerences) across partitions. Total ordering across all partitions is\npossible, but requires additional coordination [59].\nTotal order broadcast is usually described as a protocol for exchanging messages\nbetween nodes. Informally, it requires that two safety properties always be satisfied:\nReliable delivery\nNo messages are lost: if a message is delivered to one node, it is delivered to all\nnodes.\nTotally ordered delivery\nMessages are delivered to every node in the same order.\nA correct algorithm for total order broadcast must ensure that the reliability and\nordering properties are always satisfied, even if a node or the network is faulty. Of\n348 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5636584-a70f-4a40-b689-d1e51459e6fa": {"__data__": {"id_": "f5636584-a70f-4a40-b689-d1e51459e6fa", "embedding": null, "metadata": {"page_label": "349", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e25f08ce-c3aa-4165-a5f5-bc0a06378411", "node_type": "4", "metadata": {"page_label": "349", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "753439ad99efb9675d629e222d342133736021ee1591b7c7e54d8ae69b65dbf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "course, messages will not be delivered while the network is interrupted, but an algo\u2010\nrithm can keep retrying so that the messages get through when the network is even\u2010\ntually repaired (and then they must still be delivered in the correct order).\nUsing total order broadcast\nConsensus services such as ZooKeeper and etcd actually implement total order\nbroadcast. This fact is a hint that there is a strong connection between total order\nbroadcast and consensus, which we will explore later in this chapter.\nTotal order broadcast is exactly what you need for database replication: if every mes\u2010\nsage represents a write to the database, and every replica processes the same writes in\nthe same order, then the replicas will remain consistent with each other (aside from\nany temporary replication lag). This principle is known as state machine replication\n[60], and we will return to it in Chapter 11.\nSimilarly, total order broadcast can be used to implement serializable transactions: as\ndiscussed in \u201cActual Serial Execution\u201d on page 252, if every message represents a\ndeterministic transaction to be executed as a stored procedure, and if every node pro\u2010\ncesses those messages in the same order, then the partitions and replicas of the data\u2010\nbase are kept consistent with each other [61].\nAn important aspect of total order broadcast is that the order is fixed at the time the\nmessages are delivered: a node is not allowed to retroactively insert a message into an\nearlier position in the order if subsequent messages have already been delivered. This\nfact makes total order broadcast stronger than timestamp ordering.\nAnother way of looking at total order broadcast is that it is a way of creating a log (as\nin a replication log, transaction log, or write-ahead log): delivering a message is like\nappending to the log. Since all nodes must deliver the same messages in the same\norder, all nodes can read the log and see the same sequence of messages.\nTotal order broadcast is also useful for implementing a lock service that provides\nfencing tokens (see \u201cFencing tokens\u201d on page 303). Every request to acquire the lock\nis appended as a message to the log, and all messages are sequentially numbered in\nthe order they appear in the log. The sequence number can then serve as a fencing\ntoken, because it is monotonically increasing. In ZooKeeper, this sequence number is\ncalled zxid [15].\nOrdering Guarantees | 349", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3417aa77-c8dd-4474-abdf-469a528017a0": {"__data__": {"id_": "3417aa77-c8dd-4474-abdf-469a528017a0", "embedding": null, "metadata": {"page_label": "350", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a5e6544-5550-4555-85be-40e06f5acc5a", "node_type": "4", "metadata": {"page_label": "350", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e49b4288a0e774790b6e2895252ee013ada0c28a6c93957d7b11b0f70df88d03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "x. In a formal sense, a linearizable read-write register is an \u201ceasier\u201d problem. Total order broadcast is equiva\u2010\nlent to consensus [67], which has no deterministic solution in the asynchronous crash-stop model [68],\nwhereas a linearizable read-write register can be implemented in the same system model [23, 24, 25]. How\u2010\never, supporting atomic operations such as compare-and-set or increment-and-get in a register makes it\nequivalent to consensus [28]. Thus, the problems of consensus and a linearizable register are closely related.\nxi. If you don\u2019t wait, but acknowledge the write immediately after it has been enqueued, you get something\nsimilar to the memory consistency model of multi-core x86 processors [43]. That model is neither lineariza\u2010\nble nor sequentially consistent.\nImplementing linearizable storage using total order broadcast\nAs illustrated in Figure 9-4, in a linearizable system there is a total order of opera\u2010\ntions. Does that mean linearizability is the same as total order broadcast? Not quite,\nbut there are close links between the two.x \nTotal order broadcast is asynchronous: messages are guaranteed to be delivered relia\u2010\nbly in a fixed order, but there is no guarantee about when a message will be delivered\n(so one recipient may lag behind the others). By contrast, linearizability is a recency\nguarantee: a read is guaranteed to see the latest value written.\nHowever, if you have total order broadcast, you can build linearizable storage on top\nof it. For example, you can ensure that usernames uniquely identify user accounts.\nImagine that for every possible username, you can have a linearizable register with an\natomic compare-and-set operation. Every register initially has the value null (indi\u2010\ncating that the username is not taken). When a user wants to create a username, you\nexecute a compare-and-set operation on the register for that username, setting it to\nthe user account ID, under the condition that the previous register value is null. If\nmultiple users try to concurrently grab the same username, only one of the compare-\nand-set operations will succeed, because the others will see a value other than null\n(due to linearizability).\nYou can implement such a linearizable compare-and-set operation as follows by\nusing total order broadcast as an append-only log [62, 63]:\n1. Append a message to the log, tentatively indicating the username you want to\nclaim.\n2. Read the log, and wait for the message you appended to be delivered back to\nyou.xi\n3. Check for any messages claiming the username that you want. If the first message\nfor your desired username is your own message, then you are successful: you can\ncommit the username claim (perhaps by appending another message to the log)\nand acknowledge it to the client. If the first message for your desired username is\nfrom another user, you abort the operation.\n350 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2905, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1605ec7e-f09e-4ca4-a5b2-f20e9cbe68d6": {"__data__": {"id_": "1605ec7e-f09e-4ca4-a5b2-f20e9cbe68d6", "embedding": null, "metadata": {"page_label": "351", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcad1dc8-f51f-4443-9685-1add2b409bf5", "node_type": "4", "metadata": {"page_label": "351", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8e57b45b7abbddf89f0af472c36bc01e3a63d84ea452251e805c037c60909158", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because log entries are delivered to all nodes in the same order, if there are several\nconcurrent writes, all nodes will agree on which one came first. Choosing the first of\nthe conflicting writes as the winner and aborting later ones ensures that all nodes\nagree on whether a write was committed or aborted. A similar approach can be used\nto implement serializable multi-object transactions on top of a log [62].\nWhile this procedure ensures linearizable writes, it doesn\u2019t guarantee linearizable\nreads\u2014if you read from a store that is asynchronously updated from the log, it may\nbe stale. (To be precise, the procedure described here provides sequential consistency\n[47, 64], sometimes also known as timeline consistency  [65, 66], a slightly weaker\nguarantee than linearizability.) To make reads linearizable, there are a few options:\n\u2022 You can sequence reads through the log by appending a message, reading the log,\nand performing the actual read when the message is delivered back to you. The\nmessage\u2019s position in the log thus defines the point in time at which the read\nhappens. (Quorum reads in etcd work somewhat like this [16].)\n\u2022 If the log allows you to fetch the position of the latest log message in a lineariza\u2010\nble way, you can query that position, wait for all entries up to that position to be\ndelivered to you, and then perform the read. (This is the idea behind Zoo\u2010\nKeeper\u2019s sync() operation [15].)\n\u2022 You can make your read from a replica that is synchronously updated on writes,\nand is thus sure to be up to date. (This technique is used in chain replication\n[63]; see also \u201cResearch on Replication\u201d on page 155.)\nImplementing total order broadcast using linearizable storage\nThe last section showed how to build a linearizable compare-and-set operation from\ntotal order broadcast. We can also turn it around, assume that we have linearizable\nstorage, and show how to build total order broadcast from it.\nThe easiest way is to assume you have a linearizable register that stores an integer and\nthat has an atomic increment-and-get operation [ 28]. Alternatively, an atomic\ncompare-and-set operation would also do the job.\nThe algorithm is simple: for every message you want to send through total order\nbroadcast, you increment-and-get the linearizable integer, and then attach the value\nyou got from the register as a sequence number to the message. You can then send\nthe message to all nodes (resending any lost messages), and the recipients will deliver\nthe messages consecutively by sequence number.\nNote that unlike Lamport timestamps, the numbers you get from incrementing the\nlinearizable register form a sequence with no gaps. Thus, if a node has delivered mes\u2010\nsage 4 and receives an incoming message with a sequence number of 6, it knows that\nit must wait for message 5 before it can deliver message 6. The same is not the case\nOrdering Guarantees | 351", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f10aaa10-f44c-4783-81e2-c5785357f2d6": {"__data__": {"id_": "f10aaa10-f44c-4783-81e2-c5785357f2d6", "embedding": null, "metadata": {"page_label": "352", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75efab75-6103-4a86-a65a-95b3f5f6803c", "node_type": "4", "metadata": {"page_label": "352", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9811a7ec27f9a5012c380a719f1cfe844718aa8eecada7b3f5ff8dfa27258175", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with Lamport timestamps\u2014in fact, this is the key difference between total order\nbroadcast and timestamp ordering.\nHow hard could it be to make a linearizable integer with an atomic increment-and-\nget operation? As usual, if things never failed, it would be easy: you could just keep it\nin a variable on one node. The problem lies in handling the situation when network\nconnections to that node are interrupted, and restoring the value when that node fails\n[59]. In general, if you think hard enough about linearizable sequence number gener\u2010\nators, you inevitably end up with a consensus algorithm.\nThis is no coincidence: it can be proved that a linearizable compare-and-set (or\nincrement-and-get) register and total order broadcast are both equivalent to consen\u2010\nsus [28, 67]. That is, if you can solve one of these problems, you can transform it into\na solution for the others. This is quite a profound and surprising insight!\nIt is time to finally tackle the consensus problem head-on, which we will do in the\nrest of this chapter. \nDistributed Transactions and Consensus\nConsensus is one of the most important and fundamental problems in distributed\ncomputing. On the surface, it seems simple: informally, the goal is simply to get sev\u2010\neral nodes to agree on something . You might think that this shouldn\u2019t be too hard.\nUnfortunately, many broken systems have been built in the mistaken belief that this\nproblem is easy to solve.\nAlthough consensus is very important, the section about it appears late in this book\nbecause the topic is quite subtle, and appreciating the subtleties requires some pre\u2010\nrequisite knowledge. Even in the academic research community, the understanding\nof consensus only gradually crystallized over the course of decades, with many mis\u2010\nunderstandings along the way. Now that we have discussed replication ( Chapter 5),\ntransactions (Chapter 7), system models ( Chapter 8), linearizability, and total order\nbroadcast (this chapter), we are finally ready to tackle the consensus problem.\nThere are a number of situations in which it is important for nodes to agree. For\nexample:\nLeader election\nIn a database with single-leader replication, all nodes need to agree on which\nnode is the leader. The leadership position might become contested if some\nnodes can\u2019t communicate with others due to a network fault. In this case, con\u2010\nsensus is important to avoid a bad failover, resulting in a split brain situation in\nwhich two nodes both believe themselves to be the leader (see \u201cHandling Node\nOutages\u201d on page 156). If there were two leaders, they would both accept writes\nand their data would diverge, leading to inconsistency and data loss.\n352 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bcbf27c-3aa3-47f1-88a5-60118d64f2d2": {"__data__": {"id_": "2bcbf27c-3aa3-47f1-88a5-60118d64f2d2", "embedding": null, "metadata": {"page_label": "353", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3bca2b0-a931-4def-a018-aaa835af6265", "node_type": "4", "metadata": {"page_label": "353", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dc71c5d6646b43739feec773793cd040f25cf11f1ad5c70a7f9cf32ebd391700", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "xii. Atomic commit is formalized slightly differently from consensus: an atomic transaction can commit only\nif all participants vote to commit, and must abort if any participant needs to abort. Consensus is allowed to\ndecide on any value that is proposed by one of the participants. However, atomic commit and consensus are\nreducible to each other [70, 71]. Nonblocking atomic commit is harder than consensus\u2014see \u201cThree-phase\ncommit\u201d on page 359.\nAtomic commit\nIn a database that supports transactions spanning several nodes or partitions, we\nhave the problem that a transaction may fail on some nodes but succeed on oth\u2010\ners. If we want to maintain transaction atomicity (in the sense of ACID; see\n\u201cAtomicity\u201d on page 223), we have to get all nodes to agree on the outcome of the\ntransaction: either they all abort/roll back (if anything goes wrong) or they all\ncommit (if nothing goes wrong). This instance of consensus is known as the\natomic commit problem.xii\nThe Impossibility of Consensus\nYou may have heard about the FLP result [ 68]\u2014named after the authors Fischer,\nLynch, and Paterson\u2014which proves that there is no algorithm that is always able to\nreach consensus if there is a risk that a node may crash. In a distributed system, we\nmust assume that nodes may crash, so reliable consensus is impossible. Yet, here we\nare, discussing algorithms for achieving consensus. What is going on here?\nThe answer is that the FLP result is proved in the asynchronous system model (see\n\u201cSystem Model and Reality\u201d on page 306), a very restrictive model that assumes a\ndeterministic algorithm that cannot use any clocks or timeouts. If the algorithm is\nallowed to use timeouts, or some other way of identifying suspected crashed nodes\n(even if the suspicion is sometimes wrong), then consensus becomes solvable [ 67].\nEven just allowing the algorithm to use random numbers is sufficient to get around\nthe impossibility result [69].\nThus, although the FLP result about the impossibility of consensus is of great theoret\u2010\nical importance, distributed systems can usually achieve consensus in practice.\nIn this section we will first examine the atomic commit problem in more detail. In\nparticular, we will discuss the two-phase commit (2PC) algorithm, which is the most\ncommon way of solving atomic commit and which is implemented in various data\u2010\nbases, messaging systems, and application servers. It turns out that 2PC is a kind of\nconsensus algorithm\u2014but not a very good one [70, 71].\nBy learning from 2PC we will then work our way toward better consensus algorithms,\nsuch as those used in ZooKeeper (Zab) and etcd (Raft).\nDistributed Transactions and Consensus | 353", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d79deba2-1ac3-4483-878e-50dd552007db": {"__data__": {"id_": "d79deba2-1ac3-4483-878e-50dd552007db", "embedding": null, "metadata": {"page_label": "354", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4dea3af1-89f0-4cff-9fe0-e62c4e20be25", "node_type": "4", "metadata": {"page_label": "354", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4973aed0ca6fb4855aec9f2832a2326f8d7b586de5445ec3985c2b8dd72da66b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Atomic Commit and Two-Phase Commit (2PC)\nIn Chapter 7 we learned that the purpose of transaction atomicity is to provide sim\u2010\nple semantics in the case where something goes wrong in the middle of making sev\u2010\neral writes. The outcome of a transaction is either a successful commit, in which case\nall of the transaction\u2019s writes are made durable, or an abort, in which case all of the\ntransaction\u2019s writes are rolled back (i.e., undone or discarded).\nAtomicity prevents failed transactions from littering the database with half-finished\nresults and half-updated state. This is especially important for multi-object transac\u2010\ntions (see \u201cSingle-Object and Multi-Object Operations\u201d on page 228) and databases\nthat maintain secondary indexes. Each secondary index is a separate data structure\nfrom the primary data\u2014thus, if you modify some data, the corresponding change\nneeds to also be made in the secondary index. Atomicity ensures that the secondary\nindex stays consistent with the primary data (if the index became inconsistent with\nthe primary data, it would not be very useful).\nFrom single-node to distributed atomic commit\nFor transactions that execute at a single database node, atomicity is commonly imple\u2010\nmented by the storage engine. When the client asks the database node to commit the\ntransaction, the database makes the transaction\u2019s writes durable (typically in a write-\nahead log; see \u201cMaking B-trees reliable\u201d on page 82) and then appends a commit\nrecord to the log on disk. If the database crashes in the middle of this process, the\ntransaction is recovered from the log when the node restarts: if the commit record\nwas successfully written to disk before the crash, the transaction is considered com\u2010\nmitted; if not, any writes from that transaction are rolled back.\nThus, on a single node, transaction commitment crucially depends on the order in\nwhich data is durably written to disk: first the data, then the commit record [ 72]. The\nkey deciding moment for whether the transaction commits or aborts is the moment\nat which the disk finishes writing the commit record: before that moment, it is still\npossible to abort (due to a crash), but after that moment, the transaction is commit\u2010\nted (even if the database crashes). Thus, it is a single device (the controller of one par\u2010\nticular disk drive, attached to one particular node) that makes the commit atomic.\nHowever, what if multiple nodes are involved in a transaction? For example, perhaps\nyou have a multi-object transaction in a partitioned database, or a term-partitioned\nsecondary index (in which the index entry may be on a different node from the pri\u2010\nmary data; see \u201cPartitioning and Secondary Indexes\u201d on page 206). Most \u201cNoSQL\u201d\ndistributed datastores do not support such distributed transactions, but various clus\u2010\ntered relational systems do (see \u201cDistributed Transactions in Practice\u201d on page 360).\nIn these cases, it is not sufficient to simply send a commit request to all of the nodes\nand independently commit the transaction on each one. In doing so, it could easily\n354 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3098, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c10d0bf3-73f0-482b-9af2-f77b19642dfe": {"__data__": {"id_": "c10d0bf3-73f0-482b-9af2-f77b19642dfe", "embedding": null, "metadata": {"page_label": "355", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84ad2d4d-3861-4c5b-ab04-05459126052d", "node_type": "4", "metadata": {"page_label": "355", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b5f23c1ba83c7e408d8ff0521d04d1772c6b9e1e728fb1a43f4e1a1d51f22d5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "happen that the commit succeeds on some nodes and fails on other nodes, which\nwould violate the atomicity guarantee:\n\u2022 Some nodes may detect a constraint violation or conflict, making an abort neces\u2010\nsary, while other nodes are successfully able to commit.\n\u2022 Some of the commit requests might be lost in the network, eventually aborting\ndue to a timeout, while other commit requests get through.\n\u2022 Some nodes may crash before the commit record is fully written and roll back on\nrecovery, while others successfully commit.\nIf some nodes commit the transaction but others abort it, the nodes become inconsis\u2010\ntent with each other (like in Figure 7-3). And once a transaction has been committed\non one node, it cannot be retracted again if it later turns out that it was aborted on\nanother node. For this reason, a node must only commit once it is certain that all\nother nodes in the transaction are also going to commit.\nA transaction commit must be irrevocable\u2014you are not allowed to change your\nmind and retroactively abort a transaction after it has been committed. The reason\nfor this rule is that once data has been committed, it becomes visible to other transac\u2010\ntions, and thus other clients may start relying on that data; this principle forms the\nbasis of read committed isolation, discussed in \u201cRead Committed\u201d on page 234. If a\ntransaction was allowed to abort after committing, any transactions that read the\ncommitted data would be based on data that was retroactively declared not to have\nexisted\u2014so they would have to be reverted as well.\n(It is possible for the effects of a committed transaction to later be undone by\nanother, compensating transaction  [73, 74]. However, from the database\u2019s point of\nview this is a separate transaction, and thus any cross-transaction correctness\nrequirements are the application\u2019s problem.) \nIntroduction to two-phase commit\nTwo-phase commit is an algorithm for achieving atomic transaction commit across\nmultiple nodes\u2014i.e., to ensure that either all nodes commit or all nodes abort. It is a\nclassic algorithm in distributed databases [ 13, 35, 75]. 2PC is used internally in some\ndatabases and also made available to applications in the form of XA transactions [76,\n77] (which are supported by the Java Transaction API, for example) or via WS-\nAtomicTransaction for SOAP web services [78, 79].\nThe basic flow of 2PC is illustrated in Figure 9-9. Instead of a single commit request,\nas with a single-node transaction, the commit/abort process in 2PC is split into two\nphases (hence the name).\nDistributed Transactions and Consensus | 355", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d05cf104-5ec3-4ae0-a7ba-4ac2e6c77bfb": {"__data__": {"id_": "d05cf104-5ec3-4ae0-a7ba-4ac2e6c77bfb", "embedding": null, "metadata": {"page_label": "356", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5ad3909-7bc2-4fbf-b861-0ed12ef9cf7a", "node_type": "4", "metadata": {"page_label": "356", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "29d799094d75b7e0d975238fc3fc19743a717611de198e3b0419889ded5d2cfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 9-9. A successful execution of two-phase commit (2PC).\nDon\u2019t confuse 2PC and 2PL\nTwo-phase commit (2PC) and two-phase locking (see \u201cTwo-Phase\nLocking (2PL)\u201d on page 257) are two very different things. 2PC\nprovides atomic commit in a distributed database, whereas 2PL\nprovides serializable isolation. To avoid confusion, it\u2019s best to think\nof them as entirely separate concepts and to ignore the unfortunate\nsimilarity in the names.\n2PC uses a new component that does not normally appear in single-node transac\u2010\ntions: a coordinator (also known as transaction manager ). The coordinator is often\nimplemented as a library within the same application process that is requesting the\ntransaction (e.g., embedded in a Java EE container), but it can also be a separate pro\u2010\ncess or service. Examples of such coordinators include Narayana, JOTM, BTM, or\nMSDTC.\nA 2PC transaction begins with the application reading and writing data on multiple\ndatabase nodes, as normal. We call these database nodes participants in the transac\u2010\ntion. When the application is ready to commit, the coordinator begins phase 1: it\nsends a prepare request to each of the nodes, asking them whether they are able to\ncommit. The coordinator then tracks the responses from the participants:\n\u2022 If all participants reply \u201cyes,\u201d indicating they are ready to commit, then the coor\u2010\ndinator sends out a commit request in phase 2, and the commit actually takes\nplace.\n\u2022 If any of the participants replies \u201cno,\u201d the coordinator sends an abort request to\nall nodes in phase 2.\nThis process is somewhat like the traditional marriage ceremony in Western cultures:\nthe minister asks the bride and groom individually whether each wants to marry the\nother, and typically receives the answer \u201cI do\u201d from both. After receiving both\n356 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2329e98c-13e8-401c-bcab-451ba37cc0fa": {"__data__": {"id_": "2329e98c-13e8-401c-bcab-451ba37cc0fa", "embedding": null, "metadata": {"page_label": "357", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7bd547d-9bdc-41b4-bd95-b27fba8ebf35", "node_type": "4", "metadata": {"page_label": "357", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bf26e66ec280eb163734403fd81ba3abaa23c7191c7bdee1f01ae10095a05d78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "acknowledgments, the minister pronounces the couple husband and wife: the trans\u2010\naction is committed, and the happy fact is broadcast to all attendees. If either bride or\ngroom does not say \u201cyes,\u201d the ceremony is aborted [73].\nA system of promises\nFrom this short description it might not be clear why two-phase commit ensures\natomicity, while one-phase commit across several nodes does not. Surely the prepare\nand commit requests can just as easily be lost in the two-phase case. What makes 2PC\ndifferent?\nTo understand why it works, we have to break down the process in a bit more detail:\n1. When the application wants to begin a distributed transaction, it requests a\ntransaction ID from the coordinator. This transaction ID is globally unique.\n2. The application begins a single-node transaction on each of the participants, and\nattaches the globally unique transaction ID to the single-node transaction. All\nreads and writes are done in one of these single-node transactions. If anything\ngoes wrong at this stage (for example, a node crashes or a request times out), the\ncoordinator or any of the participants can abort.\n3. When the application is ready to commit, the coordinator sends a prepare\nrequest to all participants, tagged with the global transaction ID. If any of these\nrequests fails or times out, the coordinator sends an abort request for that trans\u2010\naction ID to all participants.\n4. When a participant receives the prepare request, it makes sure that it can defi\u2010\nnitely commit the transaction under all circumstances. This includes writing all\ntransaction data to disk (a crash, a power failure, or running out of disk space is\nnot an acceptable excuse for refusing to commit later), and checking for any con\u2010\nflicts or constraint violations. By replying \u201cyes\u201d to the coordinator, the node\npromises to commit the transaction without error if requested. In other words,\nthe participant surrenders the right to abort the transaction, but without actually\ncommitting it.\n5. When the coordinator has received responses to all prepare requests, it makes a\ndefinitive decision on whether to commit or abort the transaction (committing\nonly if all participants voted \u201cyes\u201d). The coordinator must write that decision to\nits transaction log on disk so that it knows which way it decided in case it subse\u2010\nquently crashes. This is called the commit point.\n6. Once the coordinator\u2019s decision has been written to disk, the commit or abort\nrequest is sent to all participants. If this request fails or times out, the coordinator\nmust retry forever until it succeeds. There is no more going back: if the decision\nwas to commit, that decision must be enforced, no matter how many retries it\ntakes. If a participant has crashed in the meantime, the transaction will be com\u2010\nDistributed Transactions and Consensus | 357", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "646b4931-b8e6-41af-9fa4-a865e5db6bab": {"__data__": {"id_": "646b4931-b8e6-41af-9fa4-a865e5db6bab", "embedding": null, "metadata": {"page_label": "358", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2900c8c7-cf10-455b-9d3a-3370dad43dac", "node_type": "4", "metadata": {"page_label": "358", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "806779d4377ea7e216ea563d48ad13a37b9b72536da1f029e5df5b3593e854c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mitted when it recovers\u2014since the participant voted \u201cyes,\u201d it cannot refuse to\ncommit when it recovers.\nThus, the protocol contains two crucial \u201cpoints of no return\u201d: when a participant\nvotes \u201cyes,\u201d it promises that it will definitely be able to commit later (although the\ncoordinator may still choose to abort); and once the coordinator decides, that deci\u2010\nsion is irrevocable. Those promises ensure the atomicity of 2PC. (Single-node atomic\ncommit lumps these two events into one: writing the commit record to the transac\u2010\ntion log.)\nReturning to the marriage analogy, before saying \u201cI do,\u201d you and your bride/groom\nhave the freedom to abort the transaction by saying \u201cNo way!\u201d (or something to that\neffect). However, after saying \u201cI do,\u201d you cannot retract that statement. If you faint\nafter saying \u201cI do\u201d and you don\u2019t hear the minister speak the words \u201cYou are now\nhusband and wife,\u201d that doesn\u2019t change the fact that the transaction was committed.\nWhen you recover consciousness later, you can find out whether you are married or\nnot by querying the minister for the status of your global transaction ID, or you can\nwait for the minister\u2019s next retry of the commit request (since the retries will have\ncontinued throughout your period of unconsciousness).\nCoordinator failure\nWe have discussed what happens if one of the participants or the network fails during\n2PC: if any of the prepare requests fail or time out, the coordinator aborts the trans\u2010\naction; if any of the commit or abort requests fail, the coordinator retries them indefi\u2010\nnitely. However, it is less clear what happens if the coordinator crashes.\nIf the coordinator fails before sending the prepare requests, a participant can safely\nabort the transaction. But once the participant has received a prepare request and\nvoted \u201cyes,\u201d it can no longer abort unilaterally\u2014it must wait to hear back from the\ncoordinator whether the transaction was committed or aborted. If the coordinator\ncrashes or the network fails at this point, the participant can do nothing but wait. A\nparticipant\u2019s transaction in this state is called in doubt or uncertain.\nThe situation is illustrated in Figure 9-10. In this particular example, the coordinator\nactually decided to commit, and database 2 received the commit request. However,\nthe coordinator crashed before it could send the commit request to database 1, and so\ndatabase 1 does not know whether to commit or abort. Even a timeout does not help\nhere: if database 1 unilaterally aborts after a timeout, it will end up inconsistent with\ndatabase 2, which has committed. Similarly, it is not safe to unilaterally commit,\nbecause another participant may have aborted.\n358 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ff5d943-cefc-4ee9-947e-b1b9299382a9": {"__data__": {"id_": "3ff5d943-cefc-4ee9-947e-b1b9299382a9", "embedding": null, "metadata": {"page_label": "359", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a96b9b3-1198-446e-b8d9-6255eebfb3be", "node_type": "4", "metadata": {"page_label": "359", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "84f110968a4e4f9b5f034c6b709a488fb97a05a46f2a5d11dba2cc5172c4d69c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 9-10. The coordinator crashes after participants vote \u201cyes.\u201d Database 1 does not\nknow whether to commit or abort.\nWithout hearing from the coordinator, the participant has no way of knowing\nwhether to commit or abort. In principle, the participants could communicate among\nthemselves to find out how each participant voted and come to some agreement, but\nthat is not part of the 2PC protocol.\nThe only way 2PC can complete is by waiting for the coordinator to recover. This is\nwhy the coordinator must write its commit or abort decision to a transaction log on\ndisk before sending commit or abort requests to participants: when the coordinator\nrecovers, it determines the status of all in-doubt transactions by reading its transac\u2010\ntion log. Any transactions that don\u2019t have a commit record in the coordinator\u2019s log\nare aborted. Thus, the commit point of 2PC comes down to a regular single-node\natomic commit on the coordinator. \nThree-phase commit\nTwo-phase commit is called a blocking atomic commit protocol due to the fact that\n2PC can become stuck waiting for the coordinator to recover. In theory, it is possible\nto make an atomic commit protocol nonblocking, so that it does not get stuck if a\nnode fails. However, making this work in practice is not so straightforward.\nAs an alternative to 2PC, an algorithm called three-phase commit (3PC) has been pro\u2010\nposed [13, 80]. However, 3PC assumes a network with bounded delay and nodes with\nbounded response times; in most practical systems with unbounded network delay\nand process pauses (see Chapter 8), it cannot guarantee atomicity.\nIn general, nonblocking atomic commit requires a perfect failure detector  [67, 71]\u2014\ni.e., a reliable mechanism for telling whether a node has crashed or not. In a network\nwith unbounded delay a timeout is not a reliable failure detector, because a request\nmay time out due to a network problem even if no node has crashed. For this reason,\n2PC continues to be used, despite the known problem with coordinator failure. \nDistributed Transactions and Consensus | 359", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "741b1bd2-52d7-468b-980a-f9423a4e8741": {"__data__": {"id_": "741b1bd2-52d7-468b-980a-f9423a4e8741", "embedding": null, "metadata": {"page_label": "360", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97eb87f5-8826-4de8-93ba-962fba5e0936", "node_type": "4", "metadata": {"page_label": "360", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "322e969d0230b6ce0c299c3b4d200e21efcccbb8b413547e51eaa74e8a76a243", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Distributed Transactions in Practice\nDistributed transactions, especially those implemented with two-phase commit, have\na mixed reputation. On the one hand, they are seen as providing an important safety\nguarantee that would be hard to achieve otherwise; on the other hand, they are criti\u2010\ncized for causing operational problems, killing performance, and promising more\nthan they can deliver [ 81, 82, 83, 84]. Many cloud services choose not to implement\ndistributed transactions due to the operational problems they engender [85, 86].\nSome implementations of distributed transactions carry a heavy performance penalty\n\u2014for example, distributed transactions in MySQL are reported to be over 10 times\nslower than single-node transactions [ 87], so it is not surprising when people advise\nagainst using them. Much of the performance cost inherent in two-phase commit is\ndue to the additional disk forcing (fsync) that is required for crash recovery [88], and\nthe additional network round-trips.\nHowever, rather than dismissing distributed transactions outright, we should exam\u2010\nine them in some more detail, because there are important lessons to be learned from\nthem. To begin, we should be precise about what we mean by \u201cdistributed transac\u2010\ntions.\u201d Two quite different types of distributed transactions are often conflated:\nDatabase-internal distributed transactions\nSome distributed databases (i.e., databases that use replication and partitioning\nin their standard configuration) support internal transactions among the nodes\nof that database. For example, VoltDB and MySQL Cluster\u2019s NDB storage engine\nhave such internal transaction support. In this case, all the nodes participating in\nthe transaction are running the same database software.\nHeterogeneous distributed transactions\nIn a heterogeneous transaction, the participants are two or more different tech\u2010\nnologies: for example, two databases from different vendors, or even non-\ndatabase systems such as message brokers. A distributed transaction across these\nsystems must ensure atomic commit, even though the systems may be entirely\ndifferent under the hood.\nDatabase-internal transactions do not have to be compatible with any other system,\nso they can use any protocol and apply optimizations specific to that particular tech\u2010\nnology. For that reason, database-internal distributed transactions can often work\nquite well. On the other hand, transactions spanning heterogeneous technologies are\na lot more challenging.\nExactly-once message processing\nHeterogeneous distributed transactions allow diverse systems to be integrated in\npowerful ways. For example, a message from a message queue can be acknowledged\nas processed if and only if the database transaction for processing the message was\n360 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f280e60-6e62-4f83-9bac-8166db5aea09": {"__data__": {"id_": "7f280e60-6e62-4f83-9bac-8166db5aea09", "embedding": null, "metadata": {"page_label": "361", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af67df2a-d9d9-4735-8d02-691dbb378710", "node_type": "4", "metadata": {"page_label": "361", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8f8669c8bab9bc0ee50c2f1fa9a732489c2979fea6a24c848f7698f0de02aa47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "successfully committed. This is implemented by atomically committing the message\nacknowledgment and the database writes in a single transaction. With distributed\ntransaction support, this is possible, even if the message broker and the database are\ntwo unrelated technologies running on different machines.\nIf either the message delivery or the database transaction fails, both are aborted, and\nso the message broker may safely redeliver the message later. Thus, by atomically\ncommitting the message and the side effects of its processing, we can ensure that the\nmessage is effectively processed exactly once, even if it required a few retries before it\nsucceeded. The abort discards any side effects of the partially completed transaction.\nSuch a distributed transaction is only possible if all systems affected by the transac\u2010\ntion are able to use the same atomic commit protocol, however. For example, say a\nside effect of processing a message is to send an email, and the email server does not\nsupport two-phase commit: it could happen that the email is sent two or more times\nif message processing fails and is retried. But if all side effects of processing a message\nare rolled back on transaction abort, then the processing step can safely be retried as\nif nothing had happened.\nWe will return to the topic of exactly-once message processing in Chapter 11. Let\u2019s\nlook first at the atomic commit protocol that allows such heterogeneous distributed\ntransactions. \nXA transactions\nX/Open XA  (short for eXtended Architecture ) is a standard for implementing two-\nphase commit across heterogeneous technologies [ 76, 77]. It was introduced in 1991\nand has been widely implemented: XA is supported by many traditional relational\ndatabases (including PostgreSQL, MySQL, DB2, SQL Server, and Oracle) and mes\u2010\nsage brokers (including ActiveMQ, HornetQ, MSMQ, and IBM MQ).\nXA is not a network protocol\u2014it is merely a C API for interfacing with a transaction\ncoordinator. Bindings for this API exist in other languages; for example, in the world\nof Java EE applications, XA transactions are implemented using the Java Transaction\nAPI (JTA), which in turn is supported by many drivers for databases using Java Data\u2010\nbase Connectivity (JDBC) and drivers for message brokers using the Java Message\nService (JMS) APIs.\nXA assumes that your application uses a network driver or client library to commu\u2010\nnicate with the participant databases or messaging services. If the driver supports XA,\nthat means it calls the XA API to find out whether an operation should be part of a\ndistributed transaction\u2014and if so, it sends the necessary information to the database\nserver. The driver also exposes callbacks through which the coordinator can ask the\nparticipant to prepare, commit, or abort.\nDistributed Transactions and Consensus | 361", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8cbc429-ae87-408c-8c1f-447823b54de9": {"__data__": {"id_": "f8cbc429-ae87-408c-8c1f-447823b54de9", "embedding": null, "metadata": {"page_label": "362", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0921d14d-082c-4af2-b132-e553b872873e", "node_type": "4", "metadata": {"page_label": "362", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3a4cd84623b7e877bb37397cd77fa62ac36f6cec3441bc2011a655ff4a27f113", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The transaction coordinator implements the XA API. The standard does not specify\nhow it should be implemented, but in practice the coordinator is often simply a\nlibrary that is loaded into the same process as the application issuing the transaction\n(not a separate service). It keeps track of the participants in a transaction, collects\npartipants\u2019 responses after asking them to prepare (via a callback into the driver), and\nuses a log on the local disk to keep track of the commit/abort decision for each trans\u2010\naction.\nIf the application process crashes, or the machine on which the application is running\ndies, the coordinator goes with it. Any participants with prepared but uncommitted\ntransactions are then stuck in doubt. Since the coordinator\u2019s log is on the application\nserver\u2019s local disk, that server must be restarted, and the coordinator library must\nread the log to recover the commit/abort outcome of each transaction. Only then can\nthe coordinator use the database driver\u2019s XA callbacks to ask participants to commit\nor abort, as appropriate. The database server cannot contact the coordinator directly,\nsince all communication must go via its client library.\nHolding locks while in doubt\nWhy do we care so much about a transaction being stuck in doubt? Can\u2019t the rest of\nthe system just get on with its work, and ignore the in-doubt transaction that will be\ncleaned up eventually?\nThe problem is with locking. As discussed in \u201cRead Committed\u201d on page 234, data\u2010\nbase transactions usually take a row-level exclusive lock on any rows they modify, to\nprevent dirty writes. In addition, if you want serializable isolation, a database using\ntwo-phase locking would also have to take a shared lock on any rows read by the\ntransaction (see \u201cTwo-Phase Locking (2PL)\u201d on page 257).\nThe database cannot release those locks until the transaction commits or aborts\n(illustrated as a shaded area in Figure 9-9). Therefore, when using two-phase commit,\na transaction must hold onto the locks throughout the time it is in doubt. If the coor\u2010\ndinator has crashed and takes 20 minutes to start up again, those locks will be held\nfor 20 minutes. If the coordinator\u2019s log is entirely lost for some reason, those locks\nwill be held forever\u2014or at least until the situation is manually resolved by an admin\u2010\nistrator.\nWhile those locks are held, no other transaction can modify those rows. Depending\non the database, other transactions may even be blocked from reading those rows.\nThus, other transactions cannot simply continue with their business\u2014if they want to\naccess that same data, they will be blocked. This can cause large parts of your applica\u2010\ntion to become unavailable until the in-doubt transaction is resolved.\n362 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d08237d-c0a9-41ee-97c3-24aff7f0e0ba": {"__data__": {"id_": "7d08237d-c0a9-41ee-97c3-24aff7f0e0ba", "embedding": null, "metadata": {"page_label": "363", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae0da04c-ccf6-4d95-9801-a037af371b77", "node_type": "4", "metadata": {"page_label": "363", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ffa8a6590a3f8d07d5b6232b87bf3d2dbc0a2377a2513139c2a0751dda4d5ba2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recovering from coordinator failure\nIn theory, if the coordinator crashes and is restarted, it should cleanly recover its state\nfrom the log and resolve any in-doubt transactions. However, in practice, orphaned\nin-doubt transactions do occur [89, 90]\u2014that is, transactions for which the coordina\u2010\ntor cannot decide the outcome for whatever reason (e.g., because the transaction log\nhas been lost or corrupted due to a software bug). These transactions cannot be\nresolved automatically, so they sit forever in the database, holding locks and blocking\nother transactions.\nEven rebooting your database servers will not fix this problem, since a correct imple\u2010\nmentation of 2PC must preserve the locks of an in-doubt transaction even across\nrestarts (otherwise it would risk violating the atomicity guarantee). It\u2019s a sticky\nsituation.\nThe only way out is for an administrator to manually decide whether to commit or\nroll back the transactions. The administrator must examine the participants of each\nin-doubt transaction, determine whether any participant has committed or aborted\nalready, and then apply the same outcome to the other participants. Resolving the\nproblem potentially requires a lot of manual effort, and most likely needs to be done\nunder high stress and time pressure during a serious production outage (otherwise,\nwhy would the coordinator be in such a bad state?).\nMany XA implementations have an emergency escape hatch called heuristic decisions:\nallowing a participant to unilaterally decide to abort or commit an in-doubt transac\u2010\ntion without a definitive decision from the coordinator [ 76, 77, 91]. To be clear, heu\u2010\nristic here is a euphemism for probably breaking atomicity, since it violates the system\nof promises in two-phase commit. Thus, heuristic decisions are intended only for\ngetting out of catastrophic situations, and not for regular use.\nLimitations of distributed transactions\nXA transactions solve the real and important problem of keeping several participant\ndata systems consistent with each other, but as we have seen, they also introduce\nmajor operational problems. In particular, the key realization is that the transaction\ncoordinator is itself a kind of database (in which transaction outcomes are stored),\nand so it needs to be approached with the same care as any other important database:\n\u2022 If the coordinator is not replicated but runs only on a single machine, it is a sin\u2010\ngle point of failure for the entire system (since its failure causes other application\nservers to block on locks held by in-doubt transactions). Surprisingly, many\ncoordinator implementations are not highly available by default, or have only\nrudimentary replication support.\n\u2022 Many server-side applications are developed in a stateless model (as favored by\nHTTP), with all persistent state stored in a database, which has the advantage\nDistributed Transactions and Consensus | 363", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9bdd4460-fcef-4e78-8b98-6c06985ba9d5": {"__data__": {"id_": "9bdd4460-fcef-4e78-8b98-6c06985ba9d5", "embedding": null, "metadata": {"page_label": "364", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df11fac4-a4f4-4a88-80c5-b88544cd60b5", "node_type": "4", "metadata": {"page_label": "364", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "550034f1b8ff38e13ae5036dee816fae000ca635318b3e288364fb4e56eb7c7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "that application servers can be added and removed at will. However, when the\ncoordinator is part of the application server, it changes the nature of the deploy\u2010\nment. Suddenly, the coordinator\u2019s logs become a crucial part of the durable sys\u2010\ntem state\u2014as important as the databases themselves, since the coordinator logs\nare required in order to recover in-doubt transactions after a crash. Such applica\u2010\ntion servers are no longer stateless.\n\u2022 Since XA needs to be compatible with a wide range of data systems, it is necessar\u2010\nily a lowest common denominator. For example, it cannot detect deadlocks\nacross different systems (since that would require a standardized protocol for\nsystems to exchange information on the locks that each transaction is waiting\nfor), and it does not work with SSI (see \u201cSerializable Snapshot Isolation (SSI)\u201d on\npage 261), since that would require a protocol for identifying conflicts across dif\u2010\nferent systems.\n\u2022 For database-internal distributed transactions (not XA), the limitations are not\nso great\u2014for example, a distributed version of SSI is possible. However, there\nremains the problem that for 2PC to successfully commit a transaction, all par\u2010\nticipants must respond. Consequently, if any part of the system is broken, the\ntransaction also fails. Distributed transactions thus have a tendency of amplifying\nfailures, which runs counter to our goal of building fault-tolerant systems.\nDo these facts mean we should give up all hope of keeping several systems consistent\nwith each other? Not quite\u2014there are alternative methods that allow us to achieve\nthe same thing without the pain of heterogeneous distributed transactions. We will\nreturn to these in Chapters 11 and 12. But first, we should wrap up the topic of\nconsensus. \nFault-Tolerant Consensus\nInformally, consensus means getting several nodes to agree on something. For exam\u2010\nple, if several people concurrently try to book the last seat on an airplane, or the same\nseat in a theater, or try to register an account with the same username, then a consen\u2010\nsus algorithm could be used to determine which one of these mutually incompatible\noperations should be the winner.\nThe consensus problem is normally formalized as follows: one or more nodes may\npropose values, and the consensus algorithm decides on one of those values. In the\nseat-booking example, when several customers are concurrently trying to buy the last\nseat, each node handling a customer request may propose the ID of the customer it is\nserving, and the decision indicates which one of those customers got the seat.\n364 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a8006abb-fa9a-4244-bb3a-02f8f54f1a8d": {"__data__": {"id_": "a8006abb-fa9a-4244-bb3a-02f8f54f1a8d", "embedding": null, "metadata": {"page_label": "365", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "addf1f2d-18a5-486d-b004-995e5ed621ce", "node_type": "4", "metadata": {"page_label": "365", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2f09426bcc395036e9abcb0d38562702760be6925cce28927096f3416bd1b4d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "xiii. This particular variant of consensus is called uniform consensus, which is equivalent to regular consensus\nin asynchronous systems with unreliable failure detectors [71]. The academic literature usually refers to pro\u2010\ncesses rather than nodes, but we use nodes here for consistency with the rest of this book.\nIn this formalism, a consensus algorithm must satisfy the following properties [25]:xiii\nUniform agreement\nNo two nodes decide differently.\nIntegrity\nNo node decides twice.\nValidity\nIf a node decides value v, then v was proposed by some node.\nTermination\nEvery node that does not crash eventually decides some value.\nThe uniform agreement and integrity properties define the core idea of consensus:\neveryone decides on the same outcome, and once you have decided, you cannot\nchange your mind. The validity property exists mostly to rule out trivial solutions: for\nexample, you could have an algorithm that always decides null, no matter what was\nproposed; this algorithm would satisfy the agreement and integrity properties, but\nnot the validity property.\nIf you don\u2019t care about fault tolerance, then satisfying the first three properties is\neasy: you can just hardcode one node to be the \u201cdictator,\u201d and let that node make all\nof the decisions. However, if that one node fails, then the system can no longer make\nany decisions. This is, in fact, what we saw in the case of two-phase commit: if the\ncoordinator fails, in-doubt participants cannot decide whether to commit or abort.\nThe termination property formalizes the idea of fault tolerance. It essentially says that\na consensus algorithm cannot simply sit around and do nothing forever\u2014in other\nwords, it must make progress. Even if some nodes fail, the other nodes must still\nreach a decision. (Termination is a liveness property, whereas the other three are\nsafety properties\u2014see \u201cSafety and liveness\u201d on page 308.)\nThe system model of consensus assumes that when a node \u201ccrashes,\u201d it suddenly dis\u2010\nappears and never comes back. (Instead of a software crash, imagine that there is an\nearthquake, and the datacenter containing your node is destroyed by a landslide. You\nmust assume that your node is buried under 30 feet of mud and is never going to\ncome back online.) In this system model, any algorithm that has to wait for a node to\nrecover is not going to be able to satisfy the termination property. In particular, 2PC\ndoes not meet the requirements for termination.\nDistributed Transactions and Consensus | 365", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5237fc7f-0bcd-4349-82da-3c48080a3796": {"__data__": {"id_": "5237fc7f-0bcd-4349-82da-3c48080a3796", "embedding": null, "metadata": {"page_label": "366", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c6532e6-85bb-49fb-9e38-c7e180fae61a", "node_type": "4", "metadata": {"page_label": "366", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f2ff778d5db007ce4c87b1d87bc4895566e90ae8d8d13e9fe5cc15aaa1dd5f0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Of course, if all nodes crash and none of them are running, then it is not possible for\nany algorithm to decide anything. There is a limit to the number of failures that an\nalgorithm can tolerate: in fact, it can be proved that any consensus algorithm requires\nat least a majority of nodes to be functioning correctly in order to assure termination\n[67]. That majority can safely form a quorum (see \u201cQuorums for reading and writ\u2010\ning\u201d on page 179).\nThus, the termination property is subject to the assumption that fewer than half of\nthe nodes are crashed or unreachable. However, most implementations of consensus\nensure that the safety properties\u2014agreement, integrity, and validity\u2014are always met,\neven if a majority of nodes fail or there is a severe network problem [ 92]. Thus, a\nlarge-scale outage can stop the system from being able to process requests, but it can\u2010\nnot corrupt the consensus system by causing it to make invalid decisions.\nMost consensus algorithms assume that there are no Byzantine faults, as discussed in\n\u201cByzantine Faults\u201d on page 304. That is, if a node does not correctly follow the proto\u2010\ncol (for example, if it sends contradictory messages to different nodes), it may break\nthe safety properties of the protocol. It is possible to make consensus robust against\nByzantine faults as long as fewer than one-third of the nodes are Byzantine-faulty [25,\n93], but we don\u2019t have space to discuss those algorithms in detail in this book.\nConsensus algorithms and total order broadcast\nThe best-known fault-tolerant consensus algorithms are Viewstamped Replication\n(VSR) [94, 95], Paxos [96, 97, 98, 99], Raft [22, 100, 101], and Zab [15, 21, 102]. There\nare quite a few similarities between these algorithms, but they are not the same [ 103].\nIn this book we won\u2019t go into full details of the different algorithms: it\u2019s sufficient to\nbe aware of some of the high-level ideas that they have in common, unless you\u2019re\nimplementing a consensus system yourself (which is probably not advisable\u2014it\u2019s\nhard [98, 104]).\nMost of these algorithms actually don\u2019t directly use the formal model described here\n(proposing and deciding on a single value, while satisfying the agreement, integrity,\nvalidity, and termination properties). Instead, they decide on a sequence of values,\nwhich makes them total order broadcast  algorithms, as discussed previously in this\nchapter (see \u201cTotal Order Broadcast\u201d on page 348).\nRemember that total order broadcast requires messages to be delivered exactly once,\nin the same order, to all nodes. If you think about it, this is equivalent to performing\nseveral rounds of consensus: in each round, nodes propose the message that they\nwant to send next, and then decide on the next message to be delivered in the total\norder [67].\nSo, total order broadcast is equivalent to repeated rounds of consensus (each consen\u2010\nsus decision corresponding to one message delivery):\n366 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b29f464c-64fe-4b0d-8776-724b1858c4ef": {"__data__": {"id_": "b29f464c-64fe-4b0d-8776-724b1858c4ef", "embedding": null, "metadata": {"page_label": "367", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45f61d53-fdd2-45e0-b68b-43316ab82c64", "node_type": "4", "metadata": {"page_label": "367", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2bb10ba37016490a1f9753f5a3e75c5774f21358ddaa530eea2cf0fc7195c6b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Due to the agreement property of consensus, all nodes decide to deliver the same\nmessages in the same order.\n\u2022 Due to the integrity property, messages are not duplicated.\n\u2022 Due to the validity property, messages are not corrupted and not fabricated out\nof thin air.\n\u2022 Due to the termination property, messages are not lost.\nViewstamped Replication, Raft, and Zab implement total order broadcast directly,\nbecause that is more efficient than doing repeated rounds of one-value-at-a-time\nconsensus. In the case of Paxos, this optimization is known as Multi-Paxos.\nSingle-leader replication and consensus\nIn Chapter 5 we discussed single-leader replication (see \u201cLeaders and Followers\u201d on\npage 152), which takes all the writes to the leader and applies them to the followers in\nthe same order, thus keeping replicas up to date. Isn\u2019t this essentially total order\nbroadcast? How come we didn\u2019t have to worry about consensus in Chapter 5?\nThe answer comes down to how the leader is chosen. If the leader is manually chosen\nand configured by the humans in your operations team, you essentially have a \u201ccon\u2010\nsensus algorithm\u201d of the dictatorial variety: only one node is allowed to accept writes\n(i.e., make decisions about the order of writes in the replication log), and if that node\ngoes down, the system becomes unavailable for writes until the operators manually\nconfigure a different node to be the leader. Such a system can work well in practice,\nbut it does not satisfy the termination property of consensus because it requires\nhuman intervention in order to make progress.\nSome databases perform automatic leader election and failover, promoting a follower\nto be the new leader if the old leader fails (see \u201cHandling Node Outages\u201d  on page\n156). This brings us closer to fault-tolerant total order broadcast, and thus to solving\nconsensus.\nHowever, there is a problem. We previously discussed the problem of split brain, and\nsaid that all nodes need to agree who the leader is\u2014otherwise two different nodes\ncould each believe themselves to be the leader, and consequently get the database into\nan inconsistent state. Thus, we need consensus in order to elect a leader. But if the\nconsensus algorithms described here are actually total order broadcast algorithms,\nand total order broadcast is like single-leader replication, and single-leader replica\u2010\ntion requires a leader, then\u2026\nIt seems that in order to elect a leader, we first need a leader. In order to solve con\u2010\nsensus, we must first solve consensus. How do we break out of this conundrum?\nDistributed Transactions and Consensus | 367", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7381cfad-2e9d-4f55-a90d-73941c0dcd75": {"__data__": {"id_": "7381cfad-2e9d-4f55-a90d-73941c0dcd75", "embedding": null, "metadata": {"page_label": "368", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bea78591-7b96-4433-9d19-b7fe32c97193", "node_type": "4", "metadata": {"page_label": "368", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c87a17a5b3c9edd33a3806e4fe520386e04afe7d8e698e1325250aca708e2367", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Epoch numbering and quorums\nAll of the consensus protocols discussed so far internally use a leader in some form or\nanother, but they don\u2019t guarantee that the leader is unique. Instead, they can make a\nweaker guarantee: the protocols define an epoch number (called the ballot number in\nPaxos, view number  in Viewstamped Replication, and term number  in Raft) and\nguarantee that within each epoch, the leader is unique.\nEvery time the current leader is thought to be dead, a vote is started among the nodes\nto elect a new leader. This election is given an incremented epoch number, and thus\nepoch numbers are totally ordered and monotonically increasing. If there is a conflict\nbetween two different leaders in two different epochs (perhaps because the previous\nleader actually wasn\u2019t dead after all), then the leader with the higher epoch number\nprevails.\nBefore a leader is allowed to decide anything, it must first check that there isn\u2019t some\nother leader with a higher epoch number which might take a conflicting decision.\nHow does a leader know that it hasn\u2019t been ousted by another node? Recall \u201cThe\nTruth Is Defined by the Majority\u201d on page 300: a node cannot necessarily trust its\nown judgment\u2014just because a node thinks that it is the leader, that does not neces\u2010\nsarily mean the other nodes accept it as their leader.\nInstead, it must collect votes from a quorum of nodes (see \u201cQuorums for reading and\nwriting\u201d on page 179). For every decision that a leader wants to make, it must send\nthe proposed value to the other nodes and wait for a quorum of nodes to respond in\nfavor of the proposal. The quorum typically, but not always, consists of a majority of\nnodes [105]. A node votes in favor of a proposal only if it is not aware of any other\nleader with a higher epoch.\nThus, we have two rounds of voting: once to choose a leader, and a second time to\nvote on a leader\u2019s proposal. The key insight is that the quorums for those two votes\nmust overlap: if a vote on a proposal succeeds, at least one of the nodes that voted for\nit must have also participated in the most recent leader election [ 105]. Thus, if the\nvote on a proposal does not reveal any higher-numbered epoch, the current leader\ncan conclude that no leader election with a higher epoch number has happened, and\ntherefore be sure that it still holds the leadership. It can then safely decide the pro\u2010\nposed value.\nThis voting process looks superficially similar to two-phase commit. The biggest dif\u2010\nferences are that in 2PC the coordinator is not elected, and that fault-tolerant consen\u2010\nsus algorithms only require votes from a majority of nodes, whereas 2PC requires a\n\u201cyes\u201d vote from every participant. Moreover, consensus algorithms define a recovery\nprocess by which nodes can get into a consistent state after a new leader is elected,\nensuring that the safety properties are always met. These differences are key to the\ncorrectness and fault tolerance of a consensus algorithm. \n368 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d94f311a-1ee0-4696-b08a-86c6f63fab4b": {"__data__": {"id_": "d94f311a-1ee0-4696-b08a-86c6f63fab4b", "embedding": null, "metadata": {"page_label": "369", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c855298-579f-4f17-b6be-3cf70cd81aa5", "node_type": "4", "metadata": {"page_label": "369", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "109b77ad962905cb796175fa1cf013ce8a7223ad7c353dfe3211f4ed7fccd159", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Limitations of consensus\nConsensus algorithms are a huge breakthrough for distributed systems: they bring\nconcrete safety properties (agreement, integrity, and validity) to systems where every\u2010\nthing else is uncertain, and they nevertheless remain fault-tolerant (able to make pro\u2010\ngress as long as a majority of nodes are working and reachable). They provide total\norder broadcast, and therefore they can also implement linearizable atomic opera\u2010\ntions in a fault-tolerant way (see \u201cImplementing linearizable storage using total order\nbroadcast\u201d on page 350).\nNevertheless, they are not used everywhere, because the benefits come at a cost.\nThe process by which nodes vote on proposals before they are decided is a kind of\nsynchronous replication. As discussed in \u201cSynchronous Versus Asynchronous Repli\u2010\ncation\u201d on page 153, databases are often configured to use asynchronous replication.\nIn this configuration, some committed data can potentially be lost on failover\u2014but\nmany people choose to accept this risk for the sake of better performance.\nConsensus systems always require a strict majority to operate. This means you need a\nminimum of three nodes in order to tolerate one failure (the remaining two out of\nthree form a majority), or a minimum of five nodes to tolerate two failures (the\nremaining three out of five form a majority). If a network failure cuts off some nodes\nfrom the rest, only the majority portion of the network can make progress, and the\nrest is blocked (see also \u201cThe Cost of Linearizability\u201d on page 335).\nMost consensus algorithms assume a fixed set of nodes that participate in voting,\nwhich means that you can\u2019t just add or remove nodes in the cluster. Dynamic mem\u2010\nbership extensions to consensus algorithms allow the set of nodes in the cluster to\nchange over time, but they are much less well understood than static membership\nalgorithms.\nConsensus systems generally rely on timeouts to detect failed nodes. In environments\nwith highly variable network delays, especially geographically distributed systems, it\noften happens that a node falsely believes the leader to have failed due to a transient\nnetwork issue. Although this error does not harm the safety properties, frequent\nleader elections result in terrible performance because the system can end up spend\u2010\ning more time choosing a leader than doing any useful work.\nSometimes, consensus algorithms are particularly sensitive to network problems. For\nexample, Raft has been shown to have unpleasant edge cases [ 106]: if the entire net\u2010\nwork is working correctly except for one particular network link that is consistently\nunreliable, Raft can get into situations where leadership continually bounces between\ntwo nodes, or the current leader is continually forced to resign, so the system effec\u2010\ntively never makes progress. Other consensus algorithms have similar problems, and\ndesigning algorithms that are more robust to unreliable networks is still an open\nresearch problem. \nDistributed Transactions and Consensus | 369", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab66a745-7c35-493d-afa7-e4ba7e39a67c": {"__data__": {"id_": "ab66a745-7c35-493d-afa7-e4ba7e39a67c", "embedding": null, "metadata": {"page_label": "370", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54f93970-accf-4166-ae70-b619f6fab915", "node_type": "4", "metadata": {"page_label": "370", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5546c80f699b02c863a616088e7de279d0b570f962ebddeea578c4c63fd6079a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Membership and Coordination Services\nProjects like ZooKeeper or etcd are often described as \u201cdistributed key-value stores\u201d\nor \u201ccoordination and configuration services.\u201d The API of such a service looks pretty\nmuch like that of a database: you can read and write the value for a given key, and\niterate over keys. So if they\u2019re basically databases, why do they go to all the effort of\nimplementing a consensus algorithm? What makes them different from any other\nkind of database?\nTo understand this, it is helpful to briefly explore how a service like ZooKeeper is\nused. As an application developer, you will rarely need to use ZooKeeper directly,\nbecause it is actually not well suited as a general-purpose database. It is more likely\nthat you will end up relying on it indirectly via some other project: for example,\nHBase, Hadoop YARN, OpenStack Nova, and Kafka all rely on ZooKeeper running\nin the background. What is it that these projects get from it?\nZooKeeper and etcd are designed to hold small amounts of data that can fit entirely\nin memory (although they still write to disk for durability)\u2014so you wouldn\u2019t want to\nstore all of your application\u2019s data here. That small amount of data is replicated\nacross all the nodes using a fault-tolerant total order broadcast algorithm. As dis\u2010\ncussed previously, total order broadcast is just what you need for database replica\u2010\ntion: if each message represents a write to the database, applying the same writes in\nthe same order keeps replicas consistent with each other.\nZooKeeper is modeled after Google\u2019s Chubby lock service [ 14, 98], implementing not\nonly total order broadcast (and hence consensus), but also an interesting set of other\nfeatures that turn out to be particularly useful when building distributed systems:\nLinearizable atomic operations\nUsing an atomic compare-and-set operation, you can implement a lock: if several\nnodes concurrently try to perform the same operation, only one of them will suc\u2010\nceed. The consensus protocol guarantees that the operation will be atomic and\nlinearizable, even if a node fails or the network is interrupted at any point. A dis\u2010\ntributed lock is usually implemented as a lease, which has an expiry time so that\nit is eventually released in case the client fails (see \u201cProcess Pauses\u201d on page 295).\nTotal ordering of operations\nAs discussed in \u201cThe leader and the lock\u201d on page 301, when some resource is\nprotected by a lock or lease, you need a fencing token to prevent clients from con\u2010\nflicting with each other in the case of a process pause. The fencing token is some\nnumber that monotonically increases every time the lock is acquired. ZooKeeper\nprovides this by totally ordering all operations and giving each operation a\nmonotonically increasing transaction ID ( zxid) and version number ( cversion)\n[15].\n370 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd94e5c3-da99-43fd-bcbe-8c84bcf690fd": {"__data__": {"id_": "fd94e5c3-da99-43fd-bcbe-8c84bcf690fd", "embedding": null, "metadata": {"page_label": "371", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adf2fa83-68e0-4d59-a937-052769668a2b", "node_type": "4", "metadata": {"page_label": "371", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "aa6650b8419b8aa346d1345261549ff786056ff2eaf6c903ab6fb7f4b9005d1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Failure detection\nClients maintain a long-lived session on ZooKeeper servers, and the client and\nserver periodically exchange heartbeats to check that the other node is still alive.\nEven if the connection is temporarily interrupted, or a ZooKeeper node fails, the\nsession remains active. However, if the heartbeats cease for a duration that is\nlonger than the session timeout, ZooKeeper declares the session to be dead. Any\nlocks held by a session can be configured to be automatically released when the\nsession times out (ZooKeeper calls these ephemeral nodes).\nChange notifications\nNot only can one client read locks and values that were created by another client,\nbut it can also watch them for changes. Thus, a client can find out when another\nclient joins the cluster (based on the value it writes to ZooKeeper), or if another\nclient fails (because its session times out and its ephemeral nodes disappear). By\nsubscribing to notifications, a client avoids having to frequently poll to find out\nabout changes.\nOf these features, only the linearizable atomic operations really require consensus.\nHowever, it is the combination of these features that makes systems like ZooKeeper\nso useful for distributed coordination.\nAllocating work to nodes\nOne example in which the ZooKeeper/Chubby model works well is if you have sev\u2010\neral instances of a process or service, and one of them needs to be chosen as leader or\nprimary. If the leader fails, one of the other nodes should take over. This is of course\nuseful for single-leader databases, but it\u2019s also useful for job schedulers and similar\nstateful systems.\nAnother example arises when you have some partitioned resource (database, message\nstreams, file storage, distributed actor system, etc.) and need to decide which parti\u2010\ntion to assign to which node. As new nodes join the cluster, some of the partitions\nneed to be moved from existing nodes to the new nodes in order to rebalance the\nload (see \u201cRebalancing Partitions\u201d on page 209). As nodes are removed or fail, other\nnodes need to take over the failed nodes\u2019 work.\nThese kinds of tasks can be achieved by judicious use of atomic operations, ephem\u2010\neral nodes, and notifications in ZooKeeper. If done correctly, this approach allows\nthe application to automatically recover from faults without human intervention. It\u2019s\nnot easy, despite the appearance of libraries such as Apache Curator [ 17] that have\nsprung up to provide higher-level tools on top of the ZooKeeper client API\u2014but it is\nstill much better than attempting to implement the necessary consensus algorithms\nfrom scratch, which has a poor success record [107].\nDistributed Transactions and Consensus | 371", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c2671cf-f32b-42ad-8070-c7682a496ffa": {"__data__": {"id_": "6c2671cf-f32b-42ad-8070-c7682a496ffa", "embedding": null, "metadata": {"page_label": "372", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dd88692-0b1e-45b3-88c4-0ccb4fbaeeae", "node_type": "4", "metadata": {"page_label": "372", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "abb018523e8e09468d2ae2f08aa9e1ba9954c5eb8ed615e52a74e083c94d05fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An application may initially run only on a single node, but eventually may grow to\nthousands of nodes. Trying to perform majority votes over so many nodes would be\nterribly inefficient. Instead, ZooKeeper runs on a fixed number of nodes (usually\nthree or five) and performs its majority votes among those nodes while supporting a\npotentially large number of clients. Thus, ZooKeeper provides a way of \u201coutsourcing\u201d\nsome of the work of coordinating nodes (consensus, operation ordering, and failure\ndetection) to an external service.\nNormally, the kind of data managed by ZooKeeper is quite slow-changing: it repre\u2010\nsents information like \u201cthe node running on 10.1.1.23 is the leader for partition 7,\u201d\nwhich may change on a timescale of minutes or hours. It is not intended for storing\nthe runtime state of the application, which may change thousands or even millions of\ntimes per second. If application state needs to be replicated from one node to\nanother, other tools (such as Apache BookKeeper [108]) can be used.\nService discovery\nZooKeeper, etcd, and Consul are also often used for service discovery\u2014that is, to find\nout which IP address you need to connect to in order to reach a particular service. In\ncloud datacenter environments, where it is common for virtual machines to continu\u2010\nally come and go, you often don\u2019t know the IP addresses of your services ahead of\ntime. Instead, you can configure your services such that when they start up they reg\u2010\nister their network endpoints in a service registry, where they can then be found by\nother services.\nHowever, it is less clear whether service discovery actually requires consensus. DNS is\nthe traditional way of looking up the IP address for a service name, and it uses multi\u2010\nple layers of caching to achieve good performance and availability. Reads from DNS\nare absolutely not linearizable, and it is usually not considered problematic if the\nresults from a DNS query are a little stale [109]. It is more important that DNS is reli\u2010\nably available and robust to network interruptions.\nAlthough service discovery does not require consensus, leader election does. Thus, if\nyour consensus system already knows who the leader is, then it can make sense to\nalso use that information to help other services discover who the leader is. For this\npurpose, some consensus systems support read-only caching replicas. These replicas\nasynchronously receive the log of all decisions of the consensus algorithm, but do not\nactively participate in voting. They are therefore able to serve read requests that do\nnot need to be linearizable.\nMembership services\nZooKeeper and friends can be seen as part of a long history of research into member\u2010\nship services , which goes back to the 1980s and has been important for building\nhighly reliable systems, e.g., for air traffic control [110].\n372 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40efb552-d122-4550-ad61-3f6fbeac35ae": {"__data__": {"id_": "40efb552-d122-4550-ad61-3f6fbeac35ae", "embedding": null, "metadata": {"page_label": "373", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e80d002-31b9-401d-aabe-ab84c20d52ab", "node_type": "4", "metadata": {"page_label": "373", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fbba6cf52d8b52e277932cd67a7cd1a47e60de46dea9869225f3e90c024a5d35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A membership service determines which nodes are currently active and live members\nof a cluster. As we saw throughout Chapter 8, due to unbounded network delays it\u2019s\nnot possible to reliably detect whether another node has failed. However, if you cou\u2010\nple failure detection with consensus, nodes can come to an agreement about which\nnodes should be considered alive or not.\nIt could still happen that a node is incorrectly declared dead by consensus, even\nthough it is actually alive. But it is nevertheless very useful for a system to have agree\u2010\nment on which nodes constitute the current membership. For example, choosing a\nleader could mean simply choosing the lowest-numbered among the current mem\u2010\nbers, but this approach would not work if different nodes have divergent opinions on\nwho the current members are. \nSummary\nIn this chapter we examined the topics of consistency and consensus from several dif\u2010\nferent angles. We looked in depth at linearizability, a popular consistency model: its\ngoal is to make replicated data appear as though there were only a single copy, and to\nmake all operations act on it atomically. Although linearizability is appealing because\nit is easy to understand\u2014it makes a database behave like a variable in a single-\nthreaded program\u2014it has the downside of being slow, especially in environments\nwith large network delays.\nWe also explored causality, which imposes an ordering on events in a system (what\nhappened before what, based on cause and effect). Unlike linearizability, which puts\nall operations in a single, totally ordered timeline, causality provides us with a weaker\nconsistency model: some things can be concurrent, so the version history is like a\ntimeline with branching and merging. Causal consistency does not have the coordi\u2010\nnation overhead of linearizability and is much less sensitive to network problems.\nHowever, even if we capture the causal ordering (for example using Lamport time\u2010\nstamps), we saw that some things cannot be implemented this way: in \u201cTimestamp\nordering is not sufficient\u201d on page 347 we considered the example of ensuring that a\nusername is unique and rejecting concurrent registrations for the same username. If\none node is going to accept a registration, it needs to somehow know that another\nnode isn\u2019t concurrently in the process of registering the same name. This problem led\nus toward consensus.\nWe saw that achieving consensus means deciding something in such a way that all\nnodes agree on what was decided, and such that the decision is irrevocable. With\nsome digging, it turns out that a wide range of problems are actually reducible to\nconsensus and are equivalent to each other (in the sense that if you have a solution\nfor one of them, you can easily transform it into a solution for one of the others).\nSuch equivalent problems include:\nSummary | 373", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2845, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40770dcc-da68-4882-b16d-8cd4915f1f37": {"__data__": {"id_": "40770dcc-da68-4882-b16d-8cd4915f1f37", "embedding": null, "metadata": {"page_label": "374", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a22269d0-f9ea-4172-a34e-41b479aacce3", "node_type": "4", "metadata": {"page_label": "374", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "99e6127878258ad1b30b2f0f52393070e0d6d94967267c51153bb66614cf890f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Linearizable compare-and-set registers\nThe register needs to atomically decide whether to set its value, based on whether\nits current value equals the parameter given in the operation.\nAtomic transaction commit\nA database must decide whether to commit or abort a distributed transaction.\nTotal order broadcast\nThe messaging system must decide on the order in which to deliver messages.\nLocks and leases\nWhen several clients are racing to grab a lock or lease, the lock decides which one\nsuccessfully acquired it.\nMembership/coordination service\nGiven a failure detector (e.g., timeouts), the system must decide which nodes are\nalive, and which should be considered dead because their sessions timed out.\nUniqueness constraint\nWhen several transactions concurrently try to create conflicting records with the\nsame key, the constraint must decide which one to allow and which should fail\nwith a constraint violation.\nAll of these are straightforward if you only have a single node, or if you are willing to\nassign the decision-making capability to a single node. This is what happens in a\nsingle-leader database: all the power to make decisions is vested in the leader, which\nis why such databases are able to provide linearizable operations, uniqueness con\u2010\nstraints, a totally ordered replication log, and more.\nHowever, if that single leader fails, or if a network interruption makes the leader\nunreachable, such a system becomes unable to make any progress. There are three\nways of handling that situation:\n1. Wait for the leader to recover, and accept that the system will be blocked in the\nmeantime. Many XA/JTA transaction coordinators choose this option. This\napproach does not fully solve consensus because it does not satisfy the termina\u2010\ntion property: if the leader does not recover, the system can be blocked forever.\n2. Manually fail over by getting humans to choose a new leader node and reconfig\u2010\nure the system to use it. Many relational databases take this approach. It is a kind\nof consensus by \u201cact of God\u201d\u2014the human operator, outside of the computer sys\u2010\ntem, makes the decision. The speed of failover is limited by the speed at which\nhumans can act, which is generally slower than computers.\n374 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1008ba1b-3c00-4d20-814b-9ff89e276f63": {"__data__": {"id_": "1008ba1b-3c00-4d20-814b-9ff89e276f63", "embedding": null, "metadata": {"page_label": "375", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4621f15-7b4d-4bd3-8711-6dbad76f7731", "node_type": "4", "metadata": {"page_label": "375", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "71d8e8bebf2decf6bcb9bc4672322bf947d96b0e207e22bffa0108e2152d3bfc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. Use an algorithm to automatically choose a new leader. This approach requires a\nconsensus algorithm, and it is advisable to use a proven algorithm that correctly\nhandles adverse network conditions [107].\nAlthough a single-leader database can provide linearizability without executing a\nconsensus algorithm on every write, it still requires consensus to maintain its leader\u2010\nship and for leadership changes. Thus, in some sense, having a leader only \u201ckicks the\ncan down the road\u201d: consensus is still required, only in a different place, and less fre\u2010\nquently. The good news is that fault-tolerant algorithms and systems for consensus\nexist, and we briefly discussed them in this chapter.\nTools like ZooKeeper play an important role in providing an \u201coutsourced\u201d consen\u2010\nsus, failure detection, and membership service that applications can use. It\u2019s not easy\nto use, but it is much better than trying to develop your own algorithms that can\nwithstand all the problems discussed in Chapter 8. If you find yourself wanting to do\none of those things that is reducible to consensus, and you want it to be fault-tolerant,\nthen it is advisable to use something like ZooKeeper.\nNevertheless, not every system necessarily requires consensus: for example, leaderless\nand multi-leader replication systems typically do not use global consensus. The con\u2010\nflicts that occur in these systems (see \u201cHandling Write Conflicts\u201d on page 171) are a\nconsequence of not having consensus across different leaders, but maybe that\u2019s okay:\nmaybe we simply need to cope without linearizability and learn to work better with\ndata that has branching and merging version histories.\nThis chapter referenced a large body of research on the theory of distributed systems.\nAlthough the theoretical papers and proofs are not always easy to understand, and\nsometimes make unrealistic assumptions, they are incredibly valuable for informing\npractical work in this field: they help us reason about what can and cannot be done,\nand help us find the counterintuitive ways in which distributed systems are often\nflawed. If you have the time, the references are well worth exploring. \nThis brings us to the end of Part II  of this book, in which we covered replication\n(Chapter 5 ), partitioning ( Chapter 6 ), transactions ( Chapter 7 ), distributed system\nfailure models ( Chapter 8), and finally consistency and consensus ( Chapter 9). Now\nthat we have laid a firm foundation of theory, in Part III we will turn once again to\nmore practical systems, and discuss how to build powerful applications from hetero\u2010\ngeneous building blocks.\nReferences\n[1] Peter Bailis and Ali Ghodsi: \u201c Eventual Consistency Today: Limitations, Exten\u2010\nsions, and Beyond ,\u201d ACM Queue , volume 11, number 3, pages 55-63, March 2013.\ndoi:10.1145/2460276.2462076\nSummary | 375", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed553707-91e6-439a-8b34-1704a58f40b9": {"__data__": {"id_": "ed553707-91e6-439a-8b34-1704a58f40b9", "embedding": null, "metadata": {"page_label": "376", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8d59f6e-509c-48ef-8805-84c3cfa94d23", "node_type": "4", "metadata": {"page_label": "376", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b55c468da0e779cb85f408e6164019e2720527acb664ebe21464499fd964b028", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] Prince Mahajan, Lorenzo Alvisi, and Mike Dahlin: \u201c Consistency, Availability, and\nConvergence,\u201d University of Texas at Austin, Department of Computer Science, Tech\nReport UTCS TR-11-22, May 2011.\n[3] Alex Scotti: \u201c Adventures in Building Your Own Database ,\u201d at All Your Base ,\nNovember 2015.\n[4] Peter Bailis, Aaron Davidson, Alan Fekete, et al.: \u201c Highly Available Transactions:\nVirtues and Limitations,\u201d at 40th International Conference on Very Large Data Bases\n(VLDB), September 2014. Extended version published as pre-print arXiv:1302.0309\n[cs.DB].\n[5] Paolo Viotti and Marko Vukoli\u0107: \u201c Consistency in Non-Transactional Distributed\nStorage Systems,\u201d arXiv:1512.00168, 12 April 2016.\n[6] Maurice P. Herlihy and Jeannette M. Wing: \u201c Linearizability: A Correctness Con\u2010\ndition for Concurrent Objects ,\u201d ACM Transactions on Programming Languages and\nSystems (TOPLAS), volume 12, number 3, pages 463\u2013492, July 1990. doi:\n10.1145/78969.78972\n[7] Leslie Lamport: \u201c On interprocess communication ,\u201d Distributed Computing, vol\u2010\nume 1, number 2, pages 77\u2013101, June 1986. doi:10.1007/BF01786228\n[8] David K. Gifford: \u201c Information Storage in a Decentralized Computer System ,\u201d\nXerox Palo Alto Research Centers, CSL-81-8, June 1981.\n[9] Martin Kleppmann: \u201c Please Stop Calling Databases CP or AP ,\u201d martin.klepp\u2010\nmann.com, May 11, 2015.\n[10] Kyle Kingsbury: \u201c Call Me Maybe: MongoDB Stale Reads ,\u201d aphyr.com, April 20,\n2015.\n[11] Kyle Kingsbury: \u201c Computational Techniques in Knossos ,\u201d aphyr.com, May 17,\n2014.\n[12] Peter Bailis: \u201c Linearizability Versus Serializability ,\u201d bailis.org, September 24,\n2014.\n[13] Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: Concurrency\nControl and Recovery in Database Systems . Addison-Wesley, 1987. ISBN:\n978-0-201-10715-9, available online at research.microsoft.com.\n[14] Mike Burrows: \u201cThe Chubby Lock Service for Loosely-Coupled Distributed Sys\u2010\ntems,\u201d at 7th USENIX Symposium on Operating System Design and Implementation\n(OSDI), November 2006.\n[15] Flavio P. Junqueira and Benjamin Reed: ZooKeeper: Distributed Process Coordi\u2010\nnation. O\u2019Reilly Media, 2013. ISBN: 978-1-449-36130-3\n[16] \u201cetcd 2.0.12 Documentation,\u201d CoreOS, Inc., 2015.\n376 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b19bf71-d287-4428-bc19-debdbe537176": {"__data__": {"id_": "1b19bf71-d287-4428-bc19-debdbe537176", "embedding": null, "metadata": {"page_label": "377", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80e080f4-3428-4c42-ba30-5a7c9d710361", "node_type": "4", "metadata": {"page_label": "377", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "797a7c91add69eadb06320c0d208694de3a6f71f56439af82aa381ba841e1278", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[17] \u201cApache Curator,\u201d Apache Software Foundation, curator.apache.org, 2015.\n[18] Morali Vallath: Oracle 10g RAC Grid, Services & Clustering . Elsevier Digital\nPress, 2006. ISBN: 978-1-555-58321-7\n[19] Peter Bailis, Alan Fekete, Michael J Franklin, et al.: \u201c Coordination-Avoiding\nDatabase Systems,\u201d Proceedings of the VLDB Endowment , volume 8, number 3, pages\n185\u2013196, November 2014.\n[20] Kyle Kingsbury: \u201cCall Me Maybe: etcd and Consul,\u201d aphyr.com, June 9, 2014.\n[21] Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini: \u201c Zab: High-\nPerformance Broadcast for Primary-Backup Systems ,\u201d at 41st IEEE International\nConference on Dependable Systems and Networks  (DSN), June 2011. doi:10.1109/\nDSN.2011.5958223\n[22] Diego Ongaro and John K. Ousterhout: \u201c In Search of an Understandable Con\u2010\nsensus Algorithm (Extended Version) ,\u201d at USENIX Annual Technical Conference\n(ATC), June 2014.\n[23] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev: \u201c Sharing Memory Robustly in\nMessage-Passing Systems ,\u201d Journal of the ACM , volume 42, number 1, pages 124\u2013\n142, January 1995. doi:10.1145/200836.200869\n[24] Nancy Lynch and Alex Shvartsman: \u201c Robust Emulation of Shared Memory\nUsing Dynamic Quorum-Acknowledged Broadcasts ,\u201d at 27th Annual International\nSymposium on Fault-Tolerant Computing  (FTCS), June 1997. doi:10.1109/FTCS.\n1997.614100\n[25] Christian Cachin, Rachid Guerraoui, and Lu\u00eds Rodrigues: Introduction to Relia\u2010\nble and Secure Distributed Programming , 2nd edition. Springer, 2011. ISBN:\n978-3-642-15259-7, doi:10.1007/978-3-642-15260-3\n[26] Sam Elliott, Mark Allen, and Martin Kleppmann: personal communication ,\nthread on twitter.com, October 15, 2015.\n[27] Niklas Ekstr\u00f6m, Mikhail Panchenko, and Jonathan Ellis: \u201c Possible Issue with\nRead Repair?,\u201d email thread on cassandra-dev mailing list, October 2012.\n[28] Maurice P. Herlihy: \u201c Wait-Free Synchronization ,\u201d ACM Transactions on Pro\u2010\ngramming Languages and Systems  (TOPLAS), volume 13, number 1, pages 124\u2013149,\nJanuary 1991. doi:10.1145/114005.102808\n[29] Armando Fox and Eric A. Brewer: \u201c Harvest, Yield, and Scalable Tolerant Sys\u2010\ntems,\u201d at 7th Workshop on Hot Topics in Operating Systems  (HotOS), March 1999.\ndoi:10.1109/HOTOS.1999.798396\nSummary | 377", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "354f0f70-2e2f-4d67-86fe-f76b33ee59fc": {"__data__": {"id_": "354f0f70-2e2f-4d67-86fe-f76b33ee59fc", "embedding": null, "metadata": {"page_label": "378", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1b36253-3fdb-4b05-9ed9-ac2f7a5fb212", "node_type": "4", "metadata": {"page_label": "378", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "79b5733e03c034162b1f31588804894960e78e6a5cca0ecce4b9fbb50237bfda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[30] Seth Gilbert and Nancy Lynch: \u201c Brewer\u2019s Conjecture and the Feasibility of Con\u2010\nsistent, Available, Partition-Tolerant Web Services,\u201d ACM SIGACT News, volume 33,\nnumber 2, pages 51\u201359, June 2002. doi:10.1145/564585.564601\n[31] Seth Gilbert and Nancy Lynch: \u201c Perspectives on the CAP Theorem ,\u201d IEEE Com\u2010\nputer Magazine, volume 45, number 2, pages 30\u201336, February 2012. doi:10.1109/MC.\n2011.389\n[32] Eric A. Brewer: \u201cCAP Twelve Years Later: How the \u2018Rules\u2019 Have Changed,\u201d IEEE\nComputer Magazine , volume 45, number 2, pages 23\u201329, February 2012. doi:\n10.1109/MC.2012.37\n[33] Susan B. Davidson, Hector Garcia-Molina, and Dale Skeen: \u201c Consistency in Par\u2010\ntitioned Networks,\u201d ACM Computing Surveys, volume 17, number 3, pages 341\u2013370,\nSeptember 1985. doi:10.1145/5505.5508\n[34] Paul R. Johnson and Robert H. Thomas: \u201c RFC 677: The Maintenance of Dupli\u2010\ncate Databases,\u201d Network Working Group, January 27, 1975.\n[35] Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.: \u201c Notes on Dis\u2010\ntributed Databases,\u201d IBM Research, Research Report RJ2571(33471), July 1979.\n[36] Michael J. Fischer and Alan Michael: \u201c Sacrificing Serializability to Attain High\nAvailability of Data in an Unreliable Network ,\u201d at 1st ACM Symposium on Principles\nof Database Systems (PODS), March 1982. doi:10.1145/588111.588124\n[37] Eric A. Brewer: \u201cNoSQL: Past, Present, Future,\u201d at QCon San Francisco, Novem\u2010\nber 2012.\n[38] Henry Robinson: \u201c CAP Confusion: Problems with \u2018Partition Tolerance,\u2019 \u201d\nblog.cloudera.com, April 26, 2010.\n[39] Adrian Cockcroft: \u201cMigrating to Microservices,\u201d at QCon London, March 2014.\n[40] Martin Kleppmann: \u201c A Critique of the CAP Theorem ,\u201d arXiv:1509.05393, Sep\u2010\ntember 17, 2015.\n[41] Nancy A. Lynch: \u201c A Hundred Impossibility Proofs for Distributed Computing ,\u201d\nat 8th ACM Symposium on Principles of Distributed Computing  (PODC), August\n1989. doi:10.1145/72981.72982\n[42] Hagit Attiya, Faith Ellen, and Adam Morrison: \u201c Limitations of Highly-Available\nEventually-Consistent Data Stores ,\u201d at ACM Symposium on Principles of Distributed\nComputing (PODC), July 2015. doi:10.1145/2767386.2767419\n[43] Peter Sewell, Susmit Sarkar, Scott Owens, et al.: \u201c x86-TSO: A Rigorous and Usa\u2010\nble Programmer\u2019s Model for x86 Multiprocessors ,\u201d Communications of the ACM ,\nvolume 53, number 7, pages 89\u201397, July 2010. doi:10.1145/1785414.1785443\n378 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1027363b-a0bf-4b1f-8574-5c9ca5167056": {"__data__": {"id_": "1027363b-a0bf-4b1f-8574-5c9ca5167056", "embedding": null, "metadata": {"page_label": "379", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32076889-1c93-4889-b785-b1203477eab4", "node_type": "4", "metadata": {"page_label": "379", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "100453ef226e122db8076a2106f9d1fdd20d7c5c3b61194903c23c0688e5aee0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[44] Martin Thompson: \u201c Memory Barriers/Fences ,\u201d mechanical-\nsympathy.blogspot.co.uk, July 24, 2011.\n[45] Ulrich Drepper: \u201c What Every Programmer Should Know About Memory ,\u201d\nakkadia.org, November 21, 2007.\n[46] Daniel J. Abadi: \u201cConsistency Tradeoffs in Modern Distributed Database System\nDesign,\u201d IEEE Computer Magazine , volume 45, number 2, pages 37\u201342, February\n2012. doi:10.1109/MC.2012.33\n[47] Hagit Attiya and Jennifer L. Welch: \u201cSequential Consistency Versus Linearizabil\u2010\nity,\u201d ACM Transactions on Computer Systems  (TOCS), volume 12, number 2, pages\n91\u2013122, May 1994. doi:10.1145/176575.176576\n[48] Mustaque Ahamad, Gil Neiger, James E. Burns, et al.: \u201c Causal Memory: Defini\u2010\ntions, Implementation, and Programming ,\u201d Distributed Computing, volume 9, num\u2010\nber 1, pages 37\u201349, March 1995. doi:10.1007/BF01784241\n[49] Wyatt Lloyd, Michael J. Freedman, Michael Kaminsky, and David G. Andersen:\n\u201cStronger Semantics for Low-Latency Geo-Replicated Storage,\u201d at 10th USENIX Sym\u2010\nposium on Networked Systems Design and Implementation (NSDI), April 2013.\n[50] Marek Zawirski, Annette Bieniusa, Valter Balegas, et al.: \u201c SwiftCloud: Fault-\nTolerant Geo-Replication Integrated All the Way to the Client Machine ,\u201d INRIA\nResearch Report 8347, August 2013.\n[51] Peter Bailis, Ali Ghodsi, Joseph M Hellerstein, and Ion Stoica: \u201c Bolt-on Causal\nConsistency,\u201d at ACM International Conference on Management of Data  (SIGMOD),\nJune 2013.\n[52] Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: \u201cChallenges to Adopting\nStronger Consistency at Scale ,\u201d at 15th USENIX Workshop on Hot Topics in Operat\u2010\ning Systems (HotOS), May 2015.\n[53] Peter Bailis: \u201cCausality Is Expensive (and What to Do About It) ,\u201d bailis.org, Feb\u2010\nruary 5, 2014.\n[54] Ricardo Gon\u00e7alves, Paulo S\u00e9rgio Almeida, Carlos Baquero, and Victor Fonte:\n\u201cConcise Server-Wide Causality Management for Eventually Consistent Data Stores,\u201d\nat 15th IFIP International Conference on Distributed Applications and Interoperable\nSystems (DAIS), June 2015. doi:10.1007/978-3-319-19129-4_6\n[55] Rob Conery: \u201c A Better ID Generator for PostgreSQL ,\u201d rob.conery.io, May 29,\n2014.\n[56] Leslie Lamport: \u201cTime, Clocks, and the Ordering of Events in a Distributed Sys\u2010\ntem,\u201d Communications of the ACM , volume 21, number 7, pages 558\u2013565, July 1978.\ndoi:10.1145/359545.359563\nSummary | 379", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19221bb1-c45d-40b5-b096-257cbc71b5cc": {"__data__": {"id_": "19221bb1-c45d-40b5-b096-257cbc71b5cc", "embedding": null, "metadata": {"page_label": "380", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3311af09-25a9-43a2-8f45-c4edf1d0a72e", "node_type": "4", "metadata": {"page_label": "380", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "72b905ee3339e249c87fcfb6327be735850f6e40496b704501852c55cf475c55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[57] Xavier D\u00e9fago, Andr\u00e9 Schiper, and P\u00e9ter Urb\u00e1n: \u201c Total Order Broadcast and\nMulticast Algorithms: Taxonomy and Survey ,\u201d ACM Computing Surveys, volume 36,\nnumber 4, pages 372\u2013421, December 2004. doi:10.1145/1041680.1041682\n[58] Hagit Attiya and Jennifer Welch: Distributed Computing: Fundamentals, Simula\u2010\ntions and Advanced Topics , 2nd edition. John Wiley & Sons, 2004. ISBN:\n978-0-471-45324-6, doi:10.1002/0471478210\n[59] Mahesh Balakrishnan, Dahlia Malkhi, Vijayan Prabhakaran, et al.: \u201c CORFU: A\nShared Log Design for Flash Clusters ,\u201d at 9th USENIX Symposium on Networked Sys\u2010\ntems Design and Implementation (NSDI), April 2012.\n[60] Fred B. Schneider: \u201c Implementing Fault-Tolerant Services Using the State\nMachine Approach: A Tutorial ,\u201d ACM Computing Surveys , volume 22, number 4,\npages 299\u2013319, December 1990.\n[61] Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, et al.: \u201c Calvin: Fast\nDistributed Transactions for Partitioned Database Systems ,\u201d at ACM International\nConference on Management of Data (SIGMOD), May 2012.\n[62] Mahesh Balakrishnan, Dahlia Malkhi, Ted Wobber, et al.: \u201c Tango: Distributed\nData Structures over a Shared Log ,\u201d at 24th ACM Symposium on Operating Systems\nPrinciples (SOSP), November 2013. doi:10.1145/2517349.2522732\n[63] Robbert van Renesse and Fred B. Schneider: \u201c Chain Replication for Supporting\nHigh Throughput and Availability ,\u201d at 6th USENIX Symposium on Operating System\nDesign and Implementation (OSDI), December 2004.\n[64] Leslie Lamport: \u201c How to Make a Multiprocessor Computer That Correctly Exe\u2010\ncutes Multiprocess Programs,\u201d IEEE Transactions on Computers, volume 28, number\n9, pages 690\u2013691, September 1979. doi:10.1109/TC.1979.1675439\n[65] Enis S\u00f6ztutar, Devaraj Das, and Carter Shanklin: \u201cApache HBase High Availabil\u2010\nity at the Next Level,\u201d hortonworks.com, January 22, 2015.\n[66] Brian F Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, et al.: \u201c PNUTS:\nYahoo!\u2019s Hosted Data Serving Platform ,\u201d at 34th International Conference on Very\nLarge Data Bases (VLDB), August 2008. doi:10.14778/1454159.1454167\n[67] Tushar Deepak Chandra and Sam Toueg: \u201c Unreliable Failure Detectors for Reli\u2010\nable Distributed Systems,\u201d Journal of the ACM, volume 43, number 2, pages 225\u2013267,\nMarch 1996. doi:10.1145/226643.226647\n[68] Michael J. Fischer, Nancy Lynch, and Michael S. Paterson: \u201c Impossibility of Dis\u2010\ntributed Consensus with One Faulty Process ,\u201d Journal of the ACM , volume 32, num\u2010\nber 2, pages 374\u2013382, April 1985. doi:10.1145/3149.214121\n380 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9d339f5e-b6c8-4665-8625-6fa639d47f9f": {"__data__": {"id_": "9d339f5e-b6c8-4665-8625-6fa639d47f9f", "embedding": null, "metadata": {"page_label": "381", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a06ebf6-0823-4253-bc6c-7edc3aa13df6", "node_type": "4", "metadata": {"page_label": "381", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "84ef38cb7f8a009e0eca306d58ac87f93adb7141e9b2a6fc57bc44858f7e8e56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[69] Michael Ben-Or: \u201cAnother Advantage of Free Choice: Completely Asynchro\u2010\nnous Agreement Protocols,\u201d at 2nd ACM Symposium on Principles of Distributed\nComputing (PODC), August 1983. doi:10.1145/800221.806707\n[70] Jim N. Gray and Leslie Lamport: \u201c Consensus on Transaction Commit ,\u201d ACM\nTransactions on Database Systems  (TODS), volume 31, number 1, pages 133\u2013160,\nMarch 2006. doi:10.1145/1132863.1132867\n[71] Rachid Guerraoui: \u201c Revisiting the Relationship Between Non-Blocking Atomic\nCommitment and Consensus ,\u201d at 9th International Workshop on Distributed Algo\u2010\nrithms (WDAG), September 1995. doi:10.1007/BFb0022140\n[72] Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram, Ramnatthan Ala\u2010\ngappan, et al.: \u201c All File Systems Are Not Created Equal: On the Complexity of Craft\u2010\ning Crash-Consistent Applications ,\u201d at 11th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI), October 2014.\n[73] Jim Gray: \u201c The Transaction Concept: Virtues and Limitations ,\u201d at 7th Interna\u2010\ntional Conference on Very Large Data Bases (VLDB), September 1981.\n[74] Hector Garcia-Molina and Kenneth Salem: \u201c Sagas,\u201d at ACM International Con\u2010\nference on Management of Data (SIGMOD), May 1987. doi:10.1145/38713.38742\n[75] C. Mohan, Bruce G. Lindsay, and Ron Obermarck: \u201cTransaction Management in\nthe R* Distributed Database Management System ,\u201d ACM Transactions on Database\nSystems, volume 11, number 4, pages 378\u2013396, December 1986. doi:\n10.1145/7239.7266\n[76] \u201cDistributed Transaction Processing: The XA Specification ,\u201d X/Open Company\nLtd., Technical Standard XO/CAE/91/300, December 1991. ISBN: 978-1-872-63024-3\n[77] Mike Spille: \u201cXA Exposed, Part II,\u201d jroller.com, April 3, 2004.\n[78] Ivan Silva Neto and Francisco Reverbel: \u201c Lessons Learned from Implementing\nWS-Coordination and WS-AtomicTransaction ,\u201d at 7th IEEE/ACIS International\nConference on Computer and Information Science  (ICIS), May 2008. doi:10.1109/\nICIS.2008.75\n[79] James E. Johnson, David E. Langworthy, Leslie Lamport, and Friedrich H. Vogt:\n\u201cFormal Specification of a Web Services Protocol ,\u201d at 1st International Workshop on\nWeb Services and Formal Methods  (WS-FM), February 2004. doi:10.1016/j.entcs.\n2004.02.022\n[80] Dale Skeen: \u201cNonblocking Commit Protocols,\u201d at ACM International Conference\non Management of Data (SIGMOD), April 1981. doi:10.1145/582318.582339\n[81] Gregor Hohpe: \u201cYour Coffee Shop Doesn\u2019t Use Two-Phase Commit,\u201d IEEE Soft\u2010\nware, volume 22, number 2, pages 64\u201366, March 2005. doi:10.1109/MS.2005.52\nSummary | 381", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99fab3bf-7a6e-4401-8a15-d83176c447a6": {"__data__": {"id_": "99fab3bf-7a6e-4401-8a15-d83176c447a6", "embedding": null, "metadata": {"page_label": "382", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c440d27-4be8-4565-b4bb-f853c17399df", "node_type": "4", "metadata": {"page_label": "382", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d51205f0545e99795a58bfc1e55ab9f2d3db9aa8964b8b74869119d7c373f706", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[82] Pat Helland: \u201c Life Beyond Distributed Transactions: An Apostate\u2019s Opinion ,\u201d at\n3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 2007.\n[83] Jonathan Oliver: \u201c My Beef with MSDTC and Two-Phase Commits ,\u201d blog.jona\u2010\nthanoliver.com, April 4, 2011.\n[84] Oren Eini (Ahende Rahien): \u201c The Fallacy of Distributed Transactions ,\u201d\nayende.com, July 17, 2014.\n[85] Clemens Vasters: \u201c Transactions in Windows Azure (with Service Bus) \u2013 An\nEmail Discussion,\u201d vasters.com, July 30, 2012.\n[86] \u201cUnderstanding Transactionality in Azure ,\u201d NServiceBus Documentation, Par\u2010\nticular Software, 2015.\n[87] Randy Wigginton, Ryan Lowe, Marcos Albe, and Fernando Ipar: \u201c Distributed\nTransactions in MySQL,\u201d at MySQL Conference and Expo, April 2013.\n[88] Mike Spille: \u201cXA Exposed, Part I,\u201d jroller.com, April 3, 2004.\n[89] Ajmer Dhariwal: \u201c Orphaned MSDTC Transactions (-2 spids) ,\u201d eraofdata.com,\nDecember 12, 2008.\n[90] Paul Randal: \u201c Real World Story of DBCC PAGE Saving the Day ,\u201d sqlskills.com,\nJune 19, 2013.\n[91] \u201cin-doubt xact resolution Server Configuration Option ,\u201d SQL Server 2016 docu\u2010\nmentation, Microsoft, Inc., 2016.\n[92] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: \u201c Consensus in the Pres\u2010\nence of Partial Synchrony ,\u201d Journal of the ACM , volume 35, number 2, pages 288\u2013\n323, April 1988. doi:10.1145/42282.42283\n[93] Miguel Castro and Barbara H. Liskov: \u201c Practical Byzantine Fault Tolerance and\nProactive Recovery,\u201d ACM Transactions on Computer Systems, volume 20, number 4,\npages 396\u2013461, November 2002. doi:10.1145/571637.571640\n[94] Brian M. Oki and Barbara H. Liskov: \u201cViewstamped Replication: A New Primary\nCopy Method to Support Highly-Available Distributed Systems,\u201d at 7th ACM Sympo\u2010\nsium on Principles of Distributed Computing  (PODC), August 1988. doi:\n10.1145/62546.62549\n[95] Barbara H. Liskov and James Cowling: \u201c Viewstamped Replication Revisited ,\u201d\nMassachusetts Institute of Technology, Tech Report MIT-CSAIL-TR-2012-021, July\n2012.\n[96] Leslie Lamport: \u201c The Part-Time Parliament ,\u201d ACM Transactions on Computer\nSystems, volume 16, number 2, pages 133\u2013169, May 1998. doi:10.1145/279227.279229\n382 | Chapter 9: Consistency and Consensus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2184, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75fdd03e-9023-4afc-b987-f30603e6fb4f": {"__data__": {"id_": "75fdd03e-9023-4afc-b987-f30603e6fb4f", "embedding": null, "metadata": {"page_label": "383", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f719e8f8-2109-4fc1-a46c-145ac5021b50", "node_type": "4", "metadata": {"page_label": "383", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "83815badfdfb427ff01e721aa8f77150a8a9558c7c7ad19dd6b1dc7f7ad4d112", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[97] Leslie Lamport: \u201cPaxos Made Simple,\u201d ACM SIGACT News, volume 32, number\n4, pages 51\u201358, December 2001.\n[98] Tushar Deepak Chandra, Robert Griesemer, and Joshua Redstone: \u201c Paxos Made\nLive \u2013 An Engineering Perspective ,\u201d at 26th ACM Symposium on Principles of Dis\u2010\ntributed Computing (PODC), June 2007.\n[99] Robbert van Renesse: \u201c Paxos Made Moderately Complex ,\u201d cs.cornell.edu, March\n2011.\n[100] Diego Ongaro: \u201c Consensus: Bridging Theory and Practice ,\u201d PhD Thesis, Stan\u2010\nford University, August 2014.\n[101] Heidi Howard, Malte Schwarzkopf, Anil Madhavapeddy, and Jon Crowcroft:\n\u201cRaft Refloated: Do We Have Consensus? ,\u201d ACM SIGOPS Operating Systems Review ,\nvolume 49, number 1, pages 12\u201321, January 2015. doi:10.1145/2723872.2723876\n[102] Andr\u00e9 Medeiros: \u201c ZooKeeper\u2019s Atomic Broadcast Protocol: Theory and Prac\u2010\ntice,\u201d Aalto University School of Science, March 20, 2012.\n[103] Robbert van Renesse, Nicolas Schiper, and Fred B. Schneider: \u201c Vive La Diff\u00e9r\u2010\nence: Paxos vs. Viewstamped Replication vs. Zab ,\u201d IEEE Transactions on Dependable\nand Secure Computing , volume 12, number 4, pages 472\u2013484, September 2014. doi:\n10.1109/TDSC.2014.2355848\n[104] Will Portnoy: \u201c Lessons Learned from Implementing Paxos ,\u201d blog.willport\u2010\nnoy.com, June 14, 2012.\n[105] Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman: \u201c Flexible Paxos:\nQuorum Intersection Revisited,\u201d arXiv:1608.06696, August 24, 2016.\n[106] Heidi Howard and Jon Crowcroft: \u201cCoracle: Evaluating Consensus at the Inter\u2010\nnet Edge,\u201d at Annual Conference of the ACM Special Interest Group on Data Commu\u2010\nnication (SIGCOMM), August 2015. doi:10.1145/2829988.2790010\n[107] Kyle Kingsbury: \u201c Call Me Maybe: Elasticsearch 1.5.0 ,\u201d aphyr.com, April 27,\n2015.\n[108] Ivan Kelly: \u201cBookKeeper Tutorial,\u201d github.com, October 2014.\n[109] Camille Fournier: \u201c Consensus Systems for the Skeptical Architect ,\u201d at Craft\nConference, Budapest, Hungary, April 2015.\n[110] Kenneth P. Birman: \u201c A History of the Virtual Synchrony Replication Model ,\u201d\nin Replication: Theory and Practice , Springer LNCS volume 5959, chapter 6, pages\n91\u2013120, 2010. ISBN: 978-3-642-11293-5, doi:10.1007/978-3-642-11294-2_6\nSummary | 383", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9e27290-8b7b-408e-98f3-b3a22ed3067d": {"__data__": {"id_": "d9e27290-8b7b-408e-98f3-b3a22ed3067d", "embedding": null, "metadata": {"page_label": "384", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7055f343-6d9f-4afc-a566-1fe19af54aa3", "node_type": "4", "metadata": {"page_label": "384", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e02410c2-077a-4f3f-88d6-cfebbcf3209d": {"__data__": {"id_": "e02410c2-077a-4f3f-88d6-cfebbcf3209d", "embedding": null, "metadata": {"page_label": "385", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28772a37-b87f-4a6f-ad17-d1081cd2900b", "node_type": "4", "metadata": {"page_label": "385", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f873f7ac2a5a120b5aa7abbfa3ebb8cfcd5cbc388a198f74563b6b476b8171db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PART III\nDerived Data\nIn Parts I and II of this book, we assembled from the ground up all the major consid\u2010\nerations that go into a distributed database, from the layout of data on disk all the\nway to the limits of distributed consistency in the presence of faults. However, this\ndiscussion assumed that there was only one database in the application.\nIn reality, data systems are often more complex. In a large application you often need\nto be able to access and process data in many different ways, and there is no one data\u2010\nbase that can satisfy all those different needs simultaneously. Applications thus com\u2010\nmonly use a combination of several different datastores, indexes, caches, analytics\nsystems, etc. and implement mechanisms for moving data from one store to another.\nIn this final part of the book, we will examine the issues around integrating multiple\ndifferent data systems, potentially with different data models and optimized for dif\u2010\nferent access patterns, into one coherent application architecture. This aspect of\nsystem-building is often overlooked by vendors who claim that their product can sat\u2010\nisfy all your needs. In reality, integrating disparate systems is one of the most impor\u2010\ntant things that needs to be done in a nontrivial application.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57b84501-5d6a-455e-84e6-3832a8625a32": {"__data__": {"id_": "57b84501-5d6a-455e-84e6-3832a8625a32", "embedding": null, "metadata": {"page_label": "386", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e1a3ecb-9dae-4847-965d-598c86a43865", "node_type": "4", "metadata": {"page_label": "386", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "537ea3e243184055ca75bf4416a585c90ac745bb3cacd49a4c5a8574ffc1a63b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Systems of Record and Derived Data\nOn a high level, systems that store and process data can be grouped into two broad\ncategories:\nSystems of record\nA system of record, also known as source of truth, holds the authoritative version\nof your data. When new data comes in, e.g., as user input, it is first written here.\nEach fact is represented exactly once (the representation is typically normalized).\nIf there is any discrepancy between another system and the system of record,\nthen the value in the system of record is (by definition) the correct one.\nDerived data systems\nData in a derived system is the result of taking some existing data from another\nsystem and transforming or processing it in some way. If you lose derived data,\nyou can recreate it from the original source. A classic example is a cache: data can\nbe served from the cache if present, but if the cache doesn\u2019t contain what you\nneed, you can fall back to the underlying database. Denormalized values, indexes,\nand materialized views also fall into this category. In recommendation systems,\npredictive summary data is often derived from usage logs.\nTechnically speaking, derived data is redundant, in the sense that it duplicates exist\u2010\ning information. However, it is often essential for getting good performance on read\nqueries. It is commonly denormalized. You can derive several different datasets from\na single source, enabling you to look at the data from different \u201cpoints of view.\u201d\nNot all systems make a clear distinction between systems of record and derived data\nin their architecture, but it\u2019s a very helpful distinction to make, because it clarifies the\ndataflow through your system: it makes explicit which parts of the system have which\ninputs and which outputs, and how they depend on each other.\nMost databases, storage engines, and query languages are not inherently either a sys\u2010\ntem of record or a derived system. A database is just a tool: how you use it is up to\nyou. The distinction between system of record and derived data system depends not\non the tool, but on how you use it in your application.\nBy being clear about which data is derived from which other data, you can bring\nclarity to an otherwise confusing system architecture. This point will be a running\ntheme throughout this part of the book.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a6c33ac-27fb-4402-acde-0644490055e1": {"__data__": {"id_": "8a6c33ac-27fb-4402-acde-0644490055e1", "embedding": null, "metadata": {"page_label": "387", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0842b10d-a6fa-4dc2-b017-5b14af8da28c", "node_type": "4", "metadata": {"page_label": "387", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "009100eb7f5f03f0c06263a3fc7543db751d5b03d121d8af2a1792058aa4f475", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Overview of Chapters\nWe will start in Chapter 10  by examining batch-oriented dataflow systems such as\nMapReduce, and see how they give us good tools and principles for building large-\nscale data systems. In Chapter 11  we will take those ideas and apply them to data\nstreams, which allow us to do the same kinds of things with lower delays. Chapter 12\nconcludes the book by exploring ideas about how we might use these tools to build\nreliable, scalable, and maintainable applications in the future.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "734e1815-3c0d-41f2-91f4-28b6d6926b03": {"__data__": {"id_": "734e1815-3c0d-41f2-91f4-28b6d6926b03", "embedding": null, "metadata": {"page_label": "388", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3dc193bd-bba1-4922-8a69-fdcd6642501b", "node_type": "4", "metadata": {"page_label": "388", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65365ce8-d0bc-48b3-bf6d-a11936fc4520": {"__data__": {"id_": "65365ce8-d0bc-48b3-bf6d-a11936fc4520", "embedding": null, "metadata": {"page_label": "389", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4389bd7a-45a2-497a-8acb-3175ba8ef500", "node_type": "4", "metadata": {"page_label": "389", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "21da96198ffdd21ec94c94743ad22471794c460647e43d91c14290bc22eacd41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 10\nBatch Processing\nA system cannot be successful if it is too strongly influenced by a single person. Once the\ninitial design is complete and fairly robust, the real test begins as people with many different\nviewpoints undertake their own experiments.\n\u2014Donald Knuth\nIn the first two parts of this book we talked a lot about requests and queries, and the\ncorresponding responses or results. This style of data processing is assumed in many\nmodern data systems: you ask for something, or you send an instruction, and some\ntime later the system (hopefully) gives you an answer. Databases, caches, search\nindexes, web servers, and many other systems work this way.\nIn such online systems, whether it\u2019s a web browser requesting a page or a service call\u2010\ning a remote API, we generally assume that the request is triggered by a human user,\nand that the user is waiting for the response. They shouldn\u2019t have to wait too long, so\nwe pay a lot of attention to the response time of these systems (see \u201cDescribing Perfor\u2010\nmance\u201d on page 13).\nThe web, and increasing numbers of HTTP/REST-based APIs, has made the request/\nresponse style of interaction so common that it\u2019s easy to take it for granted. But we\nshould remember that it\u2019s not the only way of building systems, and that other\napproaches have their merits too. Let\u2019s distinguish three different types of systems:\nServices (online systems)\nA service waits for a request or instruction from a client to arrive. When one is\nreceived, the service tries to handle it as quickly as possible and sends a response\nback. Response time is usually the primary measure of performance of a service,\nand availability is often very important (if the client can\u2019t reach the service, the\nuser will probably get an error message).\n389", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1773, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aac8f3ed-b1fa-4cad-af85-41e34aaa5e28": {"__data__": {"id_": "aac8f3ed-b1fa-4cad-af85-41e34aaa5e28", "embedding": null, "metadata": {"page_label": "390", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf2a28be-b1bc-4be7-a797-b894e1a9e90c", "node_type": "4", "metadata": {"page_label": "390", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c2611425d135acab694f60aa60a3a57e21e53768567b5b53f5e055e3fc08459f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Batch processing systems (offline systems)\nA batch processing system takes a large amount of input data, runs a job to pro\u2010\ncess it, and produces some output data. Jobs often take a while (from a few\nminutes to several days), so there normally isn\u2019t a user waiting for the job to fin\u2010\nish. Instead, batch jobs are often scheduled to run periodically (for example, once\na day). The primary performance measure of a batch job is usually throughput\n(the time it takes to crunch through an input dataset of a certain size). We dis\u2010\ncuss batch processing in this chapter.\nStream processing systems (near-real-time systems)\nStream processing is somewhere between online and offline/batch processing (so\nit is sometimes called near-real-time or nearline processing). Like a batch pro\u2010\ncessing system, a stream processor consumes inputs and produces outputs\n(rather than responding to requests). However, a stream job operates on events\nshortly after they happen, whereas a batch job operates on a fixed set of input\ndata. This difference allows stream processing systems to have lower latency than\nthe equivalent batch systems. As stream processing builds upon batch process\u2010\ning, we discuss it in Chapter 11.\nAs we shall see in this chapter, batch processing is an important building block in our\nquest to build reliable, scalable, and maintainable applications. For example, Map\u2010\nReduce, a batch processing algorithm published in 2004 [ 1], was (perhaps over-\nenthusiastically) called \u201cthe algorithm that makes Google so massively scalable\u201d [2]. It\nwas subsequently implemented in various open source data systems, including\nHadoop, CouchDB, and MongoDB.\nMapReduce is a fairly low-level programming model compared to the parallel pro\u2010\ncessing systems that were developed for data warehouses many years previously [ 3,\n4], but it was a major step forward in terms of the scale of processing that could be\nachieved on commodity hardware. Although the importance of MapReduce is now\ndeclining [ 5], it is still worth understanding, because it provides a clear picture of\nwhy and how batch processing is useful.\nIn fact, batch processing is a very old form of computing. Long before programmable\ndigital computers were invented, punch card tabulating machines\u2014such as the Hol\u2010\nlerith machines used in the 1890 US Census [ 6]\u2014implemented a semi-mechanized\nform of batch processing to compute aggregate statistics from large inputs. And Map\u2010\nReduce bears an uncanny resemblance to the electromechanical IBM card-sorting\nmachines that were widely used for business data processing in the 1940s and 1950s\n[7]. As usual, history has a tendency of repeating itself.\nIn this chapter, we will look at MapReduce and several other batch processing algo\u2010\nrithms and frameworks, and explore how they are used in modern data systems. But\nfirst, to get started, we will look at data processing using standard Unix tools. Even if\nyou are already familiar with them, a reminder about the Unix philosophy is worth\u2010\n390 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cdbac4ef-2d1b-4d49-b450-70c5a84972f1": {"__data__": {"id_": "cdbac4ef-2d1b-4d49-b450-70c5a84972f1", "embedding": null, "metadata": {"page_label": "391", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "684cb30f-884f-4c91-a90e-11279924a187", "node_type": "4", "metadata": {"page_label": "391", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8d1a118cb64b54f82bd74754756483f85467582c03a3f65a0c35360d99d887ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. Some people love to point out that cat is unnecessary here, as the input file could be given directly as an\nargument to awk. However, the linear pipeline is more apparent when written like this.\nwhile because the ideas and lessons from Unix carry over to large-scale, heterogene\u2010\nous distributed data systems.\nBatch Processing with Unix Tools\nLet\u2019s start with a simple example. Say you have a web server that appends a line to a\nlog file every time it serves a request. For example, using the nginx default access log\nformat, one line of the log might look like this:\n216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] \"GET /css/typography.css HTTP/1.1\"\n200 3377 \"http://martin.kleppmann.com/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X\n10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115\nSafari/537.36\"\n(That is actually one line; it\u2019s only broken onto multiple lines here for readability.)\nThere\u2019s a lot of information in that line. In order to interpret it, you need to look at\nthe definition of the log format, which is as follows:\n$remote_addr - $remote_user [$time_local] \"$request\"\n$status $body_bytes_sent \"$http_referer\" \"$http_user_agent\"\nSo, this one line of the log indicates that on February 27, 2015, at 17:55:11 UTC, the\nserver received a request for the file /css/typography.css from the client IP address\n216.58.210.78. The user was not authenticated, so $remote_user is set to a hyphen\n(-). The response status was 200 (i.e., the request was successful), and the response\nwas 3,377 bytes in size. The web browser was Chrome 40, and it loaded the file\nbecause it was referenced in the page at the URL http://martin.kleppmann.com/.\nSimple Log Analysis\nVarious tools can take these log files and produce pretty reports about your website\ntraffic, but for the sake of exercise, let\u2019s build our own, using basic Unix tools. For\nexample, say you want to find the five most popular pages on your website. You can\ndo this in a Unix shell as follows:i \ncat /var/log/nginx/access.log | \n  awk '{print $7}' | \n  sort             | \n  uniq -c          | \n  sort -r -n       | \n  head -n 5          \nRead the log file.\nBatch Processing with Unix Tools | 391", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0af58246-6f30-40b0-b928-d8f576b481d7": {"__data__": {"id_": "0af58246-6f30-40b0-b928-d8f576b481d7", "embedding": null, "metadata": {"page_label": "392", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bbe6e75-21e8-4dcc-95be-cba9027560b7", "node_type": "4", "metadata": {"page_label": "392", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f8cc0bf2bc901d9629dbb5cde16ef152faf3c1f2f27c33fe83244d183d38b93c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Split each line into fields by whitespace, and output only the seventh such field\nfrom each line, which happens to be the requested URL. In our example line, this\nrequest URL is /css/typography.css.\nAlphabetically sort the list of requested URLs. If some URL has been requested\nn times, then after sorting, the file contains the same URL repeated n times in a\nrow.\nThe uniq command filters out repeated lines in its input by checking whether\ntwo adjacent lines are the same. The -c option tells it to also output a counter: for\nevery distinct URL, it reports how many times that URL appeared in the input.\nThe second sort sorts by the number ( -n) at the start of each line, which is the\nnumber of times the URL was requested. It then returns the results in reverse\n(-r) order, i.e. with the largest number first.\nFinally, head outputs just the first five lines (-n 5) of input, and discards the rest.\nThe output of that series of commands looks something like this:\n4189 /favicon.ico\n3631 /2013/05/24/improving-security-of-ssh-private-keys.html\n2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html\n1369 /\n 915 /css/typography.css\nAlthough the preceding command line likely looks a bit obscure if you\u2019re unfamiliar\nwith Unix tools, it is incredibly powerful. It will process gigabytes of log files in a\nmatter of seconds, and you can easily modify the analysis to suit your needs. For\nexample, if you want to omit CSS files from the report, change the awk argument to\n'$7 !~ /\\.css$/ {print $7}'. If you want to count top client IP addresses instead\nof top pages, change the awk argument to '{print $1}'. And so on.\nWe don\u2019t have space in this book to explore Unix tools in detail, but they are very\nmuch worth learning about. Surprisingly many data analyses can be done in a few\nminutes using some combination of awk, sed, grep, sort, uniq, and xargs, and they\nperform surprisingly well [8].\nChain of commands versus custom program\nInstead of the chain of Unix commands, you could write a simple program to do the\nsame thing. For example, in Ruby, it might look something like this:\n392 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01d2fab5-48d8-4ab4-b338-4e3f86d2f0c0": {"__data__": {"id_": "01d2fab5-48d8-4ab4-b338-4e3f86d2f0c0", "embedding": null, "metadata": {"page_label": "393", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e8afd3e-c770-415a-838c-60b15b8e1b78", "node_type": "4", "metadata": {"page_label": "393", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "12d8e2c7ff9c39c1d19ce51b017b93f6930d0ed4aa294af798da356c76767b42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "counts = Hash.new(0) \nFile.open('/var/log/nginx/access.log') do |file|\n  file.each do |line|\n    url = line.split[6] \n    counts[url] += 1 \n  end\nend\ntop5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5] \ntop5.each{|count, url| puts \"#{count} #{url}\" } \ncounts is a hash table that keeps a counter for the number of times we\u2019ve seen\neach URL. A counter is zero by default.\nFrom each line of the log, we take the URL to be the seventh whitespace-\nseparated field (the array index here is 6 because Ruby\u2019s arrays are zero-indexed).\nIncrement the counter for the URL in the current line of the log.\nSort the hash table contents by counter value (descending), and take the top five\nentries.\nPrint out those top five entries.\nThis program is not as concise as the chain of Unix pipes, but it\u2019s fairly readable, and\nwhich of the two you prefer is partly a matter of taste. However, besides the superfi\u2010\ncial syntactic differences between the two, there is a big difference in the execution\nflow, which becomes apparent if you run this analysis on a large file.\nSorting versus in-memory aggregation\nThe Ruby script keeps an in-memory hash table of URLs, where each URL is mapped\nto the number of times it has been seen. The Unix pipeline example does not have\nsuch a hash table, but instead relies on sorting a list of URLs in which multiple occur\u2010\nrences of the same URL are simply repeated.\nWhich approach is better? It depends how many different URLs you have. For most\nsmall to mid-sized websites, you can probably fit all distinct URLs, and a counter for\neach URL, in (say) 1 GB of memory. In this example, the working set of the job (the\namount of memory to which the job needs random access) depends only on the\nnumber of distinct URLs: if there are a million log entries for a single URL, the space\nrequired in the hash table is still just one URL plus the size of the counter. If this\nworking set is small enough, an in-memory hash table works fine\u2014even on a laptop.\nOn the other hand, if the job\u2019s working set is larger than the available memory, the\nsorting approach has the advantage that it can make efficient use of disks. It\u2019s the\nBatch Processing with Unix Tools | 393", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc466f1c-52b3-45c1-982d-dd6e97acf071": {"__data__": {"id_": "cc466f1c-52b3-45c1-982d-dd6e97acf071", "embedding": null, "metadata": {"page_label": "394", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9566c3f6-07c9-40a0-9738-9c200d9de840", "node_type": "4", "metadata": {"page_label": "394", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5614cb81775d7635c3c1e0e8902c060ad62d58cb30a8f14cb8d1b88cdcdadcd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "same principle as we discussed in \u201cSSTables and LSM-Trees\u201d on page 76: chunks of\ndata can be sorted in memory and written out to disk as segment files, and then mul\u2010\ntiple sorted segments can be merged into a larger sorted file. Mergesort has sequential\naccess patterns that perform well on disks. (Remember that optimizing for sequential\nI/O was a recurring theme in Chapter 3. The same pattern reappears here.)\nThe sort utility in GNU Coreutils (Linux) automatically handles larger-than-\nmemory datasets by spilling to disk, and automatically parallelizes sorting across\nmultiple CPU cores [9]. This means that the simple chain of Unix commands we saw\nearlier easily scales to large datasets, without running out of memory. The bottleneck\nis likely to be the rate at which the input file can be read from disk. \nThe Unix Philosophy\nIt\u2019s no coincidence that we were able to analyze a log file quite easily, using a chain of\ncommands like in the previous example: this was in fact one of the key design ideas of\nUnix, and it remains astonishingly relevant today. Let\u2019s look at it in some more depth\nso that we can borrow some ideas from Unix [10].\nDoug McIlroy, the inventor of Unix pipes, first described them like this in 1964 [ 11]:\n\u201cWe should have some ways of connecting programs like [a] garden hose\u2014screw in\nanother segment when it becomes necessary to massage data in another way. This is\nthe way of I/O also.\u201d The plumbing analogy stuck, and the idea of connecting pro\u2010\ngrams with pipes became part of what is now known as the Unix philosophy\u2014a set of\ndesign principles that became popular among the developers and users of Unix. The\nphilosophy was described in 1978 as follows [12, 13]:\n1. Make each program do one thing well. To do a new job, build afresh rather than\ncomplicate old programs by adding new \u201cfeatures\u201d.\n2. Expect the output of every program to become the input to another, as yet\nunknown, program. Don\u2019t clutter output with extraneous information. Avoid\nstringently columnar or binary input formats. Don\u2019t insist on interactive input.\n3. Design and build software, even operating systems, to be tried early, ideally within\nweeks. Don\u2019t hesitate to throw away the clumsy parts and rebuild them.\n4. Use tools in preference to unskilled help to lighten a programming task, even if\nyou have to detour to build the tools and expect to throw some of them out after\nyou\u2019ve finished using them.\nThis approach\u2014automation, rapid prototyping, incremental iteration, being friendly\nto experimentation, and breaking down large projects into manageable chunks\u2014\nsounds remarkably like the Agile and DevOps movements of today. Surprisingly little\nhas changed in four decades.\n394 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b064d87-5acd-4449-badd-4dc0f9d0dc98": {"__data__": {"id_": "7b064d87-5acd-4449-badd-4dc0f9d0dc98", "embedding": null, "metadata": {"page_label": "395", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a76d14f-c044-4894-a9ae-b358f3e8b501", "node_type": "4", "metadata": {"page_label": "395", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a3058eebfe2bd6cdeae95aaf9d8d9d72c7c61f73d357e40d5783509c0479b466", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. Another example of a uniform interface is URLs and HTTP, the foundations of the web. A URL identifies\na particular thing (resource) on a website, and you can link to any URL from any other website. A user with a\nweb browser can thus seamlessly jump between websites by following links, even though the servers may be\noperated by entirely unrelated organizations. This principle seems obvious today, but it was a key insight in\nmaking the web the success that it is today. Prior systems were not so uniform: for example, in the era of\nbulletin board systems (BBSs), each system had its own phone number and baud rate configuration. A refer\u2010\nence from one BBS to another would have to be in the form of a phone number and modem settings; the user\nwould have to hang up, dial the other BBS, and then manually find the information they were looking for. It\nwasn\u2019t possible to link directly to some piece of content inside another BBS.\nThe sort tool is a great example of a program that does one thing well. It is arguably\na better sorting implementation than most programming languages have in their\nstandard libraries (which do not spill to disk and do not use multiple threads, even\nwhen that would be beneficial). And yet, sort is barely useful in isolation. It only\nbecomes powerful in combination with the other Unix tools, such as uniq.\nA Unix shell like bash lets us easily compose these small programs into surprisingly\npowerful data processing jobs. Even though many of these programs are written by\ndifferent groups of people, they can be joined together in flexible ways. What does\nUnix do to enable this composability?\nA uniform interface\nIf you expect the output of one program to become the input to another program,\nthat means those programs must use the same data format\u2014in other words, a com\u2010\npatible interface. If you want to be able to connect any program\u2019s output to any pro\u2010\ngram\u2019s input, that means that all programs must use the same input/output interface.\nIn Unix, that interface is a file (or, more precisely, a file descriptor). A file is just an\nordered sequence of bytes. Because that is such a simple interface, many different\nthings can be represented using the same interface: an actual file on the filesystem, a\ncommunication channel to another process (Unix socket, stdin, stdout), a device\ndriver (say /dev/audio or /dev/lp0), a socket representing a TCP connection, and so\non. It\u2019s easy to take this for granted, but it\u2019s actually quite remarkable that these very\ndifferent things can share a uniform interface, so they can easily be plugged together.ii\nBy convention, many (but not all) Unix programs treat this sequence of bytes as\nASCII text. Our log analysis example used this fact: awk, sort, uniq, and head all treat\ntheir input file as a list of records separated by the \\n (newline, ASCII 0x0A) charac\u2010\nter. The choice of \\n is arbitrary\u2014arguably, the ASCII record separator 0x1E would\nhave been a better choice, since it\u2019s intended for this purpose [ 14]\u2014but in any case,\nthe fact that all these programs have standardized on using the same record separator\nallows them to interoperate.\nBatch Processing with Unix Tools | 395", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b101ca7d-68b7-4e28-91ee-5a53ddb45242": {"__data__": {"id_": "b101ca7d-68b7-4e28-91ee-5a53ddb45242", "embedding": null, "metadata": {"page_label": "396", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5672d9cb-a86b-4556-ac6c-ea32f804b33e", "node_type": "4", "metadata": {"page_label": "396", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ae867891d5bf61fab94729ed1fc01352d0d2e35432637cb0a93deece079e893a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The parsing of each record (i.e., a line of input) is more vague. Unix tools commonly\nsplit a line into fields by whitespace or tab characters, but CSV (comma-separated),\npipe-separated, and other encodings are also used. Even a fairly simple tool like\nxargs has half a dozen command-line options for specifying how its input should be\nparsed.\nThe uniform interface of ASCII text mostly works, but it\u2019s not exactly beautiful: our\nlog analysis example used {print $7} to extract the URL, which is not very readable.\nIn an ideal world this could have perhaps been {print $request_url} or something\nof that sort. We will return to this idea later.\nAlthough it\u2019s not perfect, even decades later, the uniform interface of Unix is still\nsomething remarkable. Not many pieces of software interoperate and compose as\nwell as Unix tools do: you can\u2019t easily pipe the contents of your email account and\nyour online shopping history through a custom analysis tool into a spreadsheet and\npost the results to a social network or a wiki. Today it\u2019s an exception, not the norm,\nto have programs that work together as smoothly as Unix tools do.\nEven databases with the same data model  often don\u2019t make it easy to get data out of\none and into the other. This lack of integration leads to Balkanization of data.\nSeparation of logic and wiring\nAnother characteristic feature of Unix tools is their use of standard input (stdin) and\nstandard output ( stdout). If you run a program and don\u2019t specify anything else,\nstdin comes from the keyboard and stdout goes to the screen. However, you can\nalso take input from a file and/or redirect output to a file. Pipes let you attach the\nstdout of one process to the stdin of another process (with a small in-memory\nbuffer, and without writing the entire intermediate data stream to disk).\nA program can still read and write files directly if it needs to, but the Unix approach\nworks best if a program doesn\u2019t worry about particular file paths and simply uses\nstdin and stdout. This allows a shell user to wire up the input and output in what\u2010\never way they want; the program doesn\u2019t know or care where the input is coming\nfrom and where the output is going to. (One could say this is a form of loose coupling,\nlate binding  [15], or inversion of control  [16].) Separating the input/output wiring\nfrom the program logic makes it easier to compose small tools into bigger systems.\nYou can even write your own programs and combine them with the tools provided\nby the operating system. Your program just needs to read input from stdin and write\noutput to stdout, and it can participate in data processing pipelines. In the log analy\u2010\nsis example, you could write a tool that translates user-agent strings into more sensi\u2010\nble browser identifiers, or a tool that translates IP addresses into country codes, and\nsimply plug it into the pipeline. The sort program doesn\u2019t care whether it\u2019s commu\u2010\nnicating with another part of the operating system or with a program written by you.\n396 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9d3953e-843e-4c75-a6ea-3aac1268744f": {"__data__": {"id_": "b9d3953e-843e-4c75-a6ea-3aac1268744f", "embedding": null, "metadata": {"page_label": "397", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f11afa31-403d-4507-907c-10cd9b848534", "node_type": "4", "metadata": {"page_label": "397", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b6e106be2b1dc260cf571134aa75caee41190faf8457230a27a0a07a58babb29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. Except by using a separate tool, such as netcat or curl. Unix started out trying to represent everything as\nfiles, but the BSD sockets API deviated from that convention [17]. The research operating systems Plan 9 and\nInferno are more consistent in their use of files: they represent a TCP connection as a file in /net/tcp [18].\nHowever, there are limits to what you can do with stdin and stdout. Programs that\nneed multiple inputs or outputs are possible but tricky. You can\u2019t pipe a program\u2019s\noutput into a network connection [17, 18].iii If a program directly opens files for read\u2010\ning and writing, or starts another program as a subprocess, or opens a network con\u2010\nnection, then that I/O is wired up by the program itself. It can still be configurable\n(through command-line options, for example), but the flexibility of wiring up inputs\nand outputs in a shell is reduced.\nTransparency and experimentation\nPart of what makes Unix tools so successful is that they make it quite easy to see what\nis going on:\n\u2022 The input files to Unix commands are normally treated as immutable. This\nmeans you can run the commands as often as you want, trying various\ncommand-line options, without damaging the input files.\n\u2022 You can end the pipeline at any point, pipe the output into less, and look at it to\nsee if it has the expected form. This ability to inspect is great for debugging.\n\u2022 You can write the output of one pipeline stage to a file and use that file as input\nto the next stage. This allows you to restart the later stage without rerunning the\nentire pipeline.\nThus, even though Unix tools are quite blunt, simple tools compared to a query opti\u2010\nmizer of a relational database, they remain amazingly useful, especially for experi\u2010\nmentation.\nHowever, the biggest limitation of Unix tools is that they run only on a single\nmachine\u2014and that\u2019s where tools like Hadoop come in. \nMapReduce and Distributed Filesystems\nMapReduce is a bit like Unix tools, but distributed across potentially thousands of\nmachines. Like Unix tools, it is a fairly blunt, brute-force, but surprisingly effective\ntool. A single MapReduce job is comparable to a single Unix process: it takes one or\nmore inputs and produces one or more outputs.\nAs with most Unix tools, running a MapReduce job normally does not modify the\ninput and does not have any side effects other than producing the output. The output\nMapReduce and Distributed Filesystems | 397", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2429, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df58522f-9c7b-481d-9d6b-1d281d2f164c": {"__data__": {"id_": "df58522f-9c7b-481d-9d6b-1d281d2f164c", "embedding": null, "metadata": {"page_label": "398", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d53a1cb-7313-430c-b35f-76e44bfe5a0c", "node_type": "4", "metadata": {"page_label": "398", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dca4dc275274ea2ff7276b2aad1a1d205e2aff676ae4a8133fb6160fef08adfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iv. One difference is that with HDFS, computing tasks can be scheduled to run on the machine that stores a\ncopy of a particular file, whereas object stores usually keep storage and computation separate. Reading from a\nlocal disk has a performance advantage if network bandwidth is a bottleneck. Note however that if erasure\ncoding is used, the locality advantage is lost, because the data from several machines must be combined in\norder to reconstitute the original file [20].\nfiles are written once, in a sequential fashion (not modifying any existing part of a file\nonce it has been written).\nWhile Unix tools use stdin and stdout as input and output, MapReduce jobs read\nand write files on a distributed filesystem. In Hadoop\u2019s implementation of Map\u2010\nReduce, that filesystem is called HDFS (Hadoop Distributed File System), an open\nsource reimplementation of the Google File System (GFS) [19].\nVarious other distributed filesystems besides HDFS exist, such as GlusterFS and the\nQuantcast File System (QFS) [ 20]. Object storage services such as Amazon S3, Azure\nBlob Storage, and OpenStack Swift [21] are similar in many ways.iv In this chapter we\nwill mostly use HDFS as a running example, but the principles apply to any dis\u2010\ntributed filesystem.\nHDFS is based on the shared-nothing principle (see the introduction to Part II ), in\ncontrast to the shared-disk approach of Network Attached Storage (NAS) and Storage\nArea Network (SAN) architectures. Shared-disk storage is implemented by a central\u2010\nized storage appliance, often using custom hardware and special network infrastruc\u2010\nture such as Fibre Channel. On the other hand, the shared-nothing approach requires\nno special hardware, only computers connected by a conventional datacenter net\u2010\nwork.\nHDFS consists of a daemon process running on each machine, exposing a network\nservice that allows other nodes to access files stored on that machine (assuming that\nevery general-purpose machine in a datacenter has some disks attached to it). A cen\u2010\ntral server called the NameNode keeps track of which file blocks are stored on which\nmachine. Thus, HDFS conceptually creates one big filesystem that can use the space\non the disks of all machines running the daemon.\nIn order to tolerate machine and disk failures, file blocks are replicated on multiple\nmachines. Replication may mean simply several copies of the same data on multiple\nmachines, as in Chapter 5, or an erasure coding scheme such as Reed\u2013Solomon codes,\nwhich allows lost data to be recovered with lower storage overhead than full replica\u2010\ntion [20, 22]. The techniques are similar to RAID, which provides redundancy across\nseveral disks attached to the same machine; the difference is that in a distributed file\u2010\nsystem, file access and replication are done over a conventional datacenter network\nwithout special hardware.\n398 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53d2dab7-4d80-4ffd-a1e8-ea6811484742": {"__data__": {"id_": "53d2dab7-4d80-4ffd-a1e8-ea6811484742", "embedding": null, "metadata": {"page_label": "399", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98137909-efe0-49e2-95a9-82cab4f68d19", "node_type": "4", "metadata": {"page_label": "399", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c1d379030dea395ad2a04920ce97b201f8d2dcdf0516f9fa1583288dee9b0fe0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "HDFS has scaled well: at the time of writing, the biggest HDFS deployments run on\ntens of thousands of machines, with combined storage capacity of hundreds of peta\u2010\nbytes [ 23]. Such large scale has become viable because the cost of data storage and\naccess on HDFS, using commodity hardware and open source software, is much\nlower than that of the equivalent capacity on a dedicated storage appliance [24]. \nMapReduce Job Execution\nMapReduce is a programming framework with which you can write code to process\nlarge datasets in a distributed filesystem like HDFS. The easiest way of understanding\nit is by referring back to the web server log analysis example in \u201cSimple Log Analysis\u201d\non page 391. The pattern of data processing in MapReduce is very similar to this\nexample:\n1. Read a set of input files, and break it up into records. In the web server log exam\u2010\nple, each record is one line in the log (that is, \\n is the record separator).\n2. Call the mapper function to extract a key and value from each input record. In\nthe preceding example, the mapper function is awk '{print $7}': it extracts the\nURL ($7) as the key, and leaves the value empty.\n3. Sort all of the key-value pairs by key. In the log example, this is done by the first\nsort command.\n4. Call the reducer function to iterate over the sorted key-value pairs. If there are\nmultiple occurrences of the same key, the sorting has made them adjacent in the\nlist, so it is easy to combine those values without having to keep a lot of state in\nmemory. In the preceding example, the reducer is implemented by the command\nuniq -c, which counts the number of adjacent records with the same key.\nThose four steps can be performed by one MapReduce job. Steps 2 (map) and 4\n(reduce) are where you write your custom data processing code. Step 1 (breaking files\ninto records) is handled by the input format parser. Step 3, the sort step, is implicit\nin MapReduce\u2014you don\u2019t have to write it, because the output from the mapper is\nalways sorted before it is given to the reducer.\nTo create a MapReduce job, you need to implement two callback functions, the map\u2010\nper and reducer, which behave as follows (see also \u201cMapReduce Querying\u201d on page\n46):\nMapper\nThe mapper is called once for every input record, and its job is to extract the key\nand value from the input record. For each input, it may generate any number of\nkey-value pairs (including none). It does not keep any state from one input\nrecord to the next, so each record is handled independently.\nMapReduce and Distributed Filesystems | 399", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c61f8765-247b-495b-a57d-cf6a1d4526e8": {"__data__": {"id_": "c61f8765-247b-495b-a57d-cf6a1d4526e8", "embedding": null, "metadata": {"page_label": "400", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0a8ae2d-72c8-4247-8b63-60632d8febcc", "node_type": "4", "metadata": {"page_label": "400", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7df70765b091ff8f85836ecdb684355801b8aab30a958de295eeb00aace0e02d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reducer\nThe MapReduce framework takes the key-value pairs produced by the mappers,\ncollects all the values belonging to the same key, and calls the reducer with an\niterator over that collection of values. The reducer can produce output records\n(such as the number of occurrences of the same URL).\nIn the web server log example, we had a second sort command in step 5, which\nranked URLs by number of requests. In MapReduce, if you need a second sorting\nstage, you can implement it by writing a second MapReduce job and using the output\nof the first job as input to the second job. Viewed like this, the role of the mapper is to\nprepare the data by putting it into a form that is suitable for sorting, and the role of\nthe reducer is to process the data that has been sorted. \nDistributed execution of MapReduce\nThe main difference from pipelines of Unix commands is that MapReduce can paral\u2010\nlelize a computation across many machines, without you having to write code to\nexplicitly handle the parallelism. The mapper and reducer only operate on one record\nat a time; they don\u2019t need to know where their input is coming from or their output is\ngoing to, so the framework can handle the complexities of moving data between\nmachines.\nIt is possible to use standard Unix tools as mappers and reducers in a distributed\ncomputation [25], but more commonly they are implemented as functions in a con\u2010\nventional programming language. In Hadoop MapReduce, the mapper and reducer\nare each a Java class that implements a particular interface. In MongoDB and\nCouchDB, mappers and reducers are JavaScript functions (see \u201cMapReduce Query\u2010\ning\u201d on page 46).\nFigure 10-1  shows the dataflow in a Hadoop MapReduce job. Its parallelization is\nbased on partitioning (see Chapter 6 ): the input to a job is typically a directory in\nHDFS, and each file or file block within the input directory is considered to be a sepa\u2010\nrate partition that can be processed by a separate map task (marked by m 1, m 2, and\nm 3 in Figure 10-1).\nEach input file is typically hundreds of megabytes in size. The MapReduce scheduler\n(not shown in the diagram) tries to run each mapper on one of the machines that\nstores a replica of the input file, provided that machine has enough spare RAM and\nCPU resources to run the map task [ 26]. This principle is known as putting the com\u2010\nputation near the data [27]: it saves copying the input file over the network, reducing\nnetwork load and increasing locality.\n400 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2497, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce3ceb0a-2bac-4c67-bd77-fa348e11c68c": {"__data__": {"id_": "ce3ceb0a-2bac-4c67-bd77-fa348e11c68c", "embedding": null, "metadata": {"page_label": "401", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "311873fc-c662-42de-a871-fde4739a3499", "node_type": "4", "metadata": {"page_label": "401", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3eb53958fcc68114c2c4a8054e059d2dcdb6239f14a57e9245b6b1a16f5ef481", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 10-1. A MapReduce job with three mappers and three reducers.\nIn most cases, the application code that should run in the map task is not yet present\non the machine that is assigned the task of running it, so the MapReduce framework\nfirst copies the code (e.g., JAR files in the case of a Java program) to the appropriate\nmachines. It then starts the map task and begins reading the input file, passing one\nrecord at a time to the mapper callback. The output of the mapper consists of key-\nvalue pairs.\nThe reduce side of the computation is also partitioned. While the number of map\ntasks is determined by the number of input file blocks, the number of reduce tasks is\nconfigured by the job author (it can be different from the number of map tasks). To\nensure that all key-value pairs with the same key end up at the same reducer, the\nframework uses a hash of the key to determine which reduce task should receive a\nparticular key-value pair (see \u201cPartitioning by Hash of Key\u201d on page 203).\nThe key-value pairs must be sorted, but the dataset is likely too large to be sorted with\na conventional sorting algorithm on a single machine. Instead, the sorting is per\u2010\nformed in stages. First, each map task partitions its output by reducer, based on the\nhash of the key. Each of these partitions is written to a sorted file on the mapper\u2019s\nlocal disk, using a technique similar to what we discussed in \u201cSSTables and LSM-\nTrees\u201d on page 76.\nMapReduce and Distributed Filesystems | 401", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13052a63-5af9-45aa-bb9f-961019e1cd87": {"__data__": {"id_": "13052a63-5af9-45aa-bb9f-961019e1cd87", "embedding": null, "metadata": {"page_label": "402", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4b40766-a5ea-486a-941c-f53d4e5c928f", "node_type": "4", "metadata": {"page_label": "402", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "811567d346a2d65139904b5b802001ad78f4324807abb4640023792fc442e947", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Whenever a mapper finishes reading its input file and writing its sorted output files,\nthe MapReduce scheduler notifies the reducers that they can start fetching the output\nfiles from that mapper. The reducers connect to each of the mappers and download\nthe files of sorted key-value pairs for their partition. The process of partitioning by\nreducer, sorting, and copying data partitions from mappers to reducers is known as\nthe shuffle [26] (a confusing term\u2014unlike shuffling a deck of cards, there is no ran\u2010\ndomness in MapReduce).\nThe reduce task takes the files from the mappers and merges them together, preserv\u2010\ning the sort order. Thus, if different mappers produced records with the same key,\nthey will be adjacent in the merged reducer input.\nThe reducer is called with a key and an iterator that incrementally scans over all\nrecords with the same key (which may in some cases not all fit in memory). The\nreducer can use arbitrary logic to process these records, and can generate any number\nof output records. These output records are written to a file on the distributed filesys\u2010\ntem (usually, one copy on the local disk of the machine running the reducer, with\nreplicas on other machines).\nMapReduce workflows\nThe range of problems you can solve with a single MapReduce job is limited. Refer\u2010\nring back to the log analysis example, a single MapReduce job could determine the\nnumber of page views per URL, but not the most popular URLs, since that requires a\nsecond round of sorting.\nThus, it is very common for MapReduce jobs to be chained together into workflows,\nsuch that the output of one job becomes the input to the next job. The Hadoop Map\u2010\nReduce framework does not have any particular support for workflows, so this chain\u2010\ning is done implicitly by directory name: the first job must be configured to write its\noutput to a designated directory in HDFS, and the second job must be configured to\nread that same directory name as its input. From the MapReduce framework\u2019s point\nof view, they are two independent jobs.\nChained MapReduce jobs are therefore less like pipelines of Unix commands (which\npass the output of one process as input to another process directly, using only a small\nin-memory buffer) and more like a sequence of commands where each command\u2019s\noutput is written to a temporary file, and the next command reads from the tempo\u2010\nrary file. This design has advantages and disadvantages, which we will discuss in\n\u201cMaterialization of Intermediate State\u201d on page 419.\nA batch job\u2019s output is only considered valid when the job has completed successfully\n(MapReduce discards the partial output of a failed job). Therefore, one job in a work\u2010\nflow can only start when the prior jobs\u2014that is, the jobs that produce its input direc\u2010\ntories\u2014have completed successfully. To handle these dependencies between job\n402 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2869, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97d643d7-8421-4827-b893-0a1f77d074a2": {"__data__": {"id_": "97d643d7-8421-4827-b893-0a1f77d074a2", "embedding": null, "metadata": {"page_label": "403", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd1239c-7778-443c-b15d-589a0124e1da", "node_type": "4", "metadata": {"page_label": "403", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "adaccd94a33e9d056b2ed958eda7b8d8024cb39e805a6312f8ea14452515c6cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "v. The joins we talk about in this book are generally equi-joins, the most common type of join, in which a\nrecord is associated with other records that have an identical value in a particular field (such as an ID). Some\ndatabases support more general types of joins, for example using a less-than operator instead of an equality\noperator, but we do not have space to cover them here.\nexecutions, various workflow schedulers for Hadoop have been developed, including\nOozie, Azkaban, Luigi, Airflow, and Pinball [28].\nThese schedulers also have management features that are useful when maintaining a\nlarge collection of batch jobs. Workflows consisting of 50 to 100 MapReduce jobs are\ncommon when building recommendation systems [ 29], and in a large organization,\nmany different teams may be running different jobs that read each other\u2019s output.\nTool support is important for managing such complex dataflows.\nVarious higher-level tools for Hadoop, such as Pig [ 30], Hive [ 31], Cascading [ 32],\nCrunch [ 33], and FlumeJava [ 34], also set up workflows of multiple MapReduce\nstages that are automatically wired together appropriately. \nReduce-Side Joins and Grouping\nWe discussed joins in Chapter 2 in the context of data models and query languages,\nbut we have not delved into how joins are actually implemented. It is time that we\npick up that thread again.\nIn many datasets it is common for one record to have an association with another\nrecord: a foreign key  in a relational model, a document reference  in a document\nmodel, or an edge in a graph model. A join is necessary whenever you have some\ncode that needs to access records on both sides of that association (both the record\nthat holds the reference and the record being referenced). As discussed in Chapter 2,\ndenormalization can reduce the need for joins but generally not remove it entirely.v\nIn a database, if you execute a query that involves only a small number of records, the\ndatabase will typically use an index to quickly locate the records of interest (see Chap\u2010\nter 3). If the query involves joins, it may require multiple index lookups. However,\nMapReduce has no concept of indexes\u2014at least not in the usual sense.\nWhen a MapReduce job is given a set of files as input, it reads the entire content of all\nof those files; a database would call this operation a full table scan. If you only want to\nread a small number of records, a full table scan is outrageously expensive compared\nto an index lookup. However, in analytic queries (see \u201cTransaction Processing or\nAnalytics?\u201d on page 90) it is common to want to calculate aggregates over a large\nnumber of records. In this case, scanning the entire input might be quite a reasonable\nthing to do, especially if you can parallelize the processing across multiple machines.\nMapReduce and Distributed Filesystems | 403", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7cb41ea-a9c9-4b4f-8631-aa80efe3783c": {"__data__": {"id_": "a7cb41ea-a9c9-4b4f-8631-aa80efe3783c", "embedding": null, "metadata": {"page_label": "404", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ccfaec2-a9b1-4d76-87f8-dae6ba181373", "node_type": "4", "metadata": {"page_label": "404", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c015acbf138d48f31c35767046f3cbdd0dd9f0bdcdce2069b7ff7bb3ad102625", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When we talk about joins in the context of batch processing, we mean resolving all\noccurrences of some association within a dataset. For example, we assume that a job\nis processing the data for all users simultaneously, not merely looking up the data for\none particular user (which would be done far more efficiently with an index).\nExample: analysis of user activity events\nA typical example of a join in a batch job is illustrated in Figure 10-2. On the left is a\nlog of events describing the things that logged-in users did on a website (known as\nactivity events or clickstream data), and on the right is a database of users. You can\nthink of this example as being part of a star schema (see \u201cStars and Snowflakes: Sche\u2010\nmas for Analytics\u201d on page 93): the log of events is the fact table, and the user data\u2010\nbase is one of the dimensions.\nFigure 10-2. A join between a log of user activity events and a database of user profiles.\nAn analytics task may need to correlate user activity with user profile information:\nfor example, if the profile contains the user\u2019s age or date of birth, the system could\ndetermine which pages are most popular with which age groups. However, the activ\u2010\nity events contain only the user ID, not the full user profile information. Embedding\nthat profile information in every single activity event would most likely be too waste\u2010\nful. Therefore, the activity events need to be joined with the user profile database.\nThe simplest implementation of this join would go over the activity events one by\none and query the user database (on a remote server) for every user ID it encounters.\nThis is possible, but it would most likely suffer from very poor performance: the pro\u2010\ncessing throughput would be limited by the round-trip time to the database server,\nthe effectiveness of a local cache would depend very much on the distribution of data,\nand running a large number of queries in parallel could easily overwhelm the data\u2010\nbase [35].\n404 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "556eeec3-fe84-4ccf-862a-8d1762daa777": {"__data__": {"id_": "556eeec3-fe84-4ccf-862a-8d1762daa777", "embedding": null, "metadata": {"page_label": "405", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fc83394-5da9-4f8a-84eb-0148832b3c55", "node_type": "4", "metadata": {"page_label": "405", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "77f916c1550efe2f793e5f1b08585433715167ffa8f36c7a3b7b1acfd9d22c4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to achieve good throughput in a batch process, the computation must be (as\nmuch as possible) local to one machine. Making random-access requests over the\nnetwork for every record you want to process is too slow. Moreover, querying a\nremote database would mean that the batch job becomes nondeterministic, because\nthe data in the remote database might change.\nThus, a better approach would be to take a copy of the user database (for example,\nextracted from a database backup using an ETL process\u2014see \u201cData Warehousing\u201d on\npage 91) and to put it in the same distributed filesystem as the log of user activity\nevents. You would then have the user database in one set of files in HDFS and the\nuser activity records in another set of files, and could use MapReduce to bring\ntogether all of the relevant records in the same place and process them efficiently.\nSort-merge joins\nRecall that the purpose of the mapper is to extract a key and value from each input\nrecord. In the case of Figure 10-2, this key would be the user ID: one set of mappers\nwould go over the activity events (extracting the user ID as the key and the activity\nevent as the value), while another set of mappers would go over the user database\n(extracting the user ID as the key and the user\u2019s date of birth as the value). This pro\u2010\ncess is illustrated in Figure 10-3.\nFigure 10-3. A reduce-side sort-merge join on user ID. If the input datasets are parti\u2010\ntioned into multiple files, each could be processed with multiple mappers in parallel.\nWhen the MapReduce framework partitions the mapper output by key and then sorts\nthe key-value pairs, the effect is that all the activity events and the user record with\nthe same user ID become adjacent to each other in the reducer input. The Map\u2010\nReduce job can even arrange the records to be sorted such that the reducer always\nMapReduce and Distributed Filesystems | 405", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1891, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0d440cb-b701-4c96-8515-1982e0b47d61": {"__data__": {"id_": "a0d440cb-b701-4c96-8515-1982e0b47d61", "embedding": null, "metadata": {"page_label": "406", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2de1ac1-0d16-453e-841a-20d054c85f8b", "node_type": "4", "metadata": {"page_label": "406", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7185948a52310f20305b3b67b8037f43df149ff956507c34726f123fa4f1ae46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "sees the record from the user database first, followed by the activity events in time\u2010\nstamp order\u2014this technique is known as a secondary sort [26].\nThe reducer can then perform the actual join logic easily: the reducer function is\ncalled once for every user ID, and thanks to the secondary sort, the first value is\nexpected to be the date-of-birth record from the user database. The reducer stores the\ndate of birth in a local variable and then iterates over the activity events with the same\nuser ID, outputting pairs of viewed-url and viewer-age-in-years. Subsequent Map\u2010\nReduce jobs could then calculate the distribution of viewer ages for each URL, and\ncluster by age group.\nSince the reducer processes all of the records for a particular user ID in one go, it only\nneeds to keep one user record in memory at any one time, and it never needs to make\nany requests over the network. This algorithm is known as a sort-merge join , since\nmapper output is sorted by key, and the reducers then merge together the sorted lists\nof records from both sides of the join.\nBringing related data together in the same place\nIn a sort-merge join, the mappers and the sorting process make sure that all the nec\u2010\nessary data to perform the join operation for a particular user ID is brought together\nin the same place: a single call to the reducer. Having lined up all the required data in\nadvance, the reducer can be a fairly simple, single-threaded piece of code that can\nchurn through records with high throughput and low memory overhead.\nOne way of looking at this architecture is that mappers \u201csend messages\u201d to the reduc\u2010\ners. When a mapper emits a key-value pair, the key acts like the destination address\nto which the value should be delivered. Even though the key is just an arbitrary string\n(not an actual network address like an IP address and port number), it behaves like\nan address: all key-value pairs with the same key will be delivered to the same desti\u2010\nnation (a call to the reducer).\nUsing the MapReduce programming model has separated the physical network com\u2010\nmunication aspects of the computation (getting the data to the right machine) from\nthe application logic (processing the data once you have it). This separation contrasts\nwith the typical use of databases, where a request to fetch data from a database often\noccurs somewhere deep inside a piece of application code [ 36]. Since MapReduce\nhandles all network communication, it also shields the application code from having\nto worry about partial failures, such as the crash of another node: MapReduce trans\u2010\nparently retries failed tasks without affecting the application logic.\nGROUP BY\nBesides joins, another common use of the \u201cbringing related data to the same place\u201d\npattern is grouping records by some key (as in the GROUP BY clause in SQL). All\n406 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2853, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e58aeaa-8a6a-406a-bd5b-adaa0e9207a8": {"__data__": {"id_": "6e58aeaa-8a6a-406a-bd5b-adaa0e9207a8", "embedding": null, "metadata": {"page_label": "407", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a800053-18d3-4b3f-9ae6-2b083f34a392", "node_type": "4", "metadata": {"page_label": "407", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "872a77c360a31922ecee060581de4515649f55bb5e07de78e86f2dbed3dea920", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "records with the same key form a group, and the next step is often to perform some\nkind of aggregation within each group\u2014for example:\n\u2022 Counting the number of records in each group (like in our example of counting\npage views, which you would express as a COUNT(*) aggregation in SQL)\n\u2022 Adding up the values in one particular field (SUM(fieldname)) in SQL\n\u2022 Picking the top k records according to some ranking function\nThe simplest way of implementing such a grouping operation with MapReduce is to\nset up the mappers so that the key-value pairs they produce use the desired grouping\nkey. The partitioning and sorting process then brings together all the records with the\nsame key in the same reducer. Thus, grouping and joining look quite similar when\nimplemented on top of MapReduce.\nAnother common use for grouping is collating all the activity events for a particular\nuser session, in order to find out the sequence of actions that the user took\u2014a pro\u2010\ncess called sessionization [37]. For example, such analysis could be used to work out\nwhether users who were shown a new version of your website are more likely to make\na purchase than those who were shown the old version (A/B testing), or to calculate\nwhether some marketing activity is worthwhile.\nIf you have multiple web servers handling user requests, the activity events for a par\u2010\nticular user are most likely scattered across various different servers\u2019 log files. You can\nimplement sessionization by using a session cookie, user ID, or similar identifier as\nthe grouping key and bringing all the activity events for a particular user together in\none place, while distributing different users\u2019 events across different partitions.\nHandling skew\nThe pattern of \u201cbringing all records with the same key to the same place\u201d breaks\ndown if there is a very large amount of data related to a single key. For example, in a\nsocial network, most users might be connected to a few hundred people, but a small\nnumber of celebrities may have many millions of followers. Such disproportionately\nactive database records are known as linchpin objects [38] or hot keys.\nCollecting all activity related to a celebrity (e.g., replies to something they posted) in a\nsingle reducer can lead to significant skew (also known as hot spots )\u2014that is, one\nreducer that must process significantly more records than the others (see \u201cSkewed\nWorkloads and Relieving Hot Spots\u201d on page 205). Since a MapReduce job is only\ncomplete when all of its mappers and reducers have completed, any subsequent jobs\nmust wait for the slowest reducer to complete before they can start.\nIf a join input has hot keys, there are a few algorithms you can use to compensate.\nFor example, the skewed join method in Pig first runs a sampling job to determine\nwhich keys are hot [ 39]. When performing the actual join, the mappers send any\nMapReduce and Distributed Filesystems | 407", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b458f0e-76d5-49c2-a651-dadd129a24e4": {"__data__": {"id_": "3b458f0e-76d5-49c2-a651-dadd129a24e4", "embedding": null, "metadata": {"page_label": "408", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a713f84e-7055-42f0-a623-2ff9653d3ed4", "node_type": "4", "metadata": {"page_label": "408", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f66669805e83c8ff54313d39604e8918535edee8e11a4eee16ed49c8f7a1c492", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "records relating to a hot key to one of several reducers, chosen at random (in contrast\nto conventional MapReduce, which chooses a reducer deterministically based on a\nhash of the key). For the other input to the join, records relating to the hot key need\nto be replicated to all reducers handling that key [40].\nThis technique spreads the work of handling the hot key over several reducers, which\nallows it to be parallelized better, at the cost of having to replicate the other join input\nto multiple reducers. The sharded join method in Crunch is similar, but requires the\nhot keys to be specified explicitly rather than using a sampling job. This technique is\nalso very similar to one we discussed in \u201cSkewed Workloads and Relieving Hot\nSpots\u201d on page 205, using randomization to alleviate hot spots in a partitioned data\u2010\nbase.\nHive\u2019s skewed join optimization takes an alternative approach. It requires hot keys to\nbe specified explicitly in the table metadata, and it stores records related to those keys\nin separate files from the rest. When performing a join on that table, it uses a map-\nside join (see the next section) for the hot keys.\nWhen grouping records by a hot key and aggregating them, you can perform the\ngrouping in two stages. The first MapReduce stage sends records to a random\nreducer, so that each reducer performs the grouping on a subset of records for the\nhot key and outputs a more compact aggregated value per key. The second Map\u2010\nReduce job then combines the values from all of the first-stage reducers into a single\nvalue per key. \nMap-Side Joins\nThe join algorithms described in the last section perform the actual join logic in the\nreducers, and are hence known as reduce-side joins. The mappers take the role of pre\u2010\nparing the input data: extracting the key and value from each input record, assigning\nthe key-value pairs to a reducer partition, and sorting by key.\nThe reduce-side approach has the advantage that you do not need to make any\nassumptions about the input data: whatever its properties and structure, the mappers\ncan prepare the data to be ready for joining. However, the downside is that all that\nsorting, copying to reducers, and merging of reducer inputs can be quite expensive.\nDepending on the available memory buffers, data may be written to disk several\ntimes as it passes through the stages of MapReduce [37].\nOn the other hand, if you can make certain assumptions about your input data, it is\npossible to make joins faster by using a so-called map-side join. This approach uses a\ncut-down MapReduce job in which there are no reducers and no sorting. Instead,\neach mapper simply reads one input file block from the distributed filesystem and\nwrites one output file to the filesystem\u2014that is all.\n408 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac576e8f-b787-4ff3-bd54-ca75ee8d8d0f": {"__data__": {"id_": "ac576e8f-b787-4ff3-bd54-ca75ee8d8d0f", "embedding": null, "metadata": {"page_label": "409", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9b1cc965-6768-4867-9a27-cfc85f41de16", "node_type": "4", "metadata": {"page_label": "409", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7c78d9a6eda7080f1787dc052ba5d61c1850d063f37dd371ad1d032c7e112da6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vi. This example assumes that there is exactly one entry for each key in the hash table, which is probably true\nwith a user database (a user ID uniquely identifies a user). In general, the hash table may need to contain\nseveral entries with the same key, and the join operator will output all matches for a key.\nBroadcast hash joins\nThe simplest way of performing a map-side join applies in the case where a large\ndataset is joined with a small dataset. In particular, the small dataset needs to be small\nenough that it can be loaded entirely into memory in each of the mappers.\nFor example, imagine in the case of Figure 10-2  that the user database is small\nenough to fit in memory. In this case, when a mapper starts up, it can first read the\nuser database from the distributed filesystem into an in-memory hash table. Once\nthis is done, the mapper can scan over the user activity events and simply look up the\nuser ID for each event in the hash table.vi\nThere can still be several map tasks: one for each file block of the large input to the\njoin (in the example of Figure 10-2, the activity events are the large input). Each of\nthese mappers loads the small input entirely into memory.\nThis simple but effective algorithm is called a broadcast hash join: the word broadcast\nreflects the fact that each mapper for a partition of the large input reads the entirety\nof the small input (so the small input is effectively \u201cbroadcast\u201d to all partitions of the\nlarge input), and the word hash reflects its use of a hash table. This join method is\nsupported by Pig (under the name \u201creplicated join\u201d), Hive (\u201cMapJoin\u201d), Cascading,\nand Crunch. It is also used in data warehouse query engines such as Impala [41].\nInstead of loading the small join input into an in-memory hash table, an alternative is\nto store the small join input in a read-only index on the local disk [ 42]. The fre\u2010\nquently used parts of this index will remain in the operating system\u2019s page cache, so\nthis approach can provide random-access lookups almost as fast as an in-memory\nhash table, but without actually requiring the dataset to fit in memory.\nPartitioned hash joins\nIf the inputs to the map-side join are partitioned in the same way, then the hash join\napproach can be applied to each partition independently. In the case of Figure 10-2,\nyou might arrange for the activity events and the user database to each be partitioned\nbased on the last decimal digit of the user ID (so there are 10 partitions on either\nside). For example, mapper 3 first loads all users with an ID ending in 3 into a hash\ntable, and then scans over all the activity events for each user whose ID ends in 3.\nIf the partitioning is done correctly, you can be sure that all the records you might\nwant to join are located in the same numbered partition, and so it is sufficient for\neach mapper to only read one partition from each of the input datasets. This has the\nadvantage that each mapper can load a smaller amount of data into its hash table.\nMapReduce and Distributed Filesystems | 409", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7934df17-32bf-4175-83d0-d97a66c8d31a": {"__data__": {"id_": "7934df17-32bf-4175-83d0-d97a66c8d31a", "embedding": null, "metadata": {"page_label": "410", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bd41b34-a1d2-49a6-b1b8-83ea4323ba5d", "node_type": "4", "metadata": {"page_label": "410", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e5964ac380f2d8cae2a724b013e7dd19d017e6843785565f3ef747898a4a15e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This approach only works if both of the join\u2019s inputs have the same number of parti\u2010\ntions, with records assigned to partitions based on the same key and the same hash\nfunction. If the inputs are generated by prior MapReduce jobs that already perform\nthis grouping, then this can be a reasonable assumption to make.\nPartitioned hash joins are known as bucketed map joins in Hive [37].\nMap-side merge joins\nAnother variant of a map-side join applies if the input datasets are not only parti\u2010\ntioned in the same way, but also sorted based on the same key. In this case, it does not\nmatter whether the inputs are small enough to fit in memory, because a mapper can\nperform the same merging operation that would normally be done by a reducer:\nreading both input files incrementally, in order of ascending key, and matching\nrecords with the same key.\nIf a map-side merge join is possible, it probably means that prior MapReduce jobs\nbrought the input datasets into this partitioned and sorted form in the first place. In\nprinciple, this join could have been performed in the reduce stage of the prior job.\nHowever, it may still be appropriate to perform the merge join in a separate map-\nonly job, for example if the partitioned and sorted datasets are also needed for other\npurposes besides this particular join.\nMapReduce workflows with map-side joins\nWhen the output of a MapReduce join is consumed by downstream jobs, the choice\nof map-side or reduce-side join affects the structure of the output. The output of a\nreduce-side join is partitioned and sorted by the join key, whereas the output of a\nmap-side join is partitioned and sorted in the same way as the large input (since one\nmap task is started for each file block of the join\u2019s large input, regardless of whether a\npartitioned or broadcast join is used).\nAs discussed, map-side joins also make more assumptions about the size, sorting, and\npartitioning of their input datasets. Knowing about the physical layout of datasets in\nthe distributed filesystem becomes important when optimizing join strategies: it is\nnot sufficient to just know the encoding format and the name of the directory in\nwhich the data is stored; you must also know the number of partitions and the keys\nby which the data is partitioned and sorted.\nIn the Hadoop ecosystem, this kind of metadata about the partitioning of datasets is\noften maintained in HCatalog and the Hive metastore [37]. \n410 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7036e587-75ea-4984-b7d2-8ca4613c5a27": {"__data__": {"id_": "7036e587-75ea-4984-b7d2-8ca4613c5a27", "embedding": null, "metadata": {"page_label": "411", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7a5009b-c4a1-420b-8a9c-e3ca17a7bba4", "node_type": "4", "metadata": {"page_label": "411", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "98ce2ebc9fb6ee7fa73a845d64838b558767eeedb6440820a93c7e0f5f85f974", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Output of Batch Workflows\nWe have talked a lot about the various algorithms for implementing workflows of\nMapReduce jobs, but we neglected an important question: what is the result of all of\nthat processing, once it is done? Why are we running all these jobs in the first place?\nIn the case of database queries, we distinguished transaction processing (OLTP) pur\u2010\nposes from analytic purposes (see \u201cTransaction Processing or Analytics?\u201d on page\n90). We saw that OLTP queries generally look up a small number of records by key,\nusing indexes, in order to present them to a user (for example, on a web page). On\nthe other hand, analytic queries often scan over a large number of records, perform\u2010\ning groupings and aggregations, and the output often has the form of a report: a\ngraph showing the change in a metric over time, or the top 10 items according to\nsome ranking, or a breakdown of some quantity into subcategories. The consumer of\nsuch a report is often an analyst or a manager who needs to make business decisions.\nWhere does batch processing fit in? It is not transaction processing, nor is it analyt\u2010\nics. It is closer to analytics, in that a batch process typically scans over large portions\nof an input dataset. However, a workflow of MapReduce jobs is not the same as a\nSQL query used for analytic purposes (see \u201cComparing Hadoop to Distributed Data\u2010\nbases\u201d on page 414). The output of a batch process is often not a report, but some\nother kind of structure.\nBuilding search indexes\nGoogle\u2019s original use of MapReduce was to build indexes for its search engine, which\nwas implemented as a workflow of 5 to 10 MapReduce jobs [ 1]. Although Google\nlater moved away from using MapReduce for this purpose [ 43], it helps to under\u2010\nstand MapReduce if you look at it through the lens of building a search index. (Even\ntoday, Hadoop MapReduce remains a good way of building indexes for Lucene/Solr\n[44].)\nWe saw briefly in \u201cFull-text search and fuzzy indexes\u201d on page 88 how a full-text\nsearch index such as Lucene works: it is a file (the term dictionary) in which you can\nefficiently look up a particular keyword and find the list of all the document IDs con\u2010\ntaining that keyword (the postings list). This is a very simplified view of a search\nindex\u2014in reality it requires various additional data, in order to rank search results by\nrelevance, correct misspellings, resolve synonyms, and so on\u2014but the principle\nholds.\nIf you need to perform a full-text search over a fixed set of documents, then a batch\nprocess is a very effective way of building the indexes: the mappers partition the set of\ndocuments as needed, each reducer builds the index for its partition, and the index\nfiles are written to the distributed filesystem. Building such document-partitioned\nindexes (see \u201cPartitioning and Secondary Indexes\u201d on page 206) parallelizes very well.\nMapReduce and Distributed Filesystems | 411", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2907, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cd87e09a-5461-405e-a0a4-c6dc70bbd0c6": {"__data__": {"id_": "cd87e09a-5461-405e-a0a4-c6dc70bbd0c6", "embedding": null, "metadata": {"page_label": "412", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddec3343-bef6-4269-b65e-e767294dc820", "node_type": "4", "metadata": {"page_label": "412", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "48608f57358edd5fc835b61bc79194d1c66ed5671a87fe4c6176fcde37a2a653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since querying a search index by keyword is a read-only operation, these index files\nare immutable once they have been created.\nIf the indexed set of documents changes, one option is to periodically rerun the entire\nindexing workflow for the entire set of documents, and replace the previous index\nfiles wholesale with the new index files when it is done. This approach can be compu\u2010\ntationally expensive if only a small number of documents have changed, but it has the\nadvantage that the indexing process is very easy to reason about: documents in,\nindexes out.\nAlternatively, it is possible to build indexes incrementally. As discussed in Chapter 3,\nif you want to add, remove, or update documents in an index, Lucene writes out new\nsegment files and asynchronously merges and compacts segment files in the back\u2010\nground. We will see more on such incremental processing in Chapter 11.\nKey-value stores as batch process output\nSearch indexes are just one example of the possible outputs of a batch processing\nworkflow. Another common use for batch processing is to build machine learning\nsystems such as classifiers (e.g., spam filters, anomaly detection, image recognition)\nand recommendation systems (e.g., people you may know, products you may be\ninterested in, or related searches [29]).\nThe output of those batch jobs is often some kind of database: for example, a data\u2010\nbase that can be queried by user ID to obtain suggested friends for that user, or a\ndatabase that can be queried by product ID to get a list of related products [45].\nThese databases need to be queried from the web application that handles user\nrequests, which is usually separate from the Hadoop infrastructure. So how does the\noutput from the batch process get back into a database where the web application can\nquery it?\nThe most obvious choice might be to use the client library for your favorite database\ndirectly within a mapper or reducer, and to write from the batch job directly to the\ndatabase server, one record at a time. This will work (assuming your firewall rules\nallow direct access from your Hadoop environment to your production databases),\nbut it is a bad idea for several reasons:\n\u2022 As discussed previously in the context of joins, making a network request for\nevery single record is orders of magnitude slower than the normal throughput of\na batch task. Even if the client library supports batching, performance is likely to\nbe poor.\n\u2022 MapReduce jobs often run many tasks in parallel. If all the mappers or reducers\nconcurrently write to the same output database, with a rate expected of a batch\nprocess, that database can easily be overwhelmed, and its performance for quer\u2010\n412 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddfd46c7-a179-427c-b09d-2f8914c12272": {"__data__": {"id_": "ddfd46c7-a179-427c-b09d-2f8914c12272", "embedding": null, "metadata": {"page_label": "413", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b142e80-6c3c-4b3f-88f7-2b38b6d9f797", "node_type": "4", "metadata": {"page_label": "413", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "10896a32a27c6784a62b4d0328920ee115a4aedd60caf341c006459aec24ef17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ies is likely to suffer. This can in turn cause operational problems in other parts\nof the system [35].\n\u2022 Normally, MapReduce provides a clean all-or-nothing guarantee for job output:\nif a job succeeds, the result is the output of running every task exactly once, even\nif some tasks failed and had to be retried along the way; if the entire job fails, no\noutput is produced. However, writing to an external system from inside a job\nproduces externally visible side effects that cannot be hidden in this way. Thus,\nyou have to worry about the results from partially completed jobs being visible to\nother systems, and the complexities of Hadoop task attempts and speculative\nexecution.\nA much better solution is to build a brand-new database inside the batch job and\nwrite it as files to the job\u2019s output directory in the distributed filesystem, just like the\nsearch indexes in the last section. Those data files are then immutable once written,\nand can be loaded in bulk into servers that handle read-only queries. Various key-\nvalue stores support building database files in MapReduce jobs, including Voldemort\n[46], Terrapin [47], ElephantDB [48], and HBase bulk loading [49].\nBuilding these database files is a good use of MapReduce: using a mapper to extract a\nkey and then sorting by that key is already a lot of the work required to build an\nindex. Since most of these key-value stores are read-only (the files can only be written\nonce by a batch job and are then immutable), the data structures are quite simple. For\nexample, they do not require a WAL (see \u201cMaking B-trees reliable\u201d on page 82).\nWhen loading data into Voldemort, the server continues serving requests to the old\ndata files while the new data files are copied from the distributed filesystem to the\nserver\u2019s local disk. Once the copying is complete, the server atomically switches over\nto querying the new files. If anything goes wrong in this process, it can easily switch\nback to the old files again, since they are still there and immutable [46]. \nPhilosophy of batch process outputs\nThe Unix philosophy that we discussed earlier in this chapter ( \u201cThe Unix Philoso\u2010\nphy\u201d on page 394) encourages experimentation by being very explicit about dataflow:\na program reads its input and writes its output. In the process, the input is left\nunchanged, any previous output is completely replaced with the new output, and\nthere are no other side effects. This means that you can rerun a command as often as\nyou like, tweaking or debugging it, without messing up the state of your system.\nThe handling of output from MapReduce jobs follows the same philosophy. By treat\u2010\ning inputs as immutable and avoiding side effects (such as writing to external data\u2010\nbases), batch jobs not only achieve good performance but also become much easier to\nmaintain:\nMapReduce and Distributed Filesystems | 413", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "140882aa-ffda-4d6d-a5ac-529bccd29869": {"__data__": {"id_": "140882aa-ffda-4d6d-a5ac-529bccd29869", "embedding": null, "metadata": {"page_label": "414", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "162430e6-5ea3-4f9a-85f4-5d89a5385c8e", "node_type": "4", "metadata": {"page_label": "414", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ea36c17d30f790f075d4558fff6925702e54498e6a6b2b15947cc15101c57a97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If you introduce a bug into the code and the output is wrong or corrupted, you\ncan simply roll back to a previous version of the code and rerun the job, and the\noutput will be correct again. Or, even simpler, you can keep the old output in a\ndifferent directory and simply switch back to it. Databases with read-write trans\u2010\nactions do not have this property: if you deploy buggy code that writes bad data\nto the database, then rolling back the code will do nothing to fix the data in the\ndatabase. (The idea of being able to recover from buggy code has been called\nhuman fault tolerance [50].)\n\u2022 As a consequence of this ease of rolling back, feature development can proceed\nmore quickly than in an environment where mistakes could mean irreversible\ndamage. This principle of minimizing irreversibility  is beneficial for Agile soft\u2010\nware development [51].\n\u2022 If a map or reduce task fails, the MapReduce framework automatically re-\nschedules it and runs it again on the same input. If the failure is due to a bug in\nthe code, it will keep crashing and eventually cause the job to fail after a few\nattempts; but if the failure is due to a transient issue, the fault is tolerated. This\nautomatic retry is only safe because inputs are immutable and outputs from\nfailed tasks are discarded by the MapReduce framework.\n\u2022 The same set of files can be used as input for various different jobs, including\nmonitoring jobs that calculate metrics and evaluate whether a job\u2019s output has\nthe expected characteristics (for example, by comparing it to the output from the\nprevious run and measuring discrepancies).\n\u2022 Like Unix tools, MapReduce jobs separate logic from wiring (configuring the\ninput and output directories), which provides a separation of concerns and ena\u2010\nbles potential reuse of code: one team can focus on implementing a job that does\none thing well, while other teams can decide where and when to run that job.\nIn these areas, the design principles that worked well for Unix also seem to be work\u2010\ning well for Hadoop\u2014but Unix and Hadoop also differ in some ways. For example,\nbecause most Unix tools assume untyped text files, they have to do a lot of input\nparsing (our log analysis example at the beginning of the chapter used {print $7} to\nextract the URL). On Hadoop, some of those low-value syntactic conversions are\neliminated by using more structured file formats: Avro (see \u201cAvro\u201d on page 122) and\nParquet (see \u201cColumn-Oriented Storage\u201d on page 95) are often used, as they provide\nefficient schema-based encoding and allow evolution of their schemas over time (see\nChapter 4). \nComparing Hadoop to Distributed Databases\nAs we have seen, Hadoop is somewhat like a distributed version of Unix, where\nHDFS is the filesystem and MapReduce is a quirky implementation of a Unix process\n414 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7bf33fbd-e388-440b-99a6-6ff3f3bf58a9": {"__data__": {"id_": "7bf33fbd-e388-440b-99a6-6ff3f3bf58a9", "embedding": null, "metadata": {"page_label": "415", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "141c6cc9-ab51-4ded-8fe2-cac73bc41f75", "node_type": "4", "metadata": {"page_label": "415", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "05d353251cddec3c92833327e8e2dc054d274d1924b572985c057c0bc8a2a39d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(which happens to always run the sort utility between the map phase and the reduce\nphase). We saw how you can implement various join and grouping operations on top\nof these primitives.\nWhen the MapReduce paper [ 1] was published, it was\u2014in some sense\u2014not at all\nnew. All of the processing and parallel join algorithms that we discussed in the last\nfew sections had already been implemented in so-called massively parallel processing\n(MPP) databases more than a decade previously [ 3, 40]. For example, the Gamma\ndatabase machine, Teradata, and Tandem NonStop SQL were pioneers in this area\n[52].\nThe biggest difference is that MPP databases focus on parallel execution of analytic\nSQL queries on a cluster of machines, while the combination of MapReduce and a\ndistributed filesystem [ 19] provides something much more like a general-purpose\noperating system that can run arbitrary programs.\nDiversity of storage\nDatabases require you to structure data according to a particular model (e.g., rela\u2010\ntional or documents), whereas files in a distributed filesystem are just byte sequences,\nwhich can be written using any data model and encoding. They might be collections\nof database records, but they can equally well be text, images, videos, sensor readings,\nsparse matrices, feature vectors, genome sequences, or any other kind of data.\nTo put it bluntly, Hadoop opened up the possibility of indiscriminately dumping data\ninto HDFS, and only later figuring out how to process it further [ 53]. By contrast,\nMPP databases typically require careful up-front modeling of the data and query pat\u2010\nterns before importing the data into the database\u2019s proprietary storage format.\nFrom a purist\u2019s point of view, it may seem that this careful modeling and import is\ndesirable, because it means users of the database have better-quality data to work\nwith. However, in practice, it appears that simply making data available quickly\u2014\neven if it is in a quirky, difficult-to-use, raw format\u2014is often more valuable than try\u2010\ning to decide on the ideal data model up front [54].\nThe idea is similar to a data warehouse (see \u201cData Warehousing\u201d on page 91): simply\nbringing data from various parts of a large organization together in one place is val\u2010\nuable, because it enables joins across datasets that were previously disparate. The\ncareful schema design required by an MPP database slows down that centralized data\ncollection; collecting data in its raw form, and worrying about schema design later,\nallows the data collection to be speeded up (a concept sometimes known as a \u201cdata\nlake\u201d or \u201centerprise data hub\u201d [55]).\nIndiscriminate data dumping shifts the burden of interpreting the data: instead of\nforcing the producer of a dataset to bring it into a standardized format, the interpre\u2010\ntation of the data becomes the consumer\u2019s problem (the schema-on-read approach\nMapReduce and Distributed Filesystems | 415", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e87f6f78-accf-457c-ae9e-131b41f98e0c": {"__data__": {"id_": "e87f6f78-accf-457c-ae9e-131b41f98e0c", "embedding": null, "metadata": {"page_label": "416", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a513c3fe-3f32-42db-b7bd-f3d23a9fc402", "node_type": "4", "metadata": {"page_label": "416", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a17a74ab992da7aed738e22643aed0950bd6e12a22413b88e621fe94a29b3399", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[56]; see \u201cSchema flexibility in the document model\u201d on page 39). This can be an\nadvantage if the producer and consumers are different teams with different priorities.\nThere may not even be one ideal data model, but rather different views onto the data\nthat are suitable for different purposes. Simply dumping data in its raw form allows\nfor several such transformations. This approach has been dubbed the sushi principle:\n\u201craw data is better\u201d [57].\nThus, Hadoop has often been used for implementing ETL processes (see \u201cData Ware\u2010\nhousing\u201d on page 91): data from transaction processing systems is dumped into the\ndistributed filesystem in some raw form, and then MapReduce jobs are written to\nclean up that data, transform it into a relational form, and import it into an MPP data\nwarehouse for analytic purposes. Data modeling still happens, but it is in a separate\nstep, decoupled from the data collection. This decoupling is possible because a dis\u2010\ntributed filesystem supports data encoded in any format.\nDiversity of processing models\nMPP databases are monolithic, tightly integrated pieces of software that take care of\nstorage layout on disk, query planning, scheduling, and execution. Since these com\u2010\nponents can all be tuned and optimized for the specific needs of the database, the sys\u2010\ntem as a whole can achieve very good performance on the types of queries for which\nit is designed. Moreover, the SQL query language allows expressive queries and ele\u2010\ngant semantics without the need to write code, making it accessible to graphical tools\nused by business analysts (such as Tableau).\nOn the other hand, not all kinds of processing can be sensibly expressed as SQL quer\u2010\nies. For example, if you are building machine learning and recommendation systems,\nor full-text search indexes with relevance ranking models, or performing image anal\u2010\nysis, you most likely need a more general model of data processing. These kinds of\nprocessing are often very specific to a particular application (e.g., feature engineering\nfor machine learning, natural language models for machine translation, risk estima\u2010\ntion functions for fraud prediction), so they inevitably require writing code, not just\nqueries.\nMapReduce gave engineers the ability to easily run their own code over large data\u2010\nsets. If you have HDFS and MapReduce, you can build a SQL query execution engine\non top of it, and indeed this is what the Hive project did [ 31]. However, you can also\nwrite many other forms of batch processes that do not lend themselves to being\nexpressed as a SQL query.\nSubsequently, people found that MapReduce was too limiting and performed too\nbadly for some types of processing, so various other processing models were devel\u2010\noped on top of Hadoop (we will see some of them in \u201cBeyond MapReduce\u201d on page\n419). Having two processing models, SQL and MapReduce, was not enough: even\nmore different models were needed! And due to the openness of the Hadoop plat\u2010\n416 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "39f3e16d-771f-42f1-9d2a-af6eacd98d2b": {"__data__": {"id_": "39f3e16d-771f-42f1-9d2a-af6eacd98d2b", "embedding": null, "metadata": {"page_label": "417", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1a3367d-07f0-4546-84da-b64d32f5628d", "node_type": "4", "metadata": {"page_label": "417", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dcd8cbcc6c7657a9b0e5023a810688b819897510e48d13af57dd9f2ada0cc8c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "form, it was feasible to implement a whole range of approaches, which would not\nhave been possible within the confines of a monolithic MPP database [58].\nCrucially, those various processing models can all be run on a single shared-use clus\u2010\nter of machines, all accessing the same files on the distributed filesystem. In the\nHadoop approach, there is no need to import the data into several different special\u2010\nized systems for different kinds of processing: the system is flexible enough to sup\u2010\nport a diverse set of workloads within the same cluster. Not having to move data\naround makes it a lot easier to derive value from the data, and a lot easier to experi\u2010\nment with new processing models.\nThe Hadoop ecosystem includes both random-access OLTP databases such as HBase\n(see \u201cSSTables and LSM-Trees\u201d on page 76) and MPP-style analytic databases such as\nImpala [ 41]. Neither HBase nor Impala uses MapReduce, but both use HDFS for\nstorage. They are very different approaches to accessing and processing data, but they\ncan nevertheless coexist and be integrated in the same system.\nDesigning for frequent faults\nWhen comparing MapReduce to MPP databases, two more differences in design\napproach stand out: the handling of faults and the use of memory and disk. Batch\nprocesses are less sensitive to faults than online systems, because they do not immedi\u2010\nately affect users if they fail and they can always be run again.\nIf a node crashes while a query is executing, most MPP databases abort the entire\nquery, and either let the user resubmit the query or automatically run it again [ 3]. As\nqueries normally run for a few seconds or a few minutes at most, this way of handling\nerrors is acceptable, since the cost of retrying is not too great. MPP databases also\nprefer to keep as much data as possible in memory (e.g., using hash joins) to avoid\nthe cost of reading from disk.\nOn the other hand, MapReduce can tolerate the failure of a map or reduce task\nwithout it affecting the job as a whole by retrying work at the granularity of an indi\u2010\nvidual task. It is also very eager to write data to disk, partly for fault tolerance, and\npartly on the assumption that the dataset will be too big to fit in memory anyway.\nThe MapReduce approach is more appropriate for larger jobs: jobs that process so\nmuch data and run for such a long time that they are likely to experience at least one\ntask failure along the way. In that case, rerunning the entire job due to a single task\nfailure would be wasteful. Even if recovery at the granularity of an individual task\nintroduces overheads that make fault-free processing slower, it can still be a reason\u2010\nable trade-off if the rate of task failures is high enough.\nBut how realistic are these assumptions? In most clusters, machine failures do occur,\nbut they are not very frequent\u2014probably rare enough that most jobs will not experi\u2010\nMapReduce and Distributed Filesystems | 417", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0fda21f-51fb-4cd6-b2f0-7f18c703df39": {"__data__": {"id_": "e0fda21f-51fb-4cd6-b2f0-7f18c703df39", "embedding": null, "metadata": {"page_label": "418", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f04e1ae7-0130-4947-927f-b0daf049078d", "node_type": "4", "metadata": {"page_label": "418", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b16f485317a5445ab6c615fbe0a3453ec0f73882829b3f38ebb7ff0269e11fc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ence a machine failure. Is it really worth incurring significant overheads for the sake\nof fault tolerance?\nTo understand the reasons for MapReduce\u2019s sparing use of memory and task-level\nrecovery, it is helpful to look at the environment for which MapReduce was originally\ndesigned. Google has mixed-use datacenters, in which online production services and\noffline batch jobs run on the same machines. Every task has a resource allocation\n(CPU cores, RAM, disk space, etc.) that is enforced using containers. Every task also\nhas a priority, and if a higher-priority task needs more resources, lower-priority tasks\non the same machine can be terminated (preempted) in order to free up resources.\nPriority also determines pricing of the computing resources: teams must pay for the\nresources they use, and higher-priority processes cost more [59].\nThis architecture allows non-production (low-priority) computing resources to be\novercommitted, because the system knows that it can reclaim the resources if neces\u2010\nsary. Overcommitting resources in turn allows better utilization of machines and\ngreater efficiency compared to systems that segregate production and non-\nproduction tasks. However, as MapReduce jobs run at low priority, they run the risk\nof being preempted at any time because a higher-priority process requires their\nresources. Batch jobs effectively \u201cpick up the scraps under the table,\u201d using any com\u2010\nputing resources that remain after the high-priority processes have taken what they\nneed.\nAt Google, a MapReduce task that runs for an hour has an approximately 5% risk of\nbeing terminated to make space for a higher-priority process. This rate is more than\nan order of magnitude higher than the rate of failures due to hardware issues,\nmachine reboot, or other reasons [ 59]. At this rate of preemptions, if a job has 100\ntasks that each run for 10 minutes, there is a risk greater than 50% that at least one\ntask will be terminated before it is finished.\nAnd this is why MapReduce is designed to tolerate frequent unexpected task termina\u2010\ntion: it\u2019s not because the hardware is particularly unreliable, it\u2019s because the freedom\nto arbitrarily terminate processes enables better resource utilization in a computing\ncluster.\nAmong open source cluster schedulers, preemption is less widely used. YARN\u2019s\nCapacityScheduler supports preemption for balancing the resource allocation of dif\u2010\nferent queues [ 58], but general priority preemption is not supported in YARN,\nMesos, or Kubernetes at the time of writing [ 60]. In an environment where tasks are\nnot so often terminated, the design decisions of MapReduce make less sense. In the\nnext section, we will look at some alternatives to MapReduce that make different\ndesign decisions. \n418 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ecbfd787-41da-465f-b804-15509f7fa6e7": {"__data__": {"id_": "ecbfd787-41da-465f-b804-15509f7fa6e7", "embedding": null, "metadata": {"page_label": "419", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa41a984-149b-411e-a492-2ff95084de3f", "node_type": "4", "metadata": {"page_label": "419", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4174c33c4222d287928cc44e2c8f0522d6d710c4959ff6960eae40f7a452fc1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Beyond MapReduce\nAlthough MapReduce became very popular and received a lot of hype in the late\n2000s, it is just one among many possible programming models for distributed sys\u2010\ntems. Depending on the volume of data, the structure of the data, and the type of pro\u2010\ncessing being done with it, other tools may be more appropriate for expressing a\ncomputation.\nWe nevertheless spent a lot of time in this chapter discussing MapReduce because it\nis a useful learning tool, as it is a fairly clear and simple abstraction on top of a dis\u2010\ntributed filesystem. That is, simple in the sense of being able to understand what it is\ndoing, not in the sense of being easy to use. Quite the opposite: implementing a com\u2010\nplex processing job using the raw MapReduce APIs is actually quite hard and labori\u2010\nous\u2014for instance, you would need to implement any join algorithms from scratch\n[37].\nIn response to the difficulty of using MapReduce directly, various higher-level pro\u2010\ngramming models (Pig, Hive, Cascading, Crunch) were created as abstractions on top\nof MapReduce. If you understand how MapReduce works, they are fairly easy to\nlearn, and their higher-level constructs make many common batch processing tasks\nsignificantly easier to implement.\nHowever, there are also problems with the MapReduce execution model itself, which\nare not fixed by adding another level of abstraction and which manifest themselves as\npoor performance for some kinds of processing. On the one hand, MapReduce is\nvery robust: you can use it to process almost arbitrarily large quantities of data on an\nunreliable multi-tenant system with frequent task terminations, and it will still get the\njob done (albeit slowly). On the other hand, other tools are sometimes orders of mag\u2010\nnitude faster for some kinds of processing.\nIn the rest of this chapter, we will look at some of those alternatives for batch process\u2010\ning. In Chapter 11  we will move to stream processing, which can be regarded as\nanother way of speeding up batch processing.\nMaterialization of Intermediate State\nAs discussed previously, every MapReduce job is independent from every other job.\nThe main contact points of a job with the rest of the world are its input and output\ndirectories on the distributed filesystem. If you want the output of one job to become\nthe input to a second job, you need to configure the second job\u2019s input directory to be\nthe same as the first job\u2019s output directory, and an external workflow scheduler must\nstart the second job only once the first job has completed.\nThis setup is reasonable if the output from the first job is a dataset that you want to\npublish widely within your organization. In that case, you need to be able to refer to it\nBeyond MapReduce | 419", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "029844a3-5ba5-448e-b565-a09d87e3ebbc": {"__data__": {"id_": "029844a3-5ba5-448e-b565-a09d87e3ebbc", "embedding": null, "metadata": {"page_label": "420", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9de3e58-a408-4d1d-b213-c6f2e02648af", "node_type": "4", "metadata": {"page_label": "420", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2d13beb924fd1c17f4dbc993004ae02adc250505982531270daf6b5fdb5a7fc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "by name and reuse it as input to several different jobs (including jobs developed by\nother teams). Publishing data to a well-known location in the distributed filesystem\nallows loose coupling so that jobs don\u2019t need to know who is producing their input or\nconsuming their output (see \u201cSeparation of logic and wiring\u201d on page 396).\nHowever, in many cases, you know that the output of one job is only ever used as\ninput to one other job, which is maintained by the same team. In this case, the files\non the distributed filesystem are simply intermediate state: a means of passing data\nfrom one job to the next. In the complex workflows used to build recommendation\nsystems consisting of 50 or 100 MapReduce jobs [29], there is a lot of such intermedi\u2010\nate state.\nThe process of writing out this intermediate state to files is called materialization.\n(We came across the term previously in the context of materialized views, in \u201cAggre\u2010\ngation: Data Cubes and Materialized Views\u201d on page 101. It means to eagerly com\u2010\npute the result of some operation and write it out, rather than computing it on\ndemand when requested.)\nBy contrast, the log analysis example at the beginning of the chapter used Unix pipes\nto connect the output of one command with the input of another. Pipes do not fully\nmaterialize the intermediate state, but instead stream the output to the input incre\u2010\nmentally, using only a small in-memory buffer.\nMapReduce\u2019s approach of fully materializing intermediate state has downsides com\u2010\npared to Unix pipes:\n\u2022 A MapReduce job can only start when all tasks in the preceding jobs (that gener\u2010\nate its inputs) have completed, whereas processes connected by a Unix pipe are\nstarted at the same time, with output being consumed as soon as it is produced.\nSkew or varying load on different machines means that a job often has a few\nstraggler tasks that take much longer to complete than the others. Having to wait\nuntil all of the preceding job\u2019s tasks have completed slows down the execution of\nthe workflow as a whole.\n\u2022 Mappers are often redundant: they just read back the same file that was just writ\u2010\nten by a reducer, and prepare it for the next stage of partitioning and sorting. In\nmany cases, the mapper code could be part of the previous reducer: if the reducer\noutput was partitioned and sorted in the same way as mapper output, then\nreducers could be chained together directly, without interleaving with mapper\nstages.\n\u2022 Storing intermediate state in a distributed filesystem means those files are repli\u2010\ncated across several nodes, which is often overkill for such temporary data.\n420 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "988a997a-689f-4dfe-9c55-e47e7ddd8ba7": {"__data__": {"id_": "988a997a-689f-4dfe-9c55-e47e7ddd8ba7", "embedding": null, "metadata": {"page_label": "421", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3813571e-45be-49ca-90b3-54b9b4ab303d", "node_type": "4", "metadata": {"page_label": "421", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "39d2079d1efafce1dbf7a58959a04fb09a360b954035b225f2b0f3c9618fe0a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dataflow engines\nIn order to fix these problems with MapReduce, several new execution engines for\ndistributed batch computations were developed, the most well known of which are\nSpark [61, 62], Tez [ 63, 64], and Flink [ 65, 66]. There are various differences in the\nway they are designed, but they have one thing in common: they handle an entire\nworkflow as one job, rather than breaking it up into independent subjobs.\nSince they explicitly model the flow of data through several processing stages, these\nsystems are known as dataflow engines . Like MapReduce, they work by repeatedly\ncalling a user-defined function to process one record at a time on a single thread.\nThey parallelize work by partitioning inputs, and they copy the output of one func\u2010\ntion over the network to become the input to another function.\nUnlike in MapReduce, these functions need not take the strict roles of alternating\nmap and reduce, but instead can be assembled in more flexible ways. We call these\nfunctions operators, and the dataflow engine provides several different options for\nconnecting one operator\u2019s output to another\u2019s input:\n\u2022 One option is to repartition and sort records by key, like in the shuffle stage of\nMapReduce (see \u201cDistributed execution of MapReduce\u201d on page 400). This fea\u2010\nture enables sort-merge joins and grouping in the same way as in MapReduce.\n\u2022 Another possibility is to take several inputs and to partition them in the same\nway, but skip the sorting. This saves effort on partitioned hash joins, where the\npartitioning of records is important but the order is irrelevant because building\nthe hash table randomizes the order anyway.\n\u2022 For broadcast hash joins, the same output from one operator can be sent to all\npartitions of the join operator.\nThis style of processing engine is based on research systems like Dryad [ 67] and\nNephele [68], and it offers several advantages compared to the MapReduce model:\n\u2022 Expensive work such as sorting need only be performed in places where it is\nactually required, rather than always happening by default between every map\nand reduce stage.\n\u2022 There are no unnecessary map tasks, since the work done by a mapper can often\nbe incorporated into the preceding reduce operator (because a mapper does not\nchange the partitioning of a dataset).\n\u2022 Because all joins and data dependencies in a workflow are explicitly declared, the\nscheduler has an overview of what data is required where, so it can make locality\noptimizations. For example, it can try to place the task that consumes some data\non the same machine as the task that produces it, so that the data can be\nBeyond MapReduce | 421", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55b3d3da-97b5-40b8-9231-ffcead09b3ac": {"__data__": {"id_": "55b3d3da-97b5-40b8-9231-ffcead09b3ac", "embedding": null, "metadata": {"page_label": "422", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1aa33c2-fce2-425d-82a6-352a0eed3b42", "node_type": "4", "metadata": {"page_label": "422", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fd590c3147fe914735a4872bf2b2bbe16a86da30177e9eb485e965a93e18cb50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "exchanged through a shared memory buffer rather than having to copy it over\nthe network.\n\u2022 It is usually sufficient for intermediate state between operators to be kept in\nmemory or written to local disk, which requires less I/O than writing it to HDFS\n(where it must be replicated to several machines and written to disk on each rep\u2010\nlica). MapReduce already uses this optimization for mapper output, but dataflow\nengines generalize the idea to all intermediate state.\n\u2022 Operators can start executing as soon as their input is ready; there is no need to\nwait for the entire preceding stage to finish before the next one starts.\n\u2022 Existing Java Virtual Machine (JVM) processes can be reused to run new opera\u2010\ntors, reducing startup overheads compared to MapReduce (which launches a\nnew JVM for each task).\nYou can use dataflow engines to implement the same computations as MapReduce\nworkflows, and they usually execute significantly faster due to the optimizations\ndescribed here. Since operators are a generalization of map and reduce, the same pro\u2010\ncessing code can run on either execution engine: workflows implemented in Pig,\nHive, or Cascading can be switched from MapReduce to Tez or Spark with a simple\nconfiguration change, without modifying code [64].\nTez is a fairly thin library that relies on the YARN shuffle service for the actual copy\u2010\ning of data between nodes [ 58], whereas Spark and Flink are big frameworks that\ninclude their own network communication layer, scheduler, and user-facing APIs.\nWe will discuss those high-level APIs shortly.\nFault tolerance\nAn advantage of fully materializing intermediate state to a distributed filesystem is\nthat it is durable, which makes fault tolerance fairly easy in MapReduce: if a task fails,\nit can just be restarted on another machine and read the same input again from the\nfilesystem.\nSpark, Flink, and Tez avoid writing intermediate state to HDFS, so they take a differ\u2010\nent approach to tolerating faults: if a machine fails and the intermediate state on that\nmachine is lost, it is recomputed from other data that is still available (a prior inter\u2010\nmediary stage if possible, or otherwise the original input data, which is normally on\nHDFS).\nTo enable this recomputation, the framework must keep track of how a given piece of\ndata was computed\u2014which input partitions it used, and which operators were\napplied to it. Spark uses the resilient distributed dataset (RDD) abstraction for track\u2010\ning the ancestry of data [ 61], while Flink checkpoints operator state, allowing it to\nresume running an operator that ran into a fault during its execution [66].\n422 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c451913-c2ef-4c58-b637-ea8b046115d1": {"__data__": {"id_": "3c451913-c2ef-4c58-b637-ea8b046115d1", "embedding": null, "metadata": {"page_label": "423", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7457fbf-ec07-4c45-959b-e13f8e0968f9", "node_type": "4", "metadata": {"page_label": "423", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d6f48b0f88c2856d57f98e521314f3af877f6350e07804f7f4cf281b0804b330", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When recomputing data, it is important to know whether the computation is deter\u2010\nministic: that is, given the same input data, do the operators always produce the same\noutput? This question matters if some of the lost data has already been sent to down\u2010\nstream operators. If the operator is restarted and the recomputed data is not the same\nas the original lost data, it becomes very hard for downstream operators to resolve the\ncontradictions between the old and new data. The solution in the case of nondeter\u2010\nministic operators is normally to kill the downstream operators as well, and run them\nagain on the new data.\nIn order to avoid such cascading faults, it is better to make operators deterministic.\nNote however that it is easy for nondeterministic behavior to accidentally creep in:\nfor example, many programming languages do not guarantee any particular order\nwhen iterating over elements of a hash table, many probabilistic and statistical\nalgorithms explicitly rely on using random numbers, and any use of the system clock\nor external data sources is nondeterministic. Such causes of nondeterminism need to\nbe removed in order to reliably recover from faults, for example by generating\npseudorandom numbers using a fixed seed.\nRecovering from faults by recomputing data is not always the right answer: if the\nintermediate data is much smaller than the source data, or if the computation is very\nCPU-intensive, it is probably cheaper to materialize the intermediate data to files\nthan to recompute it.\nDiscussion of materialization\nReturning to the Unix analogy, we saw that MapReduce is like writing the output of\neach command to a temporary file, whereas dataflow engines look much more like\nUnix pipes. Flink especially is built around the idea of pipelined execution: that is,\nincrementally passing the output of an operator to other operators, and not waiting\nfor the input to be complete before starting to process it.\nA sorting operation inevitably needs to consume its entire input before it can pro\u2010\nduce any output, because it\u2019s possible that the very last input record is the one with\nthe lowest key and thus needs to be the very first output record. Any operator that\nrequires sorting will thus need to accumulate state, at least temporarily. But many\nother parts of a workflow can be executed in a pipelined manner.\nWhen the job completes, its output needs to go somewhere durable so that users can\nfind it and use it\u2014most likely, it is written to the distributed filesystem again. Thus,\nwhen using a dataflow engine, materialized datasets on HDFS are still usually the\ninputs and the final outputs of a job. Like with MapReduce, the inputs are immutable\nand the output is completely replaced. The improvement over MapReduce is that you\nsave yourself writing all the intermediate state to the filesystem as well. \nBeyond MapReduce | 423", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0efc5463-0d2c-449a-83e1-83d1d90a0a50": {"__data__": {"id_": "0efc5463-0d2c-449a-83e1-83d1d90a0a50", "embedding": null, "metadata": {"page_label": "424", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec2ad01f-991e-4a1b-acac-7109e76e8137", "node_type": "4", "metadata": {"page_label": "424", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "717feed36419fdd0b8766881e6a7f3057b346d5eace244af5c919d67b5fb7f9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Graphs and Iterative Processing\nIn \u201cGraph-Like Data Models\u201d on page 49 we discussed using graphs for modeling\ndata, and using graph query languages to traverse the edges and vertices in a graph.\nThe discussion in Chapter 2 was focused around OLTP-style use: quickly executing\nqueries to find a small number of vertices matching certain criteria.\nIt is also interesting to look at graphs in a batch processing context, where the goal is\nto perform some kind of offline processing or analysis on an entire graph. This need\noften arises in machine learning applications such as recommendation engines, or in\nranking systems. For example, one of the most famous graph analysis algorithms is\nPageRank [69], which tries to estimate the popularity of a web page based on what\nother web pages link to it. It is used as part of the formula that determines the order\nin which web search engines present their results.\nDataflow engines like Spark, Flink, and Tez (see \u201cMaterialization of\nIntermediate State\u201d on page 419) typically arrange the operators in\na job as a directed acyclic graph (DAG). This is not the same as\ngraph processing: in dataflow engines, the flow of data from one\noperator to another  is structured as a graph, while the data itself\ntypically consists of relational-style tuples. In graph processing, the\ndata itself  has the form of a graph. Another unfortunate naming\nconfusion!\nMany graph algorithms are expressed by traversing one edge at a time, joining one\nvertex with an adjacent vertex in order to propagate some information, and repeating\nuntil some condition is met\u2014for example, until there are no more edges to follow, or\nuntil some metric converges. We saw an example in Figure 2-6, which made a list of\nall the locations in North America contained in a database by repeatedly following\nedges indicating which location is within which other location (this kind of algorithm\nis called a transitive closure).\nIt is possible to store a graph in a distributed filesystem (in files containing lists of\nvertices and edges), but this idea of \u201crepeating until done\u201d cannot be expressed in\nplain MapReduce, since it only performs a single pass over the data. This kind of\nalgorithm is thus often implemented in an iterative style:\n1. An external scheduler runs a batch process to calculate one step of the algorithm.\n2. When the batch process completes, the scheduler checks whether it has finished\n(based on the completion condition\u2014e.g., there are no more edges to follow, or\nthe change compared to the last iteration is below some threshold).\n3. If it has not yet finished, the scheduler goes back to step 1 and runs another\nround of the batch process.\n424 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8d69b19-ca7f-4b14-a616-2074968b4ef2": {"__data__": {"id_": "d8d69b19-ca7f-4b14-a616-2074968b4ef2", "embedding": null, "metadata": {"page_label": "425", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46f07b8b-004b-4221-8799-c9224afb7e58", "node_type": "4", "metadata": {"page_label": "425", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "90136ccec6141bcd3d69c6afd00f3afb9239129dcffd4c15e3dc3f657368c530", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This approach works, but implementing it with MapReduce is often very inefficient,\nbecause MapReduce does not account for the iterative nature of the algorithm: it will\nalways read the entire input dataset and produce a completely new output dataset,\neven if only a small part of the graph has changed compared to the last iteration.\nThe Pregel processing model\nAs an optimization for batch processing graphs, the bulk synchronous parallel  (BSP)\nmodel of computation [70] has become popular. Among others, it is implemented by\nApache Giraph [ 37], Spark\u2019s GraphX API, and Flink\u2019s Gelly API [ 71]. It is also\nknown as the Pregel model, as Google\u2019s Pregel paper popularized this approach for\nprocessing graphs [72].\nRecall that in MapReduce, mappers conceptually \u201csend a message\u201d to a particular call\nof the reducer because the framework collects together all the mapper outputs with\nthe same key. A similar idea is behind Pregel: one vertex can \u201csend a message\u201d to\nanother vertex, and typically those messages are sent along the edges in a graph.\nIn each iteration, a function is called for each vertex, passing it all the messages that\nwere sent to it\u2014much like a call to the reducer. The difference from MapReduce is\nthat in the Pregel model, a vertex remembers its state in memory from one iteration\nto the next, so the function only needs to process new incoming messages. If no mes\u2010\nsages are being sent in some part of the graph, no work needs to be done.\nIt\u2019s a bit similar to the actor model (see \u201cDistributed actor frameworks\u201d on page 138),\nif you think of each vertex as an actor, except that vertex state and messages between\nvertices are fault-tolerant and durable, and communication proceeds in fixed rounds:\nat every iteration, the framework delivers all messages sent in the previous iteration.\nActors normally have no such timing guarantee.\nFault tolerance\nThe fact that vertices can only communicate by message passing (not by querying\neach other directly) helps improve the performance of Pregel jobs, since messages can\nbe batched and there is less waiting for communication. The only waiting is between\niterations: since the Pregel model guarantees that all messages sent in one iteration\nare delivered in the next iteration, the prior iteration must completely finish, and all\nof its messages must be copied over the network, before the next one can start.\nEven though the underlying network may drop, duplicate, or arbitrarily delay mes\u2010\nsages (see \u201cUnreliable Networks\u201d on page 277), Pregel implementations guarantee\nthat messages are processed exactly once at their destination vertex in the following\niteration. Like MapReduce, the framework transparently recovers from faults in\norder to simplify the programming model for algorithms on top of Pregel.\nBeyond MapReduce | 425", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82d17465-b07b-4f14-8f21-17d721755217": {"__data__": {"id_": "82d17465-b07b-4f14-8f21-17d721755217", "embedding": null, "metadata": {"page_label": "426", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d90a2264-16d4-468b-9ae4-806739cca1b2", "node_type": "4", "metadata": {"page_label": "426", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "807d7d38914f105fcff4bac26992cd2f7972d5d4ed7fdd4b9cad779a1d1bc2ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This fault tolerance is achieved by periodically checkpointing the state of all vertices\nat the end of an iteration\u2014i.e., writing their full state to durable storage. If a node\nfails and its in-memory state is lost, the simplest solution is to roll back the entire\ngraph computation to the last checkpoint and restart the computation. If the algo\u2010\nrithm is deterministic and messages are logged, it is also possible to selectively\nrecover only the partition that was lost (like we previously discussed for dataflow\nengines) [72].\nParallel execution\nA vertex does not need to know on which physical machine it is executing; when it\nsends messages to other vertices, it simply sends them to a vertex ID. It is up to the\nframework to partition the graph\u2014i.e., to decide which vertex runs on which\nmachine, and how to route messages over the network so that they end up in the\nright place.\nBecause the programming model deals with just one vertex at a time (sometimes\ncalled \u201cthinking like a vertex\u201d), the framework may partition the graph in arbitrary\nways. Ideally it would be partitioned such that vertices are colocated on the same\nmachine if they need to communicate a lot. However, finding such an optimized par\u2010\ntitioning is hard\u2014in practice, the graph is often simply partitioned by an arbitrarily\nassigned vertex ID, making no attempt to group related vertices together.\nAs a result, graph algorithms often have a lot of cross-machine communication over\u2010\nhead, and the intermediate state (messages sent between nodes) is often bigger than\nthe original graph. The overhead of sending messages over the network can signifi\u2010\ncantly slow down distributed graph algorithms.\nFor this reason, if your graph can fit in memory on a single computer, it\u2019s quite likely\nthat a single-machine (maybe even single-threaded) algorithm will outperform a dis\u2010\ntributed batch process [73, 74]. Even if the graph is bigger than memory, it can fit on\nthe disks of a single computer, single-machine processing using a framework such as\nGraphChi is a viable option [ 75]. If the graph is too big to fit on a single machine, a\ndistributed approach such as Pregel is unavoidable; efficiently parallelizing graph\nalgorithms is an area of ongoing research [76]. \nHigh-Level APIs and Languages\nOver the years since MapReduce first became popular, the execution engines for dis\u2010\ntributed batch processing have matured. By now, the infrastructure has become\nrobust enough to store and process many petabytes of data on clusters of over 10,000\nmachines. As the problem of physically operating batch processes at such scale has\nbeen considered more or less solved, attention has turned to other areas: improving\nthe programming model, improving the efficiency of processing, and broadening the\nset of problems that these technologies can solve.\n426 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db83a839-6773-40e5-a08d-80fac0b13b12": {"__data__": {"id_": "db83a839-6773-40e5-a08d-80fac0b13b12", "embedding": null, "metadata": {"page_label": "427", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5827da53-13ed-415b-b577-ca6514d5b27f", "node_type": "4", "metadata": {"page_label": "427", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "591c33b3027c5115880e21ad45951827bda185c406dd0b34872b976a053d80c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As discussed previously, higher-level languages and APIs such as Hive, Pig, Cascad\u2010\ning, and Crunch became popular because programming MapReduce jobs by hand is\nquite laborious. As Tez emerged, these high-level languages had the additional bene\u2010\nfit of being able to move to the new dataflow execution engine without the need to\nrewrite job code. Spark and Flink also include their own high-level dataflow APIs,\noften taking inspiration from FlumeJava [34].\nThese dataflow APIs generally use relational-style building blocks to express a com\u2010\nputation: joining datasets on the value of some field; grouping tuples by key; filtering\nby some condition; and aggregating tuples by counting, summing, or other functions.\nInternally, these operations are implemented using the various join and grouping\nalgorithms that we discussed earlier in this chapter.\nBesides the obvious advantage of requiring less code, these high-level interfaces also\nallow interactive use, in which you write analysis code incrementally in a shell and\nrun it frequently to observe what it is doing. This style of development is very helpful\nwhen exploring a dataset and experimenting with approaches for processing it. It is\nalso reminiscent of the Unix philosophy, which we discussed in \u201cThe Unix Philoso\u2010\nphy\u201d on page 394.\nMoreover, these high-level interfaces not only make the humans using the system\nmore productive, but they also improve the job execution efficiency at a machine\nlevel.\nThe move toward declarative query languages\nAn advantage of specifying joins as relational operators, compared to spelling out the\ncode that performs the join, is that the framework can analyze the properties of the\njoin inputs and automatically decide which of the aforementioned join algorithms\nwould be most suitable for the task at hand. Hive, Spark, and Flink have cost-based\nquery optimizers that can do this, and even change the order of joins so that the\namount of intermediate state is minimized [66, 77, 78, 79].\nThe choice of join algorithm can make a big difference to the performance of a batch\njob, and it is nice not to have to understand and remember all the various join algo\u2010\nrithms we discussed in this chapter. This is possible if joins are specified in a declara\u2010\ntive way: the application simply states which joins are required, and the query\noptimizer decides how they can best be executed. We previously came across this idea\nin \u201cQuery Languages for Data\u201d on page 42.\nHowever, in other ways, MapReduce and its dataflow successors are very different\nfrom the fully declarative query model of SQL. MapReduce was built around the idea\nof function callbacks: for each record or group of records, a user-defined function\n(the mapper or reducer) is called, and that function is free to call arbitrary code in\norder to decide what to output. This approach has the advantage that you can draw\nBeyond MapReduce | 427", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2bfd7494-d6b3-4934-84b7-e113c354d575": {"__data__": {"id_": "2bfd7494-d6b3-4934-84b7-e113c354d575", "embedding": null, "metadata": {"page_label": "428", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "365bcf18-7006-498b-9573-a2c1c1405f7e", "node_type": "4", "metadata": {"page_label": "428", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3046de597f30cf9117abab6e680add85a66754d6f4bb37fb8c3ddbc1102a27ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "upon a large ecosystem of existing libraries to do things like parsing, natural language\nanalysis, image analysis, and running numerical or statistical algorithms.\nThe freedom to easily run arbitrary code is what has long distinguished batch pro\u2010\ncessing systems of MapReduce heritage from MPP databases (see \u201cComparing\nHadoop to Distributed Databases\u201d on page 414); although databases have facilities\nfor writing user-defined functions, they are often cumbersome to use and not well\nintegrated with the package managers and dependency management systems that are\nwidely used in most programming languages (such as Maven for Java, npm for Java\u2010\nScript, and Rubygems for Ruby).\nHowever, dataflow engines have found that there are also advantages to incorporat\u2010\ning more declarative features in areas besides joins. For example, if a callback func\u2010\ntion contains only a simple filtering condition, or it just selects some fields from a\nrecord, then there is significant CPU overhead in calling the function on every\nrecord. If such simple filtering and mapping operations are expressed in a declarative\nway, the query optimizer can take advantage of column-oriented storage layouts (see\n\u201cColumn-Oriented Storage\u201d on page 95) and read only the required columns from\ndisk. Hive, Spark DataFrames, and Impala also use vectorized execution (see \u201cMem\u2010\nory bandwidth and vectorized processing\u201d on page 99): iterating over data in a tight\ninner loop that is friendly to CPU caches, and avoiding function calls. Spark gener\u2010\nates JVM bytecode [ 79] and Impala uses LLVM to generate native code for these\ninner loops [41].\nBy incorporating declarative aspects in their high-level APIs, and having query opti\u2010\nmizers that can take advantage of them during execution, batch processing frame\u2010\nworks begin to look more like MPP databases (and can achieve comparable\nperformance). At the same time, by having the extensibility of being able to run arbi\u2010\ntrary code and read data in arbitrary formats, they retain their flexibility advantage.\nSpecialization for different domains\nWhile the extensibility of being able to run arbitrary code is useful, there are also\nmany common cases where standard processing patterns keep reoccurring, and so it\nis worth having reusable implementations of the common building blocks. Tradition\u2010\nally, MPP databases have served the needs of business intelligence analysts and busi\u2010\nness reporting, but that is just one among many domains in which batch processing\nis used.\nAnother domain of increasing importance is statistical and numerical algorithms,\nwhich are needed for machine learning applications such as classification and recom\u2010\nmendation systems. Reusable implementations are emerging: for example, Mahout\nimplements various algorithms for machine learning on top of MapReduce, Spark,\nand Flink, while MADlib implements similar functionality inside a relational MPP\ndatabase (Apache HAWQ) [54].\n428 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4c1e291-9e9b-4726-924a-831a589db950": {"__data__": {"id_": "b4c1e291-9e9b-4726-924a-831a589db950", "embedding": null, "metadata": {"page_label": "429", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8210b0c6-b22d-4537-bbd3-72854118a090", "node_type": "4", "metadata": {"page_label": "429", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b9c55dde042b36efea48ac7bce1f00ff5773ed2eee0de0171d927b165e878b17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also useful are spatial algorithms such as k-nearest neighbors [80], which searches for\nitems that are close to a given item in some multi-dimensional space\u2014a kind of simi\u2010\nlarity search. Approximate search is also important for genome analysis algorithms,\nwhich need to find strings that are similar but not identical [81].\nBatch processing engines are being used for distributed execution of algorithms from\nan increasingly wide range of domains. As batch processing systems gain built-in\nfunctionality and high-level declarative operators, and as MPP databases become\nmore programmable and flexible, the two are beginning to look more alike: in the\nend, they are all just systems for storing and processing data. \nSummary\nIn this chapter we explored the topic of batch processing. We started by looking at\nUnix tools such as awk, grep, and sort, and we saw how the design philosophy of\nthose tools is carried forward into MapReduce and more recent dataflow engines.\nSome of those design principles are that inputs are immutable, outputs are intended\nto become the input to another (as yet unknown) program, and complex problems\nare solved by composing small tools that \u201cdo one thing well.\u201d\nIn the Unix world, the uniform interface that allows one program to be composed\nwith another is files and pipes; in MapReduce, that interface is a distributed filesys\u2010\ntem. We saw that dataflow engines add their own pipe-like data transport mecha\u2010\nnisms to avoid materializing intermediate state to the distributed filesystem, but the\ninitial input and final output of a job is still usually HDFS.\nThe two main problems that distributed batch processing frameworks need to solve\nare:\nPartitioning\nIn MapReduce, mappers are partitioned according to input file blocks. The out\u2010\nput of mappers is repartitioned, sorted, and merged into a configurable number\nof reducer partitions. The purpose of this process is to bring all the related data\u2014\ne.g., all the records with the same key\u2014together in the same place.\nPost-MapReduce dataflow engines try to avoid sorting unless it is required, but\nthey otherwise take a broadly similar approach to partitioning.\nFault tolerance\nMapReduce frequently writes to disk, which makes it easy to recover from an\nindividual failed task without restarting the entire job but slows down execution\nin the failure-free case. Dataflow engines perform less materialization of inter\u2010\nmediate state and keep more in memory, which means that they need to recom\u2010\npute more data if a node fails. Deterministic operators reduce the amount of data\nthat needs to be recomputed.\nSummary | 429", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60b89021-aaec-4a2b-8367-03cf30ddcd0d": {"__data__": {"id_": "60b89021-aaec-4a2b-8367-03cf30ddcd0d", "embedding": null, "metadata": {"page_label": "430", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "934eb9d2-92c3-4f32-b94b-7d6e7428cf41", "node_type": "4", "metadata": {"page_label": "430", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "870e16db0745f3b322757ca7e1baef6c50b62b8692e7a55c0f46ef0f3a46e26a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We discussed several join algorithms for MapReduce, most of which are also inter\u2010\nnally used in MPP databases and dataflow engines. They also provide a good illustra\u2010\ntion of how partitioned algorithms work:\nSort-merge joins\nEach of the inputs being joined goes through a mapper that extracts the join key.\nBy partitioning, sorting, and merging, all the records with the same key end up\ngoing to the same call of the reducer. This function can then output the joined\nrecords.\nBroadcast hash joins\nOne of the two join inputs is small, so it is not partitioned and it can be entirely\nloaded into a hash table. Thus, you can start a mapper for each partition of the\nlarge join input, load the hash table for the small input into each mapper, and\nthen scan over the large input one record at a time, querying the hash table for\neach record.\nPartitioned hash joins\nIf the two join inputs are partitioned in the same way (using the same key, same\nhash function, and same number of partitions), then the hash table approach can\nbe used independently for each partition.\nDistributed batch processing engines have a deliberately restricted programming\nmodel: callback functions (such as mappers and reducers) are assumed to be stateless\nand to have no externally visible side effects besides their designated output. This\nrestriction allows the framework to hide some of the hard distributed systems prob\u2010\nlems behind its abstraction: in the face of crashes and network issues, tasks can be\nretried safely, and the output from any failed tasks is discarded. If several tasks for a\npartition succeed, only one of them actually makes its output visible.\nThanks to the framework, your code in a batch processing job does not need to worry\nabout implementing fault-tolerance mechanisms: the framework can guarantee that\nthe final output of a job is the same as if no faults had occurred, even though in real\u2010\nity various tasks perhaps had to be retried. These reliable semantics are much stron\u2010\nger than what you usually have in online services that handle user requests and that\nwrite to databases as a side effect of processing a request.\nThe distinguishing feature of a batch processing job is that it reads some input data\nand produces some output data, without modifying the input\u2014in other words, the\noutput is derived from the input. Crucially, the input data is bounded: it has a known,\nfixed size (for example, it consists of a set of log files at some point in time, or a snap\u2010\nshot of a database\u2019s contents). Because it is bounded, a job knows when it has finished\nreading the entire input, and so a job eventually completes when it is done.\nIn the next chapter, we will turn to stream processing, in which the input is unboun\u2010\nded\u2014that is, you still have a job, but its inputs are never-ending streams of data. In\n430 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2846, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed136d59-d9c5-42f3-abe0-60b7078ac7b7": {"__data__": {"id_": "ed136d59-d9c5-42f3-abe0-60b7078ac7b7", "embedding": null, "metadata": {"page_label": "431", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14773323-390a-4edd-9ff0-adc79735bac4", "node_type": "4", "metadata": {"page_label": "431", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fab401e66030635ebd22c059fd99e646312b1df3622e175c53c70420b1abdbd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "this case, a job is never complete, because at any time there may still be more work\ncoming in. We shall see that stream and batch processing are similar in some\nrespects, but the assumption of unbounded streams also changes a lot about how we\nbuild systems. \nReferences\n[1] Jeffrey Dean and Sanjay Ghemawat: \u201c MapReduce: Simplified Data Processing on\nLarge Clusters,\u201d at 6th USENIX Symposium on Operating System Design and Imple\u2010\nmentation (OSDI), December 2004.\n[2] Joel Spolsky: \u201cThe Perils of JavaSchools,\u201d joelonsoftware.com, December 25, 2005.\n[3] Shivnath Babu and Herodotos Herodotou: \u201c Massively Parallel Databases and\nMapReduce Systems ,\u201d Foundations and Trends in Databases , volume 5, number 1,\npages 1\u2013104, November 2013. doi:10.1561/1900000036\n[4] David J. DeWitt and Michael Stonebraker: \u201c MapReduce: A Major Step Back\u2010\nwards,\u201d originally published at databasecolumn.vertica.com, January 17, 2008.\n[5] Henry Robinson: \u201c The Elephant Was a Trojan Horse: On the Death of Map-\nReduce at Google,\u201d the-paper-trail.org, June 25, 2014.\n[6] \u201cThe Hollerith Machine,\u201d United States Census Bureau, census.gov.\n[7] \u201c IBM 82, 83, and 84 Sorters Reference Manual ,\u201d Edition A24-1034-1, Interna\u2010\ntional Business Machines Corporation, July 1962.\n[8] Adam Drake: \u201c Command-Line Tools Can Be 235x Faster than Your Hadoop\nCluster,\u201d aadrake.com, January 25, 2014.\n[9] \u201cGNU Coreutils 8.23 Documentation,\u201d Free Software Foundation, Inc., 2014.\n[10] Martin Kleppmann: \u201c Kafka, Samza, and the Unix Philosophy of Distributed\nData,\u201d martin.kleppmann.com, August 5, 2015.\n[11] Doug McIlroy: Internal Bell Labs memo , October 1964. Cited in: Dennis M.\nRichie: \u201cAdvice from Doug McIlroy,\u201d cm.bell-labs.com.\n[12] M. D. McIlroy, E. N. Pinson, and B. A. Tague: \u201c UNIX Time-Sharing System:\nForeword,\u201d The Bell System Technical Journal , volume 57, number 6, pages 1899\u2013\n1904, July 1978.\n[13] Eric S. Raymond: The Art of UNIX Programming . Addison-Wesley, 2003. ISBN:\n978-0-13-142901-7\n[14] Ronald Duncan: \u201cText File Formats \u2013 ASCII Delimited Text \u2013 Not CSV or TAB\nDelimited Text,\u201d ronaldduncan.wordpress.com, October 31, 2009.\n[15] Alan Kay: \u201cIs \u2018Software Engineering\u2019 an Oxymoron?,\u201d tinlizzie.org.\nSummary | 431", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87f141bc-a64b-4028-be2b-47877aabfd6a": {"__data__": {"id_": "87f141bc-a64b-4028-be2b-47877aabfd6a", "embedding": null, "metadata": {"page_label": "432", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a99d9ab7-1192-4469-a10c-332562365e28", "node_type": "4", "metadata": {"page_label": "432", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "aca20be60d87dc64c9a8006f402acd4f438bc81113e8c4c5e058495801b013a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] Martin Fowler: \u201cInversionOfControl,\u201d martinfowler.com, June 26, 2005.\n[17] Daniel J. Bernstein: \u201cTwo File Descriptors for Sockets,\u201d cr.yp.to.\n[18] Rob Pike and Dennis M. Ritchie: \u201c The Styx Architecture for Distributed Sys\u2010\ntems,\u201d Bell Labs Technical Journal, volume 4, number 2, pages 146\u2013152, April 1999.\n[19] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: \u201cThe Google File Sys\u2010\ntem,\u201d at 19th ACM Symposium on Operating Systems Principles  (SOSP), October\n2003. doi:10.1145/945445.945450\n[20] Michael Ovsiannikov, Silvius Rus, Damian Reeves, et al.: \u201c The Quantcast File\nSystem,\u201d Proceedings of the VLDB Endowment , volume 6, number 11, pages 1092\u2013\n1101, August 2013. doi:10.14778/2536222.2536234\n[21] \u201c OpenStack Swift 2.6.1 Developer Documentation ,\u201d OpenStack Foundation,\ndocs.openstack.org, March 2016.\n[22] Zhe Zhang, Andrew Wang, Kai Zheng, et al.: \u201c Introduction to HDFS Erasure\nCoding in Apache Hadoop,\u201d blog.cloudera.com, September 23, 2015.\n[23] Peter Cnudde: \u201cHadoop Turns 10,\u201d yahoohadoop.tumblr.com, February 5, 2016.\n[24] Eric Baldeschwieler: \u201c Thinking About the HDFS vs. Other Storage Technolo\u2010\ngies,\u201d hortonworks.com, July 25, 2012.\n[25] Brendan Gregg: \u201cManta: Unix Meets Map Reduce,\u201d dtrace.org, June 25, 2013.\n[26] Tom White: Hadoop: The Definitive Guide , 4th edition. O\u2019Reilly Media, 2015.\nISBN: 978-1-491-90163-2\n[27] Jim N. Gray: \u201c Distributed Computing Economics ,\u201d Microsoft Research Tech\nReport MSR-TR-2003-24, March 2003.\n[28] M\u00e1rton Trencs\u00e9ni: \u201c Luigi vs Airflow vs Pinball ,\u201d bytepawn.com, February 6,\n2016.\n[29] Roshan Sumbaly, Jay Kreps, and Sam Shah: \u201c The \u2018Big Data\u2019 Ecosystem at\nLinkedIn,\u201d at ACM International Conference on Management of Data  (SIGMOD),\nJuly 2013. doi:10.1145/2463676.2463707\n[30] Alan F. Gates, Olga Natkovich, Shubham Chopra, et al.: \u201c Building a High-Level\nDataflow System on Top of Map-Reduce: The Pig Experience ,\u201d at 35th International\nConference on Very Large Data Bases (VLDB), August 2009.\n[31] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, et al.: \u201c Hive \u2013 A Petabyte Scale\nData Warehouse Using Hadoop ,\u201d at 26th IEEE International Conference on Data\nEngineering (ICDE), March 2010. doi:10.1109/ICDE.2010.5447738\n[32] \u201cCascading 3.0 User Guide,\u201d Concurrent, Inc., docs.cascading.org, January 2016.\n432 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2307, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f69ba5ef-80d2-4ca1-ad81-72281e925876": {"__data__": {"id_": "f69ba5ef-80d2-4ca1-ad81-72281e925876", "embedding": null, "metadata": {"page_label": "433", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84d75a17-6aba-4591-86fb-bc5d19de2eb0", "node_type": "4", "metadata": {"page_label": "433", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9803376310769c8a1a5c6213680dded05ffbe720eb28d8b2edb5d5039b0d4c67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[33] \u201cApache Crunch User Guide,\u201d Apache Software Foundation, crunch.apache.org.\n[34] Craig Chambers, Ashish Raniwala, Frances Perry, et al.: \u201c FlumeJava: Easy, Effi\u2010\ncient Data-Parallel Pipelines ,\u201d at 31st ACM SIGPLAN Conference on Programming\nLanguage Design and Implementation  (PLDI), June 2010. doi:\n10.1145/1806596.1806638\n[35] Jay Kreps: \u201c Why Local State is a Fundamental Primitive in Stream Processing ,\u201d\noreilly.com, July 31, 2014.\n[36] Martin Kleppmann: \u201c Rethinking Caching in Web Apps ,\u201d martin.klepp\u2010\nmann.com, October 1, 2012.\n[37] Mark Grover, Ted Malaska, Jonathan Seidman, and Gwen Shapira: Hadoop\nApplication Architectures. O\u2019Reilly Media, 2015. ISBN: 978-1-491-90004-8\n[38] Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: \u201cChallenges to Adopting\nStronger Consistency at Scale ,\u201d at 15th USENIX Workshop on Hot Topics in Operat\u2010\ning Systems (HotOS), May 2015.\n[39] Sriranjan Manjunath: \u201cSkewed Join,\u201d wiki.apache.org, 2009.\n[40] David J. DeWitt, Jeffrey F. Naughton, Donovan A. Schneider, and S. Seshadri:\n\u201cPractical Skew Handling in Parallel Joins ,\u201d at 18th International Conference on Very\nLarge Data Bases (VLDB), August 1992.\n[41] Marcel Kornacker, Alexander Behm, Victor Bittorf, et al.: \u201c Impala: A Modern,\nOpen-Source SQL Engine for Hadoop ,\u201d at 7th Biennial Conference on Innovative\nData Systems Research (CIDR), January 2015.\n[42] Matthieu Monsch: \u201c Open-Sourcing PalDB, a Lightweight Companion for Stor\u2010\ning Side Data,\u201d engineering.linkedin.com, October 26, 2015.\n[43] Daniel Peng and Frank Dabek: \u201c Large-Scale Incremental Processing Using Dis\u2010\ntributed Transactions and Notifications,\u201d at 9th USENIX conference on Operating Sys\u2010\ntems Design and Implementation (OSDI), October 2010.\n[44] \u201c\u201cCloudera Search User Guide,\u201d Cloudera, Inc., September 2015.\n[45] Lili Wu, Sam Shah, Sean Choi, et al.: \u201c The Browsemaps: Collaborative Filtering\nat LinkedIn ,\u201d at 6th Workshop on Recommender Systems and the Social Web\n(RSWeb), October 2014.\n[46] Roshan Sumbaly, Jay Kreps, Lei Gao, et al.: \u201c Serving Large-Scale Batch Compu\u2010\nted Data with Project Voldemort ,\u201d at 10th USENIX Conference on File and Storage\nTechnologies (FAST), February 2012.\n[47] Varun Sharma: \u201c Open-Sourcing Terrapin: A Serving System for Batch Gener\u2010\nated Data,\u201d engineering.pinterest.com, September 14, 2015.\nSummary | 433", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dcc86546-203e-42da-9cff-182bba8ec427": {"__data__": {"id_": "dcc86546-203e-42da-9cff-182bba8ec427", "embedding": null, "metadata": {"page_label": "434", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa5099dd-36e0-4dc9-ab6c-546dd398816e", "node_type": "4", "metadata": {"page_label": "434", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7c56c88b7628b66779a4c693e6c82d4d9daf5862860d08aaed552cfe7c89f4cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[48] Nathan Marz: \u201cElephantDB,\u201d slideshare.net, May 30, 2011.\n[49] Jean-Daniel (JD) Cryans: \u201c How-to: Use HBase Bulk Loading, and Why ,\u201d\nblog.cloudera.com, September 27, 2013.\n[50] Nathan Marz: \u201c How to Beat the CAP Theorem ,\u201d nathanmarz.com, October 13,\n2011.\n[51] Molly Bartlett Dishman and Martin Fowler: \u201c Agile Architecture ,\u201d at O\u2019Reilly\nSoftware Architecture Conference, March 2015.\n[52] David J. DeWitt and Jim N. Gray: \u201c Parallel Database Systems: The Future of\nHigh Performance Database Systems ,\u201d Communications of the ACM , volume 35,\nnumber 6, pages 85\u201398, June 1992. doi:10.1145/129888.129894\n[53] Jay Kreps: \u201c But the multi-tenancy thing is actually really really hard ,\u201d tweet\u2010\nstorm, twitter.com, October 31, 2014.\n[54] Jeffrey Cohen, Brian Dolan, Mark Dunlap, et al.: \u201c MAD Skills: New Analysis\nPractices for Big Data ,\u201d Proceedings of the VLDB Endowment , volume 2, number 2,\npages 1481\u20131492, August 2009. doi:10.14778/1687553.1687576\n[55] Ignacio Terrizzano, Peter Schwarz, Mary Roth, and John E. Colino: \u201cData Wran\u2010\ngling: The Challenging Journey from the Wild to the Lake ,\u201d at 7th Biennial Confer\u2010\nence on Innovative Data Systems Research (CIDR), January 2015.\n[56] Paige Roberts: \u201cTo Schema on Read or to Schema on Write, That Is the Hadoop\nData Lake Question,\u201d adaptivesystemsinc.com, July 2, 2015.\n[57] Bobby Johnson and Joseph Adler: \u201c The Sushi Principle: Raw Data Is Better ,\u201d at\nStrata+Hadoop World, February 2015.\n[58] Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, et al.: \u201c Apache\nHadoop YARN: Yet Another Resource Negotiator ,\u201d at 4th ACM Symposium on\nCloud Computing (SoCC), October 2013. doi:10.1145/2523616.2523633\n[59] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, et al.: \u201c Large-Scale Cluster\nManagement at Google with Borg ,\u201d at 10th European Conference on Computer Sys\u2010\ntems (EuroSys), April 2015. doi:10.1145/2741948.2741964\n[60] Malte Schwarzkopf: \u201c The Evolution of Cluster Scheduler Architectures ,\u201d firma\u2010\nment.io, March 9, 2016.\n[61] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, et al.: \u201c Resilient Dis\u2010\ntributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing ,\u201d\nat 9th USENIX Symposium on Networked Systems Design and Implementation\n(NSDI), April 2012.\n[62] Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia: Learning\nSpark. O\u2019Reilly Media, 2015. ISBN: 978-1-449-35904-1\n434 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ee6e46c-6950-42e9-851f-643cf8674306": {"__data__": {"id_": "6ee6e46c-6950-42e9-851f-643cf8674306", "embedding": null, "metadata": {"page_label": "435", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ee217a0-d49c-4749-8e89-5697487841db", "node_type": "4", "metadata": {"page_label": "435", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2b1820019841e0e96f3d30ecb2e9a8f28d9c4cd0e0d0c96dfa3663f5f0dd8036", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[63] Bikas Saha and Hitesh Shah: \u201c Apache Tez: Accelerating Hadoop Query Process\u2010\ning,\u201d at Hadoop Summit, June 2014.\n[64] Bikas Saha, Hitesh Shah, Siddharth Seth, et al.: \u201c Apache Tez: A Unifying Frame\u2010\nwork for Modeling and Building Data Processing Applications ,\u201d at ACM Interna\u2010\ntional Conference on Management of Data  (SIGMOD), June 2015. doi:\n10.1145/2723372.2742790\n[65] Kostas Tzoumas: \u201c Apache Flink: API, Runtime, and Project Roadmap ,\u201d slide\u2010\nshare.net, January 14, 2015.\n[66] Alexander Alexandrov, Rico Bergmann, Stephan Ewen, et al.: \u201c The Stratosphere\nPlatform for Big Data Analytics ,\u201d The VLDB Journal , volume 23, number 6, pages\n939\u2013964, May 2014. doi:10.1007/s00778-014-0357-y\n[67] Michael Isard, Mihai Budiu, Yuan Yu, et al.: \u201c Dryad: Distributed Data-Parallel\nPrograms from Sequential Building Blocks ,\u201d at European Conference on Computer\nSystems (EuroSys), March 2007. doi:10.1145/1272996.1273005\n[68] Daniel Warneke and Odej Kao: \u201c Nephele: Efficient Parallel Data Processing in\nthe Cloud,\u201d at 2nd Workshop on Many-Task Computing on Grids and Supercomputers\n(MTAGS), November 2009. doi:10.1145/1646468.1646476\n[69] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd: \u201c The\nPageRank Citation Ranking: Bringing Order to the Web ,\u201d Stanford InfoLab Techni\u2010\ncal Report 422, 1999.\n[70] Leslie G. Valiant: \u201c A Bridging Model for Parallel Computation ,\u201d Communica\u2010\ntions of the ACM , volume 33, number 8, pages 103\u2013111, August 1990. doi:\n10.1145/79173.79181\n[71] Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, and Volker Markl: \u201c Spin\u2010\nning Fast Iterative Data Flows ,\u201d Proceedings of the VLDB Endowment , volume 5,\nnumber 11, pages 1268-1279, July 2012. doi:10.14778/2350229.2350245\n[72] Grzegorz Malewicz, Matthew H. Austern, Aart J. C. Bik, et al.: \u201c Pregel: A System\nfor Large-Scale Graph Processing,\u201d at ACM International Conference on Management\nof Data (SIGMOD), June 2010. doi:10.1145/1807167.1807184\n[73] Frank McSherry, Michael Isard, and Derek G. Murray: \u201c Scalability! But at What\nCOST?,\u201d at 15th USENIX Workshop on Hot Topics in Operating Systems  (HotOS),\nMay 2015.\n[74] Ionel Gog, Malte Schwarzkopf, Natacha Crooks, et al.: \u201c Musketeer: All for One,\nOne for All in Data Processing Systems ,\u201d at 10th European Conference on Computer\nSystems (EuroSys), April 2015. doi:10.1145/2741948.2741968\nSummary | 435", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f435471e-2824-4ac1-8c03-5a47da58fddf": {"__data__": {"id_": "f435471e-2824-4ac1-8c03-5a47da58fddf", "embedding": null, "metadata": {"page_label": "436", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e1f0b58-7bb1-414c-b934-c5850fe5bb4a", "node_type": "4", "metadata": {"page_label": "436", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ee097a3ca966e492ba26946bac259f1ad3060ec473e99c64d611ad299a7d09e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[75] Aapo Kyrola, Guy Blelloch, and Carlos Guestrin: \u201c GraphChi: Large-Scale Graph\nComputation on Just a PC ,\u201d at 10th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI), October 2012.\n[76] Andrew Lenharth, Donald Nguyen, and Keshav Pingali: \u201c Parallel Graph Analyt\u2010\nics,\u201d Communications of the ACM, volume 59, number 5, pages 78\u201387, May 2016. doi:\n10.1145/2901919\n[77] Fabian H\u00fcske: \u201c Peeking into Apache Flink\u2019s Engine Room ,\u201d flink.apache.org,\nMarch 13, 2015.\n[78] Mostafa Mokhtar: \u201c Hive 0.14 Cost Based Optimizer (CBO) Technical Over\u2010\nview,\u201d hortonworks.com, March 2, 2015.\n[79] Michael Armbrust, Reynold S Xin, Cheng Lian, et al.: \u201c Spark SQL: Relational\nData Processing in Spark,\u201d at ACM International Conference on Management of Data\n(SIGMOD), June 2015. doi:10.1145/2723372.2742797\n[80] Daniel Blazevski: \u201c Planting Quadtrees for Apache Flink ,\u201d insightdataengineer\u2010\ning.com, March 25, 2016.\n[81] Tom White: \u201cGenome Analysis Toolkit: Now Using Apache Spark for Data Pro\u2010\ncessing,\u201d blog.cloudera.com, April 6, 2016.\n436 | Chapter 10: Batch Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1078, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf28b57e-ce26-4f2e-b50c-f748dda3373e": {"__data__": {"id_": "cf28b57e-ce26-4f2e-b50c-f748dda3373e", "embedding": null, "metadata": {"page_label": "437", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "416d6001-a1eb-44bd-b44c-853d0dbd64b0", "node_type": "4", "metadata": {"page_label": "437", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5de6c55c-4b3b-4038-a391-6c0a21ace325": {"__data__": {"id_": "5de6c55c-4b3b-4038-a391-6c0a21ace325", "embedding": null, "metadata": {"page_label": "438", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9eab09e-4d9f-45b7-bb51-ea4432e9b921", "node_type": "4", "metadata": {"page_label": "438", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fa2451c-b3e3-4970-9d44-7dceb51f15a3": {"__data__": {"id_": "8fa2451c-b3e3-4970-9d44-7dceb51f15a3", "embedding": null, "metadata": {"page_label": "439", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b2022a6-0497-44a4-a48d-d2e6146eaa10", "node_type": "4", "metadata": {"page_label": "439", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b93cbc895c16925725c1e6d6da37a8baf09cbb1f017a4d1137849076c6f2dd38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 11\nStream Processing\nA complex system that works is invariably found to have evolved from a simple system that\nworks. The inverse proposition also appears to be true: A complex system designed from\nscratch never works and cannot be made to work.\n\u2014John Gall, Systemantics (1975)\nIn Chapter 10 we discussed batch processing\u2014techniques that read a set of files as\ninput and produce a new set of output files. The output is a form of derived data; that\nis, a dataset that can be recreated by running the batch process again if necessary. We\nsaw how this simple but powerful idea can be used to create search indexes, recom\u2010\nmendation systems, analytics, and more.\nHowever, one big assumption remained throughout Chapter 10 : namely, that the\ninput is bounded\u2014i.e., of a known and finite size\u2014so the batch process knows when\nit has finished reading its input. For example, the sorting operation that is central to\nMapReduce must read its entire input before it can start producing output: it could\nhappen that the very last input record is the one with the lowest key, and thus needs\nto be the very first output record, so starting the output early is not an option.\nIn reality, a lot of data is unbounded because it arrives gradually over time: your users\nproduced data yesterday and today, and they will continue to produce more data\ntomorrow. Unless you go out of business, this process never ends, and so the dataset\nis never \u201ccomplete\u201d in any meaningful way [ 1]. Thus, batch processors must artifi\u2010\ncially divide the data into chunks of fixed duration: for example, processing a day\u2019s\nworth of data at the end of every day, or processing an hour\u2019s worth of data at the end\nof every hour.\nThe problem with daily batch processes is that changes in the input are only reflected\nin the output a day later, which is too slow for many impatient users. To reduce the\ndelay, we can run the processing more frequently\u2014say, processing a second\u2019s worth\nof data at the end of every second\u2014or even continuously, abandoning the fixed time\n439", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1616702-c8c5-4580-9b05-79363c1e6f05": {"__data__": {"id_": "b1616702-c8c5-4580-9b05-79363c1e6f05", "embedding": null, "metadata": {"page_label": "440", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d534fcc-46ec-440c-8a4f-19cc53ae45a5", "node_type": "4", "metadata": {"page_label": "440", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4a6f083acf9db10c9979bea3d4102e259ee05eed62068fc3895a78c2c4da3184", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "slices entirely and simply processing every event as it happens. That is the idea\nbehind stream processing.\nIn general, a \u201cstream\u201d refers to data that is incrementally made available over time.\nThe concept appears in many places: in the stdin and stdout of Unix, programming\nlanguages (lazy lists) [2], filesystem APIs (such as Java\u2019s FileInputStream), TCP con\u2010\nnections, delivering audio and video over the internet, and so on.\nIn this chapter we will look at event streams as a data management mechanism: the\nunbounded, incrementally processed counterpart to the batch data we saw in the\nlast chapter. We will first discuss how streams are represented, stored, and transmit\u2010\nted over a network. In \u201cDatabases and Streams\u201d on page 451 we will investigate\nthe relationship between streams and databases. And finally, in \u201cProcessing Streams\u201d\non page 464 we will explore approaches and tools for processing those streams\ncontinually, and ways that they can be used to build applications.\nTransmitting Event Streams\nIn the batch processing world, the inputs and outputs of a job are files (perhaps on a\ndistributed filesystem). What does the streaming equivalent look like?\nWhen the input is a file (a sequence of bytes), the first processing step is usually to\nparse it into a sequence of records. In a stream processing context, a record is more\ncommonly known as an event, but it is essentially the same thing: a small, self-\ncontained, immutable object containing the details of something that happened at\nsome point in time. An event usually contains a timestamp indicating when it hap\u2010\npened according to a time-of-day clock (see \u201cMonotonic Versus Time-of-Day\nClocks\u201d on page 288).\nFor example, the thing that happened might be an action that a user took, such as\nviewing a page or making a purchase. It might also originate from a machine, such as\na periodic measurement from a temperature sensor, or a CPU utilization metric. In\nthe example of \u201cBatch Processing with Unix Tools\u201d on page 391, each line of the web\nserver log is an event.\nAn event may be encoded as a text string, or JSON, or perhaps in some binary form,\nas discussed in Chapter 4. This encoding allows you to store an event, for example by\nappending it to a file, inserting it into a relational table, or writing it to a document\ndatabase. It also allows you to send the event over the network to another node in\norder to process it.\nIn batch processing, a file is written once and then potentially read by multiple jobs.\nAnalogously, in streaming terminology, an event is generated once by a producer\n(also known as a publisher or sender), and then potentially processed by multiple con\u2010\nsumers (subscribers or recipients) [ 3]. In a filesystem, a filename identifies a set of\n440 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab385e4c-eba5-4a35-bd7b-6fea18681725": {"__data__": {"id_": "ab385e4c-eba5-4a35-bd7b-6fea18681725", "embedding": null, "metadata": {"page_label": "441", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84a15434-eb42-4ff2-bfa6-9e1039197932", "node_type": "4", "metadata": {"page_label": "441", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8b662413ad463ffb3fef220b1896e112fc4a2aa02d709c57ff744e2c7f04c8f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "related records; in a streaming system, related events are usually grouped together\ninto a topic or stream.\nIn principle, a file or database is sufficient to connect producers and consumers: a\nproducer writes every event that it generates to the datastore, and each consumer\nperiodically polls the datastore to check for events that have appeared since it last ran.\nThis is essentially what a batch process does when it processes a day\u2019s worth of data at\nthe end of every day.\nHowever, when moving toward continual processing with low delays, polling\nbecomes expensive if the datastore is not designed for this kind of usage. The more\noften you poll, the lower the percentage of requests that return new events, and thus\nthe higher the overheads become. Instead, it is better for consumers to be notified\nwhen new events appear.\nDatabases have traditionally not supported this kind of notification mechanism very\nwell: relational databases commonly have triggers, which can react to a change (e.g., a\nrow being inserted into a table), but they are very limited in what they can do and\nhave been somewhat of an afterthought in database design [ 4, 5]. Instead, specialized\ntools have been developed for the purpose of delivering event notifications.\nMessaging Systems\nA common approach for notifying consumers about new events is to use a messaging\nsystem: a producer sends a message containing the event, which is then pushed to\nconsumers. We touched on these systems previously in \u201cMessage-Passing Dataflow\u201d\non page 136, but we will now go into more detail.\nA direct communication channel like a Unix pipe or TCP connection between pro\u2010\nducer and consumer would be a simple way of implementing a messaging system.\nHowever, most messaging systems expand on this basic model. In particular, Unix\npipes and TCP connect exactly one sender with one recipient, whereas a messaging\nsystem allows multiple producer nodes to send messages to the same topic and allows\nmultiple consumer nodes to receive messages in a topic.\nWithin this publish/subscribe model, different systems take a wide range of\napproaches, and there is no one right answer for all purposes. To differentiate the\nsystems, it is particularly helpful to ask the following two questions:\n1. What happens if the producers send messages faster than the consumers can pro\u2010\ncess them? Broadly speaking, there are three options: the system can drop mes\u2010\nsages, buffer messages in a queue, or apply backpressure (also known as flow\ncontrol; i.e., blocking the producer from sending more messages). For example,\nUnix pipes and TCP use backpressure: they have a small fixed-size buffer, and if\nTransmitting Event Streams | 441", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2678, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c745b5e9-7026-4801-816b-e929e2df29fe": {"__data__": {"id_": "c745b5e9-7026-4801-816b-e929e2df29fe", "embedding": null, "metadata": {"page_label": "442", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf89a88f-b93b-4995-8968-125b7cb57ead", "node_type": "4", "metadata": {"page_label": "442", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b07abfc7b447a2c2ba8ce42205bd2cb960373810ae9eb5ac96c79496f7146aa1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it fills up, the sender is blocked until the recipient takes data out of the buffer (see\n\u201cNetwork congestion and queueing\u201d on page 282).\nIf messages are buffered in a queue, it is important to understand what happens\nas that queue grows. Does the system crash if the queue no longer fits in mem\u2010\nory, or does it write messages to disk? If so, how does the disk access affect the\nperformance of the messaging system [6]?\n2. What happens if nodes crash or temporarily go offline\u2014are any messages lost?  As\nwith databases, durability may require some combination of writing to disk\nand/or replication (see the sidebar \u201cReplication and Durability\u201d on page 227),\nwhich has a cost. If you can afford to sometimes lose messages, you can probably\nget higher throughput and lower latency on the same hardware.\nWhether message loss is acceptable depends very much on the application. For exam\u2010\nple, with sensor readings and metrics that are transmitted periodically, an occasional\nmissing data point is perhaps not important, since an updated value will be sent a\nshort time later anyway. However, beware that if a large number of messages are\ndropped, it may not be immediately apparent that the metrics are incorrect [7]. If you\nare counting events, it is more important that they are delivered reliably, since every\nlost message means incorrect counters.\nA nice property of the batch processing systems we explored in Chapter 10  is that\nthey provide a strong reliability guarantee: failed tasks are automatically retried, and\npartial output from failed tasks is automatically discarded. This means the output is\nthe same as if no failures had occurred, which helps simplify the programming\nmodel. Later in this chapter we will examine how we can provide similar guarantees\nin a streaming context.\nDirect messaging from producers to consumers\nA number of messaging systems use direct network communication between produc\u2010\ners and consumers without going via intermediary nodes:\n\u2022 UDP multicast is widely used in the financial industry for streams such as stock\nmarket feeds, where low latency is important [ 8]. Although UDP itself is unrelia\u2010\nble, application-level protocols can recover lost packets (the producer must\nremember packets it has sent so that it can retransmit them on demand).\n\u2022 Brokerless messaging libraries such as ZeroMQ [ 9] and nanomsg take a similar\napproach, implementing publish/subscribe messaging over TCP or IP multicast.\n\u2022 StatsD [10] and Brubeck [7] use unreliable UDP messaging for collecting metrics\nfrom all machines on the network and monitoring them. (In the StatsD protocol,\ncounter metrics are only correct if all messages are received; using UDP makes\nthe metrics at best approximate [11]. See also \u201cTCP Versus UDP\u201d on page 283.)\n442 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2796, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c30524f2-5f2d-4a87-9d26-b3c8765c588c": {"__data__": {"id_": "c30524f2-5f2d-4a87-9d26-b3c8765c588c", "embedding": null, "metadata": {"page_label": "443", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e7cd6b6-9acf-47b8-8558-24cbde3b5e21", "node_type": "4", "metadata": {"page_label": "443", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "88af830d7599e2aeeb2bc600b55f2657580e73e35376d1e11870a533742df79f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 If the consumer exposes a service on the network, producers can make a direct\nHTTP or RPC request (see \u201cDataflow Through Services: REST and RPC\u201d on page\n131) to push messages to the consumer. This is the idea behind webhooks [ 12], a\npattern in which a callback URL of one service is registered with another service,\nand it makes a request to that URL whenever an event occurs.\nAlthough these direct messaging systems work well in the situations for which they\nare designed, they generally require the application code to be aware of the possibility\nof message loss. The faults they can tolerate are quite limited: even if the protocols\ndetect and retransmit packets that are lost in the network, they generally assume that\nproducers and consumers are constantly online.\nIf a consumer is offline, it may miss messages that were sent while it is unreachable.\nSome protocols allow the producer to retry failed message deliveries, but this\napproach may break down if the producer crashes, losing the buffer of messages that\nit was supposed to retry.\nMessage brokers\nA widely used alternative is to send messages via a message broker (also known as a\nmessage queue), which is essentially a kind of database that is optimized for handling\nmessage streams [ 13]. It runs as a server, with producers and consumers connecting\nto it as clients. Producers write messages to the broker, and consumers receive them\nby reading them from the broker.\nBy centralizing the data in the broker, these systems can more easily tolerate clients\nthat come and go (connect, disconnect, and crash), and the question of durability is\nmoved to the broker instead. Some message brokers only keep messages in memory,\nwhile others (depending on configuration) write them to disk so that they are not lost\nin case of a broker crash. Faced with slow consumers, they generally allow unboun\u2010\nded queueing (as opposed to dropping messages or backpressure), although this\nchoice may also depend on the configuration.\nA consequence of queueing is also that consumers are generally asynchronous: when\na producer sends a message, it normally only waits for the broker to confirm that it\nhas buffered the message and does not wait for the message to be processed by con\u2010\nsumers. The delivery to consumers will happen at some undetermined future point in\ntime\u2014often within a fraction of a second, but sometimes significantly later if there is\na queue backlog.\nMessage brokers compared to databases\nSome message brokers can even participate in two-phase commit protocols using XA\nor JTA (see \u201cDistributed Transactions in Practice\u201d on page 360). This feature makes\nTransmitting Event Streams | 443", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb8740b2-e499-4281-88d5-dd19f2741edf": {"__data__": {"id_": "eb8740b2-e499-4281-88d5-dd19f2741edf", "embedding": null, "metadata": {"page_label": "444", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ede781f-94cc-4ec1-a0ac-79734c148224", "node_type": "4", "metadata": {"page_label": "444", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "eae540a619e118f4520c5426a7d71135cce4135c324c0bf0224913a43569dbcb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "them quite similar in nature to databases, although there are still important practical\ndifferences between message brokers and databases:\n\u2022 Databases usually keep data until it is explicitly deleted, whereas most message\nbrokers automatically delete a message when it has been successfully delivered to\nits consumers. Such message brokers are not suitable for long-term data storage.\n\u2022 Since they quickly delete messages, most message brokers assume that their\nworking set is fairly small\u2014i.e., the queues are short. If the broker needs to buffer\na lot of messages because the consumers are slow (perhaps spilling messages to\ndisk if they no longer fit in memory), each individual message takes longer to\nprocess, and the overall throughput may degrade [6].\n\u2022 Databases often support secondary indexes and various ways of searching for\ndata, while message brokers often support some way of subscribing to a subset of\ntopics matching some pattern. The mechanisms are different, but both are essen\u2010\ntially ways for a client to select the portion of the data that it wants to know\nabout.\n\u2022 When querying a database, the result is typically based on a point-in-time snap\u2010\nshot of the data; if another client subsequently writes something to the database\nthat changes the query result, the first client does not find out that its prior result\nis now outdated (unless it repeats the query, or polls for changes). By contrast,\nmessage brokers do not support arbitrary queries, but they do notify clients when\ndata changes (i.e., when new messages become available).\nThis is the traditional view of message brokers, which is encapsulated in standards\nlike JMS [ 14] and AMQP [ 15] and implemented in software like RabbitMQ,\nActiveMQ, HornetQ, Qpid, TIBCO Enterprise Message Service, IBM MQ, Azure Ser\u2010\nvice Bus, and Google Cloud Pub/Sub [16].\nMultiple consumers\nWhen multiple consumers read messages in the same topic, two main patterns of\nmessaging are used, as illustrated in Figure 11-1:\nLoad balancing\nEach message is delivered to one of the consumers, so the consumers can share\nthe work of processing the messages in the topic. The broker may assign mes\u2010\nsages to consumers arbitrarily. This pattern is useful when the messages are\nexpensive to process, and so you want to be able to add consumers to parallelize\nthe processing. (In AMQP, you can implement load balancing by having multi\u2010\nple clients consuming from the same queue, and in JMS it is called a shared\nsubscription.)\n444 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53ca8140-9c62-42dc-8964-d43bd3bf4920": {"__data__": {"id_": "53ca8140-9c62-42dc-8964-d43bd3bf4920", "embedding": null, "metadata": {"page_label": "445", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b06ace18-964e-4f5b-a098-c0a394666dc9", "node_type": "4", "metadata": {"page_label": "445", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a88e97cd29774811b0c68419561b81a6bde3d7dcf538e647b68c54b2aadaeed1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fan-out\nEach message is delivered to all of the consumers. Fan-out allows several inde\u2010\npendent consumers to each \u201ctune in\u201d to the same broadcast of messages, without\naffecting each other\u2014the streaming equivalent of having several different batch\njobs that read the same input file. (This feature is provided by topic subscriptions\nin JMS, and exchange bindings in AMQP.)\nFigure 11-1. (a) Load balancing: sharing the work of consuming a topic among con\u2010\nsumers; (b) fan-out: delivering each message to multiple consumers.\nThe two patterns can be combined: for example, two separate groups of consumers\nmay each subscribe to a topic, such that each group collectively receives all messages,\nbut within each group only one of the nodes receives each message.\nAcknowledgments and redelivery\nConsumers may crash at any time, so it could happen that a broker delivers a mes\u2010\nsage to a consumer but the consumer never processes it, or only partially processes it\nbefore crashing. In order to ensure that the message is not lost, message brokers use\nacknowledgments: a client must explicitly tell the broker when it has finished process\u2010\ning a message so that the broker can remove it from the queue.\nIf the connection to a client is closed or times out without the broker receiving an\nacknowledgment, it assumes that the message was not processed, and therefore it\ndelivers the message again to another consumer. (Note that it could happen that the\nmessage actually was fully processed, but the acknowledgment was lost in the net\u2010\nwork. Handling this case requires an atomic commit protocol, as discussed in \u201cDis\u2010\ntributed Transactions in Practice\u201d on page 360.)\nTransmitting Event Streams | 445", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed63a022-efee-4971-a6f0-4b87082346d2": {"__data__": {"id_": "ed63a022-efee-4971-a6f0-4b87082346d2", "embedding": null, "metadata": {"page_label": "446", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b346ba6-c764-432b-9006-7a833ff633d0", "node_type": "4", "metadata": {"page_label": "446", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "241bccf93cdc8215f63cdbf68efeb3fae3e816fa2658cbdb98bfa78a201f604d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When combined with load balancing, this redelivery behavior has an interesting\neffect on the ordering of messages. In Figure 11-2, the consumers generally process\nmessages in the order they were sent by producers. However, consumer 2 crashes\nwhile processing message m3, at the same time as consumer 1 is processing message\nm4. The unacknowledged message m3 is subsequently redelivered to consumer 1,\nwith the result that consumer 1 processes messages in the order m4, m3, m5. Thus,\nm3 and m4 are not delivered in the same order as they were sent by producer 1.\nFigure 11-2. Consumer 2 crashes while processing m3, so it is redelivered to consumer 1\nat a later time.\nEven if the message broker otherwise tries to preserve the order of messages (as\nrequired by both the JMS and AMQP standards), the combination of load balancing\nwith redelivery inevitably leads to messages being reordered. To avoid this issue, you\ncan use a separate queue per consumer (i.e., not use the load balancing feature). Mes\u2010\nsage reordering is not a problem if messages are completely independent of each\nother, but it can be important if there are causal dependencies between messages, as\nwe shall see later in the chapter. \nPartitioned Logs\nSending a packet over a network or making a request to a network service is normally\na transient operation that leaves no permanent trace. Although it is possible to record\nit permanently (using packet capture and logging), we normally don\u2019t think of it that\nway. Even message brokers that durably write messages to disk quickly delete them\nagain after they have been delivered to consumers, because they are built around a\ntransient messaging mindset.\n446 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a78d55d-52b7-4e7d-9655-2fd9b8116163": {"__data__": {"id_": "6a78d55d-52b7-4e7d-9655-2fd9b8116163", "embedding": null, "metadata": {"page_label": "447", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a68d8ca-712f-400f-b87f-270898937ad7", "node_type": "4", "metadata": {"page_label": "447", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8aa11ddbede6853d2ca56e1d78a8ec36452d75876d897943bd6ac5ed414c510b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Databases and filesystems take the opposite approach: everything that is written to a\ndatabase or file is normally expected to be permanently recorded, at least until some\u2010\none explicitly chooses to delete it again.\nThis difference in mindset has a big impact on how derived data is created. A key\nfeature of batch processes, as discussed in Chapter 10 , is that you can run them\nrepeatedly, experimenting with the processing steps, without risk of damaging the\ninput (since the input is read-only). This is not the case with AMQP/JMS-style mes\u2010\nsaging: receiving a message is destructive if the acknowledgment causes it to be\ndeleted from the broker, so you cannot run the same consumer again and expect to\nget the same result.\nIf you add a new consumer to a messaging system, it typically only starts receiving\nmessages sent after the time it was registered; any prior messages are already gone\nand cannot be recovered. Contrast this with files and databases, where you can add a\nnew client at any time, and it can read data written arbitrarily far in the past (as long\nas it has not been explicitly overwritten or deleted by the application).\nWhy can we not have a hybrid, combining the durable storage approach of databases\nwith the low-latency notification facilities of messaging? This is the idea behind log-\nbased message brokers.\nUsing logs for message storage\nA log is simply an append-only sequence of records on disk. We previously discussed\nlogs in the context of log-structured storage engines and write-ahead logs in Chap\u2010\nter 3, and in the context of replication in Chapter 5.\nThe same structure can be used to implement a message broker: a producer sends a\nmessage by appending it to the end of the log, and a consumer receives messages by\nreading the log sequentially. If a consumer reaches the end of the log, it waits for a\nnotification that a new message has been appended. The Unix tool tail -f, which\nwatches a file for data being appended, essentially works like this.\nIn order to scale to higher throughput than a single disk can offer, the log can be\npartitioned (in the sense of Chapter 6). Different partitions can then be hosted on dif\u2010\nferent machines, making each partition a separate log that can be read and written\nindependently from other partitions. A topic can then be defined as a group of parti\u2010\ntions that all carry messages of the same type. This approach is illustrated in\nFigure 11-3.\nWithin each partition, the broker assigns a monotonically increasing sequence num\u2010\nber, or offset, to every message (in Figure 11-3, the numbers in boxes are message off\u2010\nsets). Such a sequence number makes sense because a partition is append-only, so the\nmessages within a partition are totally ordered. There is no ordering guarantee across\ndifferent partitions.\nTransmitting Event Streams | 447", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89240719-6ec7-4128-8c1a-6355ff0a4feb": {"__data__": {"id_": "89240719-6ec7-4128-8c1a-6355ff0a4feb", "embedding": null, "metadata": {"page_label": "448", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ba51c12-507c-4e81-9f86-55b53a2f7f69", "node_type": "4", "metadata": {"page_label": "448", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5d515d6c05578d3676a7e8368d1a1a115817e75610af5e35fc5da07e4e4dd3df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 11-3. Producers send messages by appending them to a topic-partition file, and\nconsumers read these files sequentially.\nApache Kafka [ 17, 18], Amazon Kinesis Streams [ 19], and Twitter\u2019s DistributedLog\n[20, 21] are log-based message brokers that work like this. Google Cloud Pub/Sub is\narchitecturally similar but exposes a JMS-style API rather than a log abstraction [ 16].\nEven though these message brokers write all messages to disk, they are able to achieve\nthroughput of millions of messages per second by partitioning across multiple\nmachines, and fault tolerance by replicating messages [22, 23].\nLogs compared to traditional messaging\nThe log-based approach trivially supports fan-out messaging, because several con\u2010\nsumers can independently read the log without affecting each other\u2014reading a mes\u2010\nsage does not delete it from the log. To achieve load balancing across a group of\nconsumers, instead of assigning individual messages to consumer clients, the broker\ncan assign entire partitions to nodes in the consumer group.\nEach client then consumes all the messages in the partitions it has been assigned.\nTypically, when a consumer has been assigned a log partition, it reads the messages in\nthe partition sequentially, in a straightforward single-threaded manner. This coarse-\ngrained load balancing approach has some downsides:\n448 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e8a07ce-5d0f-43d9-9043-757eab459f51": {"__data__": {"id_": "6e8a07ce-5d0f-43d9-9043-757eab459f51", "embedding": null, "metadata": {"page_label": "449", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd2be06c-6fe0-4823-8285-44c5689b8258", "node_type": "4", "metadata": {"page_label": "449", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7ea27e98247a7396bf4f0edf45775a430b97014ca0141357dd3ab7a74c149dcc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. It\u2019s possible to create a load balancing scheme in which two consumers share the work of processing a par\u2010\ntition by having both read the full set of messages, but one of them only considers messages with even-\nnumbered offsets while the other deals with the odd-numbered offsets. Alternatively, you could spread\nmessage processing over a thread pool, but that approach complicates consumer offset management. In gen\u2010\neral, single-threaded processing of a partition is preferable, and parallelism can be increased by using more\npartitions.\n\u2022 The number of nodes sharing the work of consuming a topic can be at most the\nnumber of log partitions in that topic, because messages within the same parti\u2010\ntion are delivered to the same node.i\n\u2022 If a single message is slow to process, it holds up the processing of subsequent\nmessages in that partition (a form of head-of-line blocking; see \u201cDescribing Per\u2010\nformance\u201d on page 13).\nThus, in situations where messages may be expensive to process and you want to par\u2010\nallelize processing on a message-by-message basis, and where message ordering is not\nso important, the JMS/AMQP style of message broker is preferable. On the other\nhand, in situations with high message throughput, where each message is fast to pro\u2010\ncess and where message ordering is important, the log-based approach works very\nwell.\nConsumer offsets\nConsuming a partition sequentially makes it easy to tell which messages have been\nprocessed: all messages with an offset less than a consumer\u2019s current offset have\nalready been processed, and all messages with a greater offset have not yet been seen.\nThus, the broker does not need to track acknowledgments for every single message\u2014\nit only needs to periodically record the consumer offsets. The reduced bookkeeping\noverhead and the opportunities for batching and pipelining in this approach help\nincrease the throughput of log-based systems.\nThis offset is in fact very similar to the log sequence number that is commonly found\nin single-leader database replication, and which we discussed in \u201cSetting Up New\nFollowers\u201d on page 155. In database replication, the log sequence number allows a\nfollower to reconnect to a leader after it has become disconnected, and resume repli\u2010\ncation without skipping any writes. Exactly the same principle is used here: the mes\u2010\nsage broker behaves like a leader database, and the consumer like a follower.\nIf a consumer node fails, another node in the consumer group is assigned the failed\nconsumer\u2019s partitions, and it starts consuming messages at the last recorded offset. If\nthe consumer had processed subsequent messages but not yet recorded their offset,\nthose messages will be processed a second time upon restart. We will discuss ways of\ndealing with this issue later in the chapter.\nTransmitting Event Streams | 449", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5239067f-3e5d-4801-815f-fe47d16250dc": {"__data__": {"id_": "5239067f-3e5d-4801-815f-fe47d16250dc", "embedding": null, "metadata": {"page_label": "450", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "968c7f06-2151-472f-8649-79563383d892", "node_type": "4", "metadata": {"page_label": "450", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1675f5edc409d61dd862bcb818cbb41ac87317e0bb24b8b7ba840a33bf469a19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Disk space usage\nIf you only ever append to the log, you will eventually run out of disk space. To\nreclaim disk space, the log is actually divided into segments, and from time to time\nold segments are deleted or moved to archive storage. (We\u2019ll discuss a more sophisti\u2010\ncated way of freeing disk space later.)\nThis means that if a slow consumer cannot keep up with the rate of messages, and it\nfalls so far behind that its consumer offset points to a deleted segment, it will miss\nsome of the messages. Effectively, the log implements a bounded-size buffer that dis\u2010\ncards old messages when it gets full, also known as a circular buffer  or ring buffer .\nHowever, since that buffer is on disk, it can be quite large.\nLet\u2019s do a back-of-the-envelope calculation. At the time of writing, a typical large\nhard drive has a capacity of 6 TB and a sequential write throughput of 150 MB/s. If\nyou are writing messages at the fastest possible rate, it takes about 11 hours to fill the\ndrive. Thus, the disk can buffer 11 hours\u2019 worth of messages, after which it will start\noverwriting old messages. This ratio remains the same, even if you use many hard\ndrives and machines. In practice, deployments rarely use the full write bandwidth of\nthe disk, so the log can typically keep a buffer of several days\u2019 or even weeks\u2019 worth of\nmessages.\nRegardless of how long you retain messages, the throughput of a log remains more or\nless constant, since every message is written to disk anyway [ 18]. This behavior is in\ncontrast to messaging systems that keep messages in memory by default and only\nwrite them to disk if the queue grows too large: such systems are fast when queues are\nshort and become much slower when they start writing to disk, so the throughput\ndepends on the amount of history retained.\nWhen consumers cannot keep up with producers\nAt the beginning of \u201cMessaging Systems\u201d on page 441 we discussed three choices of\nwhat to do if a consumer cannot keep up with the rate at which producers are send\u2010\ning messages: dropping messages, buffering, or applying backpressure. In this taxon\u2010\nomy, the log-based approach is a form of buffering with a large but fixed-size buffer\n(limited by the available disk space).\nIf a consumer falls so far behind that the messages it requires are older than what is\nretained on disk, it will not be able to read those messages\u2014so the broker effectively\ndrops old messages that go back further than the size of the buffer can accommodate.\nYou can monitor how far a consumer is behind the head of the log, and raise an alert\nif it falls behind significantly. As the buffer is large, there is enough time for a human\noperator to fix the slow consumer and allow it to catch up before it starts missing\nmessages.\n450 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f7b017b-c969-4cbc-9554-79ba8e21acfd": {"__data__": {"id_": "8f7b017b-c969-4cbc-9554-79ba8e21acfd", "embedding": null, "metadata": {"page_label": "451", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8695a0c-5403-4b4a-b00c-8b131d041388", "node_type": "4", "metadata": {"page_label": "451", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "61ddaede323d002837baa0e1d9d808a74a581cd75451f73467882f930ff57807", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even if a consumer does fall too far behind and starts missing messages, only that\nconsumer is affected; it does not disrupt the service for other consumers. This fact is\na big operational advantage: you can experimentally consume a production log for\ndevelopment, testing, or debugging purposes, without having to worry much about\ndisrupting production services. When a consumer is shut down or crashes, it stops\nconsuming resources\u2014the only thing that remains is its consumer offset.\nThis behavior also contrasts with traditional message brokers, where you need to be\ncareful to delete any queues whose consumers have been shut down\u2014otherwise they\ncontinue unnecessarily accumulating messages and taking away memory from con\u2010\nsumers that are still active.\nReplaying old messages\nWe noted previously that with AMQP- and JMS-style message brokers, processing\nand acknowledging messages is a destructive operation, since it causes the messages\nto be deleted on the broker. On the other hand, in a log-based message broker, con\u2010\nsuming messages is more like reading from a file: it is a read-only operation that does\nnot change the log.\nThe only side effect of processing, besides any output of the consumer, is that the\nconsumer offset moves forward. But the offset is under the consumer\u2019s control, so it\ncan easily be manipulated if necessary: for example, you can start a copy of a con\u2010\nsumer with yesterday\u2019s offsets and write the output to a different location, in order to\nreprocess the last day\u2019s worth of messages. You can repeat this any number of times,\nvarying the processing code.\nThis aspect makes log-based messaging more like the batch processes of the last\nchapter, where derived data is clearly separated from input data through a repeatable\ntransformation process. It allows more experimentation and easier recovery from\nerrors and bugs, making it a good tool for integrating dataflows within an organiza\u2010\ntion [24]. \nDatabases and Streams\nWe have drawn some comparisons between message brokers and databases. Even\nthough they have traditionally been considered separate categories of tools, we saw\nthat log-based message brokers have been successful in taking ideas from databases\nand applying them to messaging. We can also go in reverse: take ideas from messag\u2010\ning and streams, and apply them to databases.\nWe said previously that an event is a record of something that happened at some\npoint in time. The thing that happened may be a user action (e.g., typing a search\nquery), or a sensor reading, but it may also be a write to a database . The fact that\nsomething was written to a database is an event that can be captured, stored, and pro\u2010\nDatabases and Streams | 451", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b5d83bff-764e-411f-ab0a-64414049d7a7": {"__data__": {"id_": "b5d83bff-764e-411f-ab0a-64414049d7a7", "embedding": null, "metadata": {"page_label": "452", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77511991-81d6-4abd-b506-97de6f31b92b", "node_type": "4", "metadata": {"page_label": "452", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fedd558ffc2a215178f270851211ab7d288ba1c7563d3dce9e956b2bdc0aad8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "cessed. This observation suggests that the connection between databases and streams\nruns deeper than just the physical storage of logs on disk\u2014it is quite fundamental.\nIn fact, a replication log (see \u201cImplementation of Replication Logs\u201d  on page 158) is a\nstream of database write events, produced by the leader as it processes transactions.\nThe followers apply that stream of writes to their own copy of the database and thus\nend up with an accurate copy of the same data. The events in the replication log\ndescribe the data changes that occurred.\nWe also came across the state machine replication  principle in \u201cTotal Order Broad\u2010\ncast\u201d on page 348, which states: if every event represents a write to the database, and\nevery replica processes the same events in the same order, then the replicas will all\nend up in the same final state. (Processing an event is assumed to be a deterministic\noperation.) It\u2019s just another case of event streams!\nIn this section we will first look at a problem that arises in heterogeneous data sys\u2010\ntems, and then explore how we can solve it by bringing ideas from event streams to\ndatabases.\nKeeping Systems in Sync\nAs we have seen throughout this book, there is no single system that can satisfy all\ndata storage, querying, and processing needs. In practice, most nontrivial applica\u2010\ntions need to combine several different technologies in order to satisfy their require\u2010\nments: for example, using an OLTP database to serve user requests, a cache to speed\nup common requests, a full-text index to handle search queries, and a data warehouse\nfor analytics. Each of these has its own copy of the data, stored in its own representa\u2010\ntion that is optimized for its own purposes.\nAs the same or related data appears in several different places, they need to be kept in\nsync with one another: if an item is updated in the database, it also needs to be upda\u2010\nted in the cache, search indexes, and data warehouse. With data warehouses this syn\u2010\nchronization is usually performed by ETL processes (see \u201cData Warehousing\u201d on\npage 91), often by taking a full copy of a database, transforming it, and bulk-loading\nit into the data warehouse\u2014in other words, a batch process. Similarly, we saw in\n\u201cThe Output of Batch Workflows\u201d on page 411 how search indexes, recommendation\nsystems, and other derived data systems might be created using batch processes.\nIf periodic full database dumps are too slow, an alternative that is sometimes used is\ndual writes , in which the application code explicitly writes to each of the systems\nwhen data changes: for example, first writing to the database, then updating the\nsearch index, then invalidating the cache entries (or even performing those writes\nconcurrently).\nHowever, dual writes have some serious problems, one of which is a race condition\nillustrated in Figure 11-4. In this example, two clients concurrently want to update an\n452 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bec6f598-c082-49a6-b5d0-0423c575fd00": {"__data__": {"id_": "bec6f598-c082-49a6-b5d0-0423c575fd00", "embedding": null, "metadata": {"page_label": "453", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c595d90-86f7-4117-9d03-12cebbd672fa", "node_type": "4", "metadata": {"page_label": "453", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "57bbae3e8703ff686d17c4972f37f8e1d0cecdf503455e69f27f455463caea25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "item X: client 1 wants to set the value to A, and client 2 wants to set it to B. Both\nclients first write the new value to the database, then write it to the search index. Due\nto unlucky timing, the requests are interleaved: the database first sees the write from\nclient 1 setting the value to A, then the write from client 2 setting the value to B, so\nthe final value in the database is B. The search index first sees the write from client 2,\nthen client 1, so the final value in the search index is A. The two systems are now\npermanently inconsistent with each other, even though no error occurred.\nFigure 11-4. In the database, X is first set to A and then to B, while at the search index\nthe writes arrive in the opposite order.\nUnless you have some additional concurrency detection mechanism, such as the ver\u2010\nsion vectors we discussed in \u201cDetecting Concurrent Writes\u201d on page 184, you will not\neven notice that concurrent writes occurred\u2014one value will simply silently overwrite\nanother value.\nAnother problem with dual writes is that one of the writes may fail while the other\nsucceeds. This is a fault-tolerance problem rather than a concurrency problem, but it\nalso has the effect of the two systems becoming inconsistent with each other. Ensur\u2010\ning that they either both succeed or both fail is a case of the atomic commit problem,\nwhich is expensive to solve (see \u201cAtomic Commit and Two-Phase Commit (2PC)\u201d on\npage 354).\nIf you only have one replicated database with a single leader, then that leader deter\u2010\nmines the order of writes, so the state machine replication approach works among\nreplicas of the database. However, in Figure 11-4 there isn\u2019t a single leader: the data\u2010\nbase may have a leader and the search index may have a leader, but neither follows\nthe other, and so conflicts can occur (see \u201cMulti-Leader Replication\u201d on page 168).\nThe situation would be better if there really was only one leader\u2014for example, the\ndatabase\u2014and if we could make the search index a follower of the database. But is\nthis possible in practice? \nDatabases and Streams | 453", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b73a98d1-c0fe-4ddb-80b7-855974b95bf0": {"__data__": {"id_": "b73a98d1-c0fe-4ddb-80b7-855974b95bf0", "embedding": null, "metadata": {"page_label": "454", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be45592f-7e7f-4683-9a84-d84d94fbfa9a", "node_type": "4", "metadata": {"page_label": "454", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fce7ce8caf37717b66ec8aa1a5ab268b5624b892f1b364e2f8506c10647906d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Change Data Capture\nThe problem with most databases\u2019 replication logs is that they have long been consid\u2010\nered to be an internal implementation detail of the database, not a public API. Clients\nare supposed to query the database through its data model and query language, not\nparse the replication logs and try to extract data from them.\nFor decades, many databases simply did not have a documented way of getting the\nlog of changes written to them. For this reason it was difficult to take all the changes\nmade in a database and replicate them to a different storage technology such as a\nsearch index, cache, or data warehouse.\nMore recently, there has been growing interest in change data capture (CDC), which\nis the process of observing all data changes written to a database and extracting them\nin a form in which they can be replicated to other systems. CDC is especially interest\u2010\ning if changes are made available as a stream, immediately as they are written.\nFor example, you can capture the changes in a database and continually apply the\nsame changes to a search index. If the log of changes is applied in the same order, you\ncan expect the data in the search index to match the data in the database. The search\nindex and any other derived data systems are just consumers of the change stream, as\nillustrated in Figure 11-5.\nFigure 11-5. Taking data in the order it was written to one database, and applying the\nchanges to other systems in the same order.\nImplementing change data capture\nWe can call the log consumers derived data systems, as discussed in the introduction\nto Part III: the data stored in the search index and the data warehouse is just another\nview onto the data in the system of record. Change data capture is a mechanism for\nensuring that all changes made to the system of record are also reflected in the\nderived data systems so that the derived systems have an accurate copy of the data.\n454 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "102e125c-f6fa-4863-a495-82a084834b7f": {"__data__": {"id_": "102e125c-f6fa-4863-a495-82a084834b7f", "embedding": null, "metadata": {"page_label": "455", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e14ee7e-a8dc-4a4f-914e-f5418c518135", "node_type": "4", "metadata": {"page_label": "455", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9e6a21b83a5d573bca5cb4a6fcef926cb45f573108977f28679b68f6f3c60c97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Essentially, change data capture makes one database the leader (the one from which\nthe changes are captured), and turns the others into followers. A log-based message\nbroker is well suited for transporting the change events from the source database,\nsince it preserves the ordering of messages (avoiding the reordering issue of\nFigure 11-2).\nDatabase triggers can be used to implement change data capture (see \u201cTrigger-based\nreplication\u201d on page 161) by registering triggers that observe all changes to data\ntables and add corresponding entries to a changelog table. However, they tend to be\nfragile and have significant performance overheads. Parsing the replication log can be\na more robust approach, although it also comes with challenges, such as handling\nschema changes.\nLinkedIn\u2019s Databus [ 25], Facebook\u2019s Wormhole [ 26], and Yahoo!\u2019s Sherpa [ 27] use\nthis idea at large scale. Bottled Water implements CDC for PostgreSQL using an API\nthat decodes the write-ahead log [ 28], Maxwell and Debezium do something similar\nfor MySQL by parsing the binlog [ 29, 30, 31], Mongoriver reads the MongoDB oplog\n[32, 33], and GoldenGate provides similar facilities for Oracle [34, 35].\nLike message brokers, change data capture is usually asynchronous: the system of\nrecord database does not wait for the change to be applied to consumers before com\u2010\nmitting it. This design has the operational advantage that adding a slow consumer\ndoes not affect the system of record too much, but it has the downside that all the\nissues of replication lag apply (see \u201cProblems with Replication Lag\u201d on page 161).\nInitial snapshot\nIf you have the log of all changes that were ever made to a database, you can recon\u2010\nstruct the entire state of the database by replaying the log. However, in many cases,\nkeeping all changes forever would require too much disk space, and replaying it\nwould take too long, so the log needs to be truncated.\nBuilding a new full-text index, for example, requires a full copy of the entire database\n\u2014it is not sufficient to only apply a log of recent changes, since it would be missing\nitems that were not recently updated. Thus, if you don\u2019t have the entire log history,\nyou need to start with a consistent snapshot, as previously discussed in \u201cSetting Up\nNew Followers\u201d on page 155.\nThe snapshot of the database must correspond to a known position or offset in the\nchange log, so that you know at which point to start applying changes after the snap\u2010\nshot has been processed. Some CDC tools integrate this snapshot facility, while oth\u2010\ners leave it as a manual operation.\nDatabases and Streams | 455", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f4d2df9-ea45-4708-ae79-3290a72a4090": {"__data__": {"id_": "8f4d2df9-ea45-4708-ae79-3290a72a4090", "embedding": null, "metadata": {"page_label": "456", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b137b1d1-76eb-4c28-8474-f9d9cb9fb11f", "node_type": "4", "metadata": {"page_label": "456", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9dfcd2cbd65b1e1180567956e90ae64f635324c51a4fdc064c734af27540c5c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Log compaction\nIf you can only keep a limited amount of log history, you need to go through the\nsnapshot process every time you want to add a new derived data system. However,\nlog compaction provides a good alternative.\nWe discussed log compaction previously in \u201cHash Indexes\u201d on page 72, in the con\u2010\ntext of log-structured storage engines (see Figure 3-2 for an example). The principle\nis simple: the storage engine periodically looks for log records with the same key,\nthrows away any duplicates, and keeps only the most recent update for each key. This\ncompaction and merging process runs in the background.\nIn a log-structured storage engine, an update with a special null value (a tombstone)\nindicates that a key was deleted, and causes it to be removed during log compaction.\nBut as long as a key is not overwritten or deleted, it stays in the log forever. The disk\nspace required for such a compacted log depends only on the current contents of the\ndatabase, not the number of writes that have ever occurred in the database. If the\nsame key is frequently overwritten, previous values will eventually be garbage-\ncollected, and only the latest value will be retained.\nThe same idea works in the context of log-based message brokers and change data\ncapture. If the CDC system is set up such that every change has a primary key, and\nevery update for a key replaces the previous value for that key, then it\u2019s sufficient to\nkeep just the most recent write for a particular key.\nNow, whenever you want to rebuild a derived data system such as a search index, you\ncan start a new consumer from offset 0 of the log-compacted topic, and sequentially\nscan over all messages in the log. The log is guaranteed to contain the most recent\nvalue for every key in the database (and maybe some older values)\u2014in other words,\nyou can use it to obtain a full copy of the database contents without having to take\nanother snapshot of the CDC source database.\nThis log compaction feature is supported by Apache Kafka. As we shall see later in\nthis chapter, it allows the message broker to be used for durable storage, not just for\ntransient messaging.\nAPI support for change streams\nIncreasingly, databases are beginning to support change streams as a first-class inter\u2010\nface, rather than the typical retrofitted and reverse-engineered CDC efforts. For\nexample, RethinkDB allows queries to subscribe to notifications when the results of a\nquery change [ 36], Firebase [ 37] and CouchDB [ 38] provide data synchronization\nbased on a change feed that is also made available to applications, and Meteor uses\nthe MongoDB oplog to subscribe to data changes and update the user interface [39].\nVoltDB allows transactions to continuously export data from a database in the form\nof a stream [ 40]. The database represents an output stream in the relational data\n456 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ba336cd-3f58-4c84-b1e4-3eb60aa766f3": {"__data__": {"id_": "7ba336cd-3f58-4c84-b1e4-3eb60aa766f3", "embedding": null, "metadata": {"page_label": "457", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4738e651-60f3-49d9-9f0e-c6e80fc363a0", "node_type": "4", "metadata": {"page_label": "457", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "42fc94657f5f96b6af9021c7d4eefb5fd7ea01ec2bce98e7fee00098886efdd8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model as a table into which transactions can insert tuples, but which cannot be quer\u2010\nied. The stream then consists of the log of tuples that committed transactions have\nwritten to this special table, in the order they were committed. External consumers\ncan asynchronously consume this log and use it to update derived data systems.\nKafka Connect [ 41] is an effort to integrate change data capture tools for a wide\nrange of database systems with Kafka. Once the stream of change events is in Kafka, it\ncan be used to update derived data systems such as search indexes, and also feed into\nstream processing systems as discussed later in this chapter. \nEvent Sourcing\nThere are some parallels between the ideas we\u2019ve discussed here and event sourcing, a\ntechnique that was developed in the domain-driven design (DDD) community [ 42,\n43, 44]. We will discuss event sourcing briefly, because it incorporates some useful\nand relevant ideas for streaming systems.\nSimilarly to change data capture, event sourcing involves storing all changes to the\napplication state as a log of change events. The biggest difference is that event sourc\u2010\ning applies the idea at a different level of abstraction:\n\u2022 In change data capture, the application uses the database in a mutable way,\nupdating and deleting records at will. The log of changes is extracted from the\ndatabase at a low level (e.g., by parsing the replication log), which ensures that\nthe order of writes extracted from the database matches the order in which they\nwere actually written, avoiding the race condition in Figure 11-4. The application\nwriting to the database does not need to be aware that CDC is occurring.\n\u2022 In event sourcing, the application logic is explicitly built on the basis of immuta\u2010\nble events that are written to an event log. In this case, the event store is append-\nonly, and updates or deletes are discouraged or prohibited. Events are designed\nto reflect things that happened at the application level, rather than low-level state\nchanges.\nEvent sourcing is a powerful technique for data modeling: from an application point\nof view it is more meaningful to record the user\u2019s actions as immutable events, rather\nthan recording the effect of those actions on a mutable database. Event sourcing\nmakes it easier to evolve applications over time, helps with debugging by making it\neasier to understand after the fact why something happened, and guards against\napplication bugs (see \u201cAdvantages of immutable events\u201d on page 460).\nFor example, storing the event \u201cstudent cancelled their course enrollment\u201d clearly\nexpresses the intent of a single action in a neutral fashion, whereas the side effects\n\u201cone entry was deleted from the enrollments table, and one cancellation reason was\nadded to the student feedback table\u201d embed a lot of assumptions about the way the\nDatabases and Streams | 457", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d183b6af-427e-46d4-a96e-03a5fadc083b": {"__data__": {"id_": "d183b6af-427e-46d4-a96e-03a5fadc083b", "embedding": null, "metadata": {"page_label": "458", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f07ef0e3-bac2-4112-a550-4aa731fd8236", "node_type": "4", "metadata": {"page_label": "458", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d050c427434ecd7cfd11fcce83cb1d713b891ded63d1c52b075bc1620ddcc897", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "data is later going to be used. If a new application feature is introduced\u2014for example,\n\u201cthe place is offered to the next person on the waiting list\u201d\u2014the event sourcing\napproach allows that new side effect to easily be chained off the existing event.\nEvent sourcing is similar to the chronicle data model [45], and there are also similari\u2010\nties between an event log and the fact table that you find in a star schema (see \u201cStars\nand Snowflakes: Schemas for Analytics\u201d on page 93).\nSpecialized databases such as Event Store [ 46] have been developed to support appli\u2010\ncations using event sourcing, but in general the approach is independent of any par\u2010\nticular tool. A conventional database or a log-based message broker can also be used\nto build applications in this style.\nDeriving current state from the event log\nAn event log by itself is not very useful, because users generally expect to see the cur\u2010\nrent state of a system, not the history of modifications. For example, on a shopping\nwebsite, users expect to be able to see the current contents of their cart, not an\nappend-only list of all the changes they have ever made to their cart.\nThus, applications that use event sourcing need to take the log of events (representing\nthe data written to the system) and transform it into application state that is suitable\nfor showing to a user (the way in which data is read from the system [ 47]). This\ntransformation can use arbitrary logic, but it should be deterministic so that you can\nrun it again and derive the same application state from the event log.\nLike with change data capture, replaying the event log allows you to reconstruct the\ncurrent state of the system. However, log compaction needs to be handled differently:\n\u2022 A CDC event for the update of a record typically contains the entire new version\nof the record, so the current value for a primary key is entirely determined by the\nmost recent event for that primary key, and log compaction can discard previous\nevents for the same key.\n\u2022 On the other hand, with event sourcing, events are modeled at a higher level: an\nevent typically expresses the intent of a user action, not the mechanics of the state\nupdate that occurred as a result of the action. In this case, later events typically\ndo not override prior events, and so you need the full history of events to recon\u2010\nstruct the final state. Log compaction is not possible in the same way.\nApplications that use event sourcing typically have some mechanism for storing\nsnapshots of the current state that is derived from the log of events, so they don\u2019t\nneed to repeatedly reprocess the full log. However, this is only a performance optimi\u2010\nzation to speed up reads and recovery from crashes; the intention is that the system is\nable to store all raw events forever and reprocess the full event log whenever required.\nWe discuss this assumption in \u201cLimitations of immutability\u201d on page 463. \n458 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7917cce0-f165-406e-885e-7760e9931f5b": {"__data__": {"id_": "7917cce0-f165-406e-885e-7760e9931f5b", "embedding": null, "metadata": {"page_label": "459", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4b6e2d4-c5e9-412c-a263-4dd7ac67138e", "node_type": "4", "metadata": {"page_label": "459", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "33e4c1e5b57d85caa781065fbb1fd1c6a4a8a46f771be5b05f711ceb08f43681", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Commands and events\nThe event sourcing philosophy is careful to distinguish between events and com\u2010\nmands [48]. When a request from a user first arrives, it is initially a command: at this\npoint it may still fail, for example because some integrity condition is violated. The\napplication must first validate that it can execute the command. If the validation is\nsuccessful and the command is accepted, it becomes an event, which is durable and\nimmutable.\nFor example, if a user tries to register a particular username, or reserve a seat on an\nairplane or in a theater, then the application needs to check that the username or seat\nis not already taken. (We previously discussed this example in \u201cFault-Tolerant Con\u2010\nsensus\u201d on page 364.) When that check has succeeded, the application can generate\nan event to indicate that a particular username was registered by a particular user ID,\nor that a particular seat has been reserved for a particular customer.\nAt the point when the event is generated, it becomes a fact. Even if the customer later\ndecides to change or cancel the reservation, the fact remains true that they formerly\nheld a reservation for a particular seat, and the change or cancellation is a separate\nevent that is added later.\nA consumer of the event stream is not allowed to reject an event: by the time the con\u2010\nsumer sees the event, it is already an immutable part of the log, and it may have\nalready been seen by other consumers. Thus, any validation of a command needs to\nhappen synchronously, before it becomes an event\u2014for example, by using a serializa\u2010\nble transaction that atomically validates the command and publishes the event.\nAlternatively, the user request to reserve a seat could be split into two events: first a\ntentative reservation, and then a separate confirmation event once the reservation has\nbeen validated (as discussed in \u201cImplementing linearizable storage using total order\nbroadcast\u201d on page 350). This split allows the validation to take place in an asynchro\u2010\nnous process. \nState, Streams, and Immutability\nWe saw in Chapter 10  that batch processing benefits from the immutability of its\ninput files, so you can run experimental processing jobs on existing input files\nwithout fear of damaging them. This principle of immutability is also what makes\nevent sourcing and change data capture so powerful.\nWe normally think of databases as storing the current state of the application\u2014this\nrepresentation is optimized for reads, and it is usually the most convenient for serv\u2010\ning queries. The nature of state is that it changes, so databases support updating and\ndeleting data as well as inserting it. How does this fit with immutability?\nDatabases and Streams | 459", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fdf849e5-e62f-4641-8a6a-bb69fcc12901": {"__data__": {"id_": "fdf849e5-e62f-4641-8a6a-bb69fcc12901", "embedding": null, "metadata": {"page_label": "460", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "268e3d23-d175-460f-9b0c-3c63224b0b97", "node_type": "4", "metadata": {"page_label": "460", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "76b8ac3f1c81535392d4deb95d2866cd752826be3c6e02d622bd96956266aad0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Whenever you have state that changes, that state is the result of the events that muta\u2010\nted it over time. For example, your list of currently available seats is the result of the\nreservations you have processed, the current account balance is the result of the cred\u2010\nits and debits on the account, and the response time graph for your web server is an\naggregation of the individual response times of all web requests that have occurred.\nNo matter how the state changes, there was always a sequence of events that caused\nthose changes. Even as things are done and undone, the fact remains true that those\nevents occurred. The key idea is that mutable state and an append-only log of immut\u2010\nable events do not contradict each other: they are two sides of the same coin. The log\nof all changes, the changelog, represents the evolution of state over time.\nIf you are mathematically inclined, you might say that the application state is what\nyou get when you integrate an event stream over time, and a change stream is what\nyou get when you differentiate the state by time, as shown in Figure 11-6 [49, 50, 51].\nThe analogy has limitations (for example, the second derivative of state does not\nseem to be meaningful), but it\u2019s a useful starting point for thinking about data.\nFigure 11-6. The relationship between the current application state and an event\nstream.\nIf you store the changelog durably, that simply has the effect of making the state\nreproducible. If you consider the log of events to be your system of record, and any\nmutable state as being derived from it, it becomes easier to reason about the flow of\ndata through a system. As Pat Helland puts it [52]:\nTransaction logs record all the changes made to the database. High-speed appends are\nthe only way to change the log. From this perspective, the contents of the database\nhold a caching of the latest record values in the logs. The truth is the log. The database\nis a cache of a subset of the log. That cached subset happens to be the latest value of\neach record and index value from the log.\nLog compaction, as discussed in \u201cLog compaction\u201d on page 456, is one way of bridg\u2010\ning the distinction between log and database state: it retains only the latest version of\neach record, and discards overwritten versions.\nAdvantages of immutable events\nImmutability in databases is an old idea. For example, accountants have been using\nimmutability for centuries in financial bookkeeping. When a transaction occurs, it is\n460 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b4421a52-79e9-4458-aeec-6b001af320fc": {"__data__": {"id_": "b4421a52-79e9-4458-aeec-6b001af320fc", "embedding": null, "metadata": {"page_label": "461", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe45250b-25e0-46f3-be4b-8b26d7ee8be7", "node_type": "4", "metadata": {"page_label": "461", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "95db9a95c00dd53a8b778c94a11f26a032ed92cd466ff948db48de5898d15533", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "recorded in an append-only ledger, which is essentially a log of events describing\nmoney, goods, or services that have changed hands. The accounts, such as profit and\nloss or the balance sheet, are derived from the transactions in the ledger by adding\nthem up [53].\nIf a mistake is made, accountants don\u2019t erase or change the incorrect transaction in\nthe ledger\u2014instead, they add another transaction that compensates for the mistake,\nfor example refunding an incorrect charge. The incorrect transaction still remains in\nthe ledger forever, because it might be important for auditing reasons. If incorrect\nfigures, derived from the incorrect ledger, have already been published, then the fig\u2010\nures for the next accounting period include a correction. This process is entirely nor\u2010\nmal in accounting [54].\nAlthough such auditability is particularly important in financial systems, it is also\nbeneficial for many other systems that are not subject to such strict regulation. As\ndiscussed in \u201cPhilosophy of batch process outputs\u201d on page 413, if you accidentally\ndeploy buggy code that writes bad data to a database, recovery is much harder if the\ncode is able to destructively overwrite data. With an append-only log of immutable\nevents, it is much easier to diagnose what happened and recover from the problem.\nImmutable events also capture more information than just the current state. For\nexample, on a shopping website, a customer may add an item to their cart and then\nremove it again. Although the second event cancels out the first event from the point\nof view of order fulfillment, it may be useful to know for analytics purposes that the\ncustomer was considering a particular item but then decided against it. Perhaps they\nwill choose to buy it in the future, or perhaps they found a substitute. This informa\u2010\ntion is recorded in an event log, but would be lost in a database that deletes items\nwhen they are removed from the cart [42].\nDeriving several views from the same event log\nMoreover, by separating mutable state from the immutable event log, you can derive\nseveral different read-oriented representations from the same log of events. This\nworks just like having multiple consumers of a stream ( Figure 11-5): for example, the\nanalytic database Druid ingests directly from Kafka using this approach [ 55], Pista\u2010\nchio is a distributed key-value store that uses Kafka as a commit log [ 56], and Kafka\nConnect sinks can export data from Kafka to various different databases and indexes\n[41]. It would make sense for many other storage and indexing systems, such as\nsearch servers, to similarly take their input from a distributed log (see \u201cKeeping Sys\u2010\ntems in Sync\u201d on page 452).\nHaving an explicit translation step from an event log to a database makes it easier to\nevolve your application over time: if you want to introduce a new feature that\npresents your existing data in some new way, you can use the event log to build a\nseparate read-optimized view for the new feature, and run it alongside the existing\nDatabases and Streams | 461", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3053, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3afa05b5-d65a-4d39-99bd-e883b621ef19": {"__data__": {"id_": "3afa05b5-d65a-4d39-99bd-e883b621ef19", "embedding": null, "metadata": {"page_label": "462", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a33f9f31-d173-4f26-ab64-b78e454e6e7b", "node_type": "4", "metadata": {"page_label": "462", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9c0907e5293d547ca711e24d381daa91acaed77b6e23ccd716404b4a2d1167cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "systems without having to modify them. Running old and new systems side by side is\noften easier than performing a complicated schema migration in an existing system.\nOnce the old system is no longer needed, you can simply shut it down and reclaim its\nresources [47, 57].\nStoring data is normally quite straightforward if you don\u2019t have to worry about how it\nis going to be queried and accessed; many of the complexities of schema design,\nindexing, and storage engines are the result of wanting to support certain query and\naccess patterns (see Chapter 3). For this reason, you gain a lot of flexibility by sepa\u2010\nrating the form in which data is written from the form it is read, and by allowing sev\u2010\neral different read views. This idea is sometimes known as command query\nresponsibility segregation (CQRS) [42, 58, 59].\nThe traditional approach to database and schema design is based on the fallacy that\ndata must be written in the same form as it will be queried. Debates about normaliza\u2010\ntion and denormalization (see \u201cMany-to-One and Many-to-Many Relationships\u201d  on\npage 33) become largely irrelevant if you can translate data from a write-optimized\nevent log to read-optimized application state: it is entirely reasonable to denormalize\ndata in the read-optimized views, as the translation process gives you a mechanism\nfor keeping it consistent with the event log.\nIn \u201cDescribing Load\u201d on page 11 we discussed Twitter\u2019s home timelines, a cache of\nrecently written tweets by the people a particular user is following (like a mailbox).\nThis is another example of read-optimized state: home timelines are highly denor\u2010\nmalized, since your tweets are duplicated in all of the timelines of the people follow\u2010\ning you. However, the fan-out service keeps this duplicated state in sync with new\ntweets and new following relationships, which keeps the duplication manageable.\nConcurrency control\nThe biggest downside of event sourcing and change data capture is that the consum\u2010\ners of the event log are usually asynchronous, so there is a possibility that a user may\nmake a write to the log, then read from a log-derived view and find that their write\nhas not yet been reflected in the read view. We discussed this problem and potential\nsolutions previously in \u201cReading Your Own Writes\u201d on page 162.\nOne solution would be to perform the updates of the read view synchronously with\nappending the event to the log. This requires a transaction to combine the writes into\nan atomic unit, so either you need to keep the event log and the read view in the same\nstorage system, or you need a distributed transaction across the different systems.\nAlternatively, you could use the approach discussed in \u201cImplementing linearizable\nstorage using total order broadcast\u201d on page 350.\nOn the other hand, deriving the current state from an event log also simplifies some\naspects of concurrency control. Much of the need for multi-object transactions (see\n\u201cSingle-Object and Multi-Object Operations\u201d on page 228) stems from a single user\n462 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4242c3d-88b9-418d-acfe-7d17143e2b90": {"__data__": {"id_": "a4242c3d-88b9-418d-acfe-7d17143e2b90", "embedding": null, "metadata": {"page_label": "463", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "759b35bf-9700-4feb-a710-198a75996616", "node_type": "4", "metadata": {"page_label": "463", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b5d7208ba85b1c2a1d567adea9ddf6f7f9ad50373aac9a25002b54a103196ec6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "action requiring data to be changed in several different places. With event sourcing,\nyou can design an event such that it is a self-contained description of a user action.\nThe user action then requires only a single write in one place\u2014namely appending the\nevents to the log\u2014which is easy to make atomic.\nIf the event log and the application state are partitioned in the same way (for exam\u2010\nple, processing an event for a customer in partition 3 only requires updating partition\n3 of the application state), then a straightforward single-threaded log consumer needs\nno concurrency control for writes\u2014by construction, it only processes a single event\nat a time (see also \u201cActual Serial Execution\u201d on page 252). The log removes the non\u2010\ndeterminism of concurrency by defining a serial order of events in a partition [ 24]. If\nan event touches multiple state partitions, a bit more work is required, which we will\ndiscuss in Chapter 12. \nLimitations of immutability\nMany systems that don\u2019t use an event-sourced model nevertheless rely on immutabil\u2010\nity: various databases internally use immutable data structures or multi-version data\nto support point-in-time snapshots (see \u201cIndexes and snapshot isolation\u201d on page\n241). Version control systems such as Git, Mercurial, and Fossil also rely on immuta\u2010\nble data to preserve version history of files.\nTo what extent is it feasible to keep an immutable history of all changes forever? The\nanswer depends on the amount of churn in the dataset. Some workloads mostly add\ndata and rarely update or delete; they are easy to make immutable. Other workloads\nhave a high rate of updates and deletes on a comparatively small dataset; in these\ncases, the immutable history may grow prohibitively large, fragmentation may\nbecome an issue, and the performance of compaction and garbage collection\nbecomes crucial for operational robustness [60, 61].\nBesides the performance reasons, there may also be circumstances in which you need\ndata to be deleted for administrative reasons, in spite of all immutability. For exam\u2010\nple, privacy regulations may require deleting a user\u2019s personal information after they\nclose their account, data protection legislation may require erroneous information to\nbe removed, or an accidental leak of sensitive information may need to be contained.\nIn these circumstances, it\u2019s not sufficient to just append another event to the log to\nindicate that the prior data should be considered deleted\u2014you actually want to\nrewrite history and pretend that the data was never written in the first place. For\nexample, Datomic calls this feature excision [62], and the Fossil version control sys\u2010\ntem has a similar concept called shunning [63].\nTruly deleting data is surprisingly hard [ 64], since copies can live in many places: for\nexample, storage engines, filesystems, and SSDs often write to a new location rather\nthan overwriting in place [ 52], and backups are often deliberately immutable to pre\u2010\nvent accidental deletion or corruption. Deletion is more a matter of \u201cmaking it harder\nDatabases and Streams | 463", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a01450af-e5ee-48b7-ada1-b13bda805a30": {"__data__": {"id_": "a01450af-e5ee-48b7-ada1-b13bda805a30", "embedding": null, "metadata": {"page_label": "464", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cda95b9-3300-4296-bd13-f0ce7b223d37", "node_type": "4", "metadata": {"page_label": "464", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "96f1d2eee20d55fa36fc9ee5b7c0820086d82c18d35a91ba01e7e9290d2a93e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to retrieve the data\u201d than actually \u201cmaking it impossible to retrieve the data.\u201d Never\u2010\ntheless, you sometimes have to try, as we shall see in \u201cLegislation and self-regulation\u201d\non page 542. \nProcessing Streams\nSo far in this chapter we have talked about where streams come from (user activity\nevents, sensors, and writes to databases), and we have talked about how streams are\ntransported (through direct messaging, via message brokers, and in event logs).\nWhat remains is to discuss what you can do with the stream once you have it\u2014\nnamely, you can process it. Broadly, there are three options:\n1. You can take the data in the events and write it to a database, cache, search index,\nor similar storage system, from where it can then be queried by other clients. As\nshown in Figure 11-5 , this is a good way of keeping a database in sync with\nchanges happening in other parts of the system\u2014especially if the stream con\u2010\nsumer is the only client writing to the database. Writing to a storage system is the\nstreaming equivalent of what we discussed in \u201cThe Output of Batch Workflows\u201d\non page 411.\n2. You can push the events to users in some way, for example by sending email\nalerts or push notifications, or by streaming the events to a real-time dashboard\nwhere they are visualized. In this case, a human is the ultimate consumer of the\nstream.\n3. You can process one or more input streams to produce one or more output\nstreams. Streams may go through a pipeline consisting of several such processing\nstages before they eventually end up at an output (option 1 or 2).\nIn the rest of this chapter, we will discuss option 3: processing streams to produce\nother, derived streams. A piece of code that processes streams like this is known as an\noperator or a job. It is closely related to the Unix processes and MapReduce jobs we\ndiscussed in Chapter 10 , and the pattern of dataflow is similar: a stream processor\nconsumes input streams in a read-only fashion and writes its output to a different\nlocation in an append-only fashion.\nThe patterns for partitioning and parallelization in stream processors are also very\nsimilar to those in MapReduce and the dataflow engines we saw in Chapter 10, so we\nwon\u2019t repeat those topics here. Basic mapping operations such as transforming and\nfiltering records also work the same.\nThe one crucial difference to batch jobs is that a stream never ends. This difference\nhas many implications: as discussed at the start of this chapter, sorting does not make\nsense with an unbounded dataset, and so sort-merge joins (see \u201cReduce-Side Joins\nand Grouping\u201d on page 403) cannot be used. Fault-tolerance mechanisms must also\n464 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79322ace-b4a6-4be2-9497-cf78ef7d56b3": {"__data__": {"id_": "79322ace-b4a6-4be2-9497-cf78ef7d56b3", "embedding": null, "metadata": {"page_label": "465", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0325c47-3f7e-46e9-aaca-b5618ebf055c", "node_type": "4", "metadata": {"page_label": "465", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "704cedef6a1834bfb8cb840d505a1ade9f38d4952d3eb8070b23b52914e68b1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "change: with a batch job that has been running for a few minutes, a failed task can\nsimply be restarted from the beginning, but with a stream job that has been running\nfor several years, restarting from the beginning after a crash may not be a viable\noption.\nUses of Stream Processing\nStream processing has long been used for monitoring purposes, where an organiza\u2010\ntion wants to be alerted if certain things happen. For example:\n\u2022 Fraud detection systems need to determine if the usage patterns of a credit card\nhave unexpectedly changed, and block the card if it is likely to have been stolen.\n\u2022 Trading systems need to examine price changes in a financial market and execute\ntrades according to specified rules.\n\u2022 Manufacturing systems need to monitor the status of machines in a factory, and\nquickly identify the problem if there is a malfunction.\n\u2022 Military and intelligence systems need to track the activities of a potential aggres\u2010\nsor, and raise the alarm if there are signs of an attack.\nThese kinds of applications require quite sophisticated pattern matching and correla\u2010\ntions. However, other uses of stream processing have also emerged over time. In this\nsection we will briefly compare and contrast some of these applications.\nComplex event processing\nComplex event processing (CEP) is an approach developed in the 1990s for analyzing\nevent streams, especially geared toward the kind of application that requires search\u2010\ning for certain event patterns [ 65, 66]. Similarly to the way that a regular expression\nallows you to search for certain patterns of characters in a string, CEP allows you to\nspecify rules to search for certain patterns of events in a stream.\nCEP systems often use a high-level declarative query language like SQL, or a graphi\u2010\ncal user interface, to describe the patterns of events that should be detected. These\nqueries are submitted to a processing engine that consumes the input streams and\ninternally maintains a state machine that performs the required matching. When a\nmatch is found, the engine emits a complex event (hence the name) with the details of\nthe event pattern that was detected [67].\nIn these systems, the relationship between queries and data is reversed compared to\nnormal databases. Usually, a database stores data persistently and treats queries as\ntransient: when a query comes in, the database searches for data matching the query,\nand then forgets about the query when it has finished. CEP engines reverse these\nroles: queries are stored long-term, and events from the input streams continuously\nflow past them in search of a query that matches an event pattern [68].\nProcessing Streams | 465", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f6b9974-7da9-4a32-baaf-db0f144776d8": {"__data__": {"id_": "4f6b9974-7da9-4a32-baaf-db0f144776d8", "embedding": null, "metadata": {"page_label": "466", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "556f65ab-88fb-4aba-8e8c-7829a9de24a6", "node_type": "4", "metadata": {"page_label": "466", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "034a08624287f9e39dce032580e520d07e1e7ac27c9228ef177e5f12931d0e96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Implementations of CEP include Esper [ 69], IBM InfoSphere Streams [ 70], Apama,\nTIBCO StreamBase, and SQLstream. Distributed stream processors like Samza are\nalso gaining SQL support for declarative queries on streams [71].\nStream analytics\nAnother area in which stream processing is used is for analytics on streams. The\nboundary between CEP and stream analytics is blurry, but as a general rule, analytics\ntends to be less interested in finding specific event sequences and is more oriented\ntoward aggregations and statistical metrics over a large number of events\u2014for exam\u2010\nple:\n\u2022 Measuring the rate of some type of event (how often it occurs per time interval)\n\u2022 Calculating the rolling average of a value over some time period\n\u2022 Comparing current statistics to previous time intervals (e.g., to detect trends or\nto alert on metrics that are unusually high or low compared to the same time last\nweek)\nSuch statistics are usually computed over fixed time intervals\u2014for example, you\nmight want to know the average number of queries per second to a service over the\nlast 5 minutes, and their 99th percentile response time during that period. Averaging\nover a few minutes smoothes out irrelevant fluctuations from one second to the next,\nwhile still giving you a timely picture of any changes in traffic pattern. The time\ninterval over which you aggregate is known as a window, and we will look into win\u2010\ndowing in more detail in \u201cReasoning About Time\u201d on page 468.\nStream analytics systems sometimes use probabilistic algorithms, such as Bloom fil\u2010\nters (which we encountered in \u201cPerformance optimizations\u201d on page 79) for set\nmembership, HyperLogLog [ 72] for cardinality estimation, and various percentile\nestimation algorithms (see \u201cPercentiles in Practice\u201d on page 16). Probabilistic algo\u2010\nrithms produce approximate results, but have the advantage of requiring significantly\nless memory in the stream processor than exact algorithms. This use of approxima\u2010\ntion algorithms sometimes leads people to believe that stream processing systems are\nalways lossy and inexact, but that is wrong: there is nothing inherently approximate\nabout stream processing, and probabilistic algorithms are merely an optimization\n[73].\nMany open source distributed stream processing frameworks are designed with ana\u2010\nlytics in mind: for example, Apache Storm, Spark Streaming, Flink, Concord, Samza,\nand Kafka Streams [ 74]. Hosted services include Google Cloud Dataflow and Azure\nStream Analytics.\n466 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef0c9b42-428d-40ac-8f9c-4802e8f3fcb2": {"__data__": {"id_": "ef0c9b42-428d-40ac-8f9c-4802e8f3fcb2", "embedding": null, "metadata": {"page_label": "467", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dea88df1-71db-48ff-861f-cfd6d384bd06", "node_type": "4", "metadata": {"page_label": "467", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bc4ee8542b08f274e531879e7137be3c2aa419c68415d3aec4b017b3a6090944", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Maintaining materialized views\nWe saw in \u201cDatabases and Streams\u201d on page 451 that a stream of changes to a data\u2010\nbase can be used to keep derived data systems, such as caches, search indexes, and\ndata warehouses, up to date with a source database. We can regard these examples as\nspecific cases of maintaining materialized views (see \u201cAggregation: Data Cubes and\nMaterialized Views\u201d on page 101): deriving an alternative view onto some dataset so\nthat you can query it efficiently, and updating that view whenever the underlying\ndata changes [50].\nSimilarly, in event sourcing, application state is maintained by applying a log of\nevents; here the application state is also a kind of materialized view. Unlike stream\nanalytics scenarios, it is usually not sufficient to consider only events within some\ntime window: building the materialized view potentially requires all events over an\narbitrary time period, apart from any obsolete events that may be discarded by log\ncompaction (see \u201cLog compaction\u201d on page 456). In effect, you need a window that\nstretches all the way back to the beginning of time.\nIn principle, any stream processor could be used for materialized view maintenance,\nalthough the need to maintain events forever runs counter to the assumptions of\nsome analytics-oriented frameworks that mostly operate on windows of a limited\nduration. Samza and Kafka Streams support this kind of usage, building upon Kafka\u2019s\nsupport for log compaction [75].\nSearch on streams\nBesides CEP, which allows searching for patterns consisting of multiple events, there\nis also sometimes a need to search for individual events based on complex criteria,\nsuch as full-text search queries.\nFor example, media monitoring services subscribe to feeds of news articles and\nbroadcasts from media outlets, and search for any news mentioning companies,\nproducts, or topics of interest. This is done by formulating a search query in advance,\nand then continually matching the stream of news items against this query. Similar\nfeatures exist on some websites: for example, users of real estate websites can ask to\nbe notified when a new property matching their search criteria appears on the mar\u2010\nket. The percolator feature of Elasticsearch [ 76] is one option for implementing this\nkind of stream search.\nConventional search engines first index the documents and then run queries over the\nindex. By contrast, searching a stream turns the processing on its head: the queries\nare stored, and the documents run past the queries, like in CEP. In the simplest case,\nyou can test every document against every query, although this can get slow if you\nhave a large number of queries. To optimize the process, it is possible to index the\nqueries as well as the documents, and thus narrow down the set of queries that may\nmatch [77].\nProcessing Streams | 467", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c775082-e439-4cc5-a189-e99bb59ce3cd": {"__data__": {"id_": "9c775082-e439-4cc5-a189-e99bb59ce3cd", "embedding": null, "metadata": {"page_label": "468", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89a5411c-aa01-4c2b-9d00-04d430ac929b", "node_type": "4", "metadata": {"page_label": "468", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6c04d151671cdd4ed466a999c897c2e339f0284bf5d0083dbc07658d2a667e56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Message passing and RPC\nIn \u201cMessage-Passing Dataflow\u201d on page 136 we discussed message-passing systems as\nan alternative to RPC\u2014i.e., as a mechanism for services to communicate, as used for\nexample in the actor model. Although these systems are also based on messages and\nevents, we normally don\u2019t think of them as stream processors:\n\u2022 Actor frameworks are primarily a mechanism for managing concurrency and\ndistributed execution of communicating modules, whereas stream processing is\nprimarily a data management technique.\n\u2022 Communication between actors is often ephemeral and one-to-one, whereas\nevent logs are durable and multi-subscriber.\n\u2022 Actors can communicate in arbitrary ways (including cyclic request/response\npatterns), but stream processors are usually set up in acyclic pipelines where\nevery stream is the output of one particular job, and derived from a well-defined\nset of input streams.\nThat said, there is some crossover area between RPC-like systems and stream pro\u2010\ncessing. For example, Apache Storm has a feature called distributed RPC , which\nallows user queries to be farmed out to a set of nodes that also process event streams;\nthese queries are then interleaved with events from the input streams, and results can\nbe aggregated and sent back to the user [ 78]. (See also \u201cMulti-partition data process\u2010\ning\u201d on page 514.)\nIt is also possible to process streams using actor frameworks. However, many such\nframeworks do not guarantee message delivery in the case of crashes, so the process\u2010\ning is not fault-tolerant unless you implement additional retry logic.\nReasoning About Time\nStream processors often need to deal with time, especially when used for analytics\npurposes, which frequently use time windows such as \u201cthe average over the last five\nminutes.\u201d It might seem that the meaning of \u201cthe last five minutes\u201d should be unam\u2010\nbiguous and clear, but unfortunately the notion is surprisingly tricky.\nIn a batch process, the processing tasks rapidly crunch through a large collection of\nhistorical events. If some kind of breakdown by time needs to happen, the batch pro\u2010\ncess needs to look at the timestamp embedded in each event. There is no point in\nlooking at the system clock of the machine running the batch process, because the\ntime at which the process is run has nothing to do with the time at which the events\nactually occurred.\nA batch process may read a year\u2019s worth of historical events within a few minutes; in\nmost cases, the timeline of interest is the year of history, not the few minutes of pro\u2010\ncessing. Moreover, using the timestamps in the events allows the processing to be\n468 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "443f7b96-1b65-475f-92b1-e44745626f13": {"__data__": {"id_": "443f7b96-1b65-475f-92b1-e44745626f13", "embedding": null, "metadata": {"page_label": "469", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fae5da1-37ad-4a3c-8804-fdd107bbcb39", "node_type": "4", "metadata": {"page_label": "469", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b16810d4275a2efa5b32a3fd2d4b984ff4919ce093d538104e17f08be5c175dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. Thank you to Kostas Kloudas from the Flink community for coming up with this analogy.\ndeterministic: running the same process again on the same input yields the same\nresult (see \u201cFault tolerance\u201d on page 422).\nOn the other hand, many stream processing frameworks use the local system clock\non the processing machine (the processing time) to determine windowing [ 79]. This\napproach has the advantage of being simple, and it is reasonable if the delay between\nevent creation and event processing is negligibly short. However, it breaks down if\nthere is any significant processing lag\u2014i.e., if the processing may happen noticeably\nlater than the time at which the event actually occurred.\nEvent time versus processing time\nThere are many reasons why processing may be delayed: queueing, network faults\n(see \u201cUnreliable Networks\u201d on page 277), a performance issue leading to contention\nin the message broker or processor, a restart of the stream consumer, or reprocessing\nof past events (see \u201cReplaying old messages\u201d on page 451) while recovering from a\nfault or after fixing a bug in the code.\nMoreover, message delays can also lead to unpredictable ordering of messages. For\nexample, say a user first makes one web request (which is handled by web server A),\nand then a second request (which is handled by server B). A and B emit events\ndescribing the requests they handled, but B\u2019s event reaches the message broker before\nA\u2019s event does. Now stream processors will first see the B event and then the A event,\neven though they actually occurred in the opposite order.\nIf it helps to have an analogy, consider the Star Wars movies: Episode IV was released\nin 1977, Episode V in 1980, and Episode VI in 1983, followed by Episodes I, II, and\nIII in 1999, 2002, and 2005, respectively, and Episode VII in 2015 [ 80].ii If you\nwatched the movies in the order they came out, the order in which you processed the\nmovies is inconsistent with the order of their narrative. (The episode number is like\nthe event timestamp, and the date when you watched the movie is the processing\ntime.) As humans, we are able to cope with such discontinuities, but stream process\u2010\ning algorithms need to be specifically written to accommodate such timing and\nordering issues.\nConfusing event time and processing time leads to bad data. For example, say you\nhave a stream processor that measures the rate of requests (counting the number of\nrequests per second). If you redeploy the stream processor, it may be shut down for a\nminute and process the backlog of events when it comes back up. If you measure the\nrate based on the processing time, it will look as if there was a sudden anomalous\nspike of requests while processing the backlog, when in fact the real rate of requests\nwas steady (Figure 11-7).\nProcessing Streams | 469", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c539421d-cfc5-43ca-bf63-bf4c9d11ca5e": {"__data__": {"id_": "c539421d-cfc5-43ca-bf63-bf4c9d11ca5e", "embedding": null, "metadata": {"page_label": "470", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb91518e-ccca-4a97-8afa-b65ddedc40c8", "node_type": "4", "metadata": {"page_label": "470", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "35709c6b8f8bdca8fe34d94dc1b623f25f147c0bed61eacbf80975e3ab7b1b86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 11-7. Windowing by processing time introduces artifacts due to variations in\nprocessing rate.\nKnowing when you\u2019re ready\nA tricky problem when defining windows in terms of event time is that you can never\nbe sure when you have received all of the events for a particular window, or whether\nthere are some events still to come.\nFor example, say you\u2019re grouping events into one-minute windows so that you can\ncount the number of requests per minute. You have counted some number of events\nwith timestamps that fall in the 37th minute of the hour, and time has moved on;\nnow most of the incoming events fall within the 38th and 39th minutes of the hour.\nWhen do you declare that you have finished the window for the 37th minute, and\noutput its counter value?\nYou can time out and declare a window ready after you have not seen any new events\nfor a while, but it could still happen that some events were buffered on another\nmachine somewhere, delayed due to a network interruption. You need to be able to\nhandle such straggler events that arrive after the window has already been declared\ncomplete. Broadly, you have two options [1]:\n1. Ignore the straggler events, as they are probably a small percentage of events in\nnormal circumstances. You can track the number of dropped events as a metric,\nand alert if you start dropping a significant amount of data.\n2. Publish a correction, an updated value for the window with stragglers included.\nYou may also need to retract the previous output.\n470 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1529, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f086129b-5cd9-4e3d-a8b4-c81808701ddb": {"__data__": {"id_": "f086129b-5cd9-4e3d-a8b4-c81808701ddb", "embedding": null, "metadata": {"page_label": "471", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e74ee4f-2c37-4c3a-9094-67291e7aed0c", "node_type": "4", "metadata": {"page_label": "471", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6ee306258139026a0fd97ee36c9912f18e2ed4b0e6543bbddc87c48cb37fe271", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In some cases it is possible to use a special message to indicate, \u201cFrom now on there\nwill be no more messages with a timestamp earlier than t,\u201d which can be used by con\u2010\nsumers to trigger windows [ 81]. However, if several producers on different machines\nare generating events, each with their own minimum timestamp thresholds, the con\u2010\nsumers need to keep track of each producer individually. Adding and removing pro\u2010\nducers is trickier in this case.\nWhose clock are you using, anyway?\nAssigning timestamps to events is even more difficult when events can be buffered at\nseveral points in the system. For example, consider a mobile app that reports events\nfor usage metrics to a server. The app may be used while the device is offline, in\nwhich case it will buffer events locally on the device and send them to a server when\nan internet connection is next available (which may be hours or even days later). To\nany consumers of this stream, the events will appear as extremely delayed stragglers.\nIn this context, the timestamp on the events should really be the time at which the\nuser interaction occurred, according to the mobile device\u2019s local clock. However, the\nclock on a user-controlled device often cannot be trusted, as it may be accidentally or\ndeliberately set to the wrong time (see \u201cClock Synchronization and Accuracy\u201d on\npage 289). The time at which the event was received by the server (according to the\nserver\u2019s clock) is more likely to be accurate, since the server is under your control, but\nless meaningful in terms of describing the user interaction.\nTo adjust for incorrect device clocks, one approach is to log three timestamps [82]:\n\u2022 The time at which the event occurred, according to the device clock\n\u2022 The time at which the event was sent to the server, according to the device clock\n\u2022 The time at which the event was received by the server, according to the server\nclock\nBy subtracting the second timestamp from the third, you can estimate the offset\nbetween the device clock and the server clock (assuming the network delay is negligi\u2010\nble compared to the required timestamp accuracy). You can then apply that offset to\nthe event timestamp, and thus estimate the true time at which the event actually\noccurred (assuming the device clock offset did not change between the time the event\noccurred and the time it was sent to the server).\nThis problem is not unique to stream processing\u2014batch processing suffers from\nexactly the same issues of reasoning about time. It is just more noticeable in a stream\u2010\ning context, where we are more aware of the passage of time.\nProcessing Streams | 471", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2617, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ccebcd0-69bd-4d59-a113-7fa96f57b959": {"__data__": {"id_": "6ccebcd0-69bd-4d59-a113-7fa96f57b959", "embedding": null, "metadata": {"page_label": "472", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6ed040d-4599-4f63-9713-21643c08d14b", "node_type": "4", "metadata": {"page_label": "472", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b7b128b4e2c5f54602b7b319aae31601d847c31e66a736cf3e0d5fb68bec3bc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Types of windows\nOnce you know how the timestamp of an event should be determined, the next step\nis to decide how windows over time periods should be defined. The window can then\nbe used for aggregations, for example to count events, or to calculate the average of\nvalues within the window. Several types of windows are in common use [79, 83]:\nTumbling window\nA tumbling window has a fixed length, and every event belongs to exactly one\nwindow. For example, if you have a 1-minute tumbling window, all the events\nwith timestamps between 10:03:00 and 10:03:59 are grouped into one window,\nevents between 10:04:00 and 10:04:59 into the next window, and so on. You\ncould implement a 1-minute tumbling window by taking each event timestamp\nand rounding it down to the nearest minute to determine the window that it\nbelongs to.\nHopping window\nA hopping window also has a fixed length, but allows windows to overlap in\norder to provide some smoothing. For example, a 5-minute window with a hop\nsize of 1 minute would contain the events between 10:03:00 and 10:07:59, then\nthe next window would cover events between 10:04:00 and 10:08:59, and so on.\nYou can implement this hopping window by first calculating 1-minute tumbling\nwindows, and then aggregating over several adjacent windows.\nSliding window\nA sliding window contains all the events that occur within some interval of each\nother. For example, a 5-minute sliding window would cover events at 10:03:39\nand 10:08:12, because they are less than 5 minutes apart (note that tumbling and\nhopping 5-minute windows would not have put these two events in the same\nwindow, as they use fixed boundaries). A sliding window can be implemented by\nkeeping a buffer of events sorted by time and removing old events when they\nexpire from the window.\nSession window\nUnlike the other window types, a session window has no fixed duration. Instead,\nit is defined by grouping together all events for the same user that occur closely\ntogether in time, and the window ends when the user has been inactive for some\ntime (for example, if there have been no events for 30 minutes). Sessionization is\na common requirement for website analytics (see \u201cGROUP BY\u201d on page 406). \nStream Joins\nIn Chapter 10 we discussed how batch jobs can join datasets by key, and how such\njoins form an important part of data pipelines. Since stream processing generalizes\n472 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21ac0cbf-9094-4a7e-8934-d0791f6423fd": {"__data__": {"id_": "21ac0cbf-9094-4a7e-8934-d0791f6423fd", "embedding": null, "metadata": {"page_label": "473", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2507874c-147f-4ba2-b52d-0bce0275324f", "node_type": "4", "metadata": {"page_label": "473", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "210f158a315e3919c4ab09c6a9250a172e879431458e5f3c31a2dd2864cacc70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "data pipelines to incremental processing of unbounded datasets, there is exactly the\nsame need for joins on streams.\nHowever, the fact that new events can appear anytime on a stream makes joins on\nstreams more challenging than in batch jobs. To understand the situation better, let\u2019s\ndistinguish three different types of joins: stream-stream joins, stream-table joins, and\ntable-table joins [84]. In the following sections we\u2019ll illustrate each by example.\nStream-stream join (window join)\nSay you have a search feature on your website, and you want to detect recent trends\nin searched-for URLs. Every time someone types a search query, you log an event\ncontaining the query and the results returned. Every time someone clicks one of the\nsearch results, you log another event recording the click. In order to calculate the\nclick-through rate for each URL in the search results, you need to bring together the\nevents for the search action and the click action, which are connected by having the\nsame session ID. Similar analyses are needed in advertising systems [85].\nThe click may never come if the user abandons their search, and even if it comes, the\ntime between the search and the click may be highly variable: in many cases it might\nbe a few seconds, but it could be as long as days or weeks (if a user runs a search,\nforgets about that browser tab, and then returns to the tab and clicks a result some\u2010\ntime later). Due to variable network delays, the click event may even arrive before the\nsearch event. You can choose a suitable window for the join\u2014for example, you may\nchoose to join a click with a search if they occur at most one hour apart.\nNote that embedding the details of the search in the click event is not equivalent to\njoining the events: doing so would only tell you about the cases where the user\nclicked a search result, not about the searches where the user did not click any of the\nresults. In order to measure search quality, you need accurate click-through rates, for\nwhich you need both the search events and the click events.\nTo implement this type of join, a stream processor needs to maintain state: for exam\u2010\nple, all the events that occurred in the last hour, indexed by session ID. Whenever a\nsearch event or click event occurs, it is added to the appropriate index, and the\nstream processor also checks the other index to see if another event for the same ses\u2010\nsion ID has already arrived. If there is a matching event, you emit an event saying\nwhich search result was clicked. If the search event expires without you seeing a\nmatching click event, you emit an event saying which search results were not clicked.\nStream-table join (stream enrichment)\nIn \u201cExample: analysis of user activity events\u201d on page 404 (Figure 10-2) we saw an\nexample of a batch job joining two datasets: a set of user activity events and a data\u2010\nbase of user profiles. It is natural to think of the user activity events as a stream, and\nto perform the same join on a continuous basis in a stream processor: the input is a\nProcessing Streams | 473", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3057, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2537d46-2f96-46f8-b062-a2039c1d5119": {"__data__": {"id_": "f2537d46-2f96-46f8-b062-a2039c1d5119", "embedding": null, "metadata": {"page_label": "474", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aae6d196-35aa-4723-a43d-12ee2e12dad4", "node_type": "4", "metadata": {"page_label": "474", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6f4f1253584b4ad4472e4597ebf2e0ce5148684e64f2f6b8faa241bc853ff24b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "stream of activity events containing a user ID, and the output is a stream of activity\nevents in which the user ID has been augmented with profile information about the\nuser. This process is sometimes known as enriching the activity events with informa\u2010\ntion from the database.\nTo perform this join, the stream process needs to look at one activity event at a time,\nlook up the event\u2019s user ID in the database, and add the profile information to the\nactivity event. The database lookup could be implemented by querying a remote\ndatabase; however, as discussed in \u201cExample: analysis of user activity events\u201d  on page\n404, such remote queries are likely to be slow and risk overloading the database [75].\nAnother approach is to load a copy of the database into the stream processor so that\nit can be queried locally without a network round-trip. This technique is very similar\nto the hash joins we discussed in \u201cMap-Side Joins\u201d on page 408: the local copy of the\ndatabase might be an in-memory hash table if it is small enough, or an index on the\nlocal disk.\nThe difference to batch jobs is that a batch job uses a point-in-time snapshot of the\ndatabase as input, whereas a stream processor is long-running, and the contents of\nthe database are likely to change over time, so the stream processor\u2019s local copy of the\ndatabase needs to be kept up to date. This issue can be solved by change data capture:\nthe stream processor can subscribe to a changelog of the user profile database as well\nas the stream of activity events. When a profile is created or modified, the stream\nprocessor updates its local copy. Thus, we obtain a join between two streams: the\nactivity events and the profile updates.\nA stream-table join is actually very similar to a stream-stream join; the biggest differ\u2010\nence is that for the table changelog stream, the join uses a window that reaches back\nto the \u201cbeginning of time\u201d (a conceptually infinite window), with newer versions of\nrecords overwriting older ones. For the stream input, the join might not maintain a\nwindow at all.\nTable-table join (materialized view maintenance)\nConsider the Twitter timeline example that we discussed in \u201cDescribing Load\u201d  on\npage 11. We said that when a user wants to view their home timeline, it is too expen\u2010\nsive to iterate over all the people the user is following, find their recent tweets, and\nmerge them.\nInstead, we want a timeline cache: a kind of per-user \u201cinbox\u201d to which tweets are\nwritten as they are sent, so that reading the timeline is a single lookup. Materializing\nand maintaining this cache requires the following event processing:\n\u2022 When user u sends a new tweet, it is added to the timeline of every user who is\nfollowing u.\n474 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7454ecad-a503-4d2b-aee3-3ce634036818": {"__data__": {"id_": "7454ecad-a503-4d2b-aee3-3ce634036818", "embedding": null, "metadata": {"page_label": "475", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "885ccbb4-34ac-456b-87b4-03cf1b17db25", "node_type": "4", "metadata": {"page_label": "475", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f82d179a75578c19a981f8938880ccc467f975edc1dda7eaeda9aae22b64617f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. If you regard a stream as the derivative of a table, as in Figure 11-6, and regard a join as a product of two\ntables u\u00b7v, something interesting happens: the stream of changes to the materialized join follows the product\nrule (u\u00b7v)\u2032 = u\u2032v + uv\u2032. In words: any change of tweets is joined with the current followers, and any change of\nfollowers is joined with the current tweets [49, 50].\n\u2022 When a user deletes a tweet, it is removed from all users\u2019 timelines.\n\u2022 When user u1 starts following user u2, recent tweets by u2 are added to u1\u2019s\ntimeline.\n\u2022 When user u1 unfollows user u2, tweets by u2 are removed from u1\u2019s timeline.\nTo implement this cache maintenance in a stream processor, you need streams of\nevents for tweets (sending and deleting) and for follow relationships (following and\nunfollowing). The stream process needs to maintain a database containing the set of\nfollowers for each user so that it knows which timelines need to be updated when a\nnew tweet arrives [86].\nAnother way of looking at this stream process is that it maintains a materialized view\nfor a query that joins two tables (tweets and follows), something like the following:\nSELECT follows.follower_id AS timeline_id,\n  array_agg(tweets.* ORDER BY tweets.timestamp DESC)\nFROM tweets\nJOIN follows ON follows.followee_id = tweets.sender_id\nGROUP BY follows.follower_id\nThe join of the streams corresponds directly to the join of the tables in that query.\nThe timelines are effectively a cache of the result of this query, updated every time the\nunderlying tables change.iii\nTime-dependence of joins\nThe three types of joins described here (stream-stream, stream-table, and table-table)\nhave a lot in common: they all require the stream processor to maintain some state\n(search and click events, user profiles, or follower list) based on one join input, and\nquery that state on messages from the other join input.\nThe order of the events that maintain the state is important (it matters whether you\nfirst follow and then unfollow, or the other way round). In a partitioned log, the\nordering of events within a single partition is preserved, but there is typically no\nordering guarantee across different streams or partitions.\nThis raises a question: if events on different streams happen around a similar time, in\nwhich order are they processed? In the stream-table join example, if a user updates\ntheir profile, which activity events are joined with the old profile (processed before\nthe profile update), and which are joined with the new profile (processed after the\nProcessing Streams | 475", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba0d06b2-5e73-4c31-b4f0-1a885f68c384": {"__data__": {"id_": "ba0d06b2-5e73-4c31-b4f0-1a885f68c384", "embedding": null, "metadata": {"page_label": "476", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff564d44-d7e6-474f-abba-fd234ef7df00", "node_type": "4", "metadata": {"page_label": "476", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c5f3f9b453d45dce7377a2f9dada8ba6505eff9170574dc665828520d438b07e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "profile update)? Put another way: if state changes over time, and you join with some\nstate, what point in time do you use for the join [45]?\nSuch time dependence can occur in many places. For example, if you sell things, you\nneed to apply the right tax rate to invoices, which depends on the country or state,\nthe type of product, and the date of sale (since tax rates change from time to time).\nWhen joining sales to a table of tax rates, you probably want to join with the tax rate\nat the time of the sale, which may be different from the current tax rate if you are\nreprocessing historical data.\nIf the ordering of events across streams is undetermined, the join becomes nondeter\u2010\nministic [ 87], which means you cannot rerun the same job on the same input and\nnecessarily get the same result: the events on the input streams may be interleaved in\na different way when you run the job again.\nIn data warehouses, this issue is known as a slowly changing dimension (SCD), and it\nis often addressed by using a unique identifier for a particular version of the joined\nrecord: for example, every time the tax rate changes, it is given a new identifier, and\nthe invoice includes the identifier for the tax rate at the time of sale [ 88, 89]. This\nchange makes the join deterministic, but has the consequence that log compaction is\nnot possible, since all versions of the records in the table need to be retained. \nFault Tolerance\nIn the final section of this chapter, let\u2019s consider how stream processors can tolerate\nfaults. We saw in Chapter 10  that batch processing frameworks can tolerate faults\nfairly easily: if a task in a MapReduce job fails, it can simply be started again on\nanother machine, and the output of the failed task is discarded. This transparent retry\nis possible because input files are immutable, each task writes its output to a separate\nfile on HDFS, and output is only made visible when a task completes successfully.\nIn particular, the batch approach to fault tolerance ensures that the output of the\nbatch job is the same as if nothing had gone wrong, even if in fact some tasks did fail.\nIt appears as though every input record was processed exactly once\u2014no records are\nskipped, and none are processed twice. Although restarting tasks means that records\nmay in fact be processed multiple times, the visible effect in the output is as if they\nhad only been processed once. This principle is known as exactly-once semantics ,\nalthough effectively-once would be a more descriptive term [90].\nThe same issue of fault tolerance arises in stream processing, but it is less straightfor\u2010\nward to handle: waiting until a task is finished before making its output visible is not\nan option, because a stream is infinite and so you can never finish processing it.\n476 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4de2793c-4712-4165-b783-bd094e6b0811": {"__data__": {"id_": "4de2793c-4712-4165-b783-bd094e6b0811", "embedding": null, "metadata": {"page_label": "477", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e22e822-31c6-4813-82e3-3b0dd629067a", "node_type": "4", "metadata": {"page_label": "477", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "535ed5cee9c7ae6dce46e72bbc438d4f1ad2bf9738866f9bfc88d34dc67b1130", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Microbatching and checkpointing\nOne solution is to break the stream into small blocks, and treat each block like a min\u2010\niature batch process. This approach is called microbatching, and it is used in Spark\nStreaming [91]. The batch size is typically around one second, which is the result of a\nperformance compromise: smaller batches incur greater scheduling and coordination\noverhead, while larger batches mean a longer delay before results of the stream pro\u2010\ncessor become visible.\nMicrobatching also implicitly provides a tumbling window equal to the batch size\n(windowed by processing time, not event timestamps); any jobs that require larger\nwindows need to explicitly carry over state from one microbatch to the next.\nA variant approach, used in Apache Flink, is to periodically generate rolling check\u2010\npoints of state and write them to durable storage [ 92, 93]. If a stream operator\ncrashes, it can restart from its most recent checkpoint and discard any output gener\u2010\nated between the last checkpoint and the crash. The checkpoints are triggered by bar\u2010\nriers in the message stream, similar to the boundaries between microbatches, but\nwithout forcing a particular window size.\nWithin the confines of the stream processing framework, the microbatching and\ncheckpointing approaches provide the same exactly-once semantics as batch process\u2010\ning. However, as soon as output leaves the stream processor (for example, by writing\nto a database, sending messages to an external message broker, or sending emails),\nthe framework is no longer able to discard the output of a failed batch. In this case,\nrestarting a failed task causes the external side effect to happen twice, and micro\u2010\nbatching or checkpointing alone is not sufficient to prevent this problem.\nAtomic commit revisited\nIn order to give the appearance of exactly-once processing in the presence of faults,\nwe need to ensure that all outputs and side effects of processing an event take effect if\nand only if  the processing is successful. Those effects include any messages sent to\ndownstream operators or external messaging systems (including email or push notifi\u2010\ncations), any database writes, any changes to operator state, and any acknowledg\u2010\nment of input messages (including moving the consumer offset forward in a log-\nbased message broker).\nThose things either all need to happen atomically, or none of them must happen, but\nthey should not go out of sync with each other. If this approach sounds familiar, it is\nbecause we discussed it in \u201cExactly-once message processing\u201d on page 360 in the con\u2010\ntext of distributed transactions and two-phase commit.\nIn Chapter 9  we discussed the problems in the traditional implementations of dis\u2010\ntributed transactions, such as XA. However, in more restricted environments it is\npossible to implement such an atomic commit facility efficiently. This approach is\nProcessing Streams | 477", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0f19198-d87a-430f-a53f-a4059a0884c3": {"__data__": {"id_": "b0f19198-d87a-430f-a53f-a4059a0884c3", "embedding": null, "metadata": {"page_label": "478", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13238669-c93a-448f-8db4-b4ad15071394", "node_type": "4", "metadata": {"page_label": "478", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3e839f259f87e47b98290be29f4aba8e0f59278e4bb9c3633765cced591348f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "used in Google Cloud Dataflow [ 81, 92] and VoltDB [ 94], and there are plans to add\nsimilar features to Apache Kafka [ 95, 96]. Unlike XA, these implementations do not\nattempt to provide transactions across heterogeneous technologies, but instead keep\nthem internal by managing both state changes and messaging within the stream pro\u2010\ncessing framework. The overhead of the transaction protocol can be amortized by\nprocessing several input messages within a single transaction.\nIdempotence\nOur goal is to discard the partial output of any failed tasks so that they can be safely\nretried without taking effect twice. Distributed transactions are one way of achieving\nthat goal, but another way is to rely on idempotence [97].\nAn idempotent operation is one that you can perform multiple times, and it has the\nsame effect as if you performed it only once. For example, setting a key in a key-value\nstore to some fixed value is idempotent (writing the value again simply overwrites the\nvalue with an identical value), whereas incrementing a counter is not idempotent\n(performing the increment again means the value is incremented twice).\nEven if an operation is not naturally idempotent, it can often be made idempotent\nwith a bit of extra metadata. For example, when consuming messages from Kafka,\nevery message has a persistent, monotonically increasing offset. When writing a value\nto an external database, you can include the offset of the message that triggered the\nlast write with the value. Thus, you can tell whether an update has already been\napplied, and avoid performing the same update again.\nThe state handling in Storm\u2019s Trident is based on a similar idea [ 78]. Relying on\nidempotence implies several assumptions: restarting a failed task must replay the\nsame messages in the same order (a log-based message broker does this), the process\u2010\ning must be deterministic, and no other node may concurrently update the same\nvalue [98, 99].\nWhen failing over from one processing node to another, fencing may be required (see\n\u201cThe leader and the lock\u201d on page 301) to prevent interference from a node that is\nthought to be dead but is actually alive. Despite all those caveats, idempotent opera\u2010\ntions can be an effective way of achieving exactly-once semantics with only a small\noverhead.\nRebuilding state after a failure\nAny stream process that requires state\u2014for example, any windowed aggregations\n(such as counters, averages, and histograms) and any tables and indexes used for\njoins\u2014must ensure that this state can be recovered after a failure.\nOne option is to keep the state in a remote datastore and replicate it, although having\nto query a remote database for each individual message can be slow, as discussed in\n478 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc295ccb-67b5-456f-b56d-bf9f2c0bbdfa": {"__data__": {"id_": "dc295ccb-67b5-456f-b56d-bf9f2c0bbdfa", "embedding": null, "metadata": {"page_label": "479", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fc8790d-3459-4432-9a9a-2dc4513c3c3f", "node_type": "4", "metadata": {"page_label": "479", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4cbf8333b40ecf58666d0b052b57c16fad54c69ce8580117cf66076a0e856ffa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u201cStream-table join (stream enrichment)\u201d on page 473. An alternative is to keep state\nlocal to the stream processor, and replicate it periodically. Then, when the stream\nprocessor is recovering from a failure, the new task can read the replicated state and\nresume processing without data loss.\nFor example, Flink periodically captures snapshots of operator state and writes them\nto durable storage such as HDFS [ 92, 93]; Samza and Kafka Streams replicate state\nchanges by sending them to a dedicated Kafka topic with log compaction, similar to\nchange data capture [ 84, 100]. VoltDB replicates state by redundantly processing\neach input message on several nodes (see \u201cActual Serial Execution\u201d on page 252).\nIn some cases, it may not even be necessary to replicate the state, because it can be\nrebuilt from the input streams. For example, if the state consists of aggregations over\na fairly short window, it may be fast enough to simply replay the input events corre\u2010\nsponding to that window. If the state is a local replica of a database, maintained by\nchange data capture, the database can also be rebuilt from the log-compacted change\nstream (see \u201cLog compaction\u201d on page 456).\nHowever, all of these trade-offs depend on the performance characteristics of the\nunderlying infrastructure: in some systems, network delay may be lower than disk\naccess latency, and network bandwidth may be comparable to disk bandwidth. There\nis no universally ideal trade-off for all situations, and the merits of local versus\nremote state may also shift as storage and networking technologies evolve. \nSummary\nIn this chapter we have discussed event streams, what purposes they serve, and how\nto process them. In some ways, stream processing is very much like the batch pro\u2010\ncessing we discussed in Chapter 10 , but done continuously on unbounded (never-\nending) streams rather than on a fixed-size input. From this perspective, message\nbrokers and event logs serve as the streaming equivalent of a filesystem.\nWe spent some time comparing two types of message brokers:\nAMQP/JMS-style message broker\nThe broker assigns individual messages to consumers, and consumers acknowl\u2010\nedge individual messages when they have been successfully processed. Messages\nare deleted from the broker once they have been acknowledged. This approach is\nappropriate as an asynchronous form of RPC (see also \u201cMessage-Passing Data\u2010\nflow\u201d on page 136), for example in a task queue, where the exact order of mes\u2010\nsage processing is not important and where there is no need to go back and read\nold messages again after they have been processed.\nSummary | 479", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2615, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3613c13b-e0a7-4228-ab85-a7b88f3c4a83": {"__data__": {"id_": "3613c13b-e0a7-4228-ab85-a7b88f3c4a83", "embedding": null, "metadata": {"page_label": "480", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3ae67c8-10a6-492d-98fd-caf2df38d93a", "node_type": "4", "metadata": {"page_label": "480", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5877b646117d024d66b376f0227430f6a93bcc61528dbae258e1da0e1a49a501", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Log-based message broker\nThe broker assigns all messages in a partition to the same consumer node, and\nalways delivers messages in the same order. Parallelism is achieved through par\u2010\ntitioning, and consumers track their progress by checkpointing the offset of the\nlast message they have processed. The broker retains messages on disk, so it is\npossible to jump back and reread old messages if necessary.\nThe log-based approach has similarities to the replication logs found in databases\n(see Chapter 5) and log-structured storage engines (see Chapter 3). We saw that this\napproach is especially appropriate for stream processing systems that consume input\nstreams and generate derived state or derived output streams.\nIn terms of where streams come from, we discussed several possibilities: user activity\nevents, sensors providing periodic readings, and data feeds (e.g., market data in\nfinance) are naturally represented as streams. We saw that it can also be useful to\nthink of the writes to a database as a stream: we can capture the changelog\u2014i.e., the\nhistory of all changes made to a database\u2014either implicitly through change data cap\u2010\nture or explicitly through event sourcing. Log compaction allows the stream to retain\na full copy of the contents of a database.\nRepresenting databases as streams opens up powerful opportunities for integrating\nsystems. You can keep derived data systems such as search indexes, caches, and ana\u2010\nlytics systems continually up to date by consuming the log of changes and applying\nthem to the derived system. You can even build fresh views onto existing data by\nstarting from scratch and consuming the log of changes from the beginning all the\nway to the present.\nThe facilities for maintaining state as streams and replaying messages are also the\nbasis for the techniques that enable stream joins and fault tolerance in various stream\nprocessing frameworks. We discussed several purposes of stream processing, includ\u2010\ning searching for event patterns (complex event processing), computing windowed\naggregations (stream analytics), and keeping derived data systems up to date (materi\u2010\nalized views).\nWe then discussed the difficulties of reasoning about time in a stream processor,\nincluding the distinction between processing time and event timestamps, and the\nproblem of dealing with straggler events that arrive after you thought your window\nwas complete.\nWe distinguished three types of joins that may appear in stream processes:\nStream-stream joins\nBoth input streams consist of activity events, and the join operator searches for\nrelated events that occur within some window of time. For example, it may\nmatch two actions taken by the same user within 30 minutes of each other. The\n480 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b86e8f6a-376a-465d-a70a-b301ced72c89": {"__data__": {"id_": "b86e8f6a-376a-465d-a70a-b301ced72c89", "embedding": null, "metadata": {"page_label": "481", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c973714-1327-4f17-a8ad-3e6bbbe6c8c7", "node_type": "4", "metadata": {"page_label": "481", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "411f911b46cffe57ba832707819df2963d3f76758936d00d3bdd2503e2c65104", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "two join inputs may in fact be the same stream (a self-join) if you want to find\nrelated events within that one stream.\nStream-table joins\nOne input stream consists of activity events, while the other is a database change\u2010\nlog. The changelog keeps a local copy of the database up to date. For each activity\nevent, the join operator queries the database and outputs an enriched activity\nevent.\nTable-table joins\nBoth input streams are database changelogs. In this case, every change on one\nside is joined with the latest state of the other side. The result is a stream of\nchanges to the materialized view of the join between the two tables.\nFinally, we discussed techniques for achieving fault tolerance and exactly-once\nsemantics in a stream processor. As with batch processing, we need to discard the\npartial output of any failed tasks. However, since a stream process is long-running\nand produces output continuously, we can\u2019t simply discard all output. Instead, a\nfiner-grained recovery mechanism can be used, based on microbatching, checkpoint\u2010\ning, transactions, or idempotent writes. \nReferences\n[1] Tyler Akidau, Robert Bradshaw, Craig Chambers, et al.: \u201c The Dataflow Model: A\nPractical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale,\nUnbounded, Out-of-Order Data Processing ,\u201d Proceedings of the VLDB Endowment ,\nvolume 8, number 12, pages 1792\u20131803, August 2015. doi:10.14778/2824032.2824076\n[2] Harold Abelson, Gerald Jay Sussman, and Julie Sussman: Structure and Interpre\u2010\ntation of Computer Programs , 2nd edition. MIT Press, 1996. ISBN:\n978-0-262-51087-5, available online at mitpress.mit.edu\n[3] Patrick Th. Eugster, Pascal A. Felber, Rachid Guerraoui, and Anne-Marie Ker\u2010\nmarrec: \u201c The Many Faces of Publish/Subscribe ,\u201d ACM Computing Surveys , volume\n35, number 2, pages 114\u2013131, June 2003. doi:10.1145/857076.857078\n[4] Joseph M. Hellerstein and Michael Stonebraker: Readings in Database Systems ,\n4th edition. MIT Press, 2005. ISBN: 978-0-262-69314-1, available online at red\u2010\nbook.cs.berkeley.edu\n[5] Don Carney, U\u011fur \u00c7etintemel, Mitch Cherniack, et al.: \u201c Monitoring Streams \u2013 A\nNew Class of Data Management Applications ,\u201d at 28th International Conference on\nVery Large Data Bases (VLDB), August 2002.\n[6] Matthew Sackman: \u201cPushing Back,\u201d lshift.net, May 5, 2016.\nSummary | 481", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad71549c-8290-491a-b8fa-de15fab0163b": {"__data__": {"id_": "ad71549c-8290-491a-b8fa-de15fab0163b", "embedding": null, "metadata": {"page_label": "482", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2463e0f9-f295-4325-9a22-a2489ed55899", "node_type": "4", "metadata": {"page_label": "482", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cbada9f054ab9b63bc84e21e61b85ee9876420f5f278dda521c15a2e93e988f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[7] Vicent Mart\u00ed: \u201c Brubeck, a statsd-Compatible Metrics Aggregator ,\u201d githubengin\u2010\neering.com, June 15, 2015.\n[8] Seth Lowenberger: \u201c MoldUDP64 Protocol Specification V 1.00 ,\u201d nasdaq\u2010\ntrader.com, July 2009.\n[9] Pieter Hintjens: ZeroMQ \u2013 The Guide . O\u2019Reilly Media, 2013. ISBN:\n978-1-449-33404-8\n[10] Ian Malpass: \u201c Measure Anything, Measure Everything ,\u201d codeascraft.com, Febru\u2010\nary 15, 2011.\n[11] Dieter Plaetinck: \u201c 25 Graphite, Grafana and statsd Gotchas ,\u201d blog.raintank.io,\nMarch 3, 2016.\n[12] Jeff Lindsay: \u201c Web Hooks to Revolutionize the Web ,\u201d progrium.com, May 3,\n2007.\n[13] Jim N. Gray: \u201c Queues Are Databases ,\u201d Microsoft Research Technical Report\nMSR-TR-95-56, December 1995.\n[14] Mark Hapner, Rich Burridge, Rahul Sharma, et al.: \u201c JSR-343 Java Message Ser\u2010\nvice (JMS) 2.0 Specification,\u201d jms-spec.java.net, March 2013.\n[15] Sanjay Aiyagari, Matthew Arrott, Mark Atwell, et al.: \u201c AMQP: Advanced Mes\u2010\nsage Queuing Protocol Specification,\u201d Version 0-9-1, November 2008.\n[16] \u201cGoogle Cloud Pub/Sub: A Google-Scale Messaging Service ,\u201d cloud.google.com,\n2016.\n[17] \u201cApache Kafka 0.9 Documentation,\u201d kafka.apache.org, November 2015.\n[18] Jay Kreps, Neha Narkhede, and Jun Rao: \u201c Kafka: A Distributed Messaging Sys\u2010\ntem for Log Processing ,\u201d at 6th International Workshop on Networking Meets Data\u2010\nbases (NetDB), June 2011.\n[19] \u201cAmazon Kinesis Streams Developer Guide,\u201d docs.aws.amazon.com, April 2016.\n[20] Leigh Stewart and Sijie Guo: \u201c Building DistributedLog: Twitter\u2019s High-\nPerformance Replicated Log Service,\u201d blog.twitter.com, September 16, 2015.\n[21] \u201cDistributedLog Documentation,\u201d Twitter, Inc., distributedlog.io, May 2016.\n[22] Jay Kreps: \u201c Benchmarking Apache Kafka: 2 Million Writes Per Second (On\nThree Cheap Machines),\u201d engineering.linkedin.com, April 27, 2014.\n[23] Kartik Paramasivam: \u201c How We\u2019re Improving and Advancing Kafka at\nLinkedIn,\u201d engineering.linkedin.com, September 2, 2015.\n[24] Jay Kreps: \u201c The Log: What Every Software Engineer Should Know About Real-\nTime Data\u2019s Unifying Abstraction,\u201d engineering.linkedin.com, December 16, 2013.\n482 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "498cc788-2eb3-4341-9088-ddd75b485c07": {"__data__": {"id_": "498cc788-2eb3-4341-9088-ddd75b485c07", "embedding": null, "metadata": {"page_label": "483", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8aa29d93-878f-441e-a49d-089ece12d7b0", "node_type": "4", "metadata": {"page_label": "483", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "adce5d3e89834d2d4845d8ce88fbc43299d7803e4d00d56e256a25fa355cd6fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[25] Shirshanka Das, Chavdar Botev, Kapil Surlaker, et al.: \u201c All Aboard the Data\u2010\nbus!,\u201d at 3rd ACM Symposium on Cloud Computing (SoCC), October 2012.\n[26] Yogeshwer Sharma, Philippe Ajoux, Petchean Ang, et al.: \u201c Wormhole: Reliable\nPub-Sub to Support Geo-Replicated Internet Services ,\u201d at 12th USENIX Symposium\non Networked Systems Design and Implementation (NSDI), May 2015.\n[27] P. P. S. Narayan: \u201cSherpa Update,\u201d developer.yahoo.com, June 8, .\n[28] Martin Kleppmann: \u201c Bottled Water: Real-Time Integration of PostgreSQL and\nKafka,\u201d martin.kleppmann.com, April 23, 2015.\n[29] Ben Osheroff: \u201cIntroducing Maxwell, a mysql-to-kafka Binlog Processor ,\u201d devel\u2010\noper.zendesk.com, August 20, 2015.\n[30] Randall Hauch: \u201cDebezium 0.2.1 Released,\u201d debezium.io, June 10, 2016.\n[31] Prem Santosh Udaya Shankar: \u201c Streaming MySQL Tables in Real-Time to\nKafka,\u201d engineeringblog.yelp.com, August 1, 2016.\n[32] \u201cMongoriver,\u201d Stripe, Inc., github.com, September 2014.\n[33] Dan Harvey: \u201c Change Data Capture with Mongo + Kafka ,\u201d at Hadoop Users\nGroup UK, August 2015.\n[34] \u201cOracle GoldenGate 12c: Real-Time Access to Real-Time Information ,\u201d Oracle\nWhite Paper, March 2015.\n[35] \u201c Oracle GoldenGate Fundamentals: How Oracle GoldenGate Works ,\u201d Oracle\nCorporation, youtube.com, November 2012.\n[36] Slava Akhmechet: \u201c Advancing the Realtime Web ,\u201d rethinkdb.com, January 27,\n2015.\n[37] \u201cFirebase Realtime Database Documentation ,\u201d Google, Inc., firebase.google.com,\nMay 2016.\n[38] \u201cApache CouchDB 1.6 Documentation,\u201d docs.couchdb.org, 2014.\n[39] Matt DeBergalis: \u201c Meteor 0.7.0: Scalable Database Queries Using MongoDB\nOplog Instead of Poll-and-Diff,\u201d info.meteor.com, December 17, 2013.\n[40] \u201c Chapter 15. Importing and Exporting Live Data ,\u201d VoltDB 6.4 User Manual,\ndocs.voltdb.com, June 2016.\n[41] Neha Narkhede: \u201c Announcing Kafka Connect: Building Large-Scale Low-\nLatency Data Pipelines,\u201d confluent.io, February 18, 2016.\n[42] Greg Young: \u201cCQRS and Event Sourcing,\u201d at Code on the Beach, August 2014.\n[43] Martin Fowler: \u201cEvent Sourcing,\u201d martinfowler.com, December 12, 2005.\nSummary | 483", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "099df565-1fea-455b-b414-774a18f0c18c": {"__data__": {"id_": "099df565-1fea-455b-b414-774a18f0c18c", "embedding": null, "metadata": {"page_label": "484", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d290785-3a12-4b06-a821-4e7adf4d3912", "node_type": "4", "metadata": {"page_label": "484", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "62bf4b592c5dfcc7c821c0d87f591960586379375857228914a360576eb7e89a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[44] Vaughn Vernon: Implementing Domain-Driven Design. Addison-Wesley Profes\u2010\nsional, 2013. ISBN: 978-0-321-83457-7\n[45] H. V. Jagadish, Inderpal Singh Mumick, and Abraham Silberschatz: \u201c View\nMaintenance Issues for the Chronicle Data Model ,\u201d at 14th ACM SIGACT-SIGMOD-\nSIGART Symposium on Principles of Database Systems  (PODS), May 1995. doi:\n10.1145/212433.220201\n[46] \u201c Event Store 3.5.0 Documentation ,\u201d Event Store LLP, docs.geteventstore.com,\nFebruary 2016.\n[47] Martin Kleppmann: Making Sense of Stream Processing . Report, O\u2019Reilly Media,\nMay 2016.\n[48] Sander Mak: \u201c Event-Sourced Architectures with Akka ,\u201d at JavaOne, September\n2014.\n[49] Julian Hyde: personal communication, June 2016.\n[50] Ashish Gupta and Inderpal Singh Mumick: Materialized Views: Techniques,\nImplementations, and Applications. MIT Press, 1999. ISBN: 978-0-262-57122-7\n[51] Timothy Griffin and Leonid Libkin: \u201c Incremental Maintenance of Views with\nDuplicates,\u201d at ACM International Conference on Management of Data  (SIGMOD),\nMay 1995. doi:10.1145/223784.223849\n[52] Pat Helland: \u201c Immutability Changes Everything ,\u201d at 7th Biennial Conference on\nInnovative Data Systems Research (CIDR), January 2015.\n[53] Martin Kleppmann: \u201c Accounting for Computer Scientists ,\u201d martin.klepp\u2010\nmann.com, March 7, 2011.\n[54] Pat Helland: \u201cAccountants Don\u2019t Use Erasers,\u201d blogs.msdn.com, June 14, 2007.\n[55] Fangjin Yang: \u201cDogfooding with Druid, Samza, and Kafka: Metametrics at Met\u2010\namarkets,\u201d metamarkets.com, June 3, 2015.\n[56] Gavin Li, Jianqiu Lv, and Hang Qi: \u201cPistachio: Co-Locate the Data and Compute\nfor Fastest Cloud Compute,\u201d yahoohadoop.tumblr.com, April 13, 2015.\n[57] Kartik Paramasivam: \u201c Stream Processing Hard Problems \u2013 Part 1: Killing\nLambda,\u201d engineering.linkedin.com, June 27, 2016.\n[58] Martin Fowler: \u201cCQRS,\u201d martinfowler.com, July 14, 2011.\n[59] Greg Young: \u201cCQRS Documents,\u201d cqrs.files.wordpress.com, November 2010.\n[60] Baron Schwartz: \u201c Immutability, MVCC, and Garbage Collection ,\u201d xaprb.com,\nDecember 28, 2013.\n484 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28070354-b27b-4996-a730-3c5cf87ac7d1": {"__data__": {"id_": "28070354-b27b-4996-a730-3c5cf87ac7d1", "embedding": null, "metadata": {"page_label": "485", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4aed5dff-ca5c-43dd-bde8-1c4102935cbf", "node_type": "4", "metadata": {"page_label": "485", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9177f01f40a1cabab8e680a1d582d0328e17ed877d2ac4071aa3377d5fb5b261", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[61] Daniel Eloff, Slava Akhmechet, Jay Kreps, et al.: \u201cRe: Turning the Database\nInside-out with Apache Samza ,\u201d Hacker News discussion, news.ycombinator.com,\nMarch 4, 2015.\n[62] \u201cDatomic Development Resources: Excision,\u201d Cognitect, Inc., docs.datomic.com.\n[63] \u201cFossil Documentation: Deleting Content from Fossil,\u201d fossil-scm.org, 2016.\n[64] Jay Kreps: \u201c The irony of distributed systems is that data loss is really easy but\ndeleting data is surprisingly hard,\u201d twitter.com, March 30, 2015.\n[65] David C. Luckham: \u201c What\u2019s the Difference Between ESP and CEP? ,\u201d complexe\u2010\nvents.com, August 1, 2006.\n[66] Srinath Perera: \u201c How Is Stream Processing and Complex Event Processing\n(CEP) Different?,\u201d quora.com, December 3, 2015.\n[67] Arvind Arasu, Shivnath Babu, and Jennifer Widom: \u201c The CQL Continuous\nQuery Language: Semantic Foundations and Query Execution ,\u201d The VLDB Journal ,\nvolume 15, number 2, pages 121\u2013142, June 2006. doi:10.1007/s00778-004-0147-z\n[68] Julian Hyde: \u201c Data in Flight: How Streaming SQL Technology Can Help Solve\nthe Web 2.0 Data Crunch ,\u201d ACM Queue, volume 7, number 11, December 2009. doi:\n10.1145/1661785.1667562\n[69] \u201cEsper Reference, Version 5.4.0,\u201d EsperTech, Inc., espertech.com, April 2016.\n[70] Zubair Nabi, Eric Bouillet, Andrew Bainbridge, and Chris Thomas: \u201c Of Streams\nand Storms,\u201d IBM technical report, developer.ibm.com, April 2014.\n[71] Milinda Pathirage, Julian Hyde, Yi Pan, and Beth Plale: \u201c SamzaSQL: Scalable\nFast Data Management with Streaming SQL ,\u201d at IEEE International Workshop on\nHigh-Performance Big Data Computing  (HPBDC), May 2016. doi:10.1109/IPDPSW.\n2016.141\n[72] Philippe Flajolet, \u00c9ric Fusy, Olivier Gandouet, and Fr\u00e9d\u00e9ric Meunier: \u201cHyperLog\nLog: The Analysis of a Near-Optimal Cardinality Estimation Algorithm ,\u201d at Confer\u2010\nence on Analysis of Algorithms (AofA), June 2007.\n[73] Jay Kreps: \u201cQuestioning the Lambda Architecture,\u201d oreilly.com, July 2, 2014.\n[74] Ian Hellstr\u00f6m: \u201c An Overview of Apache Streaming Technologies ,\u201d database\u2010\nline.wordpress.com, March 12, 2016.\n[75] Jay Kreps: \u201c Why Local State Is a Fundamental Primitive in Stream Processing ,\u201d\noreilly.com, July 31, 2014.\n[76] Shay Banon: \u201cPercolator,\u201d elastic.co, February 8, 2011.\n[77] Alan Woodward and Martin Kleppmann: \u201c Real-Time Full-Text Search with\nLuwak and Samza,\u201d martin.kleppmann.com, April 13, 2015.\nSummary | 485", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6149301d-90d4-4cba-8598-d905bf9a2832": {"__data__": {"id_": "6149301d-90d4-4cba-8598-d905bf9a2832", "embedding": null, "metadata": {"page_label": "486", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c80f8f5-2c34-444f-9cd2-a89f9e0e7de9", "node_type": "4", "metadata": {"page_label": "486", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "68084632b015c9eb49b69ad40f648530bbc954b2d14e76de8932dd987e02f2c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[78] \u201cApache Storm 1.0.1 Documentation,\u201d storm.apache.org, May 2016.\n[79] Tyler Akidau: \u201c The World Beyond Batch: Streaming 102 ,\u201d oreilly.com, January\n20, 2016.\n[80] Stephan Ewen: \u201cStreaming Analytics with Apache Flink,\u201d at Kafka Summit, April\n2016.\n[81] Tyler Akidau, Alex Balikov, Kaya Bekiro\u011flu, et al.: \u201c MillWheel: Fault-Tolerant\nStream Processing at Internet Scale ,\u201d at 39th International Conference on Very Large\nData Bases (VLDB), August 2013.\n[82] Alex Dean: \u201c Improving Snowplow\u2019s Understanding of Time ,\u201d snowplowanalyt\u2010\nics.com, September 15, 2015.\n[83] \u201c Windowing (Azure Stream Analytics) ,\u201d Microsoft Azure Reference,\nmsdn.microsoft.com, April 2016.\n[84] \u201c State Management ,\u201d Apache Samza 0.10 Documentation, samza.apache.org,\nDecember 2015.\n[85] Rajagopal Ananthanarayanan, Venkatesh Basker, Sumit Das, et al.: \u201c Photon:\nFault-Tolerant and Scalable Joining of Continuous Data Streams ,\u201d at ACM Interna\u2010\ntional Conference on Management of Data  (SIGMOD), June 2013. doi:\n10.1145/2463676.2465272\n[86] Martin Kleppmann: \u201cSamza Newsfeed Demo,\u201d github.com, September 2014.\n[87] Ben Kirwin: \u201cDoing the Impossible: Exactly-Once Messaging Patterns in Kafka ,\u201d\nben.kirw.in, November 28, 2014.\n[88] Pat Helland: \u201c Data on the Outside Versus Data on the Inside ,\u201d at 2nd Biennial\nConference on Innovative Data Systems Research (CIDR), January 2005.\n[89] Ralph Kimball and Margy Ross: The Data Warehouse Toolkit: The Definitive\nGuide to Dimensional Modeling , 3rd edition. John Wiley & Sons, 2013. ISBN:\n978-1-118-53080-1\n[90] Viktor Klang: \u201c I\u2019m coining the phrase \u2018effectively-once\u2019 for message processing\nwith at-least-once + idempotent operations,\u201d twitter.com, October 20, 2016.\n[91] Matei Zaharia, Tathagata Das, Haoyuan Li, et al.: \u201c Discretized Streams: An Effi\u2010\ncient and Fault-Tolerant Model for Stream Processing on Large Clusters ,\u201d at 4th\nUSENIX Conference in Hot Topics in Cloud Computing (HotCloud), June 2012.\n[92] Kostas Tzoumas, Stephan Ewen, and Robert Metzger: \u201c High-Throughput, Low-\nLatency, and Exactly-Once Stream Processing with Apache Flink,\u201d data-artisans.com,\nAugust 5, 2015.\n486 | Chapter 11: Stream Processing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16960e18-b9fd-43ad-953b-cbaf0367dc4f": {"__data__": {"id_": "16960e18-b9fd-43ad-953b-cbaf0367dc4f", "embedding": null, "metadata": {"page_label": "487", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22110560-cf2e-47d5-9618-c6eb6b2be016", "node_type": "4", "metadata": {"page_label": "487", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0b2d20ada6da8e3eecc2db710d65c7895a7e4e2c996772bec0f4bf7772421a01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[93] Paris Carbone, Gyula F\u00f3ra, Stephan Ewen, et al.: \u201c Lightweight Asynchronous\nSnapshots for Distributed Dataflows,\u201d arXiv:1506.08603 [cs.DC], June 29, 2015.\n[94] Ryan Betts and John Hugg: Fast Data: Smart and at Scale . Report, O\u2019Reilly\nMedia, October 2015.\n[95] Flavio Junqueira: \u201c Making Sense of Exactly-Once Semantics ,\u201d at Strata+Hadoop\nWorld London, June 2016.\n[96] Jason Gustafson, Flavio Junqueira, Apurva Mehta, Sriram Subramanian, and\nGuozhang Wang: \u201c KIP-98 \u2013 Exactly Once Delivery and Transactional Messaging ,\u201d\ncwiki.apache.org, November 2016.\n[97] Pat Helland: \u201cIdempotence Is Not a Medical Condition,\u201d Communications of the\nACM, volume 55, number 5, page 56, May 2012. doi:10.1145/2160718.2160734\n[98] Jay Kreps: \u201cRe: Trying to Achieve Deterministic Behavior on Recovery/Rewind ,\u201d\nemail to samza-dev mailing list, September 9, 2014.\n[99] E. N. (Mootaz) Elnozahy, Lorenzo Alvisi, Yi-Min Wang, and David B. Johnson:\n\u201cA Survey of Rollback-Recovery Protocols in Message-Passing Systems ,\u201d ACM Com\u2010\nputing Surveys , volume 34, number 3, pages 375\u2013408, September 2002. doi:\n10.1145/568522.568525\n[100] Adam Warski: \u201c Kafka Streams \u2013 How Does It Fit the Stream Processing Land\u2010\nscape?,\u201d softwaremill.com, June 1, 2016.\nSummary | 487", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7771cfb-4128-4267-a213-5f1727df684c": {"__data__": {"id_": "a7771cfb-4128-4267-a213-5f1727df684c", "embedding": null, "metadata": {"page_label": "488", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d027828-b439-4704-b10d-9ef1051c3c31", "node_type": "4", "metadata": {"page_label": "488", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb026f77-7839-4e0e-9942-4429f1794509": {"__data__": {"id_": "eb026f77-7839-4e0e-9942-4429f1794509", "embedding": null, "metadata": {"page_label": "489", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7bb47ef6-b3b7-45ff-9da9-479566d83fd7", "node_type": "4", "metadata": {"page_label": "489", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8d206db89a24fce53865d373f22aadc29f4688d13c0b8eebfe000767afe885b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CHAPTER 12\nThe Future of Data Systems\nIf a thing be ordained to another as to its end, its last end cannot consist in the preservation\nof its being. Hence a captain does not intend as a last end, the preservation of the ship\nentrusted to him, since a ship is ordained to something else as its end, viz. to navigation.\n(Often quoted as: If the highest aim of a captain was the preserve his ship, he would keep it\nin port forever.)\n\u2014St. Thomas Aquinas, Summa Theologica (1265\u20131274)\nSo far, this book has been mostly about describing things as they are at present. In\nthis final chapter, we will shift our perspective toward the future and discuss how\nthings should be: I will propose some ideas and approaches that, I believe, may funda\u2010\nmentally improve the ways we design and build applications.\nOpinions and speculation about the future are of course subjective, and so I will use\nthe first person in this chapter when writing about my personal opinions. You are\nwelcome to disagree with them and form your own opinions, but I hope that the\nideas in this chapter will at least be a starting point for a productive discussion and\nbring some clarity to concepts that are often confused.\nThe goal of this book was outlined in Chapter 1: to explore how to create applications\nand systems that are reliable, scalable, and maintainable. These themes have run\nthrough all of the chapters: for example, we discussed many fault-tolerance algo\u2010\nrithms that help improve reliability, partitioning to improve scalability, and mecha\u2010\nnisms for evolution and abstraction that improve maintainability. In this chapter we\nwill bring all of these ideas together, and build on them to envisage the future. Our\ngoal is to discover how to design applications that are better than the ones of today\u2014\nrobust, correct, evolvable, and ultimately beneficial to humanity.\n489", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7d869ef0-6dfa-4a09-8809-bdb98780f251": {"__data__": {"id_": "7d869ef0-6dfa-4a09-8809-bdb98780f251", "embedding": null, "metadata": {"page_label": "490", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3e6e739-1c89-4f43-8ce7-77ea9514c8cd", "node_type": "4", "metadata": {"page_label": "490", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d5371745f7e651a2a9373dc8a0f54f310b3a93c8fe17416ef7a0d6704011ab6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Integration\nA recurring theme in this book has been that for any given problem, there are several\nsolutions, all of which have different pros, cons, and trade-offs. For example, when\ndiscussing storage engines in Chapter 3, we saw log-structured storage, B-trees, and\ncolumn-oriented storage. When discussing replication in Chapter 5, we saw single-\nleader, multi-leader, and leaderless approaches.\nIf you have a problem such as \u201cI want to store some data and look it up again later,\u201d\nthere is no one right solution, but many different approaches that are each appropri\u2010\nate in different circumstances. A software implementation typically has to pick one\nparticular approach. It\u2019s hard enough to get one code path robust and performing\nwell\u2014trying to do everything in one piece of software almost guarantees that the\nimplementation will be poor.\nThus, the most appropriate choice of software tool also depends on the circumstan\u2010\nces. Every piece of software, even a so-called \u201cgeneral-purpose\u201d database, is designed\nfor a particular usage pattern.\nFaced with this profusion of alternatives, the first challenge is then to figure out the\nmapping between the software products and the circumstances in which they are a\ngood fit. Vendors are understandably reluctant to tell you about the kinds of work\u2010\nloads for which their software is poorly suited, but hopefully the previous chapters\nhave equipped you with some questions to ask in order to read between the lines and\nbetter understand the trade-offs.\nHowever, even if you perfectly understand the mapping between tools and circum\u2010\nstances for their use, there is another challenge: in complex applications, data is often\nused in several different ways. There is unlikely to be one piece of software that is\nsuitable for all the different circumstances in which the data is used, so you inevitably\nend up having to cobble together several different pieces of software in order to pro\u2010\nvide your application\u2019s functionality.\nCombining Specialized Tools by Deriving Data\nFor example, it is common to need to integrate an OLTP database with a full-text\nsearch index in order to handle queries for arbitrary keywords. Although some data\u2010\nbases (such as PostgreSQL) include a full-text indexing feature, which can be suffi\u2010\ncient for simple applications [1], more sophisticated search facilities require specialist\ninformation retrieval tools. Conversely, search indexes are generally not very suitable\nas a durable system of record, and so many applications need to combine two differ\u2010\nent tools in order to satisfy all of the requirements.\nWe touched on the issue of integrating data systems in \u201cKeeping Systems in Sync\u201d on\npage 452. As the number of different representations of the data increases, the inte\u2010\n490 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e900c715-67be-4e10-9a4d-5ce5da0ac1c3": {"__data__": {"id_": "e900c715-67be-4e10-9a4d-5ce5da0ac1c3", "embedding": null, "metadata": {"page_label": "491", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c08874de-13ed-4097-ba87-dff6e43fb537", "node_type": "4", "metadata": {"page_label": "491", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8487c02ec2001c2f8df86825fbf66b0597340ab46eac85c1f1dcd64647f12ebe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "gration problem becomes harder. Besides the database and the search index, perhaps\nyou need to keep copies of the data in analytics systems (data warehouses, or batch\nand stream processing systems); maintain caches or denormalized versions of objects\nthat were derived from the original data; pass the data through machine learning,\nclassification, ranking, or recommendation systems; or send notifications based on\nchanges to the data.\nSurprisingly often I see software engineers make statements like, \u201cIn my experience,\n99% of people only need X\u201d or \u201c\u2026don\u2019t need X\u201d (for various values of X). I think that\nsuch statements say more about the experience of the speaker than about the actual\nusefulness of a technology. The range of different things you might want to do with\ndata is dizzyingly wide. What one person considers to be an obscure and pointless\nfeature may well be a central requirement for someone else. The need for data inte\u2010\ngration often only becomes apparent if you zoom out and consider the dataflows\nacross an entire organization.\nReasoning about dataflows\nWhen copies of the same data need to be maintained in several storage systems in\norder to satisfy different access patterns, you need to be very clear about the inputs\nand outputs: where is data written first, and which representations are derived from\nwhich sources? How do you get data into all the right places, in the right formats?\nFor example, you might arrange for data to first be written to a system of record data\u2010\nbase, capturing the changes made to that database (see \u201cChange Data Capture\u201d on\npage 454) and then applying the changes to the search index in the same order. If\nchange data capture (CDC) is the only way of updating the index, you can be confi\u2010\ndent that the index is entirely derived from the system of record, and therefore con\u2010\nsistent with it (barring bugs in the software). Writing to the database is the only way\nof supplying new input into this system.\nAllowing the application to directly write to both the search index and the database\nintroduces the problem shown in Figure 11-4, in which two clients concurrently send\nconflicting writes, and the two storage systems process them in a different order. In\nthis case, neither the database nor the search index is \u201cin charge\u201d of determining the\norder of writes, and so they may make contradictory decisions and become perma\u2010\nnently inconsistent with each other.\nIf it is possible for you to funnel all user input through a single system that decides on\nan ordering for all writes, it becomes much easier to derive other representations of\nthe data by processing the writes in the same order. This is an application of the state\nmachine replication approach that we saw in \u201cTotal Order Broadcast\u201d  on page 348.\nWhether you use change data capture or an event sourcing log is less important than\nsimply the principle of deciding on a total order.\nData Integration | 491", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25b1798f-e663-45c0-bfb4-e86489c1c23f": {"__data__": {"id_": "25b1798f-e663-45c0-bfb4-e86489c1c23f", "embedding": null, "metadata": {"page_label": "492", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8705eb3a-1f5f-4bad-9654-edfbb8f88068", "node_type": "4", "metadata": {"page_label": "492", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "706e7e7eeb17370134ffa0c4893b08544bd5376a81dafad2ee73aed17f3f2196", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Updating a derived data system based on an event log can often be made determinis\u2010\ntic and idempotent (see \u201cIdempotence\u201d on page 478), making it quite easy to recover\nfrom faults.\nDerived data versus distributed transactions\nThe classic approach for keeping different data systems consistent with each other\ninvolves distributed transactions, as discussed in \u201cAtomic Commit and Two-Phase\nCommit (2PC)\u201d on page 354. How does the approach of using derived data systems\nfare in comparison to distributed transactions?\nAt an abstract level, they achieve a similar goal by different means. Distributed trans\u2010\nactions decide on an ordering of writes by using locks for mutual exclusion (see\n\u201cTwo-Phase Locking (2PL)\u201d on page 257), while CDC and event sourcing use a log\nfor ordering. Distributed transactions use atomic commit to ensure that changes take\neffect exactly once, while log-based systems are often based on deterministic retry\nand idempotence.\nThe biggest difference is that transaction systems usually provide linearizability (see\n\u201cLinearizability\u201d on page 324), which implies useful guarantees such as reading your\nown writes (see \u201cReading Your Own Writes\u201d on page 162). On the other hand,\nderived data systems are often updated asynchronously, and so they do not by default\noffer the same timing guarantees.\nWithin limited environments that are willing to pay the cost of distributed transac\u2010\ntions, they have been used successfully. However, I think that XA has poor fault toler\u2010\nance and performance characteristics (see \u201cDistributed Transactions in Practice\u201d  on\npage 360), which severely limit its usefulness. I believe that it might be possible to\ncreate a better protocol for distributed transactions, but getting such a protocol\nwidely adopted and integrated with existing tools would be challenging, and unlikely\nto happen soon.\nIn the absence of widespread support for a good distributed transaction protocol, I\nbelieve that log-based derived data is the most promising approach for integrating\ndifferent data systems. However, guarantees such as reading your own writes are use\u2010\nful, and I don\u2019t think that it is productive to tell everyone \u201ceventual consistency is\ninevitable\u2014suck it up and learn to deal with it\u201d (at least not without good guidance\non how to deal with it).\nIn \u201cAiming for Correctness\u201d on page 515 we will discuss some approaches for imple\u2010\nmenting stronger guarantees on top of asynchronously derived systems, and work\ntoward a middle ground between distributed transactions and asynchronous log-\nbased systems.\n492 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8daf03cd-cf7b-44ca-9550-0ae9615f1b3a": {"__data__": {"id_": "8daf03cd-cf7b-44ca-9550-0ae9615f1b3a", "embedding": null, "metadata": {"page_label": "493", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae7cf2de-36d9-4a7a-9387-66cb81cb2068", "node_type": "4", "metadata": {"page_label": "493", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f9d1fbc1b6897288a8908c31e6a4ac566579959b7da046add905513934989ac3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The limits of total ordering\nWith systems that are small enough, constructing a totally ordered event log is\nentirely feasible (as demonstrated by the popularity of databases with single-leader\nreplication, which construct precisely such a log). However, as systems are scaled\ntoward bigger and more complex workloads, limitations begin to emerge:\n\u2022 In most cases, constructing a totally ordered log requires all events to pass\nthrough a single leader node  that decides on the ordering. If the throughput of\nevents is greater than a single machine can handle, you need to partition it across\nmultiple machines (see \u201cPartitioned Logs\u201d on page 446). The order of events in\ntwo different partitions is then ambiguous.\n\u2022 If the servers are spread across multiple geographically distributed  datacenters,\nfor example in order to tolerate an entire datacenter going offline, you typically\nhave a separate leader in each datacenter, because network delays make synchro\u2010\nnous cross-datacenter coordination inefficient (see \u201cMulti-Leader Replication\u201d\non page 168). This implies an undefined ordering of events that originate in two\ndifferent datacenters.\n\u2022 When applications are deployed as microservices (see \u201cDataflow Through Serv\u2010\nices: REST and RPC\u201d on page 131), a common design choice is to deploy each\nservice and its durable state as an independent unit, with no durable state shared\nbetween services. When two events originate in different services, there is no\ndefined order for those events.\n\u2022 Some applications maintain client-side state that is updated immediately on user\ninput (without waiting for confirmation from a server), and even continue to\nwork offline (see \u201cClients with offline operation\u201d on page 170). With such appli\u2010\ncations, clients and servers are very likely to see events in different orders.\nIn formal terms, deciding on a total order of events is known as total order broadcast,\nwhich is equivalent to consensus (see \u201cConsensus algorithms and total order broad\u2010\ncast\u201d on page 366). Most consensus algorithms are designed for situations in which\nthe throughput of a single node is sufficient to process the entire stream of events,\nand these algorithms do not provide a mechanism for multiple nodes to share the\nwork of ordering the events. It is still an open research problem to design consensus\nalgorithms that can scale beyond the throughput of a single node and that work well\nin a geographically distributed setting.\nOrdering events to capture causality\nIn cases where there is no causal link between events, the lack of a total order is not a\nbig problem, since concurrent events can be ordered arbitrarily. Some other cases are\neasy to handle: for example, when there are multiple updates of the same object, they\ncan be totally ordered by routing all updates for a particular object ID to the same log\nData Integration | 493", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "32493781-67e8-4b03-a60f-ce207c00c59d": {"__data__": {"id_": "32493781-67e8-4b03-a60f-ce207c00c59d", "embedding": null, "metadata": {"page_label": "494", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2bcde958-2fa2-4bd8-88e1-f60f9966fa94", "node_type": "4", "metadata": {"page_label": "494", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "af32e4c90257e6e3d006816a1afaa649507d56799507c1487ad8601ee6f7cb45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "partition. However, causal dependencies sometimes arise in more subtle ways (see\nalso \u201cOrdering and Causality\u201d on page 339).\nFor example, consider a social networking service, and two users who were in a rela\u2010\ntionship but have just broken up. One of the users removes the other as a friend, and\nthen sends a message to their remaining friends complaining about their ex-partner.\nThe user\u2019s intention is that their ex-partner should not see the rude message, since\nthe message was sent after the friend status was revoked.\nHowever, in a system that stores friendship status in one place and messages in\nanother place, that ordering dependency between the unfriend event and the message-\nsend event may be lost. If the causal dependency is not captured, a service that sends\nnotifications about new messages may process the message-send event before the\nunfriend event, and thus incorrectly send a notification to the ex-partner.\nIn this example, the notifications are effectively a join between the messages and the\nfriend list, making it related to the timing issues of joins that we discussed previously\n(see \u201cTime-dependence of joins\u201d on page 475). Unfortunately, there does not seem to\nbe a simple answer to this problem [2, 3]. Starting points include: \n\u2022 Logical timestamps can provide total ordering without coordination (see\n\u201cSequence Number Ordering\u201d on page 343), so they may help in cases where\ntotal order broadcast is not feasible. However, they still require recipients to han\u2010\ndle events that are delivered out of order, and they require additional metadata to\nbe passed around.\n\u2022 If you can log an event to record the state of the system that the user saw before\nmaking a decision, and give that event a unique identifier, then any later events\ncan reference that event identifier in order to record the causal dependency [ 4].\nWe will return to this idea in \u201cReads are events too\u201d on page 513.\n\u2022 Conflict resolution algorithms (see \u201cAutomatic Conflict Resolution\u201d on page\n174) help with processing events that are delivered in an unexpected order. They\nare useful for maintaining state, but they do not help if actions have external side\neffects (such as sending a notification to a user).\nPerhaps, over time, patterns for application development will emerge that allow\ncausal dependencies to be captured efficiently, and derived state to be maintained\ncorrectly, without forcing all events to go through the bottleneck of total order\nbroadcast. \nBatch and Stream Processing\nI would say that the goal of data integration is to make sure that data ends up in the\nright form in all the right places. Doing so requires consuming inputs, transforming,\njoining, filtering, aggregating, training models, evaluating, and eventually writing to\n494 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b70d3a3b-8ff7-44a2-a798-a7dc9336a65b": {"__data__": {"id_": "b70d3a3b-8ff7-44a2-a798-a7dc9336a65b", "embedding": null, "metadata": {"page_label": "495", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2daa6184-1982-45da-bfee-483064f41798", "node_type": "4", "metadata": {"page_label": "495", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "18b9688f3512d21c81de9b9596ab471afc247ce40dc378cf6ffb563bdfd756da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the appropriate outputs. Batch and stream processors are the tools for achieving this\ngoal.\nThe outputs of batch and stream processes are derived datasets such as search\nindexes, materialized views, recommendations to show to users, aggregate metrics,\nand so on (see \u201cThe Output of Batch Workflows\u201d on page 411 and \u201cUses of Stream\nProcessing\u201d on page 465).\nAs we saw in Chapter 10 and Chapter 11, batch and stream processing have a lot of\nprinciples in common, and the main fundamental difference is that stream process\u2010\nors operate on unbounded datasets whereas batch process inputs are of a known,\nfinite size. There are also many detailed differences in the ways the processing\nengines are implemented, but these distinctions are beginning to blur.\nSpark performs stream processing on top of a batch processing engine by breaking\nthe stream into microbatches, whereas Apache Flink performs batch processing on\ntop of a stream processing engine [ 5]. In principle, one type of processing can be\nemulated on top of the other, although the performance characteristics vary: for\nexample, microbatching may perform poorly on hopping or sliding windows [6].\nMaintaining derived state\nBatch processing has a quite strong functional flavor (even if the code is not written\nin a functional programming language): it encourages deterministic, pure functions\nwhose output depends only on the input and which have no side effects other than\nthe explicit outputs, treating inputs as immutable and outputs as append-only.\nStream processing is similar, but it extends operators to allow managed, fault-tolerant\nstate (see \u201cRebuilding state after a failure\u201d on page 478).\nThe principle of deterministic functions with well-defined inputs and outputs is not\nonly good for fault tolerance (see \u201cIdempotence\u201d on page 478), but also simplifies\nreasoning about the dataflows in an organization [ 7]. No matter whether the derived\ndata is a search index, a statistical model, or a cache, it is helpful to think in terms of\ndata pipelines that derive one thing from another, pushing state changes in one sys\u2010\ntem through functional application code and applying the effects to derived systems.\nIn principle, derived data systems could be maintained synchronously, just like a\nrelational database updates secondary indexes synchronously within the same trans\u2010\naction as writes to the table being indexed. However, asynchrony is what makes sys\u2010\ntems based on event logs robust: it allows a fault in one part of the system to be\ncontained locally, whereas distributed transactions abort if any one participant fails,\nso they tend to amplify failures by spreading them to the rest of the system (see \u201cLim\u2010\nitations of distributed transactions\u201d on page 363).\nWe saw in \u201cPartitioning and Secondary Indexes\u201d on page 206 that secondary indexes\noften cross partition boundaries. A partitioned system with secondary indexes either\nData Integration | 495", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41ec6ef9-0334-4155-ad11-49b5d38229a9": {"__data__": {"id_": "41ec6ef9-0334-4155-ad11-49b5d38229a9", "embedding": null, "metadata": {"page_label": "496", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66cdc326-000e-4d3d-acfe-9d46440b8b04", "node_type": "4", "metadata": {"page_label": "496", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "49c5edd3becb19a1da9b52ccd04c18fb02ad1dc835a7d283017bece61c3975f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "needs to send writes to multiple partitions (if the index is term-partitioned) or send\nreads to all partitions (if the index is document-partitioned). Such cross-partition\ncommunication is also most reliable and scalable if the index is maintained asynchro\u2010\nnously [8] (see also \u201cMulti-partition data processing\u201d on page 514).\nReprocessing data for application evolution\nWhen maintaining derived data, batch and stream processing are both useful. Stream\nprocessing allows changes in the input to be reflected in derived views with low delay,\nwhereas batch processing allows large amounts of accumulated historical data to be\nreprocessed in order to derive new views onto an existing dataset.\nIn particular, reprocessing existing data provides a good mechanism for maintaining\na system, evolving it to support new features and changed requirements (see Chap\u2010\nter 4 ). Without reprocessing, schema evolution is limited to simple changes like\nadding a new optional field to a record, or adding a new type of record. This is the\ncase both in a schema-on-write and in a schema-on-read context (see \u201cSchema flexi\u2010\nbility in the document model\u201d on page 39). On the other hand, with reprocessing it is\npossible to restructure a dataset into a completely different model in order to better\nserve new requirements.\nSchema Migrations on Railways\nLarge-scale \u201cschema migrations\u201d occur in noncomputer systems as well. For example,\nin the early days of railway building in 19th-century England there were various com\u2010\npeting standards for the gauge (the distance between the two rails). Trains built for\none gauge couldn\u2019t run on tracks of another gauge, which restricted the possible\ninterconnections in the train network [9].\nAfter a single standard gauge was finally decided upon in 1846, tracks with other\ngauges had to be converted\u2014but how do you do this without shutting down the train\nline for months or years? The solution is to first convert the track to dual gauge or\nmixed gauge by adding a third rail. This conversion can be done gradually, and when\nit is done, trains of both gauges can run on the line, using two of the three rails. Even\u2010\ntually, once all trains have been converted to the standard gauge, the rail providing\nthe nonstandard gauge can be removed.\n\u201cReprocessing\u201d the existing tracks in this way, and allowing the old and new versions\nto exist side by side, makes it possible to change the gauge gradually over the course\nof years. Nevertheless, it is an expensive undertaking, which is why nonstandard\ngauges still exist today. For example, the BART system in the San Francisco Bay Area\nuses a different gauge from the majority of the US.\n496 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cedde4ec-54e2-4f3f-8a31-eb8a74ce9679": {"__data__": {"id_": "cedde4ec-54e2-4f3f-8a31-eb8a74ce9679", "embedding": null, "metadata": {"page_label": "497", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed03184b-4e03-462b-9ec0-193a951c8487", "node_type": "4", "metadata": {"page_label": "497", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f492e493d3140abf25c276bade9e48a96e4789b609937ec1e41a39149f1fdc7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Derived views allow gradual evolution. If you want to restructure a dataset, you do\nnot need to perform the migration as a sudden switch. Instead, you can maintain the\nold schema and the new schema side by side as two independently derived views onto\nthe same underlying data. You can then start shifting a small number of users to the\nnew view in order to test its performance and find any bugs, while most users con\u2010\ntinue to be routed to the old view. Gradually, you can increase the proportion of\nusers accessing the new view, and eventually you can drop the old view [10].\nThe beauty of such a gradual migration is that every stage of the process is easily\nreversible if something goes wrong: you always have a working system to go back to.\nBy reducing the risk of irreversible damage, you can be more confident about going\nahead, and thus move faster to improve your system [11].\nThe lambda architecture\nIf batch processing is used to reprocess historical data, and stream processing is used\nto process recent updates, then how do you combine the two? The lambda architec\u2010\nture [12] is a proposal in this area that has gained a lot of attention.\nThe core idea of the lambda architecture is that incoming data should be recorded by\nappending immutable events to an always-growing dataset, similarly to event sourc\u2010\ning (see \u201cEvent Sourcing\u201d on page 457). From these events, read-optimized views are\nderived. The lambda architecture proposes running two different systems in parallel:\na batch processing system such as Hadoop MapReduce, and a separate stream-\nprocessing system such as Storm.\nIn the lambda approach, the stream processor consumes the events and quickly pro\u2010\nduces an approximate update to the view; the batch processor later consumes the\nsame set of events and produces a corrected version of the derived view. The reason\u2010\ning behind this design is that batch processing is simpler and thus less prone to bugs,\nwhile stream processors are thought to be less reliable and harder to make fault-\ntolerant (see \u201cFault Tolerance\u201d on page 476). Moreover, the stream process can use\nfast approximate algorithms while the batch process uses slower exact algorithms.\nThe lambda architecture was an influential idea that shaped the design of data sys\u2010\ntems for the better, particularly by popularizing the principle of deriving views onto\nstreams of immutable events and reprocessing events when needed. However, I also\nthink that it has a number of practical problems:\n\u2022 Having to maintain the same logic to run both in a batch and in a stream pro\u2010\ncessing framework is significant additional effort. Although libraries such as\nSummingbird [ 13] provide an abstraction for computations that can be run in\neither a batch or a streaming context, the operational complexity of debugging,\ntuning, and maintaining two different systems remains [14].\nData Integration | 497", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3a0c1ed-8ee5-4625-8db9-35e595ad7fb0": {"__data__": {"id_": "d3a0c1ed-8ee5-4625-8db9-35e595ad7fb0", "embedding": null, "metadata": {"page_label": "498", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65421512-2153-418f-a356-5f80f4d63fdc", "node_type": "4", "metadata": {"page_label": "498", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f5edfeda16b16b6090059cd3ffb8fdac33266b7a4f090c60af979a868db82b9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Since the stream pipeline and the batch pipeline produce separate outputs, they\nneed to be merged in order to respond to user requests. This merge is fairly easy\nif the computation is a simple aggregation over a tumbling window, but it\nbecomes significantly harder if the view is derived using more complex opera\u2010\ntions such as joins and sessionization, or if the output is not a time series.\n\u2022 Although it is great to have the ability to reprocess the entire historical dataset,\ndoing so frequently is expensive on large datasets. Thus, the batch pipeline often\nneeds to be set up to process incremental batches (e.g., an hour\u2019s worth of data at\nthe end of every hour) rather than reprocessing everything. This raises the prob\u2010\nlems discussed in \u201cReasoning About Time\u201d on page 468, such as handling strag\u2010\nglers and handling windows that cross boundaries between batches.\nIncrementalizing a batch computation adds complexity, making it more akin to\nthe streaming layer, which runs counter to the goal of keeping the batch layer as\nsimple as possible.\nUnifying batch and stream processing\nMore recent work has enabled the benefits of the lambda architecture to be enjoyed\nwithout its downsides, by allowing both batch computations (reprocessing historical\ndata) and stream computations (processing events as they arrive) to be implemented\nin the same system [15].\nUnifying batch and stream processing in one system requires the following features,\nwhich are becoming increasingly widely available:\n\u2022 The ability to replay historical events through the same processing engine that\nhandles the stream of recent events. For example, log-based message brokers\nhave the ability to replay messages (see \u201cReplaying old messages\u201d on page 451),\nand some stream processors can read input from a distributed filesystem like\nHDFS.\n\u2022 Exactly-once semantics for stream processors\u2014that is, ensuring that the output\nis the same as if no faults had occurred, even if faults did in fact occur (see \u201cFault\nTolerance\u201d on page 476). Like with batch processing, this requires discarding the\npartial output of any failed tasks.\n\u2022 Tools for windowing by event time, not by processing time, since processing\ntime is meaningless when reprocessing historical events (see \u201cReasoning About\nTime\u201d on page 468). For example, Apache Beam provides an API for expressing\nsuch computations, which can then be run using Apache Flink or Google Cloud\nDataflow. \n498 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "faad3477-2005-4101-bcdf-585338b2e818": {"__data__": {"id_": "faad3477-2005-4101-bcdf-585338b2e818", "embedding": null, "metadata": {"page_label": "499", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf6206f9-d55a-4a4f-9245-71b7ee4a85a6", "node_type": "4", "metadata": {"page_label": "499", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "df7959413b6196b3e2d284ddc3cec3caca166c53fc4995bfbba0b58955a87b42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unbundling Databases\nAt a most abstract level, databases, Hadoop, and operating systems all perform the\nsame functions: they store some data, and they allow you to process and query that\ndata [16]. A database stores data in records of some data model (rows in tables, docu\u2010\nments, vertices in a graph, etc.) while an operating system\u2019s filesystem stores data in\nfiles\u2014but at their core, both are \u201cinformation management\u201d systems [ 17]. As we saw\nin Chapter 10, the Hadoop ecosystem is somewhat like a distributed version of Unix.\nOf course, there are many practical differences. For example, many filesystems do not\ncope very well with a directory containing 10 million small files, whereas a database\ncontaining 10 million small records is completely normal and unremarkable. Never\u2010\ntheless, the similarities and differences between operating systems and databases are\nworth exploring.\nUnix and relational databases have approached the information management prob\u2010\nlem with very different philosophies. Unix viewed its purpose as presenting program\u2010\nmers with a logical but fairly low-level hardware abstraction, whereas relational\ndatabases wanted to give application programmers a high-level abstraction that\nwould hide the complexities of data structures on disk, concurrency, crash recovery,\nand so on. Unix developed pipes and files that are just sequences of bytes, whereas\ndatabases developed SQL and transactions.\nWhich approach is better? Of course, it depends what you want. Unix is \u201csimpler\u201d in\nthe sense that it is a fairly thin wrapper around hardware resources; relational data\u2010\nbases are \u201csimpler\u201d in the sense that a short declarative query can draw on a lot of\npowerful infrastructure (query optimization, indexes, join methods, concurrency\ncontrol, replication, etc.) without the author of the query needing to understand the\nimplementation details.\nThe tension between these philosophies has lasted for decades (both Unix and the\nrelational model emerged in the early 1970s) and still isn\u2019t resolved. For example, I\nwould interpret the NoSQL movement as wanting to apply a Unix-esque approach of\nlow-level abstractions to the domain of distributed OLTP data storage.\nIn this section I will attempt to reconcile the two philosophies, in the hope that we\ncan combine the best of both worlds.\nComposing Data Storage Technologies\nOver the course of this book we have discussed various features provided by data\u2010\nbases and how they work, including:\n\u2022 Secondary indexes, which allow you to efficiently search for records based on the\nvalue of a field (see \u201cOther Indexing Structures\u201d on page 85)\nUnbundling Databases | 499", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f820a50-6787-4f35-82fe-a091f80c4be4": {"__data__": {"id_": "7f820a50-6787-4f35-82fe-a091f80c4be4", "embedding": null, "metadata": {"page_label": "500", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe9ef192-dd43-40d4-9660-73382dbb3fa4", "node_type": "4", "metadata": {"page_label": "500", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "173ce5702eba187161abc044adf834255a91f8f26461253efed0ec8b1af17669", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Materialized views, which are a kind of precomputed cache of query results (see\n\u201cAggregation: Data Cubes and Materialized Views\u201d on page 101)\n\u2022 Replication logs, which keep copies of the data on other nodes up to date (see\n\u201cImplementation of Replication Logs\u201d on page 158)\n\u2022 Full-text search indexes, which allow keyword search in text (see \u201cFull-text\nsearch and fuzzy indexes\u201d on page 88) and which are built into some relational\ndatabases [1]\nIn Chapters 10 and 11, similar themes emerged. We talked about building full-text\nsearch indexes (see \u201cThe Output of Batch Workflows\u201d on page 411), about material\u2010\nized view maintenance (see \u201cMaintaining materialized views\u201d on page 467), and\nabout replicating changes from a database to derived data systems (see \u201cChange Data\nCapture\u201d on page 454).\nIt seems that there are parallels between the features that are built into databases and\nthe derived data systems that people are building with batch and stream processors.\nCreating an index\nThink about what happens when you run CREATE INDEX to create a new index in a\nrelational database. The database has to scan over a consistent snapshot of a table,\npick out all of the field values being indexed, sort them, and write out the index. Then\nit must process the backlog of writes that have been made since the consistent snap\u2010\nshot was taken (assuming the table was not locked while creating the index, so writes\ncould continue). Once that is done, the database must continue to keep the index up\nto date whenever a transaction writes to the table.\nThis process is remarkably similar to setting up a new follower replica (see \u201cSetting\nUp New Followers\u201d on page 155), and also very similar to bootstrapping change data\ncapture in a streaming system (see \u201cInitial snapshot\u201d on page 455).\nWhenever you run CREATE INDEX, the database essentially reprocesses the existing\ndataset (as discussed in \u201cReprocessing data for application evolution\u201d  on page 496)\nand derives the index as a new view onto the existing data. The existing data may be a\nsnapshot of the state rather than a log of all changes that ever happened, but the two\nare closely related (see \u201cState, Streams, and Immutability\u201d on page 459).\nThe meta-database of everything\nIn this light, I think that the dataflow across an entire organization starts looking like\none huge database [ 7]. Whenever a batch, stream, or ETL process transports data\nfrom one place and form to another place and form, it is acting like the database sub\u2010\nsystem that keeps indexes or materialized views up to date.\n500 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ce76664-0e88-49f7-983e-dc41346532a4": {"__data__": {"id_": "5ce76664-0e88-49f7-983e-dc41346532a4", "embedding": null, "metadata": {"page_label": "501", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f655cb7-3c82-4f42-af76-6975240df378", "node_type": "4", "metadata": {"page_label": "501", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e1d3f103940d88c5e47cd55e68059023ebecd8ee7c8ec61f2fd32d52524e595f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Viewed like this, batch and stream processors are like elaborate implementations of\ntriggers, stored procedures, and materialized view maintenance routines. The derived\ndata systems they maintain are like different index types. For example, a relational\ndatabase may support B-tree indexes, hash indexes, spatial indexes (see \u201cMulti-\ncolumn indexes\u201d on page 87), and other types of indexes. In the emerging architec\u2010\nture of derived data systems, instead of implementing those facilities as features of a\nsingle integrated database product, they are provided by various different pieces of\nsoftware, running on different machines, administered by different teams.\nWhere will these developments take us in the future? If we start from the premise\nthat there is no single data model or storage format that is suitable for all access pat\u2010\nterns, I speculate that there are two avenues by which different storage and process\u2010\ning tools can nevertheless be composed into a cohesive system:\nFederated databases: unifying reads\nIt is possible to provide a unified query interface to a wide variety of underlying\nstorage engines and processing methods\u2014an approach known as a federated\ndatabase or polystore [18, 19]. For example, PostgreSQL\u2019s foreign data wrapper\nfeature fits this pattern [ 20]. Applications that need a specialized data model or\nquery interface can still access the underlying storage engines directly, while\nusers who want to combine data from disparate places can do so easily through\nthe federated interface.\nA federated query interface follows the relational tradition of a single integrated\nsystem with a high-level query language and elegant semantics, but a compli\u2010\ncated implementation.\nUnbundled databases: unifying writes\nWhile federation addresses read-only querying across several different systems, it\ndoes not have a good answer to synchronizing writes across those systems. We\nsaid that within a single database, creating a consistent index is a built-in feature.\nWhen we compose several storage systems, we similarly need to ensure that all\ndata changes end up in all the right places, even in the face of faults. Making it\neasier to reliably plug together storage systems (e.g., through change data capture\nand event logs) is like unbundling a database\u2019s index-maintenance features in a\nway that can synchronize writes across disparate technologies [7, 21].\nThe unbundled approach follows the Unix tradition of small tools that do one\nthing well [22], that communicate through a uniform low-level API (pipes), and\nthat can be composed using a higher-level language (the shell) [16].\nMaking unbundling work\nFederation and unbundling are two sides of the same coin: composing a reliable, scal\u2010\nable, and maintainable system out of diverse components. Federated read-only\nUnbundling Databases | 501", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3b2b703-04ff-4c73-95ba-ac95076fc688": {"__data__": {"id_": "c3b2b703-04ff-4c73-95ba-ac95076fc688", "embedding": null, "metadata": {"page_label": "502", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0984b0e1-8825-4bab-a777-5b25903afb80", "node_type": "4", "metadata": {"page_label": "502", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d7fc63a89604c61444fa96aa145b443c436ee94678f8148ddc2fccab86b3bbe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "querying requires mapping one data model into another, which takes some thought\nbut is ultimately quite a manageable problem. I think that keeping the writes to sev\u2010\neral storage systems in sync is the harder engineering problem, and so I will focus\non it.\nThe traditional approach to synchronizing writes requires distributed transactions\nacross heterogeneous storage systems [ 18], which I think is the wrong solution (see\n\u201cDerived data versus distributed transactions\u201d on page 492). Transactions within a\nsingle storage or stream processing system are feasible, but when data crosses the\nboundary between different technologies, I believe that an asynchronous event log\nwith idempotent writes is a much more robust and practical approach.\nFor example, distributed transactions are used within some stream processors to ach\u2010\nieve exactly-once semantics (see \u201cAtomic commit revisited\u201d on page 477), and this\ncan work quite well. However, when a transaction would need to involve systems\nwritten by different groups of people (e.g., when data is written from a stream pro\u2010\ncessor to a distributed key-value store or search index), the lack of a standardized\ntransaction protocol makes integration much harder. An ordered log of events with\nidempotent consumers (see \u201cIdempotence\u201d on page 478) is a much simpler abstrac\u2010\ntion, and thus much more feasible to implement across heterogeneous systems [7].\nThe big advantage of log-based integration is loose coupling between the various com\u2010\nponents, which manifests itself in two ways:\n1. At a system level, asynchronous event streams make the system as a whole more\nrobust to outages or performance degradation of individual components. If a\nconsumer runs slow or fails, the event log can buffer messages (see \u201cDisk space\nusage\u201d on page 450), allowing the producer and any other consumers to continue\nrunning unaffected. The faulty consumer can catch up when it is fixed, so it\ndoesn\u2019t miss any data, and the fault is contained. By contrast, the synchronous\ninteraction of distributed transactions tends to escalate local faults into large-\nscale failures (see \u201cLimitations of distributed transactions\u201d on page 363).\n2. At a human level, unbundling data systems allows different software components\nand services to be developed, improved, and maintained independently from\neach other by different teams. Specialization allows each team to focus on doing\none thing well, with well-defined interfaces to other teams\u2019 systems. Event logs\nprovide an interface that is powerful enough to capture fairly strong consistency\nproperties (due to durability and ordering of events), but also general enough to\nbe applicable to almost any kind of data.\nUnbundled versus integrated systems\nIf unbundling does indeed become the way of the future, it will not replace databases\nin their current form\u2014they will still be needed as much as ever. Databases are still\n502 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09258b97-4b8b-4c6f-82d5-1ee07e793cdb": {"__data__": {"id_": "09258b97-4b8b-4c6f-82d5-1ee07e793cdb", "embedding": null, "metadata": {"page_label": "503", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16dabdeb-269c-4d97-b25d-1602538fe2ae", "node_type": "4", "metadata": {"page_label": "503", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "58f25c8cb9f927b2c8bae9edb2ed52c718d4644037c9fdcb583dc11fe1d2491c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "required for maintaining state in stream processors, and in order to serve queries for\nthe output of batch and stream processors (see \u201cThe Output of Batch Workflows\u201d  on\npage 411 and \u201cProcessing Streams\u201d on page 464). Specialized query engines will con\u2010\ntinue to be important for particular workloads: for example, query engines in MPP\ndata warehouses are optimized for exploratory analytic queries and handle this kind\nof workload very well (see \u201cComparing Hadoop to Distributed Databases\u201d on page\n414).\nThe complexity of running several different pieces of infrastructure can be a problem:\neach piece of software has a learning curve, configuration issues, and operational\nquirks, and so it is worth deploying as few moving parts as possible. A single integra\u2010\nted software product may also be able to achieve better and more predictable perfor\u2010\nmance on the kinds of workloads for which it is designed, compared to a system\nconsisting of several tools that you have composed with application code [ 23]. As I\nsaid in the Preface, building for scale that you don\u2019t need is wasted effort and may\nlock you into an inflexible design. In effect, it is a form of premature optimization.\nThe goal of unbundling is not to compete with individual databases on performance\nfor particular workloads; the goal is to allow you to combine several different data\u2010\nbases in order to achieve good performance for a much wider range of workloads\nthan is possible with a single piece of software. It\u2019s about breadth, not depth\u2014in the\nsame vein as the diversity of storage and processing models that we discussed in\n\u201cComparing Hadoop to Distributed Databases\u201d on page 414.\nThus, if there is a single technology that does everything you need, you\u2019re most likely\nbest off simply using that product rather than trying to reimplement it yourself from\nlower-level components. The advantages of unbundling and composition only come\ninto the picture when there is no single piece of software that satisfies all your\nrequirements.\nWhat\u2019s missing?\nThe tools for composing data systems are getting better, but I think one major part is\nmissing: we don\u2019t yet have the unbundled-database equivalent of the Unix shell (i.e., a\nhigh-level language for composing storage and processing systems in a simple and\ndeclarative way).\nFor example, I would love it if we could simply declare mysql | elasticsearch, by\nanalogy to Unix pipes [ 22], which would be the unbundled equivalent of CREATE\nINDEX: it would take all the documents in a MySQL database and index them in an\nElasticsearch cluster. It would then continually capture all the changes made to the\ndatabase and automatically apply them to the search index, without us having to\nwrite custom application code. This kind of integration should be possible with\nalmost any kind of storage or indexing system.\nUnbundling Databases | 503", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e611c3de-0710-4a63-a007-e0da60b5d1c0": {"__data__": {"id_": "e611c3de-0710-4a63-a007-e0da60b5d1c0", "embedding": null, "metadata": {"page_label": "504", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e73ea35-227f-4865-afba-b5325315169b", "node_type": "4", "metadata": {"page_label": "504", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "bbd4cee66db8644b3ebc6548a7f5e9995098710a6d7a31b3210867e610d96b3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly, it would be great to be able to precompute and update caches more easily.\nRecall that a materialized view is essentially a precomputed cache, so you could imag\u2010\nine creating a cache by declaratively specifying materialized views for complex quer\u2010\nies, including recursive queries on graphs (see \u201cGraph-Like Data Models\u201d  on page\n49) and application logic. There is interesting early-stage research in this area, such as\ndifferential dataflow [24, 25], and I hope that these ideas will find their way into pro\u2010\nduction systems. \nDesigning Applications Around Dataflow\nThe approach of unbundling databases by composing specialized storage and pro\u2010\ncessing systems with application code is also becoming known as the \u201cdatabase\ninside-out\u201d approach [ 26], after the title of a conference talk I gave in 2014 [ 27].\nHowever, calling it a \u201cnew architecture\u201d is too grandiose. I see it more as a design\npattern, a starting point for discussion, and we give it a name simply so that we can\nbetter talk about it.\nThese ideas are not mine; they are simply an amalgamation of other people\u2019s ideas\nfrom which I think we should learn. In particular, there is a lot of overlap with data\u2010\nflow languages such as Oz [28] and Juttle [29], functional reactive programming (FRP)\nlanguages such as Elm [30, 31], and logic programming languages such as Bloom [32].\nThe term unbundling in this context was proposed by Jay Kreps [7].\nEven spreadsheets have dataflow programming capabilities that are miles ahead of\nmost mainstream programming languages [ 33]. In a spreadsheet, you can put a for\u2010\nmula in one cell (for example, the sum of cells in another column), and whenever any\ninput to the formula changes, the result of the formula is automatically recalculated.\nThis is exactly what we want at a data system level: when a record in a database\nchanges, we want any index for that record to be automatically updated, and any\ncached views or aggregations that depend on the record to be automatically\nrefreshed. You should not have to worry about the technical details of how this\nrefresh happens, but be able to simply trust that it works correctly.\nThus, I think that most data systems still have something to learn from the features\nthat VisiCalc already had in 1979 [ 34]. The difference from spreadsheets is that\ntoday\u2019s data systems need to be fault-tolerant, scalable, and store data durably. They\nalso need to be able to integrate disparate technologies written by different groups of\npeople over time, and reuse existing libraries and services: it is unrealistic to expect all\nsoftware to be developed using one particular language, framework, or tool.\nIn this section I will expand on these ideas and explore some ways of building appli\u2010\ncations around the ideas of unbundled databases and dataflow.\n504 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2846, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5180069a-c02c-4941-baa4-2494b5c06d85": {"__data__": {"id_": "5180069a-c02c-4941-baa4-2494b5c06d85", "embedding": null, "metadata": {"page_label": "505", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54cee247-9a64-4b4a-9526-013be196d873", "node_type": "4", "metadata": {"page_label": "505", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "47df230819e8ac63c1cdd16ba16e3e76514412bc111710b994b8153a055f4157", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Application code as a derivation function\nWhen one dataset is derived from another, it goes through some kind of transforma\u2010\ntion function. For example:\n\u2022 A secondary index is a kind of derived dataset with a straightforward transforma\u2010\ntion function: for each row or document in the base table, it picks out the values\nin the columns or fields being indexed, and sorts by those values (assuming a B-\ntree or SSTable index, which are sorted by key, as discussed in Chapter 3).\n\u2022 A full-text search index is created by applying various natural language process\u2010\ning functions such as language detection, word segmentation, stemming or lem\u2010\nmatization, spelling correction, and synonym identification, followed by building\na data structure for efficient lookups (such as an inverted index).\n\u2022 In a machine learning system, we can consider the model as being derived from\nthe training data by applying various feature extraction and statistical analysis\nfunctions. When the model is applied to new input data, the output of the model\nis derived from the input and the model (and hence, indirectly, from the training\ndata).\n\u2022 A cache often contains an aggregation of data in the form in which it is going to\nbe displayed in a user interface (UI). Populating the cache thus requires knowl\u2010\nedge of what fields are referenced in the UI; changes in the UI may require\nupdating the definition of how the cache is populated and rebuilding the cache.\nThe derivation function for a secondary index is so commonly required that it is built\ninto many databases as a core feature, and you can invoke it by merely saying CREATE\nINDEX. For full-text indexing, basic linguistic features for common languages may be\nbuilt into a database, but the more sophisticated features often require domain-\nspecific tuning. In machine learning, feature engineering is notoriously application-\nspecific, and often has to incorporate detailed knowledge about the user interaction\nand deployment of an application [35].\nWhen the function that creates a derived dataset is not a standard cookie-cutter func\u2010\ntion like creating a secondary index, custom code is required to handle the\napplication-specific aspects. And this custom code is where many databases struggle.\nAlthough relational databases commonly support triggers, stored procedures, and\nuser-defined functions, which can be used to execute application code within the\ndatabase, they have been somewhat of an afterthought in database design (see\n\u201cTransmitting Event Streams\u201d on page 440).\nSeparation of application code and state\nIn theory, databases could be deployment environments for arbitrary application\ncode, like an operating system. However, in practice they have turned out to be\nUnbundling Databases | 505", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2745, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba971737-4ff0-409c-a5a8-f748ef1739ec": {"__data__": {"id_": "ba971737-4ff0-409c-a5a8-f748ef1739ec", "embedding": null, "metadata": {"page_label": "506", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6abe6038-5b67-4bf4-bb6e-47c4a3e0f197", "node_type": "4", "metadata": {"page_label": "506", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4c3fde21304bd326602a1215355f741ddbc715d286e85ac983236bca7d726a74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "i. Explaining a joke rarely improves it, but I don\u2019t want anyone to feel left out. Here, Church is a reference to\nthe mathematician Alonzo Church, who created the lambda calculus, an early form of computation that is the\nbasis for most functional programming languages. The lambda calculus has no mutable state (i.e., no vari\u2010\nables that can be overwritten), so one could say that mutable state is separate from Church\u2019s work.\npoorly suited for this purpose. They do not fit well with the requirements of modern\napplication development, such as dependency and package management, version\ncontrol, rolling upgrades, evolvability, monitoring, metrics, calls to network services,\nand integration with external systems.\nOn the other hand, deployment and cluster management tools such as Mesos, YARN,\nDocker, Kubernetes, and others are designed specifically for the purpose of running\napplication code. By focusing on doing one thing well, they are able to do it much\nbetter than a database that provides execution of user-defined functions as one of its\nmany features.\nI think it makes sense to have some parts of a system that specialize in durable data\nstorage, and other parts that specialize in running application code. The two can\ninteract while still remaining independent.\nMost web applications today are deployed as stateless services, in which any user\nrequest can be routed to any application server, and the server forgets everything\nabout the request once it has sent the response. This style of deployment is conve\u2010\nnient, as servers can be added or removed at will, but the state has to go somewhere:\ntypically, a database. The trend has been to keep stateless application logic separate\nfrom state management (databases): not putting application logic in the database and\nnot putting persistent state in the application [ 36]. As people in the functional pro\u2010\ngramming community like to joke, \u201cWe believe in the separation of Church and\nstate\u201d [37].i\nIn this typical web application model, the database acts as a kind of mutable shared\nvariable that can be accessed synchronously over the network. The application can\nread and update the variable, and the database takes care of making it durable, pro\u2010\nviding some concurrency control and fault tolerance.\nHowever, in most programming languages you cannot subscribe to changes in a\nmutable variable\u2014you can only read it periodically. Unlike in a spreadsheet, readers\nof the variable don\u2019t get notified if the value of the variable changes. (You can imple\u2010\nment such notifications in your own code\u2014this is known as the observer pattern\u2014\nbut most languages do not have this pattern as a built-in feature.)\nDatabases have inherited this passive approach to mutable data: if you want to find\nout whether the content of the database has changed, often your only option is to poll\n(i.e., to repeat your query periodically). Subscribing to changes is only just beginning\nto emerge as a feature (see \u201cAPI support for change streams\u201d on page 456).\n506 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c3793f3-be04-484d-b65e-3383693709dd": {"__data__": {"id_": "7c3793f3-be04-484d-b65e-3383693709dd", "embedding": null, "metadata": {"page_label": "507", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f53dcc64-eb02-4465-b9ed-0625a409b519", "node_type": "4", "metadata": {"page_label": "507", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6b917c87499f3bac78d5050906c28b59216970a7ea03600d935d07d945aecf5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dataflow: Interplay between state changes and application code\nThinking about applications in terms of dataflow implies renegotiating the relation\u2010\nship between application code and state management. Instead of treating a database\nas a passive variable that is manipulated by the application, we think much more\nabout the interplay and collaboration between state, state changes, and code that pro\u2010\ncesses them. Application code responds to state changes in one place by triggering\nstate changes in another place.\nWe saw this line of thinking in \u201cDatabases and Streams\u201d on page 451, where we dis\u2010\ncussed treating the log of changes to a database as a stream of events that we can sub\u2010\nscribe to. Message-passing systems such as actors (see \u201cMessage-Passing Dataflow\u201d\non page 136) also have this concept of responding to events. Already in the 1980s, the\ntuple spaces  model explored expressing distributed computations in terms of pro\u2010\ncesses that observe state changes and react to them [38, 39].\nAs discussed, similar things happen inside a database when a trigger fires due to a\ndata change, or when a secondary index is updated to reflect a change in the table\nbeing indexed. Unbundling the database means taking this idea and applying it to the\ncreation of derived datasets outside of the primary database: caches, full-text search\nindexes, machine learning, or analytics systems. We can use stream processing and\nmessaging systems for this purpose.\nThe important thing to keep in mind is that maintaining derived data is not the same\nas asynchronous job execution, for which messaging systems are traditionally\ndesigned (see \u201cLogs compared to traditional messaging\u201d on page 448):\n\u2022 When maintaining derived data, the order of state changes is often important (if\nseveral views are derived from an event log, they need to process the events in the\nsame order so that they remain consistent with each other). As discussed in\n\u201cAcknowledgments and redelivery\u201d on page 445, many message brokers do not\nhave this property when redelivering unacknowledged messages. Dual writes are\nalso ruled out (see \u201cKeeping Systems in Sync\u201d on page 452).\n\u2022 Fault tolerance is key for derived data: losing just a single message causes the\nderived dataset to go permanently out of sync with its data source. Both message\ndelivery and derived state updates must be reliable. For example, many actor sys\u2010\ntems by default maintain actor state and messages in memory, so they are lost if\nthe machine running the actor crashes.\nStable message ordering and fault-tolerant message processing are quite stringent\ndemands, but they are much less expensive and more operationally robust than dis\u2010\ntributed transactions. Modern stream processors can provide these ordering and reli\u2010\nability guarantees at scale, and they allow application code to be run as stream\noperators.\nUnbundling Databases | 507", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "524ee179-6372-4b35-aa27-a5e3eff6a904": {"__data__": {"id_": "524ee179-6372-4b35-aa27-a5e3eff6a904", "embedding": null, "metadata": {"page_label": "508", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "681c7631-815a-4a12-affe-c1cdfff0ab8f", "node_type": "4", "metadata": {"page_label": "508", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "35ee20af1ff95a9bb11f32490c734145a8d9e05b2875140fbb54432747e25e94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ii. In the microservices approach, you could avoid the synchronous network request by caching the exchange\nrate locally in the service that processes the purchase. However, in order to keep that cache fresh, you would\nneed to periodically poll for updated exchange rates, or subscribe to a stream of changes\u2014which is exactly\nwhat happens in the dataflow approach.\nThis application code can do the arbitrary processing that built-in derivation func\u2010\ntions in databases generally don\u2019t provide. Like Unix tools chained by pipes, stream\noperators can be composed to build large systems around dataflow. Each operator\ntakes streams of state changes as input, and produces other streams of state changes\nas output.\nStream processors and services\nThe currently trendy style of application development involves breaking down func\u2010\ntionality into a set of services that communicate via synchronous network requests\nsuch as REST APIs (see \u201cDataflow Through Services: REST and RPC\u201d on page 131).\nThe advantage of such a service-oriented architecture over a single monolithic appli\u2010\ncation is primarily organizational scalability through loose coupling: different teams\ncan work on different services, which reduces coordination effort between teams (as\nlong as the services can be deployed and updated independently).\nComposing stream operators into dataflow systems has a lot of similar characteristics\nto the microservices approach [40]. However, the underlying communication mecha\u2010\nnism is very different: one-directional, asynchronous message streams rather than\nsynchronous request/response interactions.\nBesides the advantages listed in \u201cMessage-Passing Dataflow\u201d  on page 136, such as\nbetter fault tolerance, dataflow systems can also achieve better performance. For\nexample, say a customer is purchasing an item that is priced in one currency but paid\nfor in another currency. In order to perform the currency conversion, you need to\nknow the current exchange rate. This operation could be implemented in two ways\n[40, 41]:\n1. In the microservices approach, the code that processes the purchase would prob\u2010\nably query an exchange-rate service or database in order to obtain the current\nrate for a particular currency.\n2. In the dataflow approach, the code that processes purchases would subscribe to a\nstream of exchange rate updates ahead of time, and record the current rate in a\nlocal database whenever it changes. When it comes to processing the purchase, it\nonly needs to query the local database.\nThe second approach has replaced a synchronous network request to another service\nwith a query to a local database (which may be on the same machine, even in the\nsame process).ii Not only is the dataflow approach faster, but it is also more robust to\n508 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2796, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63012630-b566-458e-9929-8448e34b2e16": {"__data__": {"id_": "63012630-b566-458e-9929-8448e34b2e16", "embedding": null, "metadata": {"page_label": "509", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b50947cc-b197-4b41-a716-a767606c578f", "node_type": "4", "metadata": {"page_label": "509", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d2016be37cd3ee2b7fcd6e2fdd85a271425a9ee14cd73b691c506d39da8b4338", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the failure of another service. The fastest and most reliable network request is no net\u2010\nwork request at all! Instead of RPC, we now have a stream join between purchase\nevents and exchange rate update events (see \u201cStream-table join (stream enrichment)\u201d\non page 473).\nThe join is time-dependent: if the purchase events are reprocessed at a later point in\ntime, the exchange rate will have changed. If you want to reconstruct the original out\u2010\nput, you will need to obtain the historical exchange rate at the original time of pur\u2010\nchase. No matter whether you query a service or subscribe to a stream of exchange\nrate updates, you will need to handle this time dependence (see \u201cTime-dependence of\njoins\u201d on page 475).\nSubscribing to a stream of changes, rather than querying the current state when\nneeded, brings us closer to a spreadsheet-like model of computation: when some\npiece of data changes, any derived data that depends on it can swiftly be updated.\nThere are still many open questions, for example around issues like time-dependent\njoins, but I believe that building applications around dataflow ideas is a very promis\u2010\ning direction to go in. \nObserving Derived State\nAt an abstract level, the dataflow systems discussed in the last section give you a pro\u2010\ncess for creating derived datasets (such as search indexes, materialized views, and\npredictive models) and keeping them up to date. Let\u2019s call that process the write path:\nwhenever some piece of information is written to the system, it may go through mul\u2010\ntiple stages of batch and stream processing, and eventually every derived dataset is\nupdated to incorporate the data that was written. Figure 12-1 shows an example of\nupdating a search index.\nFigure 12-1. In a search index, writes (document updates) meet reads (queries).\nBut why do you create the derived dataset in the first place? Most likely because you\nwant to query it again at a later time. This is the read path : when serving a user\nUnbundling Databases | 509", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf3f82fc-54fc-4c25-917e-c595a64c6f7a": {"__data__": {"id_": "cf3f82fc-54fc-4c25-917e-c595a64c6f7a", "embedding": null, "metadata": {"page_label": "510", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1240fab6-21e1-4f4c-9dbe-a28e69663082", "node_type": "4", "metadata": {"page_label": "510", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4972246f531f849e1e26c9fad4b194c79b22306daaaf7a2ed1c04fa0eabc7e12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "iii. Less facetiously, the set of distinct search queries with nonempty search results is finite, assuming a finite\ncorpus. However, it would be exponential in the number of terms in the corpus, which is still pretty bad news.\nrequest you read from the derived dataset, perhaps perform some more processing\non the results, and construct the response to the user.\nTaken together, the write path and the read path encompass the whole journey of the\ndata, from the point where it is collected to the point where it is consumed (probably\nby another human). The write path is the portion of the journey that is precomputed\n\u2014i.e., that is done eagerly as soon as the data comes in, regardless of whether anyone\nhas asked to see it. The read path is the portion of the journey that only happens\nwhen someone asks for it. If you are familiar with functional programming lan\u2010\nguages, you might notice that the write path is similar to eager evaluation, and the\nread path is similar to lazy evaluation.\nThe derived dataset is the place where the write path and the read path meet, as illus\u2010\ntrated in Figure 12-1. It represents a trade-off between the amount of work that needs\nto be done at write time and the amount that needs to be done at read time.\nMaterialized views and caching\nA full-text search index is a good example: the write path updates the index, and the\nread path searches the index for keywords. Both reads and writes need to do some\nwork. Writes need to update the index entries for all terms that appear in the docu\u2010\nment. Reads need to search for each of the words in the query, and apply Boolean\nlogic to find documents that contain all of the words in the query (an AND operator),\nor any synonym of each of the words (an OR operator).\nIf you didn\u2019t have an index, a search query would have to scan over all documents\n(like grep), which would get very expensive if you had a large number of documents.\nNo index means less work on the write path (no index to update), but a lot more\nwork on the read path.\nOn the other hand, you could imagine precomputing the search results for all possi\u2010\nble queries. In that case, you would have less work to do on the read path: no Boolean\nlogic, just find the results for your query and return them. However, the write path\nwould be a lot more expensive: the set of possible search queries that could be asked\nis infinite, and thus precomputing all possible search results would require infinite\ntime and storage space. That wouldn\u2019t work so well.iii\nAnother option would be to precompute the search results for only a fixed set of the\nmost common queries, so that they can be served quickly without having to go to the\nindex. The uncommon queries can still be served from the index. This would gener\u2010\nally be called a cache of common queries, although we could also call it a materialized\n510 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c750e3b0-1d8a-4cec-ad1a-4a4469dae57d": {"__data__": {"id_": "c750e3b0-1d8a-4cec-ad1a-4a4469dae57d", "embedding": null, "metadata": {"page_label": "511", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60e0c99c-3ccb-4d86-9315-ce2b8bece4af", "node_type": "4", "metadata": {"page_label": "511", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4fe2d36f6135071b7dcab59b7f7fe5db95f41eca20d74fad292b5d9e8c4c198b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "view, as it would need to be updated when new documents appear that should be\nincluded in the results of one of the common queries.\nFrom this example we can see that an index is not the only possible boundary\nbetween the write path and the read path. Caching of common search results is possi\u2010\nble, and grep-like scanning without the index is also possible on a small number of\ndocuments. Viewed like this, the role of caches, indexes, and materialized views is\nsimple: they shift the boundary between the read path and the write path. They allow\nus to do more work on the write path, by precomputing results, in order to save effort\non the read path.\nShifting the boundary between work done on the write path and the read path was in\nfact the topic of the Twitter example at the beginning of this book, in \u201cDescribing\nLoad\u201d on page 11. In that example, we also saw how the boundary between write path\nand read path might be drawn differently for celebrities compared to ordinary users.\nAfter 500 pages we have come full circle!\nStateful, offline-capable clients\nI find the idea of a boundary between write and read paths interesting because we can\ndiscuss shifting that boundary and explore what that shift means in practical terms.\nLet\u2019s look at the idea in a different context.\nThe huge popularity of web applications in the last two decades has led us to certain\nassumptions about application development that are easy to take for granted. In par\u2010\nticular, the client/server model\u2014in which clients are largely stateless and servers have\nthe authority over data\u2014is so common that we almost forget that anything else\nexists. However, technology keeps moving on, and I think it is important to question\nthe status quo from time to time.\nTraditionally, web browsers have been stateless clients that can only do useful things\nwhen you have an internet connection (just about the only thing you could do offline\nwas to scroll up and down in a page that you had previously loaded while online).\nHowever, recent \u201csingle-page\u201d JavaScript web apps have gained a lot of stateful capa\u2010\nbilities, including client-side user interface interaction and persistent local storage in\nthe web browser. Mobile apps can similarly store a lot of state on the device and don\u2019t\nrequire a round-trip to the server for most user interactions.\nThese changing capabilities have led to a renewed interest in offline-first applications\nthat do as much as possible using a local database on the same device, without requir\u2010\ning an internet connection, and sync with remote servers in the background when a\nnetwork connection is available [ 42]. Since mobile devices often have slow and unre\u2010\nliable cellular internet connections, it\u2019s a big advantage for users if their user interface\ndoes not have to wait for synchronous network requests, and if apps mostly work off\u2010\nline (see \u201cClients with offline operation\u201d on page 170).\nUnbundling Databases | 511", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2925, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96aac05d-08a0-487d-98f1-333a5f568531": {"__data__": {"id_": "96aac05d-08a0-487d-98f1-333a5f568531", "embedding": null, "metadata": {"page_label": "512", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8c598b6-7706-4683-9167-3704832bf3c4", "node_type": "4", "metadata": {"page_label": "512", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "54558659d0235b321126c84eb89b0305b4edcd6cae8e740a2786bc8af667c332", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When we move away from the assumption of stateless clients talking to a central\ndatabase and toward state that is maintained on end-user devices, a world of new\nopportunities opens up. In particular, we can think of the on-device state as a cache of\nstate on the server . The pixels on the screen are a materialized view onto model\nobjects in the client app; the model objects are a local replica of state in a remote\ndatacenter [27].\nPushing state changes to clients\nIn a typical web page, if you load the page in a web browser and the data subse\u2010\nquently changes on the server, the browser does not find out about the change until\nyou reload the page. The browser only reads the data at one point in time, assuming\nthat it is static\u2014it does not subscribe to updates from the server. Thus, the state on\nthe device is a stale cache that is not updated unless you explicitly poll for changes.\n(HTTP-based feed subscription protocols like RSS are really just a basic form of poll\u2010\ning.)\nMore recent protocols have moved beyond the basic request/response pattern of\nHTTP: server-sent events (the EventSource API) and WebSockets provide communi\u2010\ncation channels by which a web browser can keep an open TCP connection to a\nserver, and the server can actively push messages to the browser as long as it remains\nconnected. This provides an opportunity for the server to actively inform the end-\nuser client about any changes to the state it has stored locally, reducing the staleness\nof the client-side state.\nIn terms of our model of write path and read path, actively pushing state changes all\nthe way to client devices means extending the write path all the way to the end user.\nWhen a client is first initialized, it would still need to use a read path to get its initial\nstate, but thereafter it could rely on a stream of state changes sent by the server. The\nideas we discussed around stream processing and messaging are not restricted to run\u2010\nning only in a datacenter: we can take the ideas further, and extend them all the way\nto end-user devices [43].\nThe devices will be offline some of the time, and unable to receive any notifications of\nstate changes from the server during that time. But we already solved that problem: in\n\u201cConsumer offsets\u201d on page 449 we discussed how a consumer of a log-based mes\u2010\nsage broker can reconnect after failing or becoming disconnected, and ensure that it\ndoesn\u2019t miss any messages that arrived while it was disconnected. The same techni\u2010\nque works for individual users, where each device is a small subscriber to a small\nstream of events.\nEnd-to-end event streams\nRecent tools for developing stateful clients and user interfaces, such as the Elm lan\u2010\nguage [30] and Facebook\u2019s toolchain of React, Flux, and Redux [ 44], already manage\n512 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e492b05b-1a3f-41aa-af6b-efe4cfd30074": {"__data__": {"id_": "e492b05b-1a3f-41aa-af6b-efe4cfd30074", "embedding": null, "metadata": {"page_label": "513", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fa80710-f940-4a97-b983-d00dfb02adb5", "node_type": "4", "metadata": {"page_label": "513", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "25fa8db34836171820a6f48ab010dce7984e784296a2523f4b8c2ab2b84a946d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "internal client-side state by subscribing to a stream of events representing user input\nor responses from a server, structured similarly to event sourcing (see \u201cEvent Sourc\u2010\ning\u201d on page 457).\nIt would be very natural to extend this programming model to also allow a server to\npush state-change events into this client-side event pipeline. Thus, state changes\ncould flow through an end-to-end write path: from the interaction on one device that\ntriggers a state change, via event logs and through several derived data systems and\nstream processors, all the way to the user interface of a person observing the state on\nanother device. These state changes could be propagated with fairly low delay\u2014say,\nunder one second end to end.\nSome applications, such as instant messaging and online games, already have such a\n\u201creal-time\u201d architecture (in the sense of interactions with low delay, not in the sense\nof \u201cResponse time guarantees\u201d on page 298). But why don\u2019t we build all applications\nthis way?\nThe challenge is that the assumption of stateless clients and request/response interac\u2010\ntions is very deeply ingrained in our databases, libraries, frameworks, and protocols.\nMany datastores support read and write operations where a request returns one\nresponse, but much fewer provide an ability to subscribe to changes\u2014i.e., a request\nthat returns a stream of responses over time (see \u201cAPI support for change streams\u201d\non page 456).\nIn order to extend the write path all the way to the end user, we would need to funda\u2010\nmentally rethink the way we build many of these systems: moving away from request/\nresponse interaction and toward publish/subscribe dataflow [ 27]. I think that the\nadvantages of more responsive user interfaces and better offline support would make\nit worth the effort. If you are designing data systems, I hope that you will keep in\nmind the option of subscribing to changes, not just querying the current state.\nReads are events too\nWe discussed that when a stream processor writes derived data to a store (database,\ncache, or index), and when user requests query that store, the store acts as the bound\u2010\nary between the write path and the read path. The store allows random-access read\nqueries to the data that would otherwise require scanning the whole event log.\nIn many cases, the data storage is separate from the streaming system. But recall that\nstream processors also need to maintain state to perform aggregations and joins (see\n\u201cStream Joins\u201d on page 472). This state is normally hidden inside the stream pro\u2010\ncessor, but some frameworks allow it to also be queried by outside clients [ 45], turn\u2010\ning the stream processor itself into a kind of simple database.\nI would like to take that idea further. As discussed so far, the writes to the store go\nthrough an event log, while reads are transient network requests that go directly to\nUnbundling Databases | 513", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99f7e23e-947a-4cd3-807a-953a2c398f84": {"__data__": {"id_": "99f7e23e-947a-4cd3-807a-953a2c398f84", "embedding": null, "metadata": {"page_label": "514", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11c9715d-dc9d-40a0-86f4-4aa11d06ac66", "node_type": "4", "metadata": {"page_label": "514", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f65acef174a077821b0dc88fc154107129973bf054e338b94e2d78f22fd74570", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the nodes that store the data being queried. This is a reasonable design, but not the\nonly possible one. It is also possible to represent read requests as streams of events,\nand send both the read events and the write events through a stream processor; the\nprocessor responds to read events by emitting the result of the read to an output\nstream [46].\nWhen both the writes and the reads are represented as events, and routed to the same\nstream operator in order to be handled, we are in fact performing a stream-table join\nbetween the stream of read queries and the database. The read event needs to be sent\nto the database partition holding the data (see \u201cRequest Routing\u201d on page 214), just\nlike batch and stream processors need to copartition inputs on the same key when\njoining (see \u201cReduce-Side Joins and Grouping\u201d on page 403).\nThis correspondence between serving requests and performing joins is quite funda\u2010\nmental [47]. A one-off read request just passes the request through the join operator\nand then immediately forgets it; a subscribe request is a persistent join with past and\nfuture events on the other side of the join.\nRecording a log of read events potentially also has benefits with regard to tracking\ncausal dependencies and data provenance across a system: it would allow you to\nreconstruct what the user saw before they made a particular decision. For example, in\nan online shop, it is likely that the predicted shipping date and the inventory status\nshown to a customer affect whether they choose to buy an item [ 4]. To analyze this\nconnection, you need to record the result of the user\u2019s query of the shipping and\ninventory status.\nWriting read events to durable storage thus enables better tracking of causal depen\u2010\ndencies (see \u201cOrdering events to capture causality\u201d on page 493), but it incurs addi\u2010\ntional storage and I/O cost. Optimizing such systems to reduce the overhead is still\nan open research problem [ 2]. But if you already log read requests for operational\npurposes, as a side effect of request processing, it is not such a great change to make\nthe log the source of the requests instead.\nMulti-partition data processing\nFor queries that only touch a single partition, the effort of sending queries through a\nstream and collecting a stream of responses is perhaps overkill. However, this idea\nopens the possibility of distributed execution of complex queries that need to com\u2010\nbine data from several partitions, taking advantage of the infrastructure for message\nrouting, partitioning, and joining that is already provided by stream processors.\nStorm\u2019s distributed RPC feature supports this usage pattern (see \u201cMessage passing\nand RPC\u201d on page 468). For example, it has been used to compute the number of\npeople who have seen a URL on Twitter\u2014i.e., the union of the follower sets of every\u2010\none who has tweeted that URL [ 48]. As the set of Twitter users is partitioned, this\ncomputation requires combining results from many partitions.\n514 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6074dd95-2f2e-4524-9356-6ddb60a80f6e": {"__data__": {"id_": "6074dd95-2f2e-4524-9356-6ddb60a80f6e", "embedding": null, "metadata": {"page_label": "515", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac15fe04-145d-4f53-ac4c-7f00a54a4cf3", "node_type": "4", "metadata": {"page_label": "515", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "17dcc4019cc83fc022153022b16de85f758cb3326cecd572aa0bbf193f34d077", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another example of this pattern occurs in fraud prevention: in order to assess the risk\nof whether a particular purchase event is fraudulent, you can examine the reputation\nscores of the user\u2019s IP address, email address, billing address, shipping address, and\nso on. Each of these reputation databases is itself partitioned, and so collecting the\nscores for a particular purchase event requires a sequence of joins with differently\npartitioned datasets [49].\nThe internal query execution graphs of MPP databases have similar characteristics\n(see \u201cComparing Hadoop to Distributed Databases\u201d on page 414). If you need to per\u2010\nform this kind of multi-partition join, it is probably simpler to use a database that\nprovides this feature than to implement it using a stream processor. However, treat\u2010\ning queries as streams provides an option for implementing large-scale applications\nthat run against the limits of conventional off-the-shelf solutions. \nAiming for Correctness\nWith stateless services that only read data, it is not a big deal if something goes\nwrong: you can fix the bug and restart the service, and everything returns to normal.\nStateful systems such as databases are not so simple: they are designed to remember\nthings forever (more or less), so if something goes wrong, the effects also potentially\nlast forever\u2014which means they require more careful thought [50].\nWe want to build applications that are reliable and correct (i.e., programs whose\nsemantics are well defined and understood, even in the face of various faults). For\napproximately four decades, the transaction properties of atomicity, isolation, and\ndurability (Chapter 7) have been the tools of choice for building correct applications.\nHowever, those foundations are weaker than they seem: witness for example the con\u2010\nfusion of weak isolation levels (see \u201cWeak Isolation Levels\u201d on page 233).\nIn some areas, transactions are being abandoned entirely and replaced with models\nthat offer better performance and scalability, but much messier semantics (see for\nexample \u201cLeaderless Replication\u201d on page 177). Consistency is often talked about, but\npoorly defined (see \u201cConsistency\u201d on page 224 and Chapter 9). Some people assert\nthat we should \u201cembrace weak consistency\u201d for the sake of better availability, while\nlacking a clear idea of what that actually means in practice.\nFor a topic that is so important, our understanding and our engineering methods are\nsurprisingly flaky. For example, it is very difficult to determine whether it is safe to\nrun a particular application at a particular transaction isolation level or replication\nconfiguration [51, 52]. Often simple solutions appear to work correctly when concur\u2010\nrency is low and there are no faults, but turn out to have many subtle bugs in more\ndemanding circumstances.\nFor example, Kyle Kingsbury\u2019s Jepsen experiments [ 53] have highlighted the stark\ndiscrepancies between some products\u2019 claimed safety guarantees and their actual\nAiming for Correctness | 515", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0467500-8fc9-4bf0-8068-df0e643ab802": {"__data__": {"id_": "e0467500-8fc9-4bf0-8068-df0e643ab802", "embedding": null, "metadata": {"page_label": "516", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f2e4f884-77fd-49e8-a4c5-c53f6fb4bdb5", "node_type": "4", "metadata": {"page_label": "516", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "8252a139803ff1bab4f565e03b3a867ac5c254813b46d837167543ed0c5d25ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "behavior in the presence of network problems and crashes. Even if infrastructure\nproducts like databases were free from problems, application code would still need to\ncorrectly use the features they provide, which is error-prone if the configuration is\nhard to understand (which is the case with weak isolation levels, quorum configura\u2010\ntions, and so on).\nIf your application can tolerate occasionally corrupting or losing data in unpredicta\u2010\nble ways, life is a lot simpler, and you might be able to get away with simply crossing\nyour fingers and hoping for the best. On the other hand, if you need stronger assur\u2010\nances of correctness, then serializability and atomic commit are established\napproaches, but they come at a cost: they typically only work in a single datacenter\n(ruling out geographically distributed architectures), and they limit the scale and\nfault-tolerance properties you can achieve.\nWhile the traditional transaction approach is not going away, I also believe it is not\nthe last word in making applications correct and resilient to faults. In this section I\nwill suggest some ways of thinking about correctness in the context of dataflow archi\u2010\ntectures.\nThe End-to-End Argument for Databases\nJust because an application uses a data system that provides comparatively strong\nsafety properties, such as serializable transactions, that does not mean the application\nis guaranteed to be free from data loss or corruption. For example, if an application\nhas a bug that causes it to write incorrect data, or delete data from a database, serial\u2010\nizable transactions aren\u2019t going to save you.\nThis example may seem frivolous, but it is worth taking seriously: application bugs\noccur, and people make mistakes. I used this example in \u201cState, Streams, and Immut\u2010\nability\u201d on page 459 to argue in favor of immutable and append-only data, because it\nis easier to recover from such mistakes if you remove the ability of faulty code to\ndestroy good data.\nAlthough immutability is useful, it is not a cure-all by itself. Let\u2019s look at a more sub\u2010\ntle example of data corruption that can occur.\nExactly-once execution of an operation\nIn \u201cFault Tolerance\u201d on page 476 we encountered an idea called exactly-once (or\neffectively-once) semantics. If something goes wrong while processing a message, you\ncan either give up (drop the message\u2014i.e., incur data loss) or try again. If you try\nagain, there is the risk that it actually succeeded the first time, but you just didn\u2019t find\nout about the success, and so the message ends up being processed twice.\nProcessing twice is a form of data corruption: it is undesirable to charge a customer\ntwice for the same service (billing them too much) or increment a counter twice\n516 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23113c86-7fd9-48ca-9a1a-f9497804b1e8": {"__data__": {"id_": "23113c86-7fd9-48ca-9a1a-f9497804b1e8", "embedding": null, "metadata": {"page_label": "517", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb48d5c0-da60-4aa4-8734-06294983fb1c", "node_type": "4", "metadata": {"page_label": "517", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4bb34b26c9bf9d0952112160ed1ca324f49fe2d3152c4e225d0e489694852222", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(overstating some metric). In this context, exactly-once means arranging the compu\u2010\ntation such that the final effect is the same as if no faults had occurred, even if the\noperation actually was retried due to some fault. We previously discussed a few\napproaches for achieving this goal.\nOne of the most effective approaches is to make the operation idempotent (see\n\u201cIdempotence\u201d on page 478); that is, to ensure that it has the same effect, no matter\nwhether it is executed once or multiple times. However, taking an operation that is\nnot naturally idempotent and making it idempotent requires some effort and care:\nyou may need to maintain some additional metadata (such as the set of operation IDs\nthat have updated a value), and ensure fencing when failing over from one node to\nanother (see \u201cThe leader and the lock\u201d on page 301).\nDuplicate suppression\nThe same pattern of needing to suppress duplicates occurs in many other places\nbesides stream processing. For example, TCP uses sequence numbers on packets to\nput them in the correct order at the recipient, and to determine whether any packets\nwere lost or duplicated on the network. Any lost packets are retransmitted and any\nduplicates are removed by the TCP stack before it hands the data to an application.\nHowever, this duplicate suppression only works within the context of a single TCP\nconnection. Imagine the TCP connection is a client\u2019s connection to a database, and it\nis currently executing the transaction in Example 12-1. In many databases, a transac\u2010\ntion is tied to a client connection (if the client sends several queries, the database\nknows that they belong to the same transaction because they are sent on the same\nTCP connection). If the client suffers a network interruption and connection timeout\nafter sending the COMMIT, but before hearing back from the database server, it does\nnot know whether the transaction has been committed or aborted (Figure 8-1).\nExample 12-1. A nonidempotent transfer of money from one account to another\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;\nUPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;\nCOMMIT;\nThe client can reconnect to the database and retry the transaction, but now it is out\u2010\nside of the scope of TCP duplicate suppression. Since the transaction in Example 12-1\nis not idempotent, it could happen that $22 is transferred instead of the desired $11.\nThus, even though Example 12-1 is a standard example for transaction atomicity, it is\nactually not correct, and real banks do not work like this [3].\nTwo-phase commit (see \u201cAtomic Commit and Two-Phase Commit (2PC)\u201d  on page\n354) protocols break the 1:1 mapping between a TCP connection and a transaction,\nsince they must allow a transaction coordinator to reconnect to a database after a net\u2010\nAiming for Correctness | 517", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "120dfe82-aa7f-4acc-b3b7-57b7c1565bee": {"__data__": {"id_": "120dfe82-aa7f-4acc-b3b7-57b7c1565bee", "embedding": null, "metadata": {"page_label": "518", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2201a65-c24f-4b78-9c57-543a46df7785", "node_type": "4", "metadata": {"page_label": "518", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6d5f154af87b702f795865b8866882ff21d1627ac2643ed64c849bcd6d6150a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "work fault, and tell it whether to commit or abort an in-doubt transaction. Is this suf\u2010\nficient to ensure that the transaction will only be executed once? Unfortunately not.\nEven if we can suppress duplicate transactions between the database client and\nserver, we still need to worry about the network between the end-user device and the\napplication server. For example, if the end-user client is a web browser, it probably\nuses an HTTP POST request to submit an instruction to the server. Perhaps the user\nis on a weak cellular data connection, and they succeed in sending the POST, but the\nsignal becomes too weak before they are able to receive the response from the server.\nIn this case, the user will probably be shown an error message, and they may retry\nmanually. Web browsers warn, \u201cAre you sure you want to submit this form again?\u201d\u2014\nand the user says yes, because they wanted the operation to happen. (The Post/Redi\u2010\nrect/Get pattern [54] avoids this warning message in normal operation, but it doesn\u2019t\nhelp if the POST request times out.) From the web server\u2019s point of view the retry is a\nseparate request, and from the database\u2019s point of view it is a separate transaction.\nThe usual deduplication mechanisms don\u2019t help.\nOperation identifiers\nTo make the operation idempotent through several hops of network communication,\nit is not sufficient to rely just on a transaction mechanism provided by a database\u2014\nyou need to consider the end-to-end flow of the request.\nFor example, you could generate a unique identifier for an operation (such as a\nUUID) and include it as a hidden form field in the client application, or calculate a\nhash of all the relevant form fields to derive the operation ID [ 3]. If the web browser\nsubmits the POST request twice, the two requests will have the same operation ID.\nYou can then pass that operation ID all the way through to the database and check\nthat you only ever execute one operation with a given ID, as shown in Example 12-2.\nExample 12-2. Suppressing duplicate requests using a unique ID\nALTER TABLE requests ADD UNIQUE (request_id);\nBEGIN TRANSACTION;\nINSERT INTO requests\n  (request_id, from_account, to_account, amount)\n  VALUES('0286FDB8-D7E1-423F-B40B-792B3608036C', 4321, 1234, 11.00);\nUPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;\nUPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;\nCOMMIT;\n518 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "22d1546d-3d7d-417b-8930-82e7d47fa038": {"__data__": {"id_": "22d1546d-3d7d-417b-8930-82e7d47fa038", "embedding": null, "metadata": {"page_label": "519", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "70070691-a812-46e1-820a-72b8d72a191c", "node_type": "4", "metadata": {"page_label": "519", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "9db3c1f9f831cd815117386dc4e2a9bb3e8eaa1d0d9cfd33fc7e8b988af20087", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example 12-2  relies on a uniqueness constraint on the request_id column. If a\ntransaction attempts to insert an ID that already exists, the INSERT fails and the trans\u2010\naction is aborted, preventing it from taking effect twice. Relational databases can gen\u2010\nerally maintain a uniqueness constraint correctly, even at weak isolation levels\n(whereas an application-level check-then-insert may fail under nonserializable isola\u2010\ntion, as discussed in \u201cWrite Skew and Phantoms\u201d on page 246).\nBesides suppressing duplicate requests, the requests table in Example 12-2 acts as a\nkind of event log, hinting in the direction of event sourcing (see \u201cEvent Sourcing\u201d on\npage 457). The updates to the account balances don\u2019t actually have to happen in the\nsame transaction as the insertion of the event, since they are redundant and could be\nderived from the request event in a downstream consumer\u2014as long as the event is\nprocessed exactly once, which can again be enforced using the request ID.\nThe end-to-end argument\nThis scenario of suppressing duplicate transactions is just one example of a more\ngeneral principle called the end-to-end argument , which was articulated by Saltzer,\nReed, and Clark in 1984 [55]:\nThe function in question can completely and correctly be implemented only with the\nknowledge and help of the application standing at the endpoints of the communica\u2010\ntion system. Therefore, providing that questioned function as a feature of the commu\u2010\nnication system itself is not possible. (Sometimes an incomplete version of the function\nprovided by the communication system may be useful as a performance enhance\u2010\nment.)\nIn our example, the function in question was duplicate suppression. We saw that TCP\nsuppresses duplicate packets at the TCP connection level, and some stream process\u2010\nors provide so-called exactly-once semantics at the message processing level, but that\nis not enough to prevent a user from submitting a duplicate request if the first one\ntimes out. By themselves, TCP, database transactions, and stream processors cannot\nentirely rule out these duplicates. Solving the problem requires an end-to-end solu\u2010\ntion: a transaction identifier that is passed all the way from the end-user client to the\ndatabase.\nThe end-to-end argument also applies to checking the integrity of data: checksums\nbuilt into Ethernet, TCP, and TLS can detect corruption of packets in the network,\nbut they cannot detect corruption due to bugs in the software at the sending and\nreceiving ends of the network connection, or corruption on the disks where the data\nis stored. If you want to catch all possible sources of data corruption, you also need\nend-to-end checksums.\nA similar argument applies with encryption [ 55]: the password on your home WiFi\nnetwork protects against people snooping your WiFi traffic, but not against attackers\nelsewhere on the internet; TLS/SSL between your client and the server protects\nAiming for Correctness | 519", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2948, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "495b45b5-7118-4a83-a7a1-bc424043c912": {"__data__": {"id_": "495b45b5-7118-4a83-a7a1-bc424043c912", "embedding": null, "metadata": {"page_label": "520", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bfd0d76-acb5-4a41-8f60-295f26f0ba1a", "node_type": "4", "metadata": {"page_label": "520", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f4219f76f84e6924dd1aeeb4a8b984e58604aa647bd592ac47829ab7210a7d99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "against network attackers, but not against compromises of the server. Only end-to-\nend encryption and authentication can protect against all of these things.\nAlthough the low-level features (TCP duplicate suppression, Ethernet checksums,\nWiFi encryption) cannot provide the desired end-to-end features by themselves, they\nare still useful, since they reduce the probability of problems at the higher levels. For\nexample, HTTP requests would often get mangled if we didn\u2019t have TCP putting the\npackets back in the right order. We just need to remember that the low-level reliabil\u2010\nity features are not by themselves sufficient to ensure end-to-end correctness.\nApplying end-to-end thinking in data systems\nThis brings me back to my original thesis: just because an application uses a data sys\u2010\ntem that provides comparatively strong safety properties, such as serializable transac\u2010\ntions, that does not mean the application is guaranteed to be free from data loss or\ncorruption. The application itself needs to take end-to-end measures, such as dupli\u2010\ncate suppression, as well.\nThat is a shame, because fault-tolerance mechanisms are hard to get right. Low-level\nreliability mechanisms, such as those in TCP, work quite well, and so the remaining\nhigher-level faults occur fairly rarely. It would be really nice to wrap up the remain\u2010\ning high-level fault-tolerance machinery in an abstraction so that application code\nneedn\u2019t worry about it\u2014but I fear that we have not yet found the right abstraction.\nTransactions have long been seen as a good abstraction, and I do believe that they are\nuseful. As discussed in the introduction to Chapter 7, they take a wide range of possi\u2010\nble issues (concurrent writes, constraint violations, crashes, network interruptions,\ndisk failures) and collapse them down to two possible outcomes: commit or abort.\nThat is a huge simplification of the programming model, but I fear that it is not\nenough.\nTransactions are expensive, especially when they involve heterogeneous storage tech\u2010\nnologies (see \u201cDistributed Transactions in Practice\u201d on page 360). When we refuse to\nuse distributed transactions because they are too expensive, we end up having to\nreimplement fault-tolerance mechanisms in application code. As numerous examples\nthroughout this book have shown, reasoning about concurrency and partial failure is\ndifficult and counterintuitive, and so I suspect that most application-level mecha\u2010\nnisms do not work correctly. The consequence is lost or corrupted data.\nFor these reasons, I think it is worth exploring fault-tolerance abstractions that make\nit easy to provide application-specific end-to-end correctness properties, but also\nmaintain good performance and good operational characteristics in a large-scale dis\u2010\ntributed environment. \n520 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5109382-b8b9-4fc7-a197-78d2cd5b9034": {"__data__": {"id_": "c5109382-b8b9-4fc7-a197-78d2cd5b9034", "embedding": null, "metadata": {"page_label": "521", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e65ec52-ec77-4f0b-b62d-36ec7a0b7aa3", "node_type": "4", "metadata": {"page_label": "521", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "76286e4a953349d54351e5c58dd88c514c0fbf5bf980325d9e51739032e9a41f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Enforcing Constraints\nLet\u2019s think about correctness in the context of the ideas around unbundling databases\n(\u201cUnbundling Databases\u201d on page 499). We saw that end-to-end duplicate suppres\u2010\nsion can be achieved with a request ID that is passed all the way from the client to the\ndatabase that records the write. What about other kinds of constraints?\nIn particular, let\u2019s focus on uniqueness constraints\u2014such as the one we relied on in\nExample 12-2. In \u201cConstraints and uniqueness guarantees\u201d on page 330 we saw sev\u2010\neral other examples of application features that need to enforce uniqueness: a user\u2010\nname or email address must uniquely identify a user, a file storage service cannot\nhave more than one file with the same name, and two people cannot book the same\nseat on a flight or in a theater.\nOther kinds of constraints are very similar: for example, ensuring that an account\nbalance never goes negative, that you don\u2019t sell more items than you have in stock in\nthe warehouse, or that a meeting room does not have overlapping bookings. Techni\u2010\nques that enforce uniqueness can often be used for these kinds of constraints as well.\nUniqueness constraints require consensus\nIn Chapter 9 we saw that in a distributed setting, enforcing a uniqueness constraint\nrequires consensus: if there are several concurrent requests with the same value, the\nsystem somehow needs to decide which one of the conflicting operations is accepted,\nand reject the others as violations of the constraint.\nThe most common way of achieving this consensus is to make a single node the\nleader, and put it in charge of making all the decisions. That works fine as long as you\ndon\u2019t mind funneling all requests through a single node (even if the client is on the\nother side of the world), and as long as that node doesn\u2019t fail. If you need to tolerate\nthe leader failing, you\u2019re back at the consensus problem again (see \u201cSingle-leader rep\u2010\nlication and consensus\u201d on page 367).\nUniqueness checking can be scaled out by partitioning based on the value that needs\nto be unique. For example, if you need to ensure uniqueness by request ID, as in\nExample 12-2, you can ensure all requests with the same request ID are routed to the\nsame partition (see Chapter 6). If you need usernames to be unique, you can partition\nby hash of username.\nHowever, asynchronous multi-master replication is ruled out, because it could hap\u2010\npen that different masters concurrently accept conflicting writes, and thus the values\nare no longer unique (see \u201cImplementing Linearizable Systems\u201d on page 332). If you\nwant to be able to immediately reject any writes that would violate the constraint,\nsynchronous coordination is unavoidable [56].\nAiming for Correctness | 521", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5521481e-9acf-40ee-b452-96a6dcdf80ca": {"__data__": {"id_": "5521481e-9acf-40ee-b452-96a6dcdf80ca", "embedding": null, "metadata": {"page_label": "522", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f27bd5ff-f5f6-4e01-9e67-5ec2795a3dd4", "node_type": "4", "metadata": {"page_label": "522", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "4fa2c382b7392d5f3cc391d796b3f2070c4c8d13c2828f77bfc1d19ed94ced3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Uniqueness in log-based messaging\nThe log ensures that all consumers see messages in the same order\u2014a guarantee that\nis formally known as total order broadcast and is equivalent to consensus (see \u201cTotal\nOrder Broadcast\u201d on page 348). In the unbundled database approach with log-based\nmessaging, we can use a very similar approach to enforce uniqueness constraints.\nA stream processor consumes all the messages in a log partition sequentially on a sin\u2010\ngle thread (see \u201cLogs compared to traditional messaging\u201d on page 448). Thus, if the\nlog is partitioned based on the value that needs to be unique, a stream processor can\nunambiguously and deterministically decide which one of several conflicting opera\u2010\ntions came first. For example, in the case of several users trying to claim the same\nusername [57]:\n1. Every request for a username is encoded as a message, and appended to a parti\u2010\ntion determined by the hash of the username.\n2. A stream processor sequentially reads the requests in the log, using a local data\u2010\nbase to keep track of which usernames are taken. For every request for a user\u2010\nname that is available, it records the name as taken and emits a success message\nto an output stream. For every request for a username that is already taken, it\nemits a rejection message to an output stream.\n3. The client that requested the username watches the output stream and waits for a\nsuccess or rejection message corresponding to its request.\nThis algorithm is basically the same as in \u201cImplementing linearizable storage using\ntotal order broadcast\u201d on page 350. It scales easily to a large request throughput by\nincreasing the number of partitions, as each partition can be processed independ\u2010\nently.\nThe approach works not only for uniqueness constraints, but also for many other\nkinds of constraints. Its fundamental principle is that any writes that may conflict are\nrouted to the same partition and processed sequentially. As discussed in \u201cWhat is a\nconflict?\u201d on page 174 and \u201cWrite Skew and Phantoms\u201d on page 246, the definition of\na conflict may depend on the application, but the stream processor can use arbitrary\nlogic to validate a request. This idea is similar to the approach pioneered by Bayou in\nthe 1990s [58].\nMulti-partition request processing\nEnsuring that an operation is executed atomically, while satisfying constraints,\nbecomes more interesting when several partitions are involved. In Example 12-2 ,\nthere are potentially three partitions: the one containing the request ID, the one con\u2010\ntaining the payee account, and the one containing the payer account. There is no rea\u2010\n522 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d8c0d28-0a09-467c-bbaf-6134ee16c6a0": {"__data__": {"id_": "8d8c0d28-0a09-467c-bbaf-6134ee16c6a0", "embedding": null, "metadata": {"page_label": "523", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82b6e5e3-0b77-4480-9b8f-ecf6fac3e528", "node_type": "4", "metadata": {"page_label": "523", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "17881cb8c8224d0df92a9b4c79befbb257f230dc314c00870094fd9c256d7c0e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "son why those three things should be in the same partition, since they are all\nindependent from each other.\nIn the traditional approach to databases, executing this transaction would require an\natomic commit across all three partitions, which essentially forces it into a total order\nwith respect to all other transactions on any of those partitions. Since there is now\ncross-partition coordination, different partitions can no longer be processed inde\u2010\npendently, so throughput is likely to suffer.\nHowever, it turns out that equivalent correctness can be achieved with partitioned\nlogs, and without an atomic commit:\n1. The request to transfer money from account A to account B is given a unique\nrequest ID by the client, and appended to a log partition based on the request ID.\n2. A stream processor reads the log of requests. For each request message it emits\ntwo messages to output streams: a debit instruction to the payer account A (par\u2010\ntitioned by A), and a credit instruction to the payee account B (partitioned by B).\nThe original request ID is included in those emitted messages.\n3. Further processors consume the streams of credit and debit instructions, dedu\u2010\nplicate by request ID, and apply the changes to the account balances.\nSteps 1 and 2 are necessary because if the client directly sent the credit and debit\ninstructions, it would require an atomic commit across those two partitions to ensure\nthat either both or neither happen. To avoid the need for a distributed transaction,\nwe first durably log the request as a single message, and then derive the credit and\ndebit instructions from that first message. Single-object writes are atomic in almost\nall data systems (see \u201cSingle-object writes\u201d on page 230), and so the request either\nappears in the log or it doesn\u2019t, without any need for a multi-partition atomic com\u2010\nmit.\nIf the stream processor in step 2 crashes, it resumes processing from its last check\u2010\npoint. In doing so, it does not skip any request messages, but it may process requests\nmultiple times and produce duplicate credit and debit instructions. However, since it\nis deterministic, it will just produce the same instructions again, and the processors in\nstep 3 can easily deduplicate them using the end-to-end request ID.\nIf you want to ensure that the payer account is not overdrawn by this transfer, you\ncan additionally have a stream processor (partitioned by payer account number) that\nmaintains account balances and validates transactions. Only valid transactions would\nthen be placed in the request log in step 1.\nBy breaking down the multi-partition transaction into two differently partitioned\nstages and using the end-to-end request ID, we have achieved the same correctness\nproperty (every request is applied exactly once to both the payer and payee accounts),\neven in the presence of faults, and without using an atomic commit protocol. The\nAiming for Correctness | 523", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8009b1c1-c8bb-47cc-b01a-9f0b6b7701f6": {"__data__": {"id_": "8009b1c1-c8bb-47cc-b01a-9f0b6b7701f6", "embedding": null, "metadata": {"page_label": "524", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fc1f10d-3311-47cb-b4e6-d101dec4e87e", "node_type": "4", "metadata": {"page_label": "524", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "07baaa36723e0ac3deb8e11feaebe1aeddbcafaea94341b52231a8732d119db6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "idea of using multiple differently partitioned stages is similar to what we discussed in\n\u201cMulti-partition data processing\u201d on page 514 (see also \u201cConcurrency control\u201d on\npage 462). \nTimeliness and Integrity\nA convenient property of transactions is that they are typically linearizable (see \u201cLin\u2010\nearizability\u201d on page 324): that is, a writer waits until a transaction is committed, and\nthereafter its writes are immediately visible to all readers.\nThis is not the case when unbundling an operation across multiple stages of stream\nprocessors: consumers of a log are asynchronous by design, so a sender does not wait\nuntil its message has been processed by consumers. However, it is possible for a client\nto wait for a message to appear on an output stream. This is what we did in \u201cUnique\u2010\nness in log-based messaging\u201d on page 522 when checking whether a uniqueness con\u2010\nstraint was satisfied.\nIn this example, the correctness of the uniqueness check does not depend on whether\nthe sender of the message waits for the outcome. The waiting only has the purpose of\nsynchronously informing the sender whether or not the uniqueness check succeeded,\nbut this notification can be decoupled from the effects of processing the message.\nMore generally, I think the term consistency conflates two different requirements that\nare worth considering separately:\nTimeliness\nTimeliness means ensuring that users observe the system in an up-to-date state.\nWe saw previously that if a user reads from a stale copy of the data, they may\nobserve it in an inconsistent state (see \u201cProblems with Replication Lag\u201d on page\n161). However, that inconsistency is temporary, and will eventually be resolved\nsimply by waiting and trying again.\nThe CAP theorem (see \u201cThe Cost of Linearizability\u201d on page 335) uses consis\u2010\ntency in the sense of linearizability, which is a strong way of achieving timeliness.\nWeaker timeliness properties like read-after-write consistency (see \u201cReading\nYour Own Writes\u201d on page 162) can also be useful.\nIntegrity\nIntegrity means absence of corruption; i.e., no data loss, and no contradictory or\nfalse data. In particular, if some derived dataset is maintained as a view onto\nsome underlying data (see \u201cDeriving current state from the event log\u201d on page\n458), the derivation must be correct. For example, a database index must cor\u2010\nrectly reflect the contents of the database\u2014an index in which some records are\nmissing is not very useful.\n524 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a9fce72-0464-4609-895b-c98cf4727c4a": {"__data__": {"id_": "9a9fce72-0464-4609-895b-c98cf4727c4a", "embedding": null, "metadata": {"page_label": "525", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7738e21-a5b4-416b-8377-e673f3f8f5f6", "node_type": "4", "metadata": {"page_label": "525", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cbc8646ae801f30c3ce154d49ec3c5be1b8311a30394bebb761bf5bad0aefc67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If integrity is violated, the inconsistency is permanent: waiting and trying again is\nnot going to fix database corruption in most cases. Instead, explicit checking and\nrepair is needed. In the context of ACID transactions (see \u201cThe Meaning of\nACID\u201d on page 223), consistency is usually understood as some kind of\napplication-specific notion of integrity. Atomicity and durability are important\ntools for preserving integrity.\nIn slogan form: violations of timeliness are \u201ceventual consistency,\u201d whereas violations\nof integrity are \u201cperpetual inconsistency.\u201d\nI am going to assert that in most applications, integrity is much more important than\ntimeliness. Violations of timeliness can be annoying and confusing, but violations of\nintegrity can be catastrophic.\nFor example, on your credit card statement, it is not surprising if a transaction that\nyou made within the last 24 hours does not yet appear\u2014it is normal that these sys\u2010\ntems have a certain lag. We know that banks reconcile and settle transactions asyn\u2010\nchronously, and timeliness is not very important here [ 3]. However, it would be very\nbad if the statement balance was not equal to the sum of the transactions plus the\nprevious statement balance (an error in the sums), or if a transaction was charged to\nyou but not paid to the merchant (disappearing money). Such problems would be\nviolations of the integrity of the system.\nCorrectness of dataflow systems\nACID transactions usually provide both timeliness (e.g., linearizability) and integrity\n(e.g., atomic commit) guarantees. Thus, if you approach application correctness from\nthe point of view of ACID transactions, the distinction between timeliness and integ\u2010\nrity is fairly inconsequential.\nOn the other hand, an interesting property of the event-based dataflow systems that\nwe have discussed in this chapter is that they decouple timeliness and integrity. When\nprocessing event streams asynchronously, there is no guarantee of timeliness, unless\nyou explicitly build consumers that wait for a message to arrive before returning. But\nintegrity is in fact central to streaming systems.\nExactly-once or effectively-once semantics (see \u201cFault Tolerance\u201d  on page 476) is a\nmechanism for preserving integrity. If an event is lost, or if an event takes effect\ntwice, the integrity of a data system could be violated. Thus, fault-tolerant message\ndelivery and duplicate suppression (e.g., idempotent operations) are important for\nmaintaining the integrity of a data system in the face of faults.\nAs we saw in the last section, reliable stream processing systems can preserve integ\u2010\nrity without requiring distributed transactions and an atomic commit protocol,\nwhich means they can potentially achieve comparable correctness with much better\nAiming for Correctness | 525", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19151ddf-e2ce-4d90-937c-fd7c96b4cc83": {"__data__": {"id_": "19151ddf-e2ce-4d90-937c-fd7c96b4cc83", "embedding": null, "metadata": {"page_label": "526", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80f0582b-5836-4bdc-81a1-01afee9c253a", "node_type": "4", "metadata": {"page_label": "526", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "459b046fc27f321355c8e1e5af754e329c87c017ebf2814a6b398b02ae3f1fdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "performance and operational robustness. We achieved this integrity through a com\u2010\nbination of mechanisms:\n\u2022 Representing the content of the write operation as a single message, which can\neasily be written atomically\u2014an approach that fits very well with event sourcing\n(see \u201cEvent Sourcing\u201d on page 457)\n\u2022 Deriving all other state updates from that single message using deterministic der\u2010\nivation functions, similarly to stored procedures (see \u201cActual Serial Execution\u201d\non page 252 and \u201cApplication code as a derivation function\u201d on page 505)\n\u2022 Passing a client-generated request ID through all these levels of processing, ena\u2010\nbling end-to-end duplicate suppression and idempotence\n\u2022 Making messages immutable and allowing derived data to be reprocessed from\ntime to time, which makes it easier to recover from bugs (see \u201cAdvantages of\nimmutable events\u201d on page 460)\nThis combination of mechanisms seems to me a very promising direction for build\u2010\ning fault-tolerant applications in the future. \nLoosely interpreted constraints\nAs discussed previously, enforcing a uniqueness constraint requires consensus, typi\u2010\ncally implemented by funneling all events in a particular partition through a single\nnode. This limitation is unavoidable if we want the traditional form of uniqueness\nconstraint, and stream processing cannot avoid it.\nHowever, another thing to realize is that many real applications can actually get away\nwith much weaker notions of uniqueness:\n\u2022 If two people concurrently register the same username or book the same seat,\nyou can send one of them a message to apologize, and ask them to choose a dif\u2010\nferent one. This kind of change to correct a mistake is called a compensating\ntransaction [59, 60].\n\u2022 If customers order more items than you have in your warehouse, you can order\nin more stock, apologize to customers for the delay, and offer them a discount.\nThis is actually the same as what you\u2019d have to do if, say, a forklift truck ran over\nsome of the items in your warehouse, leaving you with fewer items in stock than\nyou thought you had [ 61]. Thus, the apology workflow already needs to be part\nof your business processes anyway, and so it might be unnecessary to require a\nlinearizable constraint on the number of items in stock.\n\u2022 Similarly, many airlines overbook airplanes in the expectation that some passen\u2010\ngers will miss their flight, and many hotels overbook rooms, expecting that some\nguests will cancel. In these cases, the constraint of \u201cone person per seat\u201d is delib\u2010\n526 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "65fbcd57-1afc-4fb5-82af-dca8c8e21b47": {"__data__": {"id_": "65fbcd57-1afc-4fb5-82af-dca8c8e21b47", "embedding": null, "metadata": {"page_label": "527", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c6ce5391-1394-43ec-87de-97316d6008ea", "node_type": "4", "metadata": {"page_label": "527", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0ccf258dc83247d8283eb32d183def673f926c78c9fb463ff4d8bce1358bb255", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "erately violated for business reasons, and compensation processes (refunds,\nupgrades, providing a complimentary room at a neighboring hotel) are put in\nplace to handle situations in which demand exceeds supply. Even if there was no\noverbooking, apology and compensation processes would be needed in order to\ndeal with flights being cancelled due to bad weather or staff on strike\u2014recover\u2010\ning from such issues is just a normal part of business [3].\n\u2022 If someone withdraws more money than they have in their account, the bank can\ncharge them an overdraft fee and ask them to pay back what they owe. By limit\u2010\ning the total withdrawals per day, the risk to the bank is bounded.\nIn many business contexts, it is actually acceptable to temporarily violate a constraint\nand fix it up later by apologizing. The cost of the apology (in terms of money or repu\u2010\ntation) varies, but it is often quite low: you can\u2019t unsend an email, but you can send a\nfollow-up email with a correction. If you accidentally charge a credit card twice, you\ncan refund one of the charges, and the cost to you is just the processing fees and per\u2010\nhaps a customer complaint. Once money has been paid out of an ATM, you can\u2019t\ndirectly get it back, although in principle you can send debt collectors to recover the\nmoney if the account was overdrawn and the customer won\u2019t pay it back.\nWhether the cost of the apology is acceptable is a business decision. If it is acceptable,\nthe traditional model of checking all constraints before even writing the data is\nunnecessarily restrictive, and a linearizable constraint is not needed. It may well be a\nreasonable choice to go ahead with a write optimistically, and to check the constraint\nafter the fact. You can still ensure that the validation occurs before doing things that\nwould be expensive to recover from, but that doesn\u2019t imply you must do the valida\u2010\ntion before you even write the data.\nThese applications do require integrity: you would not want to lose a reservation, or\nhave money disappear due to mismatched credits and debits. But they don\u2019t require\ntimeliness on the enforcement of the constraint: if you have sold more items than you\nhave in the warehouse, you can patch up the problem after the fact by apologizing.\nDoing so is similar to the conflict resolution approaches we discussed in \u201cHandling\nWrite Conflicts\u201d on page 171.\nCoordination-avoiding data systems\nWe have now made two interesting observations:\n1. Dataflow systems can maintain integrity guarantees on derived data without\natomic commit, linearizability, or synchronous cross-partition coordination.\n2. Although strict uniqueness constraints require timeliness and coordination,\nmany applications are actually fine with loose constraints that may be temporar\u2010\nily violated and fixed up later, as long as integrity is preserved throughout.\nAiming for Correctness | 527", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6a3b9c3-cd81-4cb1-8c36-06a89b633131": {"__data__": {"id_": "e6a3b9c3-cd81-4cb1-8c36-06a89b633131", "embedding": null, "metadata": {"page_label": "528", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "435f96e8-f705-466b-8e3c-a20eba1a0ab6", "node_type": "4", "metadata": {"page_label": "528", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "86b32744df70f3252f6b525793a52a5fd3c055b7074cf2b2cb7b31cb6c8fd124", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Taken together, these observations mean that dataflow systems can provide the data\nmanagement services for many applications without requiring coordination, while\nstill giving strong integrity guarantees. Such coordination-avoiding data systems have\na lot of appeal: they can achieve better performance and fault tolerance than systems\nthat need to perform synchronous coordination [56].\nFor example, such a system could operate distributed across multiple datacenters in a\nmulti-leader configuration, asynchronously replicating between regions. Any one\ndatacenter can continue operating independently from the others, because no syn\u2010\nchronous cross-region coordination is required. Such a system would have weak\ntimeliness guarantees\u2014it could not be linearizable without introducing coordination\n\u2014but it can still have strong integrity guarantees.\nIn this context, serializable transactions are still useful as part of maintaining derived\nstate, but they can be run at a small scope where they work well [ 8]. Heterogeneous\ndistributed transactions such as XA transactions (see \u201cDistributed Transactions in\nPractice\u201d on page 360) are not required. Synchronous coordination can still be intro\u2010\nduced in places where it is needed (for example, to enforce strict constraints before\nan operation from which recovery is not possible), but there is no need for everything\nto pay the cost of coordination if only a small part of an application needs it [43].\nAnother way of looking at coordination and constraints: they reduce the number of\napologies you have to make due to inconsistencies, but potentially also reduce the\nperformance and availability of your system, and thus potentially increase the num\u2010\nber of apologies you have to make due to outages. You cannot reduce the number of\napologies to zero, but you can aim to find the best trade-off for your needs\u2014the\nsweet spot where there are neither too many inconsistencies nor too many availability\nproblems. \nTrust, but Verify\nAll of our discussion of correctness, integrity, and fault-tolerance has been under the\nassumption that certain things might go wrong, but other things won\u2019t. We call these\nassumptions our system model  (see \u201cMapping system models to the real world\u201d on\npage 309): for example, we should assume that processes can crash, machines can\nsuddenly lose power, and the network can arbitrarily delay or drop messages. But we\nmight also assume that data written to disk is not lost after fsync, that data in mem\u2010\nory is not corrupted, and that the multiplication instruction of our CPU always\nreturns the correct result.\nThese assumptions are quite reasonable, as they are true most of the time, and it\nwould be difficult to get anything done if we had to constantly worry about our com\u2010\nputers making mistakes. Traditionally, system models take a binary approach toward\nfaults: we assume that some things can happen, and other things can never happen.\nIn reality, it is more a question of probabilities: some things are more likely, other\n528 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0489b91-cdd9-47d3-b3dc-b41210749bbe": {"__data__": {"id_": "a0489b91-cdd9-47d3-b3dc-b41210749bbe", "embedding": null, "metadata": {"page_label": "529", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a270f694-a4c1-44c4-9b76-b362fdd7be7a", "node_type": "4", "metadata": {"page_label": "529", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c619c6c8244d79ee6402af0cfb956387fd5051cd7adcabdd01ab488a1f88cba6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "things less likely. The question is whether violations of our assumptions happen often\nenough that we may encounter them in practice.\nWe have seen that data can become corrupted while it is sitting untouched on disks\n(see \u201cReplication and Durability\u201d on page 227), and data corruption on the network\ncan sometimes evade the TCP checksums (see \u201cWeak forms of lying\u201d on page 306).\nMaybe this is something we should be paying more attention to?\nOne application that I worked on in the past collected crash reports from clients, and\nsome of the reports we received could only be explained by random bit-flips in the\nmemory of those devices. It seems unlikely, but if you have enough devices running\nyour software, even very unlikely things do happen. Besides random memory corrup\u2010\ntion due to hardware faults or radiation, certain pathological memory access patterns\ncan flip bits even in memory that has no faults [ 62]\u2014an effect that can be used to\nbreak security mechanisms in operating systems [ 63] (this technique is known as\nrowhammer). Once you look closely, hardware isn\u2019t quite the perfect abstraction that\nit may seem.\nTo be clear, random bit-flips are still very rare on modern hardware [ 64]. I just want\nto point out that they are not beyond the realm of possibility, and so they deserve\nsome attention.\nMaintaining integrity in the face of software bugs\nBesides such hardware issues, there is always the risk of software bugs, which would\nnot be caught by lower-level network, memory, or filesystem checksums. Even widely\nused database software has bugs: I have personally seen cases of MySQL failing to\ncorrectly maintain a uniqueness constraint [ 65] and PostgreSQL\u2019s serializable isola\u2010\ntion level exhibiting write skew anomalies [66], even though MySQL and PostgreSQL\nare robust and well-regarded databases that have been battle-tested by many people\nfor many years. In less mature software, the situation is likely to be much worse.\nDespite considerable efforts in careful design, testing, and review, bugs still creep in.\nAlthough they are rare, and they eventually get found and fixed, there is still a period\nduring which such bugs can corrupt data.\nWhen it comes to application code, we have to assume many more bugs, since most\napplications don\u2019t receive anywhere near the amount of review and testing that data\u2010\nbase code does. Many applications don\u2019t even correctly use the features that databases\noffer for preserving integrity, such as foreign key or uniqueness constraints [36].\nConsistency in the sense of ACID (see \u201cConsistency\u201d on page 224) is based on the\nidea that the database starts off in a consistent state, and a transaction transforms it\nfrom one consistent state to another consistent state. Thus, we expect the database to\nalways be in a consistent state. However, this notion only makes sense if you assume\nthat the transaction is free from bugs. If the application uses the database incorrectly\nAiming for Correctness | 529", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69e4c1eb-c844-4b9f-8309-7ffcc57469c5": {"__data__": {"id_": "69e4c1eb-c844-4b9f-8309-7ffcc57469c5", "embedding": null, "metadata": {"page_label": "530", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e697497-e117-450e-96db-567cc1fd941f", "node_type": "4", "metadata": {"page_label": "530", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "025d4ea9eda9ba9eb5d3024e744c69e8812bd169802f5ff148da3071a776aebc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in some way, for example using a weak isolation level unsafely, the integrity of the\ndatabase cannot be guaranteed.\nDon\u2019t just blindly trust what they promise\nWith both hardware and software not always living up to the ideal that we would like\nthem to be, it seems that data corruption is inevitable sooner or later. Thus, we\nshould at least have a way of finding out if data has been corrupted so that we can fix\nit and try to track down the source of the error. Checking the integrity of data is\nknown as auditing.\nAs discussed in \u201cAdvantages of immutable events\u201d on page 460, auditing is not just\nfor financial applications. However, auditability is highly important in finance pre\u2010\ncisely because everyone knows that mistakes happen, and we all recognize the need to\nbe able to detect and fix problems.\nMature systems similarly tend to consider the possibility of unlikely things going\nwrong, and manage that risk. For example, large-scale storage systems such as HDFS\nand Amazon S3 do not fully trust disks: they run background processes that continu\u2010\nally read back files, compare them to other replicas, and move files from one disk to\nanother, in order to mitigate the risk of silent corruption [67].\nIf you want to be sure that your data is still there, you have to actually read it and\ncheck. Most of the time it will still be there, but if it isn\u2019t, you really want to find out\nsooner rather than later. By the same argument, it is important to try restoring from\nyour backups from time to time\u2014otherwise you may only find out that your backup\nis broken when it is too late and you have already lost data. Don\u2019t just blindly trust\nthat it is all working.\nA culture of verification\nSystems like HDFS and S3 still have to assume that disks work correctly most of the\ntime\u2014which is a reasonable assumption, but not the same as assuming that they\nalways work correctly. However, not many systems currently have this kind of \u201ctrust,\nbut verify\u201d approach of continually auditing themselves. Many assume that correct\u2010\nness guarantees are absolute and make no provision for the possibility of rare data\ncorruption. I hope that in the future we will see more self-validating or self-auditing\nsystems that continually check their own integrity, rather than relying on blind trust\n[68].\nI fear that the culture of ACID databases has led us toward developing applications\non the basis of blindly trusting technology (such as a transaction mechanism), and\nneglecting any sort of auditability in the process. Since the technology we trusted\nworked well enough most of the time, auditing mechanisms were not deemed worth\nthe investment.\n530 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85481bec-fe3a-4fe6-9272-b86ab47179d4": {"__data__": {"id_": "85481bec-fe3a-4fe6-9272-b86ab47179d4", "embedding": null, "metadata": {"page_label": "531", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "632b6f14-3965-45d9-a77d-e92181f5bb8c", "node_type": "4", "metadata": {"page_label": "531", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dc8e59f31d33b7566be5a06161b6ecdf20aaeb43df989147b89fccf671c319d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But then the database landscape changed: weaker consistency guarantees became the\nnorm under the banner of NoSQL, and less mature storage technologies became\nwidely used. Yet, because the audit mechanisms had not been developed, we contin\u2010\nued building applications on the basis of blind trust, even though this approach had\nnow become more dangerous. Let\u2019s think for a moment about designing for audita\u2010\nbility.\nDesigning for auditability\nIf a transaction mutates several objects in a database, it is difficult to tell after the fact\nwhat that transaction means. Even if you capture the transaction logs (see \u201cChange\nData Capture\u201d on page 454), the insertions, updates, and deletions in various tables\ndo not necessarily give a clear picture of why those mutations were performed. The\ninvocation of the application logic that decided on those mutations is transient and\ncannot be reproduced.\nBy contrast, event-based systems can provide better auditability. In the event sourc\u2010\ning approach, user input to the system is represented as a single immutable event,\nand any resulting state updates are derived from that event. The derivation can be\nmade deterministic and repeatable, so that running the same log of events through\nthe same version of the derivation code will result in the same state updates.\nBeing explicit about dataflow (see \u201cPhilosophy of batch process outputs\u201d  on page\n413) makes the provenance of data much clearer, which makes integrity checking\nmuch more feasible. For the event log, we can use hashes to check that the event stor\u2010\nage has not been corrupted. For any derived state, we can rerun the batch and stream\nprocessors that derived it from the event log in order to check whether we get the\nsame result, or even run a redundant derivation in parallel.\nA deterministic and well-defined dataflow also makes it easier to debug and trace the\nexecution of a system in order to determine why it did something [ 4, 69]. If some\u2010\nthing unexpected occurred, it is valuable to have the diagnostic capability to repro\u2010\nduce the exact circumstances that led to the unexpected event\u2014a kind of time-travel\ndebugging capability.\nThe end-to-end argument again\nIf we cannot fully trust that every individual component of the system will be free\nfrom corruption\u2014that every piece of hardware is fault-free and that every piece of\nsoftware is bug-free\u2014then we must at least periodically check the integrity of our\ndata. If we don\u2019t check, we won\u2019t find out about corruption until it is too late and it\nhas caused some downstream damage, at which point it will be much harder and\nmore expensive to track down the problem.\nChecking the integrity of data systems is best done in an end-to-end fashion (see\n\u201cThe End-to-End Argument for Databases\u201d on page 516): the more systems we can\nAiming for Correctness | 531", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd25fc63-4d0b-4d29-9670-a6119124598a": {"__data__": {"id_": "fd25fc63-4d0b-4d29-9670-a6119124598a", "embedding": null, "metadata": {"page_label": "532", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8262359e-29ed-4003-bae3-37dc012c80a6", "node_type": "4", "metadata": {"page_label": "532", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b0d627eea4631fba76bb1bf03bad4f8ff7932a371c670ce003278500ec897b3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "include in an integrity check, the fewer opportunities there are for corruption to go\nunnoticed at some stage of the process. If we can check that an entire derived data\npipeline is correct end to end, then any disks, networks, services, and algorithms\nalong the path are implicitly included in the check.\nHaving continuous end-to-end integrity checks gives you increased confidence about\nthe correctness of your systems, which in turn allows you to move faster [ 70]. Like\nautomated testing, auditing increases the chances that bugs will be found quickly,\nand thus reduces the risk that a change to the system or a new storage technology will\ncause damage. If you are not afraid of making changes, you can much better evolve\nan application to meet changing requirements.\nTools for auditable data systems\nAt present, not many data systems make auditability a top-level concern. Some appli\u2010\ncations implement their own audit mechanisms, for example by logging all changes\nto a separate audit table, but guaranteeing the integrity of the audit log and the data\u2010\nbase state is still difficult. A transaction log can be made tamper-proof by periodically\nsigning it with a hardware security module, but that does not guarantee that the right\ntransactions went into the log in the first place.\nIt would be interesting to use cryptographic tools to prove the integrity of a system in\na way that is robust to a wide range of hardware and software issues, and even poten\u2010\ntially malicious actions. Cryptocurrencies, blockchains, and distributed ledger tech\u2010\nnologies such as Bitcoin, Ethereum, Ripple, Stellar, and various others [ 71, 72, 73]\nhave sprung up to explore this area.\nI am not qualified to comment on the merits of these technologies as currencies or\nmechanisms for agreeing contracts. However, from a data systems point of view they\ncontain some interesting ideas. Essentially, they are distributed databases, with a data\nmodel and transaction mechanism, in which different replicas can be hosted by\nmutually untrusting organizations. The replicas continually check each other\u2019s integ\u2010\nrity and use a consensus protocol to agree on the transactions that should be exe\u2010\ncuted.\nI am somewhat skeptical about the Byzantine fault tolerance aspects of these technol\u2010\nogies (see \u201cByzantine Faults\u201d on page 304), and I find the technique of proof of work\n(e.g., Bitcoin mining) extraordinarily wasteful. The transaction throughput of Bitcoin\nis rather low, albeit for political and economic reasons more than for technical ones.\nHowever, the integrity checking aspects are interesting.\nCryptographic auditing and integrity checking often relies on Merkle trees  [74],\nwhich are trees of hashes that can be used to efficiently prove that a record appears in\nsome dataset (and a few other things). Outside of the hype of cryptocurrencies, certif\u2010\nicate transparency is a security technology that relies on Merkle trees to check the val\u2010\nidity of TLS/SSL certificates [75, 76].\n532 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5685792d-17fb-4614-88c2-68520751abc8": {"__data__": {"id_": "5685792d-17fb-4614-88c2-68520751abc8", "embedding": null, "metadata": {"page_label": "533", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18624e86-96dd-4ca1-80fd-cbf4788d4b7d", "node_type": "4", "metadata": {"page_label": "533", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "84123f1243d5b7b381f58beae837872b57a1bb6332a99389538ea7a39e24a456", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I could imagine integrity-checking and auditing algorithms, like those of certificate\ntransparency and distributed ledgers, becoming more widely used in data systems in\ngeneral. Some work will be needed to make them equally scalable as systems without\ncryptographic auditing, and to keep the performance penalty as low as possible. But I\nthink this is an interesting area to watch in the future. \nDoing the Right Thing\nIn the final section of this book, I would like to take a step back. Throughout this\nbook we have examined a wide range of different architectures for data systems, eval\u2010\nuated their pros and cons, and explored techniques for building reliable, scalable, and\nmaintainable applications. However, we have left out an important and fundamental\npart of the discussion, which I would now like to fill in.\nEvery system is built for a purpose; every action we take has both intended and unin\u2010\ntended consequences. The purpose may be as simple as making money, but the con\u2010\nsequences for the world may reach far beyond that original purpose. We, the\nengineers building these systems, have a responsibility to carefully consider those\nconsequences and to consciously decide what kind of world we want to live in.\nWe talk about data as an abstract thing, but remember that many datasets are about\npeople: their behavior, their interests, their identity. We must treat such data with\nhumanity and respect. Users are humans too, and human dignity is paramount.\nSoftware development increasingly involves making important ethical choices. There\nare guidelines to help software engineers navigate these issues, such as the ACM\u2019s\nSoftware Engineering Code of Ethics and Professional Practice [ 77], but they are\nrarely discussed, applied, and enforced in practice. As a result, engineers and product\nmanagers sometimes take a very cavalier attitude to privacy and potential negative\nconsequences of their products [78, 79, 80].\nA technology is not good or bad in itself\u2014what matters is how it is used and how it\naffects people. This is true for a software system like a search engine in much the\nsame way as it is for a weapon like a gun. I think it is not sufficient for software engi\u2010\nneers to focus exclusively on the technology and ignore its consequences: the ethical\nresponsibility is ours to bear also. Reasoning about ethics is difficult, but it is too\nimportant to ignore.\nPredictive Analytics\nFor example, predictive analytics is a major part of the \u201cBig Data\u201d hype. Using data\nanalysis to predict the weather, or the spread of diseases, is one thing [ 81]; it is\nanother matter to predict whether a convict is likely to reoffend, whether an applicant\nfor a loan is likely to default, or whether an insurance customer is likely to make\nexpensive claims. The latter have a direct effect on individual people\u2019s lives.\nDoing the Right Thing | 533", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eda916be-b392-4bc8-a356-0c80f1e044d6": {"__data__": {"id_": "eda916be-b392-4bc8-a356-0c80f1e044d6", "embedding": null, "metadata": {"page_label": "534", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3808f54-f124-4d0c-ab1f-618eda95593a", "node_type": "4", "metadata": {"page_label": "534", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6b159182de8022576e99a5619457a6dfe75bd87bf19c21c2bb27c26c3fa08e8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Naturally, payment networks want to prevent fraudulent transactions, banks want to\navoid bad loans, airlines want to avoid hijackings, and companies want to avoid hir\u2010\ning ineffective or untrustworthy people. From their point of view, the cost of a missed\nbusiness opportunity is low, but the cost of a bad loan or a problematic employee is\nmuch higher, so it is natural for organizations to want to be cautious. If in doubt,\nthey are better off saying no.\nHowever, as algorithmic decision-making becomes more widespread, someone who\nhas (accurately or falsely) been labeled as risky by some algorithm may suffer a large\nnumber of those \u201cno\u201d decisions. Systematically being excluded from jobs, air travel,\ninsurance coverage, property rental, financial services, and other key aspects of soci\u2010\nety is such a large constraint of the individual\u2019s freedom that it has been called \u201calgo\u2010\nrithmic prison\u201d [ 82]. In countries that respect human rights, the criminal justice\nsystem presumes innocence until proven guilty; on the other hand, automated sys\u2010\ntems can systematically and arbitrarily exclude a person from participating in society\nwithout any proof of guilt, and with little chance of appeal.\nBias and discrimination\nDecisions made by an algorithm are not necessarily any better or any worse than\nthose made by a human. Every person is likely to have biases, even if they actively try\nto counteract them, and discriminatory practices can become culturally institutional\u2010\nized. There is hope that basing decisions on data, rather than subjective and instinc\u2010\ntive assessments by people, could be more fair and give a better chance to people who\nare often overlooked in the traditional system [83].\nWhen we develop predictive analytics systems, we are not merely automating a\nhuman\u2019s decision by using software to specify the rules for when to say yes or no; we\nare even leaving the rules themselves to be inferred from data. However, the patterns\nlearned by these systems are opaque: even if there is some correlation in the data, we\nmay not know why. If there is a systematic bias in the input to an algorithm, the sys\u2010\ntem will most likely learn and amplify that bias in its output [84].\nIn many countries, anti-discrimination laws prohibit treating people differently\ndepending on protected traits such as ethnicity, age, gender, sexuality, disability, or\nbeliefs. Other features of a person\u2019s data may be analyzed, but what happens if they\nare correlated with protected traits? For example, in racially segregated neighbor\u2010\nhoods, a person\u2019s postal code or even their IP address is a strong predictor of race.\nPut like this, it seems ridiculous to believe that an algorithm could somehow take\nbiased data as input and produce fair and impartial output from it [85]. Yet this belief\noften seems to be implied by proponents of data-driven decision making, an attitude\nthat has been satirized as \u201cmachine learning is like money laundering for bias\u201d [86].\nPredictive analytics systems merely extrapolate from the past; if the past is discrimi\u2010\nnatory, they codify that discrimination. If we want the future to be better than the\n534 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f10ee03-a0b6-4b02-af48-1848d629639a": {"__data__": {"id_": "3f10ee03-a0b6-4b02-af48-1848d629639a", "embedding": null, "metadata": {"page_label": "535", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f1c5137-1bbc-4a40-bd25-db7996926143", "node_type": "4", "metadata": {"page_label": "535", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cd6e1040002e534088116939f3b2da1b5dca3f29bc237976a0818d1b78dfd018", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "past, moral imagination is required, and that\u2019s something only humans can provide\n[87]. Data and models should be our tools, not our masters.\nResponsibility and accountability\nAutomated decision making opens the question of responsibility and accountability\n[87]. If a human makes a mistake, they can be held accountable, and the person affec\u2010\nted by the decision can appeal. Algorithms make mistakes too, but who is accounta\u2010\nble if they go wrong [ 88]? When a self-driving car causes an accident, who is\nresponsible? If an automated credit scoring algorithm systematically discriminates\nagainst people of a particular race or religion, is there any recourse? If a decision by\nyour machine learning system comes under judicial review, can you explain to the\njudge how the algorithm made its decision?\nCredit rating agencies are an old example of collecting data to make decisions about\npeople. A bad credit score makes life difficult, but at least a credit score is normally\nbased on relevant facts about a person\u2019s actual borrowing history, and any errors in\nthe record can be corrected (although the agencies normally do not make this easy).\nHowever, scoring algorithms based on machine learning typically use a much wider\nrange of inputs and are much more opaque, making it harder to understand how a\nparticular decision has come about and whether someone is being treated in an\nunfair or discriminatory way [89].\nA credit score summarizes \u201cHow did you behave in the past?\u201d whereas predictive\nanalytics usually work on the basis of \u201cWho is similar to you, and how did people like\nyou behave in the past?\u201d Drawing parallels to others\u2019 behavior implies stereotyping\npeople, for example based on where they live (a close proxy for race and socioeco\u2010\nnomic class). What about people who get put in the wrong bucket? Furthermore, if a\ndecision is incorrect due to erroneous data, recourse is almost impossible [87].\nMuch data is statistical in nature, which means that even if the probability distribu\u2010\ntion on the whole is correct, individual cases may well be wrong. For example, if the\naverage life expectancy in your country is 80 years, that doesn\u2019t mean you\u2019re expected\nto drop dead on your 80th birthday. From the average and the probability distribu\u2010\ntion, you can\u2019t say much about the age to which one particular person will live. Simi\u2010\nlarly, the output of a prediction system is probabilistic and may well be wrong in\nindividual cases.\nA blind belief in the supremacy of data for making decisions is not only delusional, it\nis positively dangerous. As data-driven decision making becomes more widespread,\nwe will need to figure out how to make algorithms accountable and transparent, how\nto avoid reinforcing existing biases, and how to fix them when they inevitably make\nmistakes.\nWe will also need to figure out how to prevent data being used to harm people, and\nrealize its positive potential instead. For example, analytics can reveal financial and\nDoing the Right Thing | 535", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "059b6400-033b-4b11-a0d0-8dd0c02ca619": {"__data__": {"id_": "059b6400-033b-4b11-a0d0-8dd0c02ca619", "embedding": null, "metadata": {"page_label": "536", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e90d93d-5469-493c-ac99-d2cd7e80f725", "node_type": "4", "metadata": {"page_label": "536", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "496d450d082e93892db452519ac2235d751f75e56ad602b77dc75d0a6007b0d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "social characteristics of people\u2019s lives. On the one hand, this power could be used to\nfocus aid and support to help those people who most need it. On the other hand, it is\nsometimes used by predatory business seeking to identify vulnerable people and sell\nthem risky products such as high-cost loans and worthless college degrees [87, 90].\nFeedback loops\nEven with predictive applications that have less immediately far-reaching effects on\npeople, such as recommendation systems, there are difficult issues that we must con\u2010\nfront. When services become good at predicting what content users want to see, they\nmay end up showing people only opinions they already agree with, leading to echo\nchambers in which stereotypes, misinformation, and polarization can breed. We are\nalready seeing the impact of social media echo chambers on election campaigns [91].\nWhen predictive analytics affect people\u2019s lives, particularly pernicious problems arise\ndue to self-reinforcing feedback loops. For example, consider the case of employers\nusing credit scores to evaluate potential hires. You may be a good worker with a good\ncredit score, but suddenly find yourself in financial difficulties due to a misfortune\noutside of your control. As you miss payments on your bills, your credit score suffers,\nand you will be less likely to find work. Joblessness pushes you toward poverty, which\nfurther worsens your scores, making it even harder to find employment [ 87]. It\u2019s a\ndownward spiral due to poisonous assumptions, hidden behind a camouflage of\nmathematical rigor and data.\nWe can\u2019t always predict when such feedback loops happen. However, many conse\u2010\nquences can be predicted by thinking about the entire system (not just the computer\u2010\nized parts, but also the people interacting with it)\u2014an approach known as systems\nthinking [92]. We can try to understand how a data analysis system responds to dif\u2010\nferent behaviors, structures, or characteristics. Does the system reinforce and amplify\nexisting differences between people (e.g., making the rich richer or the poor poorer),\nor does it try to combat injustice? And even with the best intentions, we must beware\nof unintended consequences. \nPrivacy and Tracking\nBesides the problems of predictive analytics\u2014i.e., using data to make automated\ndecisions about people\u2014there are ethical problems with data collection itself. What is\nthe relationship between the organizations collecting data and the people whose data\nis being collected?\nWhen a system only stores data that a user has explicitly entered, because they want\nthe system to store and process it in a certain way, the system is performing a service\nfor the user: the user is the customer. But when a user\u2019s activity is tracked and logged\nas a side effect of other things they are doing, the relationship is less clear. The service\n536 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "339907ab-f39a-4eed-a7d6-e86106dcdd6f": {"__data__": {"id_": "339907ab-f39a-4eed-a7d6-e86106dcdd6f", "embedding": null, "metadata": {"page_label": "537", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bda2501a-784e-484b-a1df-bfdea29358a2", "node_type": "4", "metadata": {"page_label": "537", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "18db34359ebe877fef77da785afb488f8756977272a22dad60961628e821de7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "no longer just does what the user tells it to do, but it takes on interests of its own,\nwhich may conflict with the user\u2019s interests.\nTracking behavioral data has become increasingly important for user-facing features\nof many online services: tracking which search results are clicked helps improve the\nranking of search results; recommending \u201cpeople who liked X also liked Y\u201d helps\nusers discover interesting and useful things; A/B tests and user flow analysis can help\nindicate how a user interface might be improved. Those features require some\namount of tracking of user behavior, and users benefit from them.\nHowever, depending on a company\u2019s business model, tracking often doesn\u2019t stop\nthere. If the service is funded through advertising, the advertisers are the actual cus\u2010\ntomers, and the users\u2019 interests take second place. Tracking data becomes more\ndetailed, analyses become further-reaching, and data is retained for a long time in\norder to build up detailed profiles of each person for marketing purposes.\nNow the relationship between the company and the user whose data is being collec\u2010\nted starts looking quite different. The user is given a free service and is coaxed into\nengaging with it as much as possible. The tracking of the user serves not primarily\nthat individual, but rather the needs of the advertisers who are funding the service. I\nthink this relationship can be appropriately described with a word that has more sin\u2010\nister connotations: surveillance.\nSurveillance\nAs a thought experiment, try replacing the word data with surveillance, and observe if\ncommon phrases still sound so good [ 93]. How about this: \u201cIn our surveillance-\ndriven organization we collect real-time surveillance streams and store them in our\nsurveillance warehouse. Our surveillance scientists use advanced analytics and sur\u2010\nveillance processing in order to derive new insights.\u201d\nThis thought experiment is unusually polemic for this book, Designing Surveillance-\nIntensive Applications , but I think that strong words are needed to emphasize this\npoint. In our attempts to make software \u201ceat the world\u201d [ 94], we have built the great\u2010\nest mass surveillance infrastructure the world has ever seen. Rushing toward an Inter\u2010\nnet of Things, we are rapidly approaching a world in which every inhabited space\ncontains at least one internet-connected microphone, in the form of smartphones,\nsmart TVs, voice-controlled assistant devices, baby monitors, and even children\u2019s\ntoys that use cloud-based speech recognition. Many of these devices have a terrible\nsecurity record [95].\nEven the most totalitarian and repressive regimes could only dream of putting a\nmicrophone in every room and forcing every person to constantly carry a device\ncapable of tracking their location and movements. Yet we apparently voluntarily,\neven enthusiastically, throw ourselves into this world of total surveillance. The differ\u2010\nDoing the Right Thing | 537", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1442b64-4520-4316-be19-b90d40b2b9a8": {"__data__": {"id_": "c1442b64-4520-4316-be19-b90d40b2b9a8", "embedding": null, "metadata": {"page_label": "538", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7cc10ee2-2872-43b0-a6dc-059019cbfc7c", "node_type": "4", "metadata": {"page_label": "538", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0a505e8ee10d2d40b8c0b6b4567e6ea9ebe212cd20252a2df55831a9cd707452", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ence is just that the data is being collected by corporations rather than government\nagencies [96].\nNot all data collection necessarily qualifies as surveillance, but examining it as such\ncan help us understand our relationship with the data collector. Why are we seem\u2010\ningly happy to accept surveillance by corporations? Perhaps you feel you have noth\u2010\ning to hide\u2014in other words, you are totally in line with existing power structures,\nyou are not a marginalized minority, and you needn\u2019t fear persecution [ 97]. Not\neveryone is so fortunate. Or perhaps it\u2019s because the purpose seems benign\u2014it\u2019s not\novert coercion and conformance, but merely better recommendations and more per\u2010\nsonalized marketing. However, combined with the discussion of predictive analytics\nfrom the last section, that distinction seems less clear.\nWe are already seeing car insurance premiums linked to tracking devices in cars, and\nhealth insurance coverage that depends on people wearing a fitness tracking device.\nWhen surveillance is used to determine things that hold sway over important aspects\nof life, such as insurance coverage or employment, it starts to appear less benign.\nMoreover, data analysis can reveal surprisingly intrusive things: for example, the\nmovement sensor in a smartwatch or fitness tracker can be used to work out what\nyou are typing (for example, passwords) with fairly good accuracy [ 98]. And algo\u2010\nrithms for analysis are only going to get better.\nConsent and freedom of choice\nWe might assert that users voluntarily choose to use a service that tracks their activ\u2010\nity, and they have agreed to the terms of service and privacy policy, so they consent to\ndata collection. We might even claim that users are receiving a valuable service in\nreturn for the data they provide, and that the tracking is necessary in order to provide\nthe service. Undoubtedly, social networks, search engines, and various other free\nonline services are valuable to users\u2014but there are problems with this argument.\nUsers have little knowledge of what data they are feeding into our databases, or how\nit is retained and processed\u2014and most privacy policies do more to obscure than to\nilluminate. Without understanding what happens to their data, users cannot give any\nmeaningful consent. Often, data from one user also says things about other people\nwho are not users of the service and who have not agreed to any terms. The derived\ndatasets that we discussed in this part of the book\u2014in which data from the entire\nuser base may have been combined with behavioral tracking and external data sour\u2010\nces\u2014are precisely the kinds of data of which users cannot have any meaningful\nunderstanding.\nMoreover, data is extracted from users through a one-way process, not a relationship\nwith true reciprocity, and not a fair value exchange. There is no dialog, no option for\nusers to negotiate how much data they provide and what service they receive in\n538 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "317777d7-9742-47af-be50-825be566268e": {"__data__": {"id_": "317777d7-9742-47af-be50-825be566268e", "embedding": null, "metadata": {"page_label": "539", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "985383d5-8688-4d9c-92c5-0e27ee0b0112", "node_type": "4", "metadata": {"page_label": "539", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e37b5318064305fdba794f6aee8197970a4fbfc17344eebd7c1c82009c3668d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "return: the relationship between the service and the user is very asymmetric and one-\nsided. The terms are set by the service, not by the user [99].\nFor a user who does not consent to surveillance, the only real alternative is simply not\nto use a service. But this choice is not free either: if a service is so popular that it is\n\u201cregarded by most people as essential for basic social participation\u201d [99], then it is not\nreasonable to expect people to opt out of this service\u2014using it is de facto mandatory.\nFor example, in most Western social communities, it has become the norm to carry a\nsmartphone, to use Facebook for socializing, and to use Google for finding informa\u2010\ntion. Especially when a service has network effects, there is a social cost to people\nchoosing not to use it.\nDeclining to use a service due to its tracking of users is only an option for the small\nnumber of people who are privileged enough to have the time and knowledge to\nunderstand its privacy policy, and who can afford to potentially miss out on social\nparticipation or professional opportunities that may have arisen if they had participa\u2010\nted in the service. For people in a less privileged position, there is no meaningful free\u2010\ndom of choice: surveillance becomes inescapable.\nPrivacy and use of data\nSometimes people claim that \u201cprivacy is dead\u201d on the grounds that some users are\nwilling to post all sorts of things about their lives to social media, sometimes mun\u2010\ndane and sometimes deeply personal. However, this claim is false and rests on a mis\u2010\nunderstanding of the word privacy.\nHaving privacy does not mean keeping everything secret; it means having the free\u2010\ndom to choose which things to reveal to whom, what to make public, and what to\nkeep secret. The right to privacy is a decision right: it enables each person to decide\nwhere they want to be on the spectrum between secrecy and transparency in each sit\u2010\nuation [99]. It is an important aspect of a person\u2019s freedom and autonomy.\nWhen data is extracted from people through surveillance infrastructure, privacy\nrights are not necessarily eroded, but rather transferred to the data collector. Compa\u2010\nnies that acquire data essentially say \u201ctrust us to do the right thing with your data,\u201d\nwhich means that the right to decide what to reveal and what to keep secret is trans\u2010\nferred from the individual to the company.\nThe companies in turn choose to keep much of the outcome of this surveillance\nsecret, because to reveal it would be perceived as creepy, and would harm their busi\u2010\nness model (which relies on knowing more about people than other companies do).\nIntimate information about users is only revealed indirectly, for example in the form\nof tools for targeting advertisements to specific groups of people (such as those suf\u2010\nfering from a particular illness).\nDoing the Right Thing | 539", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2845, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "183d2e7a-c302-4afd-aaff-eeae2ff3f3dd": {"__data__": {"id_": "183d2e7a-c302-4afd-aaff-eeae2ff3f3dd", "embedding": null, "metadata": {"page_label": "540", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ec9c0c5-b5f6-4e0a-96e3-4cdff960da1f", "node_type": "4", "metadata": {"page_label": "540", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "11d3ea3194ff24c93c425e13cb8957c3aae25bb154f0ed4483a276f3b1a0e1d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even if particular users cannot be personally reidentified from the bucket of people\ntargeted by a particular ad, they have lost their agency about the disclosure of some\nintimate information, such as whether they suffer from some illness. It is not the user\nwho decides what is revealed to whom on the basis of their personal preferences\u2014it\nis the company that exercises the privacy right with the goal of maximizing its profit.\nMany companies have a goal of not being perceived as creepy\u2014avoiding the question\nof how intrusive their data collection actually is, and instead focusing on managing\nuser perceptions. And even these perceptions are often managed poorly: for example,\nsomething may be factually correct, but if it triggers painful memories, the user may\nnot want to be reminded about it [ 100]. With any kind of data we should expect the\npossibility that it is wrong, undesirable, or inappropriate in some way, and we need to\nbuild mechanisms for handling those failures. Whether something is \u201cundesirable\u201d or\n\u201cinappropriate\u201d is of course down to human judgment; algorithms are oblivious to\nsuch notions unless we explicitly program them to respect human needs. As engi\u2010\nneers of these systems we must be humble, accepting and planning for such failings.\nPrivacy settings that allow a user of an online service to control which aspects of their\ndata other users can see are a starting point for handing back some control to users.\nHowever, regardless of the setting, the service itself still has unfettered access to the\ndata, and is free to use it in any way permitted by the privacy policy. Even if the ser\u2010\nvice promises not to sell the data to third parties, it usually grants itself unrestricted\nrights to process and analyze the data internally, often going much further than what\nis overtly visible to users.\nThis kind of large-scale transfer of privacy rights from individuals to corporations is\nhistorically unprecedented [ 99]. Surveillance has always existed, but it used to be\nexpensive and manual, not scalable and automated. Trust relationships have always\nexisted, for example between a patient and their doctor, or between a defendant and\ntheir attorney\u2014but in these cases the use of data has been strictly governed by ethical,\nlegal, and regulatory constraints. Internet services have made it much easier to amass\nhuge amounts of sensitive information without meaningful consent, and to use it at\nmassive scale without users understanding what is happening to their private data.\nData as assets and power\nSince behavioral data is a byproduct of users interacting with a service, it is some\u2010\ntimes called \u201cdata exhaust\u201d\u2014suggesting that the data is worthless waste material.\nViewed this way, behavioral and predictive analytics can be seen as a form of recy\u2010\ncling that extracts value from data that would have otherwise been thrown away.\nMore correct would be to view it the other way round: from an economic point of\nview, if targeted advertising is what pays for a service, then behavioral data about\npeople is the service\u2019s core asset. In this case, the application with which the user\ninteracts is merely a means to lure users into feeding more and more personal infor\u2010\n540 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d19626d-b338-4847-b1ca-120e1e30f76e": {"__data__": {"id_": "6d19626d-b338-4847-b1ca-120e1e30f76e", "embedding": null, "metadata": {"page_label": "541", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de17f2e6-650c-4858-9fa3-216f1eb249e7", "node_type": "4", "metadata": {"page_label": "541", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a3824c72fdbe69ccea0106561b95f1aa011b44af88228682dfeb6236d311343d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mation into the surveillance infrastructure [ 99]. The delightful human creativity and\nsocial relationships that often find expression in online services are cynically exploi\u2010\nted by the data extraction machine.\nThe assertion that personal data is a valuable asset is supported by the existence of\ndata brokers, a shady industry operating in secrecy, purchasing, aggregating, analyz\u2010\ning, inferring, and reselling intrusive personal data about people, mostly for market\u2010\ning purposes [ 90]. Startups are valued by their user numbers, by \u201ceyeballs\u201d\u2014i.e., by\ntheir surveillance capabilities.\nBecause the data is valuable, many people want it. Of course companies want it\u2014\nthat\u2019s why they collect it in the first place. But governments want to obtain it too: by\nmeans of secret deals, coercion, legal compulsion, or simply stealing it [ 101]. When a\ncompany goes bankrupt, the personal data it has collected is one of the assets that get\nsold. Moreover, the data is difficult to secure, so breaches happen disconcertingly\noften [102].\nThese observations have led critics to saying that data is not just an asset, but a \u201ctoxic\nasset\u201d [101], or at least \u201chazardous material\u201d [103]. Even if we think that we are capa\u2010\nble of preventing abuse of data, whenever we collect data, we need to balance the ben\u2010\nefits with the risk of it falling into the wrong hands: computer systems may be\ncompromised by criminals or hostile foreign intelligence services, data may be leaked\nby insiders, the company may fall into the hands of unscrupulous management that\ndoes not share our values, or the country may be taken over by a regime that has no\nqualms about compelling us to hand over the data.\nWhen collecting data, we need to consider not just today\u2019s political environment, but\nall possible future governments. There is no guarantee that every government elected\nin future will respect human rights and civil liberties, so \u201cit is poor civic hygiene to\ninstall technologies that could someday facilitate a police state\u201d [104].\n\u201cKnowledge is power,\u201d as the old adage goes. And furthermore, \u201cto scrutinize others\nwhile avoiding scrutiny oneself is one of the most important forms of power\u201d [ 105].\nThis is why totalitarian governments want surveillance: it gives them the power to\ncontrol the population. Although today\u2019s technology companies are not overtly seek\u2010\ning political power, the data and knowledge they have accumulated nevertheless gives\nthem a lot of power over our lives, much of which is surreptitious, outside of public\noversight [106].\nRemembering the Industrial Revolution\nData is the defining feature of the information age. The internet, data storage, pro\u2010\ncessing, and software-driven automation are having a major impact on the global\neconomy and human society. As our daily lives and social organization have changed\nin the past decade, and will probably continue to radically change in the coming dec\u2010\nades, comparisons to the Industrial Revolution come to mind [87, 96].\nDoing the Right Thing | 541", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29cc148e-8e91-435d-be63-cabb63a006f0": {"__data__": {"id_": "29cc148e-8e91-435d-be63-cabb63a006f0", "embedding": null, "metadata": {"page_label": "542", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d98de347-796a-46d5-9155-f3aaa2f3a921", "node_type": "4", "metadata": {"page_label": "542", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "3cee590b10a7127b679427673e7a3a0e6dc3988123c0d07a497f75b1911d9f6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Industrial Revolution came about through major technological and agricultural\nadvances, and it brought sustained economic growth and significantly improved liv\u2010\ning standards in the long run. Yet it also came with major problems: pollution of the\nair (due to smoke and chemical processes) and the water (from industrial and human\nwaste) was dreadful. Factory owners lived in splendor, while urban workers often\nlived in very poor housing and worked long hours in harsh conditions. Child labor\nwas common, including dangerous and poorly paid work in mines.\nIt took a long time before safeguards were established, such as environmental protec\u2010\ntion regulations, safety protocols for workplaces, outlawing child labor, and health\ninspections for food. Undoubtedly the cost of doing business increased when facto\u2010\nries could no longer dump their waste into rivers, sell tainted foods, or exploit work\u2010\ners. But society as a whole benefited hugely, and few of us would want to return to a\ntime before those regulations [87].\nJust as the Industrial Revolution had a dark side that needed to be managed, our tran\u2010\nsition to the information age has major problems that we need to confront and solve.\nI believe that the collection and use of data is one of those problems. In the words of\nBruce Schneier [96]:\nData is the pollution problem of the information age, and protecting privacy is the\nenvironmental challenge. Almost all computers produce information. It stays around,\nfestering. How we deal with it\u2014how we contain it and how we dispose of it\u2014is central\nto the health of our information economy. Just as we look back today at the early deca\u2010\ndes of the industrial age and wonder how our ancestors could have ignored pollution\nin their rush to build an industrial world, our grandchildren will look back at us during\nthese early decades of the information age and judge us on how we addressed the chal\u2010\nlenge of data collection and misuse.\nWe should try to make them proud.\nLegislation and self-regulation\nData protection laws might be able to help preserve individuals\u2019 rights. For example,\nthe 1995 European Data Protection Directive states that personal data must be \u201ccol\u2010\nlected for specified, explicit and legitimate purposes and not further processed in a\nway incompatible with those purposes,\u201d and furthermore that data must be \u201cade\u2010\nquate, relevant and not excessive in relation to the purposes for which they are collec\u2010\nted\u201d [107].\nHowever, it is doubtful whether this legislation is effective in today\u2019s internet context\n[108]. These rules run directly counter to the philosophy of Big Data, which is to\nmaximize data collection, to combine it with other datasets, to experiment and to\nexplore in order to generate new insights. Exploration means using data for unfore\u2010\nseen purposes, which is the opposite of the \u201cspecified and explicit\u201d purposes for\nwhich the user gave their consent (if we can meaningfully speak of consent at all\n[109]). Updated regulations are now being developed [89].\n542 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5b4da239-7fd8-4aba-b908-dec4cb4d6342": {"__data__": {"id_": "5b4da239-7fd8-4aba-b908-dec4cb4d6342", "embedding": null, "metadata": {"page_label": "543", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6eeeb315-2816-4bf0-92b5-f5d92eed548b", "node_type": "4", "metadata": {"page_label": "543", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "e29954cd40519deacaf32e572081ef959721652da1e88ca37bb5a512e8b6a336", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Companies that collect lots of data about people oppose regulation as being a burden\nand a hindrance to innovation. To some extent that opposition is justified. For exam\u2010\nple, when sharing medical data, there are clear risks to privacy, but there are also\npotential opportunities: how many deaths could be prevented if data analysis was\nable to help us achieve better diagnostics or find better treatments [ 110]? Over-\nregulation may prevent such breakthroughs. It is difficult to balance such potential\nopportunities with the risks [105].\nFundamentally, I think we need a culture shift in the tech industry with regard to\npersonal data. We should stop regarding users as metrics to be optimized, and\nremember that they are humans who deserve respect, dignity, and agency. We should\nself-regulate our data collection and processing practices in order to establish and\nmaintain the trust of the people who depend on our software [ 111]. And we should\ntake it upon ourselves to educate end users about how their data is used, rather than\nkeeping them in the dark.\nWe should allow each individual to maintain their privacy\u2014i.e., their control over\nown data\u2014and not steal that control from them through surveillance. Our individual\nright to control our data is like the natural environment of a national park: if we\ndon\u2019t explicitly protect and care for it, it will be destroyed. It will be the tragedy of the\ncommons, and we will all be worse off for it. Ubiquitous surveillance is not inevitable\n\u2014we are still able to stop it.\nHow exactly we might achieve this is an open question. To begin with, we should not\nretain data forever, but purge it as soon as it is no longer needed [ 111, 112]. Purging\ndata runs counter to the idea of immutability (see \u201cLimitations of immutability\u201d on\npage 463), but that issue can be solved. A promising approach I see is to enforce\naccess control through cryptographic protocols, rather than merely by policy [ 113,\n114]. Overall, culture and attitude changes will be necessary. \nSummary\nIn this chapter we discussed new approaches to designing data systems, and I\nincluded my personal opinions and speculations about the future. We started with\nthe observation that there is no one single tool that can efficiently serve all possible\nuse cases, and so applications necessarily need to compose several different pieces of\nsoftware to accomplish their goals. We discussed how to solve this data integration\nproblem by using batch processing and event streams to let data changes flow\nbetween different systems.\nIn this approach, certain systems are designated as systems of record, and other data\nis derived from them through transformations. In this way we can maintain indexes,\nmaterialized views, machine learning models, statistical summaries, and more. By\nmaking these derivations and transformations asynchronous and loosely coupled, a\nSummary | 543", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "344780e3-f4d3-42d0-aa2b-974f955db383": {"__data__": {"id_": "344780e3-f4d3-42d0-aa2b-974f955db383", "embedding": null, "metadata": {"page_label": "544", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4d3d7bd-e13d-4f3a-a5cc-c53e97a1b090", "node_type": "4", "metadata": {"page_label": "544", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "541073140fd01d982af83895d8ffddf89e5856afe377b861bed6585b682e22e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "problem in one area is prevented from spreading to unrelated parts of the system,\nincreasing the robustness and fault-tolerance of the system as a whole.\nExpressing dataflows as transformations from one dataset to another also helps\nevolve applications: if you want to change one of the processing steps, for example to\nchange the structure of an index or cache, you can just rerun the new transformation\ncode on the whole input dataset in order to rederive the output. Similarly, if some\u2010\nthing goes wrong, you can fix the code and reprocess the data in order to recover.\nThese processes are quite similar to what databases already do internally, so we recast\nthe idea of dataflow applications as unbundling the components of a database, and\nbuilding an application by composing these loosely coupled components.\nDerived state can be updated by observing changes in the underlying data. Moreover,\nthe derived state itself can further be observed by downstream consumers. We can\neven take this dataflow all the way through to the end-user device that is displaying\nthe data, and thus build user interfaces that dynamically update to reflect data\nchanges and continue to work offline.\nNext, we discussed how to ensure that all of this processing remains correct in the\npresence of faults. We saw that strong integrity guarantees can be implemented scala\u2010\nbly with asynchronous event processing, by using end-to-end operation identifiers to\nmake operations idempotent and by checking constraints asynchronously. Clients\ncan either wait until the check has passed, or go ahead without waiting but risk hav\u2010\ning to apologize about a constraint violation. This approach is much more scalable\nand robust than the traditional approach of using distributed transactions, and fits\nwith how many business processes work in practice.\nBy structuring applications around dataflow and checking constraints asynchro\u2010\nnously, we can avoid most coordination and create systems that maintain integrity\nbut still perform well, even in geographically distributed scenarios and in the pres\u2010\nence of faults. We then talked a little about using audits to verify the integrity of data\nand detect corruption.\nFinally, we took a step back and examined some ethical aspects of building data-\nintensive applications. We saw that although data can be used to do good, it can also\ndo significant harm: making justifying decisions that seriously affect people\u2019s lives\nand are difficult to appeal against, leading to discrimination and exploitation, nor\u2010\nmalizing surveillance, and exposing intimate information. We also run the risk of\ndata breaches, and we may find that a well-intentioned use of data has unintended\nconsequences.\nAs software and data are having such a large impact on the world, we engineers must\nremember that we carry a responsibility to work toward the kind of world that we\nwant to live in: a world that treats people with humanity and respect. I hope that we\ncan work together toward that goal. \n544 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26ff6cb2-ae2a-4d1b-b08d-c68aa1fe6638": {"__data__": {"id_": "26ff6cb2-ae2a-4d1b-b08d-c68aa1fe6638", "embedding": null, "metadata": {"page_label": "545", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "570bc122-9b98-430e-9c45-81f50eca26e4", "node_type": "4", "metadata": {"page_label": "545", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "af6e70775738cc185eefcab4a1a3f1139b9eca05cf994c86bc82ad39a48fe66e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "References\n[1] Rachid Belaid: \u201cPostgres Full-Text Search is Good Enough! ,\u201d rachbelaid.com, July\n13, 2015.\n[2] Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: \u201c Challenges to Adopting\nStronger Consistency at Scale ,\u201d at 15th USENIX Workshop on Hot Topics in Operat\u2010\ning Systems (HotOS), May 2015.\n[3] Pat Helland and Dave Campbell: \u201c Building on Quicksand ,\u201d at 4th Biennial Con\u2010\nference on Innovative Data Systems Research (CIDR), January 2009.\n[4] Jessica Kerr: \u201c Provenance and Causality in Distributed Systems ,\u201d blog.jessi\u2010\ntron.com, September 25, 2016.\n[5] Kostas Tzoumas: \u201c Batch Is a Special Case of Streaming ,\u201d data-artisans.com, Sep\u2010\ntember 15, 2015.\n[6] Shinji Kim and Robert Blafford: \u201cStream Windowing Performance Analysis: Con\u2010\ncord and Spark Streaming,\u201d concord.io, July 6, 2016.\n[7] Jay Kreps: \u201c The Log: What Every Software Engineer Should Know About Real-\nTime Data\u2019s Unifying Abstraction,\u201d engineering.linkedin.com, December 16, 2013.\n[8] Pat Helland: \u201c Life Beyond Distributed Transactions: An Apostate\u2019s Opinion ,\u201d at\n3rd Biennial Conference on Innovative Data Systems Research (CIDR), January 2007.\n[9] \u201c Great Western Railway (1835\u20131948) ,\u201d Network Rail Virtual Archive, network\u2010\nrail.co.uk.\n[10] Jacqueline Xu: \u201cOnline Migrations at Scale,\u201d stripe.com, February 2, 2017.\n[11] Molly Bartlett Dishman and Martin Fowler: \u201c Agile Architecture ,\u201d at O\u2019Reilly\nSoftware Architecture Conference, March 2015.\n[12] Nathan Marz and James Warren: Big Data: Principles and Best Practices of Scala\u2010\nble Real-Time Data Systems. Manning, 2015. ISBN: 978-1-617-29034-3\n[13] Oscar Boykin, Sam Ritchie, Ian O\u2019Connell, and Jimmy Lin: \u201c Summingbird: A\nFramework for Integrating Batch and Online MapReduce Computations ,\u201d at 40th\nInternational Conference on Very Large Data Bases (VLDB), September 2014.\n[14] Jay Kreps: \u201cQuestioning the Lambda Architecture,\u201d oreilly.com, July 2, 2014.\n[15] Raul Castro Fernandez, Peter Pietzuch, Jay Kreps, et al.: \u201c Liquid: Unifying Near\u2010\nline and Offline Big Data Integration ,\u201d at 7th Biennial Conference on Innovative Data\nSystems Research (CIDR), January 2015.\nSummary | 545", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b08a5680-2817-4cd5-a233-dfb3b9b478f7": {"__data__": {"id_": "b08a5680-2817-4cd5-a233-dfb3b9b478f7", "embedding": null, "metadata": {"page_label": "546", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85a17d89-f7f9-4f69-a1e4-4a7f196609a6", "node_type": "4", "metadata": {"page_label": "546", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6ac0da2f1bf6dc94ee23020bbe8216ea885be31e0306ab1d0c696b706777d995", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] Dennis M. Ritchie and Ken Thompson: \u201c The UNIX Time-Sharing System ,\u201d\nCommunications of the ACM , volume 17, number 7, pages 365\u2013375, July 1974. doi:\n10.1145/361011.361061\n[17] Eric A. Brewer and Joseph M. Hellerstein: \u201c CS262a: Advanced Topics in Com\u2010\nputer Systems ,\u201d lecture notes, University of California, Berkeley, cs.berkeley.edu,\nAugust 2011.\n[18] Michael Stonebraker: \u201cThe Case for Polystores,\u201d wp.sigmod.org, July 13, 2015.\n[19] Jennie Duggan, Aaron J. Elmore, Michael Stonebraker, et al.: \u201c The BigDAWG\nPolystore System,\u201d ACM SIGMOD Record , volume 44, number 2, pages 11\u201316, June\n2015. doi:10.1145/2814710.2814713\n[20] Patrycja Dybka: \u201cForeign Data Wrappers for PostgreSQL,\u201d vertabelo.com, March\n24, 2015.\n[21] David B. Lomet, Alan Fekete, Gerhard Weikum, and Mike Zwilling: \u201c Unbun\u2010\ndling Transaction Services in the Cloud ,\u201d at 4th Biennial Conference on Innovative\nData Systems Research (CIDR), January 2009.\n[22] Martin Kleppmann and Jay Kreps: \u201c Kafka, Samza and the Unix Philosophy of\nDistributed Data,\u201d IEEE Data Engineering Bulletin, volume 38, number 4, pages 4\u201314,\nDecember 2015.\n[23] John Hugg: \u201c Winning Now and in the Future: Where VoltDB Shines ,\u201d\nvoltdb.com, March 23, 2016.\n[24] Frank McSherry, Derek G. Murray, Rebecca Isaacs, and Michael Isard: \u201c Differ\u2010\nential Dataflow ,\u201d at 6th Biennial Conference on Innovative Data Systems Research\n(CIDR), January 2013.\n[25] Derek G Murray, Frank McSherry, Rebecca Isaacs, et al.: \u201cNaiad: A Timely Data\u2010\nflow System ,\u201d at 24th ACM Symposium on Operating Systems Principles  (SOSP),\npages 439\u2013455, November 2013. doi:10.1145/2517349.2522738\n[26] Gwen Shapira: \u201cWe have a bunch of customers who are implementing \u2018database\ninside-out\u2019 concept and they all ask \u2018is anyone else doing it? are we crazy?\u2019 \u201d twit\u2010\nter.com, July 28, 2016.\n[27] Martin Kleppmann: \u201c Turning the Database Inside-out with Apache Samza, \u201d at\nStrange Loop, September 2014.\n[28] Peter Van Roy and Seif Haridi: Concepts, Techniques, and Models of Computer\nProgramming. MIT Press, 2004. ISBN: 978-0-262-22069-9\n[29] \u201cJuttle Documentation,\u201d juttle.github.io, 2016.\n546 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f35fdf52-7d2f-4503-8483-1845fdf8fae3": {"__data__": {"id_": "f35fdf52-7d2f-4503-8483-1845fdf8fae3", "embedding": null, "metadata": {"page_label": "547", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9836168d-450a-4873-be3c-64d13d4707c9", "node_type": "4", "metadata": {"page_label": "547", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7daec71c45c49550d10bb4dbe7772aabbaf8dc723eefec3465d41cdbba280b3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[30] Evan Czaplicki and Stephen Chong: \u201c Asynchronous Functional Reactive Pro\u2010\ngramming for GUIs,\u201d at 34th ACM SIGPLAN Conference on Programming Language\nDesign and Implementation (PLDI), June 2013. doi:10.1145/2491956.2462161\n[31] Engineer Bainomugisha, Andoni Lombide Carreton, Tom van Cutsem, Stijn\nMostinckx, and Wolfgang de Meuter: \u201c A Survey on Reactive Programming ,\u201d ACM\nComputing Surveys , volume 45, number 4, pages 1\u201334, August 2013. doi:\n10.1145/2501654.2501666\n[32] Peter Alvaro, Neil Conway, Joseph M. Hellerstein, and William R. Marczak:\n\u201cConsistency Analysis in Bloom: A CALM and Collected Approach ,\u201d at 5th Biennial\nConference on Innovative Data Systems Research (CIDR), January 2011.\n[33] Felienne Hermans: \u201cSpreadsheets Are Code,\u201d at Code Mesh, November 2015.\n[34] Dan Bricklin and Bob Frankston: \u201c VisiCalc: Information from Its Creators ,\u201d\ndanbricklin.com.\n[35] D. Sculley, Gary Holt, Daniel Golovin, et al.: \u201c Machine Learning: The High-\nInterest Credit Card of Technical Debt ,\u201d at NIPS Workshop on Software Engineering\nfor Machine Learning (SE4ML), December 2014.\n[36] Peter Bailis, Alan Fekete, Michael J Franklin, et al.: \u201c Feral Concurrency Control:\nAn Empirical Investigation of Modern Application Integrity ,\u201d at ACM International\nConference on Management of Data  (SIGMOD), June 2015. doi:\n10.1145/2723372.2737784\n[37] Guy Steele: \u201c Re: Need for Macros (Was Re: Icon) ,\u201d email to ll1-discuss mailing\nlist, people.csail.mit.edu, December 24, 2001.\n[38] David Gelernter: \u201c Generative Communication in Linda ,\u201d ACM Transactions on\nProgramming Languages and Systems (TOPLAS), volume 7, number 1, pages 80\u2013112,\nJanuary 1985. doi:10.1145/2363.2433\n[39] Patrick Th. Eugster, Pascal A. Felber, Rachid Guerraoui, and Anne-Marie Ker\u2010\nmarrec: \u201c The Many Faces of Publish/Subscribe ,\u201d ACM Computing Surveys , volume\n35, number 2, pages 114\u2013131, June 2003. doi:10.1145/857076.857078\n[40] Ben Stopford: \u201c Microservices in a Streaming World ,\u201d at QCon London, March\n2016.\n[41] Christian Posta: \u201c Why Microservices Should Be Event Driven: Autonomy vs\nAuthority,\u201d blog.christianposta.com, May 27, 2016.\n[42] Alex Feyerke: \u201cSay Hello to Offline First,\u201d hood.ie, November 5, 2013.\n[43] Sebastian Burckhardt, Daan Leijen, Jonathan Protzenko, and Manuel F\u00e4hndrich:\n\u201cGlobal Sequence Protocol: A Robust Abstraction for Replicated Shared State ,\u201d at\nSummary | 547", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be7a4b96-5689-46c4-a7a4-75d3bc7a8256": {"__data__": {"id_": "be7a4b96-5689-46c4-a7a4-75d3bc7a8256", "embedding": null, "metadata": {"page_label": "548", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8861b292-6a4e-462f-bf12-ebeaf3cb795b", "node_type": "4", "metadata": {"page_label": "548", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c42680d1150b2cd31fcd0de6a956fe59b5dab048c766a2652e96e1631d278c01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "29th European Conference on Object-Oriented Programming  (ECOOP), July 2015.\ndoi:10.4230/LIPIcs.ECOOP.2015.568\n[44] Mark Soper: \u201cClearing Up React Data Management Confusion with Flux, Redux,\nand Relay,\u201d medium.com, December 3, 2015.\n[45] Eno Thereska, Damian Guy, Michael Noll, and Neha Narkhede: \u201c Unifying\nStream Processing and Interactive Queries in Apache Kafka ,\u201d confluent.io, October\n26, 2016.\n[46] Frank McSherry: \u201cDataflow as Database,\u201d github.com, July 17, 2016.\n[47] Peter Alvaro: \u201cI See What You Mean,\u201d at Strange Loop, September 2015.\n[48] Nathan Marz: \u201c Trident: A High-Level Abstraction for Realtime Computation ,\u201d\nblog.twitter.com, August 2, 2012.\n[49] Edi Bice: \u201c Low Latency Web Scale Fraud Prevention with Apache Samza, Kafka\nand Friends,\u201d at Merchant Risk Council MRC Vegas Conference, March 2016.\n[50] Charity Majors: \u201cThe Accidental DBA,\u201d charity.wtf, October 2, 2016.\n[51] Arthur J. Bernstein, Philip M. Lewis, and Shiyong Lu: \u201c Semantic Conditions for\nCorrectness at Different Isolation Levels ,\u201d at 16th International Conference on Data\nEngineering (ICDE), February 2000. doi:10.1109/ICDE.2000.839387\n[52] Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan: \u201c Auto\u2010\nmating the Detection of Snapshot Isolation Anomalies,\u201d at 33rd International Confer\u2010\nence on Very Large Data Bases (VLDB), September 2007.\n[53] Kyle Kingsbury: Jepsen blog post series, aphyr.com, 2013\u20132016.\n[54] Michael Jouravlev: \u201cRedirect After Post,\u201d theserverside.com, August 1, 2004.\n[55] Jerome H. Saltzer, David P. Reed, and David D. Clark: \u201c End-to-End Arguments\nin System Design ,\u201d ACM Transactions on Computer Systems , volume 2, number 4,\npages 277\u2013288, November 1984. doi:10.1145/357401.357402\n[56] Peter Bailis, Alan Fekete, Michael J. Franklin, et al.: \u201c Coordination-Avoiding\nDatabase Systems,\u201d Proceedings of the VLDB Endowment , volume 8, number 3, pages\n185\u2013196, November 2014.\n[57] Alex Yarmula: \u201c Strong Consistency in Manhattan ,\u201d blog.twitter.com, March 17,\n2016.\n[58] Douglas B Terry, Marvin M Theimer, Karin Petersen, et al.: \u201c Managing Update\nConflicts in Bayou, a Weakly Connected Replicated Storage System ,\u201d at 15th ACM\nSymposium on Operating Systems Principles  (SOSP), pages 172\u2013182, December 1995.\ndoi:10.1145/224056.224070\n548 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30b28b21-5eb9-4624-9dbe-d444ad80fd0e": {"__data__": {"id_": "30b28b21-5eb9-4624-9dbe-d444ad80fd0e", "embedding": null, "metadata": {"page_label": "549", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19053491-90c1-4900-b3df-cec4d042a334", "node_type": "4", "metadata": {"page_label": "549", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "88bd624329bbff0970b9d32dbd7a266811720bb636d3496864efa570b56519d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[59] Jim Gray: \u201c The Transaction Concept: Virtues and Limitations ,\u201d at 7th Interna\u2010\ntional Conference on Very Large Data Bases (VLDB), September 1981.\n[60] Hector Garcia-Molina and Kenneth Salem: \u201c Sagas,\u201d at ACM International Con\u2010\nference on Management of Data (SIGMOD), May 1987. doi:10.1145/38713.38742\n[61] Pat Helland: \u201c Memories, Guesses, and Apologies ,\u201d blogs.msdn.com, May 15,\n2007.\n[62] Yoongu Kim, Ross Daly, Jeremie Kim, et al.: \u201c Flipping Bits in Memory Without\nAccessing Them: An Experimental Study of DRAM Disturbance Errors ,\u201d at 41st\nAnnual International Symposium on Computer Architecture  (ISCA), June 2014. doi:\n10.1145/2678373.2665726\n[63] Mark Seaborn and Thomas Dullien: \u201c Exploiting the DRAM Rowhammer Bug to\nGain Kernel Privileges,\u201d googleprojectzero.blogspot.co.uk, March 9, 2015.\n[64] Jim N. Gray and Catharine van Ingen: \u201c Empirical Measurements of Disk Failure\nRates and Error Rates,\u201d Microsoft Research, MSR-TR-2005-166, December 2005.\n[65] Annamalai Gurusami and Daniel Price: \u201c Bug #73170: Duplicates in Unique Sec\u2010\nondary Index Because of Fix of Bug#68021,\u201d bugs.mysql.com, July 2014.\n[66] Gary Fredericks: \u201cPostgres Serializability Bug,\u201d github.com, September 2015.\n[67] Xiao Chen: \u201cHDFS DataNode Scanners and Disk Checker Explained ,\u201d blog.clou\u2010\ndera.com, December 20, 2016.\n[68] Jay Kreps: \u201c Getting Real About Distributed System Reliability ,\u201d blog.empathy\u2010\nbox.com, March 19, 2012.\n[69] Martin Fowler: \u201cThe LMAX Architecture,\u201d martinfowler.com, July 12, 2011.\n[70] Sam Stokes: \u201cMove Fast with Confidence,\u201d blog.samstokes.co.uk, July 11, 2016.\n[71] \u201cSawtooth Lake Documentation,\u201d Intel Corporation, intelledger.github.io, 2016.\n[72] Richard Gendal Brown: \u201c Introducing R3 Corda\u2122: A Distributed Ledger\nDesigned for Financial Services,\u201d gendal.me, April 5, 2016.\n[73] Trent McConaghy, Rodolphe Marques, Andreas M\u00fcller, et al.: \u201c BigchainDB: A\nScalable Blockchain Database,\u201d bigchaindb.com, June 8, 2016.\n[74] Ralph C. Merkle: \u201c A Digital Signature Based on a Conventional Encryption\nFunction,\u201d at CRYPTO \u201987, August 1987. doi:10.1007/3-540-48184-2_32\n[75] Ben Laurie: \u201c Certificate Transparency ,\u201d ACM Queue , volume 12, number 8,\npages 10-19, August 2014. doi:10.1145/2668152.2668154\nSummary | 549", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7026a587-e6d6-4d8d-a05a-8504c68e6ccf": {"__data__": {"id_": "7026a587-e6d6-4d8d-a05a-8504c68e6ccf", "embedding": null, "metadata": {"page_label": "550", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85fc3415-8fbd-4e4d-9f77-7ed86812eba3", "node_type": "4", "metadata": {"page_label": "550", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d0b43d9708493361309da60c81a2527b15f625f1c41c26966e0eb8fbacb05dd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[76] Mark D. Ryan: \u201c Enhanced Certificate Transparency and End-to-End Encrypted\nMail,\u201d at Network and Distributed System Security Symposium  (NDSS), February\n2014. doi:10.14722/ndss.2014.23379\n[77] \u201cSoftware Engineering Code of Ethics and Professional Practice,\u201d Association for\nComputing Machinery, acm.org, 1999.\n[78] Fran\u00e7ois Chollet: \u201c Software development is starting to involve important ethical\nchoices,\u201d twitter.com, October 30, 2016.\n[79] Igor Perisic: \u201cMaking Hard Choices: The Quest for Ethics in Machine Learning,\u201d\nengineering.linkedin.com, November 2016.\n[80] John Naughton: \u201c Algorithm Writers Need a Code of Conduct ,\u201d theguar\u2010\ndian.com, December 6, 2015.\n[81] Logan Kugler: \u201c What Happens When Big Data Blunders? ,\u201d Communications of\nthe ACM, volume 59, number 6, pages 15\u201316, June 2016. doi:10.1145/2911975\n[82] Bill Davidow: \u201c Welcome to Algorithmic Prison ,\u201d theatlantic.com, February 20,\n2014.\n[83] Don Peck: \u201cThey\u2019re Watching You at Work,\u201d theatlantic.com, December 2013.\n[84] Leigh Alexander: \u201c Is an Algorithm Any Less Racist Than a Human? \u201d theguar\u2010\ndian.com, August 3, 2016.\n[85] Jesse Emspak: \u201c How a Machine Learns Prejudice ,\u201d scientificamerican.com,\nDecember 29, 2016.\n[86] Maciej Ceg\u0142owski: \u201cThe Moral Economy of Tech,\u201d idlewords.com, June 2016.\n[87] Cathy O\u2019Neil: Weapons of Math Destruction: How Big Data Increases Inequality\nand Threatens Democracy. Crown Publishing, 2016. ISBN: 978-0-553-41881-1\n[88] Julia Angwin: \u201cMake Algorithms Accountable,\u201d nytimes.com, August 1, 2016.\n[89] Bryce Goodman and Seth Flaxman: \u201c European Union Regulations on Algorith\u2010\nmic Decision-Making and a \u2018Right to Explanation\u2019 ,\u201d arXiv:1606.08813, August 31,\n2016.\n[90] \u201cA Review of the Data Broker Industry: Collection, Use, and Sale of Consumer\nData for Marketing Purposes,\u201d Staff Report, United States Senate Committee on Com\u2010\nmerce, Science, and Transportation, commerce.senate.gov, December 2013.\n[91] Olivia Solon: \u201c Facebook\u2019s Failure: Did Fake News and Polarized Politics Get\nTrump Elected?\u201d theguardian.com, November 10, 2016.\n[92] Donella H. Meadows and Diana Wright: Thinking in Systems: A Primer. Chelsea\nGreen Publishing, 2008. ISBN: 978-1-603-58055-7\n550 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29b07c41-2f88-478b-8082-927a1713397b": {"__data__": {"id_": "29b07c41-2f88-478b-8082-927a1713397b", "embedding": null, "metadata": {"page_label": "551", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7be73b89-e40e-4d0a-bcdf-76a5aa7b79cb", "node_type": "4", "metadata": {"page_label": "551", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a82a93877f8df9826a35f1847796b63339c3c32fb6ffaa095531e065b9f7c5cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[93] Daniel J. Bernstein: \u201c Listening to a \u2018big data\u2019/\u2018data science\u2019 talk ,\u201d twitter.com,\nMay 12, 2015.\n[94] Marc Andreessen: \u201cWhy Software Is Eating the World ,\u201d The Wall Street Journal,\n20 August 2011.\n[95] J. M. Porup: \u201c \u2018Internet of Things\u2019 Security Is Hilariously Broken and Getting\nWorse,\u201d arstechnica.com, January 23, 2016.\n[96] Bruce Schneier: Data and Goliath: The Hidden Battles to Collect Your Data and\nControl Your World. W. W. Norton, 2015. ISBN: 978-0-393-35217-7\n[97] The Grugq: \u201cNothing to Hide,\u201d grugq.tumblr.com, April 15, 2016.\n[98] Tony Beltramelli: \u201cDeep-Spying: Spying Using Smartwatch and Deep Learning ,\u201d\nMasters Thesis, IT University of Copenhagen, December 2015. Available at\narxiv.org/abs/1512.05616\n[99] Shoshana Zuboff: \u201c Big Other: Surveillance Capitalism and the Prospects of an\nInformation Civilization,\u201d Journal of Information Technology , volume 30, number 1,\npages 75\u201389, April 2015. doi:10.1057/jit.2015.5\n[100] Carina C. Zona: \u201c Consequences of an Insightful Algorithm ,\u201d at GOTO Berlin,\nNovember 2016.\n[101] Bruce Schneier: \u201c Data Is a Toxic Asset, So Why Not Throw It Out? ,\u201d schne\u2010\nier.com, March 1, 2016.\n[102] John E. Dunn: \u201c The UK\u2019s 15 Most Infamous Data Breaches ,\u201d techworld.com,\nNovember 18, 2016.\n[103] Cory Scott: \u201cData is not toxic - which implies no benefit - but rather hazardous\nmaterial, where we must balance need vs. want,\u201d twitter.com, March 6, 2016.\n[104] Bruce Schneier: \u201cMission Creep: When Everything Is Terrorism,\u201d schneier.com,\nJuly 16, 2013.\n[105] Lena Ulbricht and Maximilian von Grafenstein: \u201c Big Data: Big Power Shifts? ,\u201d\nInternet Policy Review, volume 5, number 1, March 2016. doi:10.14763/2016.1.406\n[106] Ellen P. Goodman and Julia Powles: \u201cFacebook and Google: Most Powerful and\nSecretive Empires We\u2019ve Ever Known,\u201d theguardian.com, September 28, 2016.\n[107] Directive 95/46/EC on the protection of individuals with regard to the process\u2010\ning of personal data and on the free movement of such data , Official Journal of the\nEuropean Communities No. L 281/31, eur-lex.europa.eu, November 1995.\n[108] Brendan Van Alsenoy: \u201cRegulating Data Protection: The Allocation of Respon\u2010\nsibility and Risk Among Actors Involved in Personal Data Processing ,\u201d Thesis, KU\nLeuven Centre for IT and IP Law, August 2016.\nSummary | 551", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79c924ef-ba3d-4498-b7e1-9423edddcf29": {"__data__": {"id_": "79c924ef-ba3d-4498-b7e1-9423edddcf29", "embedding": null, "metadata": {"page_label": "552", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02648872-446a-41e0-a655-eca95044163c", "node_type": "4", "metadata": {"page_label": "552", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d1b3599f606655188e0283da89831863bbafd4b6c06748bd8f9691fd4875c620", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[109] Michiel Rhoen: \u201c Beyond Consent: Improving Data Protection Through Con\u2010\nsumer Protection Law,\u201d Internet Policy Review, volume 5, number 1, March 2016. doi:\n10.14763/2016.1.404\n[110] Jessica Leber: \u201c Your Data Footprint Is Affecting Your Life in Ways You Can\u2019t\nEven Imagine,\u201d fastcoexist.com, March 15, 2016.\n[111] Maciej Ceg\u0142owski: \u201cHaunted by Data,\u201d idlewords.com, October 2015.\n[112] Sam Thielman: \u201c You Are Not What You Read: Librarians Purge User Data to\nProtect Privacy,\u201d theguardian.com, January 13, 2016.\n[113] Conor Friedersdorf: \u201c Edward Snowden\u2019s Other Motive for Leaking ,\u201d theatlan\u2010\ntic.com, May 13, 2014.\n[114] Phillip Rogaway: \u201c The Moral Character of Cryptographic Work ,\u201d Cryptology\nePrint 2015/1162, December 2015.\n552 | Chapter 12: The Future of Data Systems", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b0821c1-ad26-4d07-b0a2-0b75dd5800ab": {"__data__": {"id_": "6b0821c1-ad26-4d07-b0a2-0b75dd5800ab", "embedding": null, "metadata": {"page_label": "553", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87aa9734-c741-46ea-844c-5abe89d376b7", "node_type": "4", "metadata": {"page_label": "553", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "21d0c38f7e28487e9d7b19d78684fc31cb876c1a02b2186d229ff903a8a0fbbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Glossary\nPlease note that the definitions in this glossary are short and sim\u2010\nple, intended to convey the core idea but not the full subtleties of a\nterm. For more detail, please follow the references into the main\ntext.\nasynchronous\nNot waiting for something to complete\n(e.g., sending data over the network to\nanother node), and not making any\nassumptions about how long it is going to\ntake. See \u201cSynchronous Versus Asynchro\u2010\nnous Replication\u201d on page 153, \u201cSynchro\u2010\nnous Versus Asynchronous Networks\u201d on\npage 284, and \u201cSystem Model and Reality\u201d\non page 306.\natomic\n1. In the context of concurrent operations:\ndescribing an operation that appears to\ntake effect at a single point in time, so\nanother concurrent process can never\nencounter the operation in a \u201chalf-\nfinished\u201d state. See also isolation.\n2. In the context of transactions: grouping\ntogether a set of writes that must either all\nbe committed or all be rolled back, even if\nfaults occur. See \u201cAtomicity\u201d on page 223\nand \u201cAtomic Commit and Two-Phase\nCommit (2PC)\u201d on page 354.\nbackpressure\nForcing the sender of some data to slow\ndown because the recipient cannot keep\nup with it. Also known as flow control. See\n\u201cMessaging Systems\u201d on page 441.\nbatch process\nA computation that takes some fixed (and\nusually large) set of data as input and pro\u2010\nduces some other data as output, without\nmodifying the input. See Chapter 10.\nbounded\nHaving some known upper limit or size.\nUsed for example in the context of net\u2010\nwork delay (see \u201cTimeouts and Unboun\u2010\nded Delays\u201d on page 281) and datasets\n(see the introduction to Chapter 11).\nByzantine fault\nA node that behaves incorrectly in some\narbitrary way, for example by sending\ncontradictory or malicious messages to\nother nodes. See \u201cByzantine Faults\u201d on\npage 304.\ncache\nA component that remembers recently\nused data in order to speed up future\nreads of the same data. It is generally not\ncomplete: thus, if some data is missing\nfrom the cache, it has to be fetched from\nsome underlying, slower data storage\nGlossary | 553", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20552f36-f7b5-4058-8e0a-ba0045d371c7": {"__data__": {"id_": "20552f36-f7b5-4058-8e0a-ba0045d371c7", "embedding": null, "metadata": {"page_label": "554", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "703cc9b9-e2b0-48a5-95cc-1ea42d282b60", "node_type": "4", "metadata": {"page_label": "554", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "dfbb690d9bc07859b8c71600fc55b0924d0ef6f44a4257cbe7687e79948844e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "system that has a complete copy of the\ndata.\nCAP theorem\nA widely misunderstood theoretical result\nthat is not useful in practice. See \u201cThe\nCAP theorem\u201d on page 336.\ncausality\nThe dependency between events that ari\u2010\nses when one thing \u201chappens before\u201d\nanother thing in a system. For example, a\nlater event that is in response to an earlier\nevent, or builds upon an earlier event, or\nshould be understood in the light of an\nearlier event. See \u201cThe \u201chappens-before\u201d\nrelationship and concurrency\u201d on page\n186 and \u201cOrdering and Causality\u201d on page\n339.\nconsensus\nA fundamental problem in distributed\ncomputing, concerning getting several\nnodes to agree on something (for exam\u2010\nple, which node should be the leader for a\ndatabase cluster). The problem is much\nharder than it seems at first glance. See\n\u201cFault-Tolerant Consensus\u201d on page 364.\ndata warehouse\nA database in which data from several dif\u2010\nferent OLTP systems has been combined\nand prepared to be used for analytics pur\u2010\nposes. See \u201cData Warehousing\u201d on page\n91.\ndeclarative\nDescribing the properties that something\nshould have, but not the exact steps for\nhow to achieve it. In the context of quer\u2010\nies, a query optimizer takes a declarative\nquery and decides how it should best be\nexecuted. See \u201cQuery Languages for Data\u201d\non page 42.\ndenormalize\nTo introduce some amount of redun\u2010\ndancy or duplication in a normalized\ndataset, typically in the form of a cache or\nindex, in order to speed up reads. A\ndenormalized value is a kind of precom\u2010\nputed query result, similar to a material\u2010\nized view. See \u201cSingle-Object and Multi-\nObject Operations\u201d on page 228 and\n\u201cDeriving several views from the same\nevent log\u201d on page 461.\nderived data\nA dataset that is created from some other\ndata through a repeatable process, which\nyou could run again if necessary. Usually,\nderived data is needed to speed up a par\u2010\nticular kind of read access to the data.\nIndexes, caches, and materialized views\nare examples of derived data. See the\nintroduction to Part III.\ndeterministic\nDescribing a function that always pro\u2010\nduces the same output if you give it the\nsame input. This means it cannot depend\non random numbers, the time of day, net\u2010\nwork communication, or other unpredict\u2010\nable things.\ndistributed\nRunning on several nodes connected by a\nnetwork. Characterized by partial failures:\nsome part of the system may be broken\nwhile other parts are still working, and it\nis often impossible for the software to\nknow what exactly is broken. See \u201cFaults\nand Partial Failures\u201d on page 274.\ndurable\nStoring data in a way such that you\nbelieve it will not be lost, even if various\nfaults occur. See \u201cDurability\u201d on page 226.\nETL\nExtract\u2013Transform\u2013Load. The process of\nextracting data from a source database,\ntransforming it into a form that is more\nsuitable for analytic queries, and loading it\ninto a data warehouse or batch processing\nsystem. See \u201cData Warehousing\u201d on page\n91.\nfailover\nIn systems that have a single leader, fail\u2010\nover is the process of moving the leader\u2010\nship role from one node to another. See\n\u201cHandling Node Outages\u201d on page 156.\nCAP theorem\n554 | Glossary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6ac57a4-6f8f-4044-b86f-bd34617c1ba1": {"__data__": {"id_": "d6ac57a4-6f8f-4044-b86f-bd34617c1ba1", "embedding": null, "metadata": {"page_label": "555", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6935f477-88a3-44dd-98aa-f92408087518", "node_type": "4", "metadata": {"page_label": "555", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "93695ba4c499e953f94abeac5ba76f908ee993f3e5172dd40703171a41d7f1a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "fault-tolerant\nAble to recover automatically if some\u2010\nthing goes wrong (e.g., if a machine\ncrashes or a network link fails). See \u201cReli\u2010\nability\u201d on page 6.\nflow control\nSee backpressure.\nfollower\nA replica that does not directly accept any\nwrites from clients, but only processes\ndata changes that it receives from a leader.\nAlso known as a secondary, slave, read\nreplica, or hot standby . See \u201cLeaders and\nFollowers\u201d on page 152.\nfull-text search\nSearching text by arbitrary keywords,\noften with additional features such as\nmatching similarly spelled words or syno\u2010\nnyms. A full-text index is a kind of secon\u2010\ndary index that supports such queries. See\n\u201cFull-text search and fuzzy indexes\u201d on\npage 88.\ngraph\nA data structure consisting of vertices\n(things that you can refer to, also known\nas nodes or entities) and edges (connec\u2010\ntions from one vertex to another, also\nknown as relationships or arcs). See\n\u201cGraph-Like Data Models\u201d on page 49.\nhash\nA function that turns an input into a\nrandom-looking number. The same input\nalways returns the same number as out\u2010\nput. Two different inputs are very likely to\nhave two different numbers as output,\nalthough it is possible that two different\ninputs produce the same output (this is\ncalled a collision). See \u201cPartitioning by\nHash of Key\u201d on page 203.\nidempotent\nDescribing an operation that can be safely\nretried; if it is executed more than once, it\nhas the same effect as if it was only exe\u2010\ncuted once. See \u201cIdempotence\u201d on page\n478.\nindex\nA data structure that lets you efficiently\nsearch for all records that have a particu\u2010\nlar value in a particular field. See \u201cData\nStructures That Power Your Database\u201d  on\npage 70.\nisolation\nIn the context of transactions, describing\nthe degree to which concurrently execut\u2010\ning transactions can interfere with each\nother. Serializable isolation provides the\nstrongest guarantees, but weaker isolation\nlevels are also used. See \u201cIsolation\u201d on\npage 225.\njoin\nTo bring together records that have some\u2010\nthing in common. Most commonly used\nin the case where one record has a refer\u2010\nence to another (a foreign key, a docu\u2010\nment reference, an edge in a graph) and a\nquery needs to get the record that the ref\u2010\nerence points to. See \u201cMany-to-One and\nMany-to-Many Relationships\u201d on page 33\nand \u201cReduce-Side Joins and Grouping\u201d on\npage 403.\nleader\nWhen data or a service is replicated across\nseveral nodes, the leader is the designated\nreplica that is allowed to make changes. A\nleader may be elected through some pro\u2010\ntocol, or manually chosen by an adminis\u2010\ntrator. Also known as the primary or\nmaster. See \u201cLeaders and Followers\u201d on\npage 152.\nlinearizable\nBehaving as if there was only a single copy\nof data in the system, which is updated by\natomic operations. See \u201cLinearizability\u201d\non page 324.\nlocality\nA performance optimization: putting sev\u2010\neral pieces of data in the same place if they\nare frequently needed at the same time.\nSee \u201cData locality for queries\u201d on page 41.\nlocality\nGlossary | 555", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7a9e2b4-7d94-438f-962f-69543f8ab415": {"__data__": {"id_": "b7a9e2b4-7d94-438f-962f-69543f8ab415", "embedding": null, "metadata": {"page_label": "556", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3509afe1-f76a-4f0b-adfa-80cffbcd902c", "node_type": "4", "metadata": {"page_label": "556", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a825d86c7b28f3a74e1a5f84a9aa2902a79bf20c53cd003774dcce5873d1770c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "lock\nA mechanism to ensure that only one\nthread, node, or transaction can access\nsomething, and anyone else who wants to\naccess the same thing must wait until the\nlock is released. See \u201cTwo-Phase Locking\n(2PL)\u201d on page 257 and \u201cThe leader and\nthe lock\u201d on page 301.\nlog\nAn append-only file for storing data. A\nwrite-ahead log is used to make a storage\nengine resilient against crashes (see \u201cMak\u2010\ning B-trees reliable\u201d on page 82), a log-\nstructured storage engine uses logs as its\nprimary storage format (see \u201cSSTables\nand LSM-Trees\u201d on page 76), a replication\nlog is used to copy writes from a leader to\nfollowers (see \u201cLeaders and Followers\u201d on\npage 152), and an event log can represent\na data stream (see \u201cPartitioned Logs\u201d on\npage 446).\nmaterialize\nTo perform a computation eagerly and\nwrite out its result, as opposed to calculat\u2010\ning it on demand when requested. See\n\u201cAggregation: Data Cubes and Material\u2010\nized Views\u201d on page 101 and \u201cMaterializa\u2010\ntion of Intermediate State\u201d on page 419.\nnode\nAn instance of some software running on\na computer, which communicates with\nother nodes via a network in order to\naccomplish some task.\nnormalized\nStructured in such a way that there is no\nredundancy or duplication. In a normal\u2010\nized database, when some piece of data\nchanges, you only need to change it in one\nplace, not many copies in many different\nplaces. See \u201cMany-to-One and Many-to-\nMany Relationships\u201d on page 33.\nOLAP\nOnline analytic processing. Access pattern\ncharacterized by aggregating (e.g., count,\nsum, average) over a large number of\nrecords. See \u201cTransaction Processing or\nAnalytics?\u201d on page 90.\nOLTP\nOnline transaction processing. Access\npattern characterized by fast queries that\nread or write a small number of records,\nusually indexed by key. See \u201cTransaction\nProcessing or Analytics?\u201d on page 90.\npartitioning\nSplitting up a large dataset or computa\u2010\ntion that is too big for a single machine\ninto smaller parts and spreading them\nacross several machines. Also known as\nsharding. See Chapter 6.\npercentile\nA way of measuring the distribution of\nvalues by counting how many values are\nabove or below some threshold. For\nexample, the 95th percentile response\ntime during some period is the time t such\nthat 95% of requests in that period com\u2010\nplete in less than t, and 5% take longer\nthan t. See \u201cDescribing Performance\u201d on\npage 13.\nprimary key\nA value (typically a number or a string)\nthat uniquely identifies a record. In many\napplications, primary keys are generated\nby the system when a record is created\n(e.g., sequentially or randomly); they are\nnot usually set by users. See also secondary\nindex.\nquorum\nThe minimum number of nodes that need\nto vote on an operation before it can be\nconsidered successful. See \u201cQuorums for\nreading and writing\u201d on page 179.\nrebalance\nTo move data or services from one node\nto another in order to spread the load\nfairly. See \u201cRebalancing Partitions\u201d on\npage 209.\nreplication\nKeeping a copy of the same data on sev\u2010\neral nodes ( replicas) so that it remains\nlock\n556 | Glossary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a81e309-8882-4fa3-8116-4e30fcccda52": {"__data__": {"id_": "7a81e309-8882-4fa3-8116-4e30fcccda52", "embedding": null, "metadata": {"page_label": "557", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9718f708-f716-43c9-aef5-3d79f9a41216", "node_type": "4", "metadata": {"page_label": "557", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "41da5170b37cdfd230c531acbca20495656f1cc7d7c0d971b41ec0e0379eda8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "accessible if a node becomes unreachable.\nSee Chapter 5.\nschema\nA description of the structure of some\ndata, including its fields and datatypes.\nWhether some data conforms to a schema\ncan be checked at various points in the\ndata\u2019s lifetime (see \u201cSchema flexibility in\nthe document model\u201d on page 39), and a\nschema can change over time (see Chap\u2010\nter 4).\nsecondary index\nAn additional data structure that is main\u2010\ntained alongside the primary data storage\nand which allows you to efficiently search\nfor records that match a certain kind of\ncondition. See \u201cOther Indexing Struc\u2010\ntures\u201d on page 85 and \u201cPartitioning and\nSecondary Indexes\u201d on page 206.\nserializable\nA guarantee that if several transactions\nexecute concurrently, they behave the\nsame as if they had executed one at a time,\nin some serial order. See \u201cSerializability\u201d\non page 251.\nshared-nothing\nAn architecture in which independent\nnodes\u2014each with their own CPUs, mem\u2010\nory, and disks\u2014are connected via a con\u2010\nventional network, in contrast to shared-\nmemory or shared-disk architectures. See\nthe introduction to Part II.\nskew\n1. Imbalanced load across partitions, such\nthat some partitions have lots of requests\nor data, and others have much less. Also\nknown as hot spots . See \u201cSkewed Work\u2010\nloads and Relieving Hot Spots\u201d on page\n205 and \u201cHandling skew\u201d on page 407.\n2. A timing anomaly that causes events to\nappear in an unexpected, nonsequential\norder. See the discussions of read skew in\n\u201cSnapshot Isolation and Repeatable Read\u201d\non page 237, write skew  in \u201cWrite Skew\nand Phantoms\u201d on page 246, and clock\nskew in \u201cTimestamps for ordering events\u201d\non page 291.\nsplit brain\nA scenario in which two nodes simultane\u2010\nously believe themselves to be the leader,\nand which may cause system guarantees\nto be violated. See \u201cHandling Node Out\u2010\nages\u201d on page 156 and \u201cThe Truth Is\nDefined by the Majority\u201d on page 300.\nstored procedure\nA way of encoding the logic of a transac\u2010\ntion such that it can be entirely executed\non a database server, without communi\u2010\ncating back and forth with a client during\nthe transaction. See \u201cActual Serial Execu\u2010\ntion\u201d on page 252.\nstream process\nA continually running computation that\nconsumes a never-ending stream of events\nas input, and derives some output from it.\nSee Chapter 11.\nsynchronous\nThe opposite of asynchronous.\nsystem of record\nA system that holds the primary, authori\u2010\ntative version of some data, also known as\nthe source of truth. Changes are first writ\u2010\nten here, and other datasets may be\nderived from the system of record. See the\nintroduction to Part III.\ntimeout\nOne of the simplest ways of detecting a\nfault, namely by observing the lack of a\nresponse within some amount of time.\nHowever, it is impossible to know\nwhether a timeout is due to a problem\nwith the remote node, or an issue in the\nnetwork. See \u201cTimeouts and Unbounded\nDelays\u201d on page 281.\ntotal order\nA way of comparing things (e.g., time\u2010\nstamps) that allows you to always say\nwhich one of two things is greater and\nwhich one is lesser. An ordering in which\ntotal order\nGlossary | 557", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3061, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c426982-35e8-4e5a-b4fc-a57f032d2b34": {"__data__": {"id_": "0c426982-35e8-4e5a-b4fc-a57f032d2b34", "embedding": null, "metadata": {"page_label": "558", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d8940cf-4a56-40c8-a14d-65b8e7a452c6", "node_type": "4", "metadata": {"page_label": "558", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "1ecc94716c166527666a801d697eb12b62cbbf570929092c49eca4b978254219", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "some things are incomparable (you can\u2010\nnot say which is greater or smaller) is\ncalled a partial order . See \u201cThe causal\norder is not a total order\u201d on page 341.\ntransaction\nGrouping together several reads and\nwrites into a logical unit, in order to sim\u2010\nplify error handling and concurrency\nissues. See Chapter 7.\ntwo-phase commit (2PC)\nAn algorithm to ensure that several data\u2010\nbase nodes either all commit or all abort a\ntransaction. See \u201cAtomic Commit and\nTwo-Phase Commit (2PC)\u201d on page 354.\ntwo-phase locking (2PL)\nAn algorithm for achieving serializable\nisolation that works by a transaction\nacquiring a lock on all data it reads or\nwrites, and holding the lock until the end\nof the transaction. See \u201cTwo-Phase Lock\u2010\ning (2PL)\u201d on page 257.\nunbounded\nNot having any known upper limit or size.\nThe opposite of bounded.\ntransaction\n558 | Glossary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e11c3d95-5be7-4553-89e4-df6aae1257c5": {"__data__": {"id_": "e11c3d95-5be7-4553-89e4-df6aae1257c5", "embedding": null, "metadata": {"page_label": "559", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18692ed0-051a-40f8-9c9d-5b4f5776e51d", "node_type": "4", "metadata": {"page_label": "559", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ff3fc03782e1b0d5229d70ce06633acf1eefdb2225be2c0d272c4a7767940f30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Index\nA\naborts (transactions), 222, 224\nin two-phase commit, 356\nperformance of optimistic concurrency con\u2010\ntrol, 266\nretrying aborted transactions, 231\nabstraction, 21, 27, 222, 266, 321\naccess path (in network model), 37, 60\naccidental complexity, removing, 21\naccountability, 535\nACID properties (transactions), 90, 223\natomicity, 223, 228\nconsistency, 224, 529\ndurability, 226\nisolation, 225, 228\nacknowledgements (messaging), 445\nactive/active replication (see multi-leader repli\u2010\ncation)\nactive/passive replication (see leader-based rep\u2010\nlication)\nActiveMQ (messaging), 137, 444\ndistributed transaction support, 361\nActiveRecord (object-relational mapper), 30,\n232\nactor model, 138\n(see also message-passing)\ncomparison to Pregel model, 425\ncomparison to stream processing, 468\nAdvanced Message Queuing Protocol (see\nAMQP)\naerospace systems, 6, 10, 305, 372\naggregation\ndata cubes and materialized views, 101\nin batch processes, 406\nin stream processes, 466\naggregation pipeline query language, 48\nAgile, 22\nminimizing irreversibility, 414, 497\nmoving faster with confidence, 532\nUnix philosophy, 394\nagreement, 365\n(see also consensus)\nAirflow (workflow scheduler), 402\nAjax, 131\nAkka (actor framework), 139\nalgorithms\nalgorithm correctness, 308\nB-trees, 79-83\nfor distributed systems, 306\nhash indexes, 72-75\nmergesort, 76, 402, 405\nred-black trees, 78\nSSTables and LSM-trees, 76-79\nall-to-all replication topologies, 175\nAllegroGraph (database), 50\nALTER TABLE statement (SQL), 40, 111\nAmazon\nDynamo (database), 177\nAmazon Web Services (AWS), 8\nKinesis Streams (messaging), 448\nnetwork reliability, 279\npostmortems, 9\nRedShift (database), 93\nS3 (object storage), 398\nchecking data integrity, 530\namplification\nof bias, 534\nof failures, 364, 495\nIndex | 559", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63361d68-0270-4fc7-a375-1b33379c6b44": {"__data__": {"id_": "63361d68-0270-4fc7-a375-1b33379c6b44", "embedding": null, "metadata": {"page_label": "560", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c93bc452-1746-464d-a1d6-1aca61539cf8", "node_type": "4", "metadata": {"page_label": "560", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5851e561ff6b9ebb4d368c13f526163af5f424df3abdce90e3e48dffc57382ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of tail latency, 16, 207\nwrite amplification, 84\nAMQP (Advanced Message Queuing Protocol),\n444\n(see also messaging systems)\ncomparison to log-based messaging, 448,\n451\nmessage ordering, 446\nanalytics, 90\ncomparison to transaction processing, 91\ndata warehousing (see data warehousing)\nparallel query execution in MPP databases,\n415\npredictive (see predictive analytics)\nrelation to batch processing, 411\nschemas for, 93-95\nsnapshot isolation for queries, 238\nstream analytics, 466\nusing MapReduce, analysis of user activity\nevents (example), 404\nanti-caching (in-memory databases), 89\nanti-entropy, 178\nApache ActiveMQ (see ActiveMQ)\nApache Avro (see Avro)\nApache Beam (see Beam)\nApache BookKeeper (see BookKeeper)\nApache Cassandra (see Cassandra)\nApache CouchDB (see CouchDB)\nApache Curator (see Curator)\nApache Drill (see Drill)\nApache Flink (see Flink)\nApache Giraph (see Giraph)\nApache Hadoop (see Hadoop)\nApache HAWQ (see HAWQ)\nApache HBase (see HBase)\nApache Helix (see Helix)\nApache Hive (see Hive)\nApache Impala (see Impala)\nApache Jena (see Jena)\nApache Kafka (see Kafka)\nApache Lucene (see Lucene)\nApache MADlib (see MADlib)\nApache Mahout (see Mahout)\nApache Oozie (see Oozie)\nApache Parquet (see Parquet)\nApache Qpid (see Qpid)\nApache Samza (see Samza)\nApache Solr (see Solr)\nApache Spark (see Spark)\nApache Storm (see Storm)\nApache Tajo (see Tajo)\nApache Tez (see Tez)\nApache Thrift (see Thrift)\nApache ZooKeeper (see ZooKeeper)\nApama (stream analytics), 466\nappend-only B-trees, 82, 242\nappend-only files (see logs)\nApplication Programming Interfaces (APIs), 5,\n27\nfor batch processing, 403\nfor change streams, 456\nfor distributed transactions, 361\nfor graph processing, 425\nfor services, 131-136\n(see also services)\nevolvability, 136\nRESTful, 133\nSOAP, 133\napplication state (see state)\napproximate search (see similarity search)\narchival storage, data from databases, 131\narcs (see edges)\narithmetic mean, 14\nASCII text, 119, 395\nASN.1 (schema language), 127\nasynchronous networks, 278, 553\ncomparison to synchronous networks, 284\nformal model, 307\nasynchronous replication, 154, 553\nconflict detection, 172\ndata loss on failover, 157\nreads from asynchronous follower, 162\nAsynchronous Transfer Mode (ATM), 285\natomic broadcast (see total order broadcast)\natomic clocks (caesium clocks), 294, 295\n(see also clocks)\natomicity (concurrency), 553\natomic increment-and-get, 351\ncompare-and-set, 245, 327\n(see also compare-and-set operations)\nreplicated operations, 246\nwrite operations, 243\natomicity (transactions), 223, 228, 553\natomic commit, 353\navoiding, 523, 528\nblocking and nonblocking, 359\nin stream processing, 360, 477\nmaintaining derived data, 453\n560 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "447de163-9c35-42be-93c3-c6ca5ef9f0f4": {"__data__": {"id_": "447de163-9c35-42be-93c3-c6ca5ef9f0f4", "embedding": null, "metadata": {"page_label": "561", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "891e1a39-3797-4686-aac2-2324a61fac11", "node_type": "4", "metadata": {"page_label": "561", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0ca4a5ea3d3ed965c3963a1fd49cb099b885c66162ed7aa84e75f0edf14666b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for multi-object transactions, 229\nfor single-object writes, 230\nauditability, 528-533\ndesigning for, 531\nself-auditing systems, 530\nthrough immutability, 460\ntools for auditable data systems, 532\navailability, 8\n(see also fault tolerance)\nin CAP theorem, 337\nin service level agreements (SLAs), 15\nAvro (data format), 122-127\ncode generation, 127\ndynamically generated schemas, 126\nobject container files, 125, 131, 414\nreader determining writer\u2019s schema, 125\nschema evolution, 123\nuse in Hadoop, 414\nawk (Unix tool), 391\nAWS (see Amazon Web Services)\nAzure (see Microsoft)\nB\nB-trees (indexes), 79-83\nappend-only/copy-on-write variants, 82,\n242\nbranching factor, 81\ncomparison to LSM-trees, 83-85\ncrash recovery, 82\ngrowing by splitting a page, 81\noptimizations, 82\nsimilarity to dynamic partitioning, 212\nbackpressure, 441, 553\nin TCP, 282\nbackups\ndatabase snapshot for replication, 156\nintegrity of, 530\nsnapshot isolation for, 238\nuse for ETL processes, 405\nbackward compatibility, 112\nBASE, contrast to ACID, 223\nbash shell (Unix), 70, 395, 503\nbatch processing, 28, 389-431, 553\ncombining with stream processing\nlambda architecture, 497\nunifying technologies, 498\ncomparison to MPP databases, 414-418\ncomparison to stream processing, 464\ncomparison to Unix, 413-414\ndataflow engines, 421-423\nfault tolerance, 406, 414, 422, 442\nfor data integration, 494-498\ngraphs and iterative processing, 424-426\nhigh-level APIs and languages, 403, 426-429\nlog-based messaging and, 451\nmaintaining derived state, 495\nMapReduce and distributed filesystems,\n397-413\n(see also MapReduce)\nmeasuring performance, 13, 390\noutputs, 411-413\nkey-value stores, 412\nsearch indexes, 411\nusing Unix tools (example), 391-394\nBayou (database), 522\nBeam (dataflow library), 498\nbias, 534\nbig ball of mud, 20\nBigtable data model, 41, 99\nbinary data encodings, 115-128\nAvro, 122-127\nMessagePack, 116-117\nThrift and Protocol Buffers, 117-121\nbinary encoding\nbased on schemas, 127\nby network drivers, 128\nbinary strings, lack of support in JSON and\nXML, 114\nBinaryProtocol encoding (Thrift), 118\nBitcask (storage engine), 72\ncrash recovery, 74\nBitcoin (cryptocurrency), 532\nByzantine fault tolerance, 305\nconcurrency bugs in exchanges, 233\nbitmap indexes, 97\nblockchains, 532\nByzantine fault tolerance, 305\nblocking atomic commit, 359\nBloom (programming language), 504\nBloom filter (algorithm), 79, 466\nBookKeeper (replicated log), 372\nBottled Water (change data capture), 455\nbounded datasets, 430, 439, 553\n(see also batch processing)\nbounded delays, 553\nin networks, 285\nprocess pauses, 298\nbroadcast hash joins, 409\nIndex | 561", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "caf2b763-2722-4f06-a202-dc42883dbb8b": {"__data__": {"id_": "caf2b763-2722-4f06-a202-dc42883dbb8b", "embedding": null, "metadata": {"page_label": "562", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3e1383c-3df4-4f5a-851c-fc3f75c8497e", "node_type": "4", "metadata": {"page_label": "562", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "2491e9f3cfe6f054f5ad47be19e9e0e61a6b70ec720b6d022996d4d52cd583a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "brokerless messaging, 442\nBrubeck (metrics aggregator), 442\nBTM (transaction coordinator), 356\nbulk synchronous parallel (BSP) model, 425\nbursty network traffic patterns, 285\nbusiness data processing, 28, 90, 390\nbyte sequence, encoding data in, 112\nByzantine faults, 304-306, 307, 553\nByzantine fault-tolerant systems, 305, 532\nByzantine Generals Problem, 304\nconsensus algorithms and, 366\nC\ncaches, 89, 553\nand materialized views, 101\nas derived data, 386, 499-504\ndatabase as cache of transaction log, 460\nin CPUs, 99, 338, 428\ninvalidation and maintenance, 452, 467\nlinearizability, 324\nCAP theorem, 336-338, 554\nCascading (batch processing), 419, 427\nhash joins, 409\nworkflows, 403\ncascading failures, 9, 214, 281\nCascalog (batch processing), 60\nCassandra (database)\ncolumn-family data model, 41, 99\ncompaction strategy, 79\ncompound primary key, 204\ngossip protocol, 216\nhash partitioning, 203-205\nlast-write-wins conflict resolution, 186, 292\nleaderless replication, 177\nlinearizability, lack of, 335\nlog-structured storage, 78\nmulti-datacenter support, 184\npartitioning scheme, 213\nsecondary indexes, 207\nsloppy quorums, 184\ncat (Unix tool), 391\ncausal context, 191\n(see also causal dependencies)\ncausal dependencies, 186-191\ncapturing, 191, 342, 494, 514\nby total ordering, 493\ncausal ordering, 339\nin transactions, 262\nsending message to friends (example), 494\ncausality, 554\ncausal ordering, 339-343\nlinearizability and, 342\ntotal order consistent with, 344, 345\nconsistency with, 344-347\nconsistent snapshots, 340\nhappens-before relationship, 186\nin serializable transactions, 262-265\nmismatch with clocks, 292\nordering events to capture, 493\nviolations of, 165, 176, 292, 340\nwith synchronized clocks, 294\nCEP (see complex event processing)\ncertificate transparency, 532\nchain replication, 155\nlinearizable reads, 351\nchange data capture, 160, 454\nAPI support for change streams, 456\ncomparison to event sourcing, 457\nimplementing, 454\ninitial snapshot, 455\nlog compaction, 456\nchangelogs, 460\nchange data capture, 454\nfor operator state, 479\ngenerating with triggers, 455\nin stream joins, 474\nlog compaction, 456\nmaintaining derived state, 452\nChaos Monkey, 7, 280\ncheckpointing\nin batch processors, 422, 426\nin high-performance computing, 275\nin stream processors, 477, 523\nchronicle data model, 458\ncircuit-switched networks, 284\ncircular buffers, 450\ncircular replication topologies, 175\nclickstream data, analysis of, 404\nclients\ncalling services, 131\npushing state changes to, 512\nrequest routing, 214\nstateful and offline-capable, 170, 511\nclocks, 287-299\natomic (caesium) clocks, 294, 295\nconfidence interval, 293-295\nfor global snapshots, 294\nlogical (see logical clocks)\n562 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5417d6fd-3a96-4069-b649-74028b7231ca": {"__data__": {"id_": "5417d6fd-3a96-4069-b649-74028b7231ca", "embedding": null, "metadata": {"page_label": "563", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "506e2a77-43ad-4b8c-aeaa-5b97f719292e", "node_type": "4", "metadata": {"page_label": "563", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7779fc54e78dcb27eb1331f061456519027e8ea5f20360a93ebe8d00288b0a71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "skew, 291-294, 334\nslewing, 289\nsynchronization and accuracy, 289-291\nsynchronization using GPS, 287, 290, 294,\n295\ntime-of-day versus monotonic clocks, 288\ntimestamping events, 471\ncloud computing, 146, 275\nneed for service discovery, 372\nnetwork glitches, 279\nshared resources, 284\nsingle-machine reliability, 8\nCloudera Impala (see Impala)\nclustered indexes, 86\nCODASYL model, 36\n(see also network model)\ncode generation\nwith Avro, 127\nwith Thrift and Protocol Buffers, 118\nwith WSDL, 133\ncollaborative editing\nmulti-leader replication and, 170\ncolumn families (Bigtable), 41, 99\ncolumn-oriented storage, 95-101\ncolumn compression, 97\ndistinction between column families and, 99\nin batch processors, 428\nParquet, 96, 131, 414\nsort order in, 99-100\nvectorized processing, 99, 428\nwriting to, 101\ncomma-separated values (see CSV)\ncommand query responsibility segregation\n(CQRS), 462\ncommands (event sourcing), 459\ncommits (transactions), 222\natomic commit, 354-355\n(see also atomicity; transactions)\nread committed isolation, 234\nthree-phase commit (3PC), 359\ntwo-phase commit (2PC), 355-359\ncommutative operations, 246\ncompaction\nof changelogs, 456\n(see also log compaction)\nfor stream operator state, 479\nof log-structured storage, 73\nissues with, 84\nsize-tiered and leveled approaches, 79\nCompactProtocol encoding (Thrift), 119\ncompare-and-set operations, 245, 327\nimplementing locks, 370\nimplementing uniqueness constraints, 331\nimplementing with total order broadcast,\n350\nrelation to consensus, 335, 350, 352, 374\nrelation to transactions, 230\ncompatibility, 112, 128\ncalling services, 136\nproperties of encoding formats, 139\nusing databases, 129-131\nusing message-passing, 138\ncompensating transactions, 355, 461, 526\ncomplex event processing (CEP), 465\ncomplexity\ndistilling in theoretical models, 310\nhiding using abstraction, 27\nof software systems, managing, 20\ncomposing data systems (see unbundling data\u2010\nbases)\ncompute-intensive applications, 3, 275\nconcatenated indexes, 87\nin Cassandra, 204\nConcord (stream processor), 466\nconcurrency\nactor programming model, 138, 468\n(see also message-passing)\nbugs from weak transaction isolation, 233\nconflict resolution, 171, 174\ndetecting concurrent writes, 184-191\ndual writes, problems with, 453\nhappens-before relationship, 186\nin replicated systems, 161-191, 324-338\nlost updates, 243\nmulti-version concurrency control\n(MVCC), 239\noptimistic concurrency control, 261\nordering of operations, 326, 341\nreducing, through event logs, 351, 462, 507\ntime and relativity, 187\ntransaction isolation, 225\nwrite skew (transaction isolation), 246-251\nconflict-free replicated datatypes (CRDTs), 174\nconflicts\nconflict detection, 172\ncausal dependencies, 186, 342\nin consensus algorithms, 368\nin leaderless replication, 184\nIndex | 563", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bbb5831-61f8-4342-80c3-08839d713697": {"__data__": {"id_": "5bbb5831-61f8-4342-80c3-08839d713697", "embedding": null, "metadata": {"page_label": "564", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d33cdea7-d2d2-4530-95b1-a6c2bddeb32b", "node_type": "4", "metadata": {"page_label": "564", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "16b6245a311c320b072128bc8dd2992681da3de25f2c6e40c656883c264bae28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in log-based systems, 351, 521\nin nonlinearizable systems, 343\nin serializable snapshot isolation (SSI),\n264\nin two-phase commit, 357, 364\nconflict resolution\nautomatic conflict resolution, 174\nby aborting transactions, 261\nby apologizing, 527\nconvergence, 172-174\nin leaderless systems, 190\nlast write wins (LWW), 186, 292\nusing atomic operations, 246\nusing custom logic, 173\ndetermining what is a conflict, 174, 522\nin multi-leader replication, 171-175\navoiding conflicts, 172\nlost updates, 242-246\nmaterializing, 251\nrelation to operation ordering, 339\nwrite skew (transaction isolation), 246-251\ncongestion (networks)\navoidance, 282\nlimiting accuracy of clocks, 293\nqueueing delays, 282\nconsensus, 321, 364-375, 554\nalgorithms, 366-368\npreventing split brain, 367\nsafety and liveness properties, 365\nusing linearizable operations, 351\ncost of, 369\ndistributed transactions, 352-375\nin practice, 360-364\ntwo-phase commit, 354-359\nXA transactions, 361-364\nimpossibility of, 353\nmembership and coordination services,\n370-373\nrelation to compare-and-set, 335, 350, 352,\n374\nrelation to replication, 155, 349\nrelation to uniqueness constraints, 521\nconsistency, 224, 524\nacross different databases, 157, 452, 462, 492\ncausal, 339-348, 493\nconsistent prefix reads, 165-167\nconsistent snapshots, 156, 237-242, 294,\n455, 500\n(see also snapshots)\ncrash recovery, 82\nenforcing constraints (see constraints)\neventual, 162, 322\n(see also eventual consistency)\nin ACID transactions, 224, 529\nin CAP theorem, 337\nlinearizability, 324-338\nmeanings of, 224\nmonotonic reads, 164-165\nof secondary indexes, 231, 241, 354, 491,\n500\nordering guarantees, 339-352\nread-after-write, 162-164\nsequential, 351\nstrong (see linearizability)\ntimeliness and integrity, 524\nusing quorums, 181, 334\nconsistent hashing, 204\nconsistent prefix reads, 165\nconstraints (databases), 225, 248\nasynchronously checked, 526\ncoordination avoidance, 527\nensuring idempotence, 519\nin log-based systems, 521-524\nacross multiple partitions, 522\nin two-phase commit, 355, 357\nrelation to consensus, 374, 521\nrelation to event ordering, 347\nrequiring linearizability, 330\nConsul (service discovery), 372\nconsumers (message streams), 137, 440\nbackpressure, 441\nconsumer offsets in logs, 449\nfailures, 445, 449\nfan-out, 11, 445, 448\nload balancing, 444, 448\nnot keeping up with producers, 441, 450,\n502\ncontext switches, 14, 297\nconvergence (conflict resolution), 172-174, 322\ncoordination\navoidance, 527\ncross-datacenter, 168, 493\ncross-partition ordering, 256, 294, 348, 523\nservices, 330, 370-373\ncoordinator (in 2PC), 356\nfailure, 358\nin XA transactions, 361-364\nrecovery, 363\n564 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49d5413a-8404-4664-b614-78e0837cb644": {"__data__": {"id_": "49d5413a-8404-4664-b614-78e0837cb644", "embedding": null, "metadata": {"page_label": "565", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e16615fa-db1b-4c60-9dfd-398cb857304f", "node_type": "4", "metadata": {"page_label": "565", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0ebebba941d51a936967e1516ec0d0b087f4f0e60c4bced6651a3987974709a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "copy-on-write (B-trees), 82, 242\nCORBA (Common Object Request Broker\nArchitecture), 134\ncorrectness, 6\nauditability, 528-533\nByzantine fault tolerance, 305, 532\ndealing with partial failures, 274\nin log-based systems, 521-524\nof algorithm within system model, 308\nof compensating transactions, 355\nof consensus, 368\nof derived data, 497, 531\nof immutable data, 461\nof personal data, 535, 540\nof time, 176, 289-295\nof transactions, 225, 515, 529\ntimeliness and integrity, 524-528\ncorruption of data\ndetecting, 519, 530-533\ndue to pathological memory access, 529\ndue to radiation, 305\ndue to split brain, 158, 302\ndue to weak transaction isolation, 233\nformalization in consensus, 366\nintegrity as absence of, 524\nnetwork packets, 306\non disks, 227\npreventing using write-ahead logs, 82\nrecovering from, 414, 460\nCouchbase (database)\ndurability, 89\nhash partitioning, 203-204, 211\nrebalancing, 213\nrequest routing, 216\nCouchDB (database)\nB-tree storage, 242\nchange feed, 456\ndocument data model, 31\njoin support, 34\nMapReduce support, 46, 400\nreplication, 170, 173\ncovering indexes, 86\nCPUs\ncache coherence and memory barriers, 338\ncaching and pipelining, 99, 428\nincreasing parallelism, 43\nCRDTs (see conflict-free replicated datatypes)\nCREATE INDEX statement (SQL), 85, 500\ncredit rating agencies, 535\nCrunch (batch processing), 419, 427\nhash joins, 409\nsharded joins, 408\nworkflows, 403\ncryptography\ndefense against attackers, 306\nend-to-end encryption and authentication,\n519, 543\nproving integrity of data, 532\nCSS (Cascading Style Sheets), 44\nCSV (comma-separated values), 70, 114, 396\nCurator (ZooKeeper recipes), 330, 371\ncurl (Unix tool), 135, 397\ncursor stability, 243\nCypher (query language), 52\ncomparison to SPARQL, 59\nD\ndata corruption (see corruption of data)\ndata cubes, 102\ndata formats (see encoding)\ndata integration, 490-498, 543\nbatch and stream processing, 494-498\nlambda architecture, 497\nmaintaining derived state, 495\nreprocessing data, 496\nunifying, 498\nby unbundling databases, 499-515\ncomparison to federated databases, 501\ncombining tools by deriving data, 490-494\nderived data versus distributed transac\u2010\ntions, 492\nlimits of total ordering, 493\nordering events to capture causality, 493\nreasoning about dataflows, 491\nneed for, 385\ndata lakes, 415\ndata locality (see locality)\ndata models, 27-64\ngraph-like models, 49-63\nDatalog language, 60-63\nproperty graphs, 50\nRDF and triple-stores, 55-59\nquery languages, 42-48\nrelational model versus document model,\n28-42\ndata protection regulations, 542\ndata systems, 3\nabout, 4\nIndex | 565", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ce3a23c-aa96-46ec-bf3e-ced02a058149": {"__data__": {"id_": "8ce3a23c-aa96-46ec-bf3e-ced02a058149", "embedding": null, "metadata": {"page_label": "566", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e74c6fc9-111f-44b9-a5c1-040c52410f6c", "node_type": "4", "metadata": {"page_label": "566", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a00b35fbb8bb110d1a8260661f4f57d0e627455e408db0ba368a6b046d402595", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "concerns when designing, 5\nfuture of, 489-544\ncorrectness, constraints, and integrity,\n515-533\ndata integration, 490-498\nunbundling databases, 499-515\nheterogeneous, keeping in sync, 452\nmaintainability, 18-22\npossible faults in, 221\nreliability, 6-10\nhardware faults, 7\nhuman errors, 9\nimportance of, 10\nsoftware errors, 8\nscalability, 10-18\nunreliable clocks, 287-299\ndata warehousing, 91-95, 554\ncomparison to data lakes, 415\nETL (extract-transform-load), 92, 416, 452\nkeeping data systems in sync, 452\nschema design, 93\nslowly changing dimension (SCD), 476\ndata-intensive applications, 3\ndatabase triggers (see triggers)\ndatabase-internal distributed transactions, 360,\n364, 477\ndatabases\narchival storage, 131\ncomparison of message brokers to, 443\ndataflow through, 129\nend-to-end argument for, 519-520\nchecking integrity, 531\ninside-out, 504\n(see also unbundling databases)\noutput from batch workflows, 412\nrelation to event streams, 451-464\n(see also changelogs)\nAPI support for change streams, 456,\n506\nchange data capture, 454-457\nevent sourcing, 457-459\nkeeping systems in sync, 452-453\nphilosophy of immutable events,\n459-464\nunbundling, 499-515\ncomposing data storage technologies,\n499-504\ndesigning applications around dataflow,\n504-509\nobserving derived state, 509-515\ndatacenters\ngeographically distributed, 145, 164, 278,\n493\nmulti-tenancy and shared resources, 284\nnetwork architecture, 276\nnetwork faults, 279\nreplication across multiple, 169\nleaderless replication, 184\nmulti-leader replication, 168, 335\ndataflow, 128-139, 504-509\ncorrectness of dataflow systems, 525\ndifferential, 504\nmessage-passing, 136-139\nreasoning about, 491\nthrough databases, 129\nthrough services, 131-136\ndataflow engines, 421-423\ncomparison to stream processing, 464\ndirected acyclic graphs (DAG), 424\npartitioning, approach to, 429\nsupport for declarative queries, 427\nDatalog (query language), 60-63\ndatatypes\nbinary strings in XML and JSON, 114\nconflict-free, 174\nin Avro encodings, 122\nin Thrift and Protocol Buffers, 121\nnumbers in XML and JSON, 114\nDatomic (database)\nB-tree storage, 242\ndata model, 50, 57\nDatalog query language, 60\nexcision (deleting data), 463\nlanguages for transactions, 255\nserial execution of transactions, 253\ndeadlocks\ndetection, in two-phase commit (2PC), 364\nin two-phase locking (2PL), 258\nDebezium (change data capture), 455\ndeclarative languages, 42, 554\nBloom, 504\nCSS and XSL, 44\nCypher, 52\nDatalog, 60\nfor batch processing, 427\nrecursive SQL queries, 53\nrelational algebra and SQL, 42\nSPARQL, 59\n566 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2543, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90f6b2b6-a419-414c-8145-6f27d67a3aef": {"__data__": {"id_": "90f6b2b6-a419-414c-8145-6f27d67a3aef", "embedding": null, "metadata": {"page_label": "567", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d93faed3-ba2e-4915-8026-8365d2d14423", "node_type": "4", "metadata": {"page_label": "567", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c0a7bb2b499e087d3f1a35d60e83f455258955b9cad18b6b0e5c16a40fba21ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "delays\nbounded network delays, 285\nbounded process pauses, 298\nunbounded network delays, 282\nunbounded process pauses, 296\ndeleting data, 463\ndenormalization (data representation), 34, 554\ncosts, 39\nin derived data systems, 386\nmaterialized views, 101\nupdating derived data, 228, 231, 490\nversus normalization, 462\nderived data, 386, 439, 554\nfrom change data capture, 454\nin event sourcing, 458-458\nmaintaining derived state through logs,\n452-457, 459-463\nobserving, by subscribing to streams, 512\noutputs of batch and stream processing, 495\nthrough application code, 505\nversus distributed transactions, 492\ndeterministic operations, 255, 274, 554\naccidental nondeterminism, 423\nand fault tolerance, 423, 426\nand idempotence, 478, 492\ncomputing derived data, 495, 526, 531\nin state machine replication, 349, 452, 458\njoins, 476\nDevOps, 394\ndifferential dataflow, 504\ndimension tables, 94\ndimensional modeling (see star schemas)\ndirected acyclic graphs (DAGs), 424\ndirty reads (transaction isolation), 234\ndirty writes (transaction isolation), 235\ndiscrimination, 534\ndisks (see hard disks)\ndistributed actor frameworks, 138\ndistributed filesystems, 398-399\ndecoupling from query engines, 417\nindiscriminately dumping data into, 415\nuse by MapReduce, 402\ndistributed systems, 273-312, 554\nByzantine faults, 304-306\ncloud versus supercomputing, 275\ndetecting network faults, 280\nfaults and partial failures, 274-277\nformalization of consensus, 365\nimpossibility results, 338, 353\nissues with failover, 157\nlimitations of distributed transactions, 363\nmulti-datacenter, 169, 335\nnetwork problems, 277-286\nquorums, relying on, 301\nreasons for using, 145, 151\nsynchronized clocks, relying on, 291-295\nsystem models, 306-310\nuse of clocks and time, 287\ndistributed transactions (see transactions)\nDjango (web framework), 232\nDNS (Domain Name System), 216, 372\nDocker (container manager), 506\ndocument data model, 30-42\ncomparison to relational model, 38-42\ndocument references, 38, 403\ndocument-oriented databases, 31\nmany-to-many relationships and joins, 36\nmulti-object transactions, need for, 231\nversus relational model\nconvergence of models, 41\ndata locality, 41\ndocument-partitioned indexes, 206, 217, 411\ndomain-driven design (DDD), 457\nDRBD (Distributed Replicated Block Device),\n153\ndrift (clocks), 289\nDrill (query engine), 93\nDruid (database), 461\nDryad (dataflow engine), 421\ndual writes, problems with, 452, 507\nduplicates, suppression of, 517\n(see also idempotence)\nusing a unique ID, 518, 522\ndurability (transactions), 226, 554\nduration (time), 287\nmeasurement with monotonic clocks, 288\ndynamic partitioning, 212\ndynamically typed languages\nanalogy to schema-on-read, 40\ncode generation and, 127\nDynamo-style databases (see leaderless replica\u2010\ntion)\nE\nedges (in graphs), 49, 403\nproperty graph model, 50\nedit distance (full-text search), 88\neffectively-once semantics, 476, 516\nIndex | 567", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2902, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9091752-0abf-46a7-865a-43ed73a2a757": {"__data__": {"id_": "a9091752-0abf-46a7-865a-43ed73a2a757", "embedding": null, "metadata": {"page_label": "568", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48477fcc-691f-463f-b45b-2849a53d6aa5", "node_type": "4", "metadata": {"page_label": "568", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "59e914ab7b0af7ee30616641189fcb593e56f1c26c255b9176571fbe411ad114", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(see also exactly-once semantics)\npreservation of integrity, 525\nelastic systems, 17\nElasticsearch (search server)\ndocument-partitioned indexes, 207\npartition rebalancing, 211\npercolator (stream search), 467\nusage example, 4\nuse of Lucene, 79\nElephantDB (database), 413\nElm (programming language), 504, 512\nencodings (data formats), 111-128\nAvro, 122-127\nbinary variants of JSON and XML, 115\ncompatibility, 112\ncalling services, 136\nusing databases, 129-131\nusing message-passing, 138\ndefined, 113\nJSON, XML, and CSV, 114\nlanguage-specific formats, 113\nmerits of schemas, 127\nrepresentations of data, 112\nThrift and Protocol Buffers, 117-121\nend-to-end argument, 277, 519-520\nchecking integrity, 531\npublish/subscribe streams, 512\nenrichment (stream), 473\nEnterprise JavaBeans (EJB), 134\nentities (see vertices)\nepoch (consensus algorithms), 368\nepoch (Unix timestamps), 288\nequi-joins, 403\nerasure coding (error correction), 398\nErlang OTP (actor framework), 139\nerror handling\nfor network faults, 280\nin transactions, 231\nerror-correcting codes, 277, 398\nEsper (CEP engine), 466\netcd (coordination service), 370-373\nlinearizable operations, 333\nlocks and leader election, 330\nquorum reads, 351\nservice discovery, 372\nuse of Raft algorithm, 349, 353\nEthereum (blockchain), 532\nEthernet (networks), 276, 278, 285\npacket checksums, 306, 519\nEtherpad (collaborative editor), 170\nethics, 533-543\ncode of ethics and professional practice, 533\nlegislation and self-regulation, 542\npredictive analytics, 533-536\namplifying bias, 534\nfeedback loops, 536\nprivacy and tracking, 536-543\nconsent and freedom of choice, 538\ndata as assets and power, 540\nmeaning of privacy, 539\nsurveillance, 537\nrespect, dignity, and agency, 543, 544\nunintended consequences, 533, 536\nETL (extract-transform-load), 92, 405, 452, 554\nuse of Hadoop for, 416\nevent sourcing, 457-459\ncommands and events, 459\ncomparison to change data capture, 457\ncomparison to lambda architecture, 497\nderiving current state from event log, 458\nimmutability and auditability, 459, 531\nlarge, reliable data systems, 519, 526\nEvent Store (database), 458\nevent streams (see streams)\nevents, 440\ndeciding on total order of, 493\nderiving views from event log, 461\ndifference to commands, 459\nevent time versus processing time, 469, 477,\n498\nimmutable, advantages of, 460, 531\nordering to capture causality, 493\nreads as, 513\nstragglers, 470, 498\ntimestamp of, in stream processing, 471\nEventSource (browser API), 512\neventual consistency, 152, 162, 308, 322\n(see also conflicts)\nand perpetual inconsistency, 525\nevolvability, 21, 111\ncalling services, 136\ngraph-structured data, 52\nof databases, 40, 129-131, 461, 497\nof message-passing, 138\nreprocessing data, 496, 498\nschema evolution in Avro, 123\nschema evolution in Thrift and Protocol\nBuffers, 120\n568 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa55c503-693b-471c-b732-65d2c1616ae1": {"__data__": {"id_": "aa55c503-693b-471c-b732-65d2c1616ae1", "embedding": null, "metadata": {"page_label": "569", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80287f50-da5d-4e30-bcc2-86fa7f642a27", "node_type": "4", "metadata": {"page_label": "569", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f63b320534aee27975d765cb2cbcd520696dbe062f1a3423df0ce9ab2118e8ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "schema-on-read, 39, 111, 128\nexactly-once semantics, 360, 476, 516\nparity with batch processors, 498\npreservation of integrity, 525\nexclusive mode (locks), 258\neXtended Architecture transactions (see XA\ntransactions)\nextract-transform-load (see ETL)\nF\nFacebook\nPresto (query engine), 93\nReact, Flux, and Redux (user interface libra\u2010\nries), 512\nsocial graphs, 49\nWormhole (change data capture), 455\nfact tables, 93\nfailover, 157, 554\n(see also leader-based replication)\nin leaderless replication, absence of, 178\nleader election, 301, 348, 352\npotential problems, 157\nfailures\namplification by distributed transactions,\n364, 495\nfailure detection, 280\nautomatic rebalancing causing cascading\nfailures, 214\nperfect failure detectors, 359\ntimeouts and unbounded delays, 282,\n284\nusing ZooKeeper, 371\nfaults versus, 7\npartial failures in distributed systems,\n275-277, 310\nfan-out (messaging systems), 11, 445\nfault tolerance, 6-10, 555\nabstractions for, 321\nformalization in consensus, 365-369\nuse of replication, 367\nhuman fault tolerance, 414\nin batch processing, 406, 414, 422, 425\nin log-based systems, 520, 524-526\nin stream processing, 476-479\natomic commit, 477\nidempotence, 478\nmaintaining derived state, 495\nmicrobatching and checkpointing, 477\nrebuilding state after a failure, 478\nof distributed transactions, 362-364\ntransaction atomicity, 223, 354-361\nfaults, 6\nByzantine faults, 304-306\nfailures versus, 7\nhandled by transactions, 221\nhandling in supercomputers and cloud\ncomputing, 275\nhardware, 7\nin batch processing versus distributed data\u2010\nbases, 417\nin distributed systems, 274-277\nintroducing deliberately, 7, 280\nnetwork faults, 279-281\nasymmetric faults, 300\ndetecting, 280\ntolerance of, in multi-leader replication,\n169\nsoftware errors, 8\ntolerating (see fault tolerance)\nfederated databases, 501\nfence (CPU instruction), 338\nfencing (preventing split brain), 158, 302-304\ngenerating fencing tokens, 349, 370\nproperties of fencing tokens, 308\nstream processors writing to databases, 478,\n517\nFibre Channel (networks), 398\nfield tags (Thrift and Protocol Buffers), 119-121\nfile descriptors (Unix), 395\nfinancial data, 460\nFirebase (database), 456\nFlink (processing framework), 421-423\ndataflow APIs, 427\nfault tolerance, 422, 477, 479\nGelly API (graph processing), 425\nintegration of batch and stream processing,\n495, 498\nmachine learning, 428\nquery optimizer, 427\nstream processing, 466\nflow control, 282, 441, 555\nFLP result (on consensus), 353\nFlumeJava (dataflow library), 403, 427\nfollowers, 152, 555\n(see also leader-based replication)\nforeign keys, 38, 403\nforward compatibility, 112\nforward decay (algorithm), 16\nIndex | 569", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6c9df40-ffc1-49dd-ad02-c73fd103b973": {"__data__": {"id_": "c6c9df40-ffc1-49dd-ad02-c73fd103b973", "embedding": null, "metadata": {"page_label": "570", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e1922ff-baa8-48e7-b731-94c1daca510b", "node_type": "4", "metadata": {"page_label": "570", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "45cfbd8fb1c39a9472cf48403c527ef785dc74f650328987edfb5cff1e4cd8b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fossil (version control system), 463\nshunning (deleting data), 463\nFoundationDB (database)\nserializable transactions, 261, 265, 364\nfractal trees, 83\nfull table scans, 403\nfull-text search, 555\nand fuzzy indexes, 88\nbuilding search indexes, 411\nLucene storage engine, 79\nfunctional reactive programming (FRP), 504\nfunctional requirements, 22\nfutures (asynchronous operations), 135\nfuzzy search (see similarity search)\nG\ngarbage collection\nimmutability and, 463\nprocess pauses for, 14, 296-299, 301\n(see also process pauses)\ngenome analysis, 63, 429\ngeographically distributed datacenters, 145,\n164, 278, 493\ngeospatial indexes, 87\nGiraph (graph processing), 425\nGit (version control system), 174, 342, 463\nGitHub, postmortems, 157, 158, 309\nglobal indexes (see term-partitioned indexes)\nGlusterFS (distributed filesystem), 398\nGNU Coreutils (Linux), 394\nGoldenGate (change data capture), 161, 170,\n455\n(see also Oracle)\nGoogle\nBigtable (database)\ndata model (see Bigtable data model)\npartitioning scheme, 199, 202\nstorage layout, 78\nChubby (lock service), 370\nCloud Dataflow (stream processor), 466,\n477, 498\n(see also Beam)\nCloud Pub/Sub (messaging), 444, 448\nDocs (collaborative editor), 170\nDremel (query engine), 93, 96\nFlumeJava (dataflow library), 403, 427\nGFS (distributed file system), 398\ngRPC (RPC framework), 135\nMapReduce (batch processing), 390\n(see also MapReduce)\nbuilding search indexes, 411\ntask preemption, 418\nPregel (graph processing), 425\nSpanner (see Spanner)\nTrueTime (clock API), 294\ngossip protocol, 216\ngovernment use of data, 541\nGPS (Global Positioning System)\nuse for clock synchronization, 287, 290, 294,\n295\nGraphChi (graph processing), 426\ngraphs, 555\nas data models, 49-63\nexample of graph-structured data, 49\nproperty graphs, 50\nRDF and triple-stores, 55-59\nversus the network model, 60\nprocessing and analysis, 424-426\nfault tolerance, 425\nPregel processing model, 425\nquery languages\nCypher, 52\nDatalog, 60-63\nrecursive SQL queries, 53\nSPARQL, 59-59\nGremlin (graph query language), 50\ngrep (Unix tool), 392\nGROUP BY clause (SQL), 406\ngrouping records in MapReduce, 406\nhandling skew, 407\nH\nHadoop (data infrastructure)\ncomparison to distributed databases, 390\ncomparison to MPP databases, 414-418\ncomparison to Unix, 413-414, 499\ndiverse processing models in ecosystem, 417\nHDFS distributed filesystem (see HDFS)\nhigher-level tools, 403\njoin algorithms, 403-410\n(see also MapReduce)\nMapReduce (see MapReduce)\nYARN (see YARN)\nhappens-before relationship, 340\ncapturing, 187\nconcurrency and, 186\nhard disks\naccess patterns, 84\n570 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f61bf414-8a45-4bd1-b582-744bb2e980c6": {"__data__": {"id_": "f61bf414-8a45-4bd1-b582-744bb2e980c6", "embedding": null, "metadata": {"page_label": "571", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36a7f4b5-af65-4b7e-a0ea-10d884b2f7b7", "node_type": "4", "metadata": {"page_label": "571", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "17c90097d08d076f68eb8f9e4c03ef7074b7fb4e6be0f673369a1d4e0318955f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "detecting corruption, 519, 530\nfaults in, 7, 227\nsequential write throughput, 75, 450\nhardware faults, 7\nhash indexes, 72-75\nbroadcast hash joins, 409\npartitioned hash joins, 409\nhash partitioning, 203-205, 217\nconsistent hashing, 204\nproblems with hash mod N, 210\nrange queries, 204\nsuitable hash functions, 203\nwith fixed number of partitions, 210\nHAWQ (database), 428\nHBase (database)\nbug due to lack of fencing, 302\nbulk loading, 413\ncolumn-family data model, 41, 99\ndynamic partitioning, 212\nkey-range partitioning, 202\nlog-structured storage, 78\nrequest routing, 216\nsize-tiered compaction, 79\nuse of HDFS, 417\nuse of ZooKeeper, 370\nHDFS (Hadoop Distributed File System),\n398-399\n(see also distributed filesystems)\nchecking data integrity, 530\ndecoupling from query engines, 417\nindiscriminately dumping data into, 415\nmetadata about datasets, 410\nNameNode, 398\nuse by Flink, 479\nuse by HBase, 212\nuse by MapReduce, 402\nHdrHistogram (numerical library), 16\nhead (Unix tool), 392\nhead vertex (property graphs), 51\nhead-of-line blocking, 15\nheap files (databases), 86\nHelix (cluster manager), 216\nheterogeneous distributed transactions, 360,\n364\nheuristic decisions (in 2PC), 363\nHibernate (object-relational mapper), 30\nhierarchical model, 36\nhigh availability (see fault tolerance)\nhigh-frequency trading, 290, 299\nhigh-performance computing (HPC), 275\nhinted handoff, 183\nhistograms, 16\nHive (query engine), 419, 427\nfor data warehouses, 93\nHCatalog and metastore, 410\nmap-side joins, 409\nquery optimizer, 427\nskewed joins, 408\nworkflows, 403\nHollerith machines, 390\nhopping windows (stream processing), 472\n(see also windows)\nhorizontal scaling (see scaling out)\nHornetQ (messaging), 137, 444\ndistributed transaction support, 361\nhot spots, 201\ndue to celebrities, 205\nfor time-series data, 203\nin batch processing, 407\nrelieving, 205\nhot standbys (see leader-based replication)\nHTTP, use in APIs (see services)\nhuman errors, 9, 279, 414\nHyperDex (database), 88\nHyperLogLog (algorithm), 466\nI\nI/O operations, waiting for, 297\nIBM\nDB2 (database)\ndistributed transaction support, 361\nrecursive query support, 54\nserializable isolation, 242, 257\nXML and JSON support, 30, 42\nelectromechanical card-sorting machines,\n390\nIMS (database), 36\nimperative query APIs, 46\nInfoSphere Streams (CEP engine), 466\nMQ (messaging), 444\ndistributed transaction support, 361\nSystem R (database), 222\nWebSphere (messaging), 137\nidempotence, 134, 478, 555\nby giving operations unique IDs, 518, 522\nidempotent operations, 517\nimmutability\nadvantages of, 460, 531\nIndex | 571", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c3011dd-0e32-4c59-be7c-eeace2eb27eb": {"__data__": {"id_": "2c3011dd-0e32-4c59-be7c-eeace2eb27eb", "embedding": null, "metadata": {"page_label": "572", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63ee8a48-1f9a-4aa6-9ecf-14c016d732d2", "node_type": "4", "metadata": {"page_label": "572", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "15604b393f5329eea6cce0ba4420068b49f3830077492d4889acb2ac70c55ad1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "deriving state from event log, 459-464\nfor crash recovery, 75\nin B-trees, 82, 242\nin event sourcing, 457\ninputs to Unix commands, 397\nlimitations of, 463\nImpala (query engine)\nfor data warehouses, 93\nhash joins, 409\nnative code generation, 428\nuse of HDFS, 417\nimpedance mismatch, 29\nimperative languages, 42\nsetting element styles (example), 45\nin doubt (transaction status), 358\nholding locks, 362\norphaned transactions, 363\nin-memory databases, 88\ndurability, 227\nserial transaction execution, 253\nincidents\ncascading failures, 9\ncrashes due to leap seconds, 290\ndata corruption and financial losses due to\nconcurrency bugs, 233\ndata corruption on hard disks, 227\ndata loss due to last-write-wins, 173, 292\ndata on disks unreadable, 309\ndeleted items reappearing, 174\ndisclosure of sensitive data due to primary\nkey reuse, 157\nerrors in transaction serializability, 529\ngigabit network interface with 1 Kb/s\nthroughput, 311\nnetwork faults, 279\nnetwork interface dropping only inbound\npackets, 279\nnetwork partitions and whole-datacenter\nfailures, 275\npoor handling of network faults, 280\nsending message to ex-partner, 494\nsharks biting undersea cables, 279\nsplit brain due to 1-minute packet delay,\n158, 279\nvibrations in server rack, 14\nviolation of uniqueness constraint, 529\nindexes, 71, 555\nand snapshot isolation, 241\nas derived data, 386, 499-504\nB-trees, 79-83\nbuilding in batch processes, 411\nclustered, 86\ncomparison of B-trees and LSM-trees, 83-85\nconcatenated, 87\ncovering (with included columns), 86\ncreating, 500\nfull-text search, 88\ngeospatial, 87\nhash, 72-75\nindex-range locking, 260\nmulti-column, 87\npartitioning and secondary indexes,\n206-209, 217\nsecondary, 85\n(see also secondary indexes)\nproblems with dual writes, 452, 491\nSSTables and LSM-trees, 76-79\nupdating when data changes, 452, 467\nIndustrial Revolution, 541\nInfiniBand (networks), 285\nInfiniteGraph (database), 50\nInnoDB (storage engine)\nclustered index on primary key, 86\nnot preventing lost updates, 245\npreventing write skew, 248, 257\nserializable isolation, 257\nsnapshot isolation support, 239\ninside-out databases, 504\n(see also unbundling databases)\nintegrating different data systems (see data\nintegration)\nintegrity, 524\ncoordination-avoiding data systems, 528\ncorrectness of dataflow systems, 525\nin consensus formalization, 365\nintegrity checks, 530\n(see also auditing)\nend-to-end, 519, 531\nuse of snapshot isolation, 238\nmaintaining despite software bugs, 529\nInterface Definition Language (IDL), 117, 122\nintermediate state, materialization of, 420-423\ninternet services, systems for implementing,\n275\ninvariants, 225\n(see also constraints)\ninversion of control, 396\nIP (Internet Protocol)\n572 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7d44800-1881-4576-9697-6103d0371f4e": {"__data__": {"id_": "c7d44800-1881-4576-9697-6103d0371f4e", "embedding": null, "metadata": {"page_label": "573", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "749e268c-6685-49d1-8911-06d614772d2c", "node_type": "4", "metadata": {"page_label": "573", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "656813989b4d12035443718ae72fb908a0af5afa714731bae36c2fb06ea2e660", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "unreliability of, 277\nISDN (Integrated Services Digital Network),\n284\nisolation (in transactions), 225, 228, 555\ncorrectness and, 515\nfor single-object writes, 230\nserializability, 251-266\nactual serial execution, 252-256\nserializable snapshot isolation (SSI),\n261-266\ntwo-phase locking (2PL), 257-261\nviolating, 228\nweak isolation levels, 233-251\npreventing lost updates, 242-246\nread committed, 234-237\nsnapshot isolation, 237-242\niterative processing, 424-426\nJ\nJava Database Connectivity (JDBC)\ndistributed transaction support, 361\nnetwork drivers, 128\nJava Enterprise Edition (EE), 134, 356, 361\nJava Message Service (JMS), 444\n(see also messaging systems)\ncomparison to log-based messaging, 448,\n451\ndistributed transaction support, 361\nmessage ordering, 446\nJava Transaction API (JTA), 355, 361\nJava Virtual Machine (JVM)\nbytecode generation, 428\ngarbage collection pauses, 296\nprocess reuse in batch processors, 422\nJavaScript\nin MapReduce querying, 46\nsetting element styles (example), 45\nuse in advanced queries, 48\nJena (RDF framework), 57\nJepsen (fault tolerance testing), 515\njitter (network delay), 284\njoins, 555\nby index lookup, 403\nexpressing as relational operators, 427\nin relational and document databases, 34\nMapReduce map-side joins, 408-410\nbroadcast hash joins, 409\nmerge joins, 410\npartitioned hash joins, 409\nMapReduce reduce-side joins, 403-408\nhandling skew, 407\nsort-merge joins, 405\nparallel execution of, 415\nsecondary indexes and, 85\nstream joins, 472-476\nstream-stream join, 473\nstream-table join, 473\ntable-table join, 474\ntime-dependence of, 475\nsupport in document databases, 42\nJOTM (transaction coordinator), 356\nJSON\nAvro schema representation, 122\nbinary variants, 115\nfor application data, issues with, 114\nin relational databases, 30, 42\nrepresenting a r\u00e9sum\u00e9 (example), 31\nJuttle (query language), 504\nK\nk-nearest neighbors, 429\nKafka (messaging), 137, 448\nKafka Connect (database integration), 457,\n461\nKafka Streams (stream processor), 466, 467\nfault tolerance, 479\nleader-based replication, 153\nlog compaction, 456, 467\nmessage offsets, 447, 478\nrequest routing, 216\ntransaction support, 477\nusage example, 4\nKetama (partitioning library), 213\nkey-value stores, 70\nas batch process output, 412\nhash indexes, 72-75\nin-memory, 89\npartitioning, 201-205\nby hash of key, 203, 217\nby key range, 202, 217\ndynamic partitioning, 212\nskew and hot spots, 205\nKryo (Java), 113\nKubernetes (cluster manager), 418, 506\nL\nlambda architecture, 497\nLamport timestamps, 345\nIndex | 573", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1fb3d4c8-0a73-463b-9ae8-223de1733891": {"__data__": {"id_": "1fb3d4c8-0a73-463b-9ae8-223de1733891", "embedding": null, "metadata": {"page_label": "574", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0273c2ed-70a8-46c6-a209-dec0015be041", "node_type": "4", "metadata": {"page_label": "574", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "5416dea9c6a495d478ebc4b6b0f975fcc6236cc4499ad3dc318e5842f23a0d45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Hadron Collider (LHC), 64\nlast write wins (LWW), 173, 334\ndiscarding concurrent writes, 186\nproblems with, 292\nprone to lost updates, 246\nlate binding, 396\nlatency\ninstability under two-phase locking, 259\nnetwork latency and resource utilization,\n286\nresponse time versus, 14\ntail latency, 15, 207\nleader-based replication, 152-161\n(see also replication)\nfailover, 157, 301\nhandling node outages, 156\nimplementation of replication logs\nchange data capture, 454-457\n(see also changelogs)\nstatement-based, 158\ntrigger-based replication, 161\nwrite-ahead log (WAL) shipping, 159\nlinearizability of operations, 333\nlocking and leader election, 330\nlog sequence number, 156, 449\nread-scaling architecture, 161\nrelation to consensus, 367\nsetting up new followers, 155\nsynchronous versus asynchronous, 153-155\nleaderless replication, 177-191\n(see also replication)\ndetecting concurrent writes, 184-191\ncapturing happens-before relationship,\n187\nhappens-before relationship and concur\u2010\nrency, 186\nlast write wins, 186\nmerging concurrently written values,\n190\nversion vectors, 191\nmulti-datacenter, 184\nquorums, 179-182\nconsistency limitations, 181-183, 334\nsloppy quorums and hinted handoff, 183\nread repair and anti-entropy, 178\nleap seconds, 8, 290\nin time-of-day clocks, 288\nleases, 295\nimplementation with ZooKeeper, 370\nneed for fencing, 302\nledgers, 460\ndistributed ledger technologies, 532\nlegacy systems, maintenance of, 18\nless (Unix tool), 397\nLevelDB (storage engine), 78\nleveled compaction, 79\nLevenshtein automata, 88\nlimping (partial failure), 311\nlinearizability, 324-338, 555\ncost of, 335-338\nCAP theorem, 336\nmemory on multi-core CPUs, 338\ndefinition, 325-329\nimplementing with total order broadcast,\n350\nin ZooKeeper, 370\nof derived data systems, 492, 524\navoiding coordination, 527\nof different replication methods, 332-335\nusing quorums, 334\nrelying on, 330-332\nconstraints and uniqueness, 330\ncross-channel timing dependencies, 331\nlocking and leader election, 330\nstronger than causal consistency, 342\nusing to implement total order broadcast,\n351\nversus serializability, 329\nLinkedIn\nAzkaban (workflow scheduler), 402\nDatabus (change data capture), 161, 455\nEspresso (database), 31, 126, 130, 153, 216\nHelix (cluster manager) (see Helix)\nprofile (example), 30\nreference to company entity (example), 34\nRest.li (RPC framework), 135\nVoldemort (database) (see Voldemort)\nLinux, leap second bug, 8, 290\nliveness properties, 308\nLMDB (storage engine), 82, 242\nload\napproaches to coping with, 17\ndescribing, 11\nload testing, 16\nload balancing (messaging), 444\nlocal indexes (see document-partitioned\nindexes)\nlocality (data access), 32, 41, 555\n574 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "168012dd-7ed7-4a11-a10d-aaf5e606b579": {"__data__": {"id_": "168012dd-7ed7-4a11-a10d-aaf5e606b579", "embedding": null, "metadata": {"page_label": "575", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c9636ae-360b-4680-a22c-b48120ff1284", "node_type": "4", "metadata": {"page_label": "575", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "61a2d06868dea22946d81063d4acc1a7bb3b8dea5ea707967eda63ded3c93143", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in batch processing, 400, 405, 421\nin stateful clients, 170, 511\nin stream processing, 474, 478, 508, 522\nlocation transparency, 134\nin the actor model, 138\nlocks, 556\ndeadlock, 258\ndistributed locking, 301-304, 330\nfencing tokens, 303\nimplementation with ZooKeeper, 370\nrelation to consensus, 374\nfor transaction isolation\nin snapshot isolation, 239\nin two-phase locking (2PL), 257-261\nmaking operations atomic, 243\nperformance, 258\npreventing dirty writes, 236\npreventing phantoms with index-range\nlocks, 260, 265\nread locks (shared mode), 236, 258\nshared mode and exclusive mode, 258\nin two-phase commit (2PC)\ndeadlock detection, 364\nin-doubt transactions holding locks, 362\nmaterializing conflicts with, 251\npreventing lost updates by explicit locking,\n244\nlog sequence number, 156, 449\nlogic programming languages, 504\nlogical clocks, 293, 343, 494\nfor read-after-write consistency, 164\nlogical logs, 160\nlogs (data structure), 71, 556\nadvantages of immutability, 460\ncompaction, 73, 79, 456, 460\nfor stream operator state, 479\ncreating using total order broadcast, 349\nimplementing uniqueness constraints, 522\nlog-based messaging, 446-451\ncomparison to traditional messaging,\n448, 451\nconsumer offsets, 449\ndisk space usage, 450\nreplaying old messages, 451, 496, 498\nslow consumers, 450\nusing logs for message storage, 447\nlog-structured storage, 71-79\nlog-structured merge tree (see LSM-\ntrees)\nreplication, 152, 158-161\nchange data capture, 454-457\n(see also changelogs)\ncoordination with snapshot, 156\nlogical (row-based) replication, 160\nstatement-based replication, 158\ntrigger-based replication, 161\nwrite-ahead log (WAL) shipping, 159\nscalability limits, 493\nloose coupling, 396, 419, 502\nlost updates (see updates)\nLSM-trees (indexes), 78-79\ncomparison to B-trees, 83-85\nLucene (storage engine), 79\nbuilding indexes in batch processes, 411\nsimilarity search, 88\nLuigi (workflow scheduler), 402\nLWW (see last write wins)\nM\nmachine learning\nethical considerations, 534\n(see also ethics)\niterative processing, 424\nmodels derived from training data, 505\nstatistical and numerical algorithms, 428\nMADlib (machine learning toolkit), 428\nmagic scaling sauce, 18\nMahout (machine learning toolkit), 428\nmaintainability, 18-22, 489\ndefined, 23\ndesign principles for software systems, 19\nevolvability (see evolvability)\noperability, 19\nsimplicity and managing complexity, 20\nmany-to-many relationships\nin document model versus relational model,\n39\nmodeling as graphs, 49\nmany-to-one and many-to-many relationships,\n33-36\nmany-to-one relationships, 34\nMapReduce (batch processing), 390, 399-400\naccessing external services within job, 404,\n412\ncomparison to distributed databases\ndesigning for frequent faults, 417\ndiversity of processing models, 416\ndiversity of storage, 415\nIndex | 575", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f32e30a1-40a6-4c2e-96b3-ed3533296b1f": {"__data__": {"id_": "f32e30a1-40a6-4c2e-96b3-ed3533296b1f", "embedding": null, "metadata": {"page_label": "576", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4c2a7d84-7bb9-4173-aba2-b8fd1e180b45", "node_type": "4", "metadata": {"page_label": "576", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "6bb9ec21001342f52d716e8b349db6786659d2b37dda30e7b72399951b82bc4c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "comparison to stream processing, 464\ncomparison to Unix, 413-414\ndisadvantages and limitations of, 419\nfault tolerance, 406, 414, 422\nhigher-level tools, 403, 426\nimplementation in Hadoop, 400-403\nthe shuffle, 402\nimplementation in MongoDB, 46-48\nmachine learning, 428\nmap-side processing, 408-410\nbroadcast hash joins, 409\nmerge joins, 410\npartitioned hash joins, 409\nmapper and reducer functions, 399\nmaterialization of intermediate state,\n419-423\noutput of batch workflows, 411-413\nbuilding search indexes, 411\nkey-value stores, 412\nreduce-side processing, 403-408\nanalysis of user activity events (exam\u2010\nple), 404\ngrouping records by same key, 406\nhandling skew, 407\nsort-merge joins, 405\nworkflows, 402\nmarshalling (see encoding)\nmassively parallel processing (MPP), 216\ncomparison to composing storage technolo\u2010\ngies, 502\ncomparison to Hadoop, 414-418, 428\nmaster-master replication (see multi-leader\nreplication)\nmaster-slave replication (see leader-based repli\u2010\ncation)\nmaterialization, 556\naggregate values, 101\nconflicts, 251\nintermediate state (batch processing),\n420-423\nmaterialized views, 101\nas derived data, 386, 499-504\nmaintaining, using stream processing,\n467, 475\nMaven (Java build tool), 428\nMaxwell (change data capture), 455\nmean, 14\nmedia monitoring, 467\nmedian, 14\nmeeting room booking (example), 249, 259,\n521\nmembership services, 372\nMemcached (caching server), 4, 89\nmemory\nin-memory databases, 88\ndurability, 227\nserial transaction execution, 253\nin-memory representation of data, 112\nrandom bit-flips in, 529\nuse by indexes, 72, 77\nmemory barrier (CPU instruction), 338\nMemSQL (database)\nin-memory storage, 89\nread committed isolation, 236\nmemtable (in LSM-trees), 78\nMercurial (version control system), 463\nmerge joins, MapReduce map-side, 410\nmergeable persistent data structures, 174\nmerging sorted files, 76, 402, 405\nMerkle trees, 532\nMesos (cluster manager), 418, 506\nmessage brokers (see messaging systems)\nmessage-passing, 136-139\nadvantages over direct RPC, 137\ndistributed actor frameworks, 138\nevolvability, 138\nMessagePack (encoding format), 116\nmessages\nexactly-once semantics, 360, 476\nloss of, 442\nusing total order broadcast, 348\nmessaging systems, 440-451\n(see also streams)\nbackpressure, buffering, or dropping mes\u2010\nsages, 441\nbrokerless messaging, 442\nevent logs, 446-451\ncomparison to traditional messaging,\n448, 451\nconsumer offsets, 449\nreplaying old messages, 451, 496, 498\nslow consumers, 450\nmessage brokers, 443-446\nacknowledgements and redelivery, 445\ncomparison to event logs, 448, 451\nmultiple consumers of same topic, 444\nreliability, 442\nuniqueness in log-based messaging, 522\n576 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0437c4c-8716-4354-a64c-cac858c3935a": {"__data__": {"id_": "e0437c4c-8716-4354-a64c-cac858c3935a", "embedding": null, "metadata": {"page_label": "577", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be85ea15-9607-435f-a738-b0994566d1c8", "node_type": "4", "metadata": {"page_label": "577", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "cd55f9a6808fdbf5837d9e8f13c187e1baf522517485120abd08078ac299747e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Meteor (web framework), 456\nmicrobatching, 477, 495\nmicroservices, 132\n(see also services)\ncausal dependencies across services, 493\nloose coupling, 502\nrelation to batch/stream processors, 389,\n508\nMicrosoft\nAzure Service Bus (messaging), 444\nAzure Storage, 155, 398\nAzure Stream Analytics, 466\nDCOM (Distributed Component Object\nModel), 134\nMSDTC (transaction coordinator), 356\nOrleans (see Orleans)\nSQL Server (see SQL Server)\nmigrating (rewriting) data, 40, 130, 461, 497\nmodulus operator (%), 210\nMongoDB (database)\naggregation pipeline, 48\natomic operations, 243\nBSON, 41\ndocument data model, 31\nhash partitioning (sharding), 203-204\nkey-range partitioning, 202\nlack of join support, 34, 42\nleader-based replication, 153\nMapReduce support, 46, 400\noplog parsing, 455, 456\npartition splitting, 212\nrequest routing, 216\nsecondary indexes, 207\nMongoriver (change data capture), 455\nmonitoring, 10, 19\nmonotonic clocks, 288\nmonotonic reads, 164\nMPP (see massively parallel processing)\nMSMQ (messaging), 361\nmulti-column indexes, 87\nmulti-leader replication, 168-177\n(see also replication)\nhandling write conflicts, 171\nconflict avoidance, 172\nconverging toward a consistent state,\n172\ncustom conflict resolution logic, 173\ndetermining what is a conflict, 174\nlinearizability, lack of, 333\nreplication topologies, 175-177\nuse cases, 168\nclients with offline operation, 170\ncollaborative editing, 170\nmulti-datacenter replication, 168, 335\nmulti-object transactions, 228\nneed for, 231\nMulti-Paxos (total order broadcast), 367\nmulti-table index cluster tables (Oracle), 41\nmulti-tenancy, 284\nmulti-version concurrency control (MVCC),\n239, 266\ndetecting stale MVCC reads, 263\nindexes and snapshot isolation, 241\nmutual exclusion, 261\n(see also locks)\nMySQL (database)\nbinlog coordinates, 156\nbinlog parsing for change data capture, 455\ncircular replication topology, 175\nconsistent snapshots, 156\ndistributed transaction support, 361\nInnoDB storage engine (see InnoDB)\nJSON support, 30, 42\nleader-based replication, 153\nperformance of XA transactions, 360\nrow-based replication, 160\nschema changes in, 40\nsnapshot isolation support, 242\n(see also InnoDB)\nstatement-based replication, 159\nTungsten Replicator (multi-leader replica\u2010\ntion), 170\nconflict detection, 177\nN\nnanomsg (messaging library), 442\nNarayana (transaction coordinator), 356\nNATS (messaging), 137\nnear-real-time (nearline) processing, 390\n(see also stream processing)\nNeo4j (database)\nCypher query language, 52\ngraph data model, 50\nNephele (dataflow engine), 421\nnetcat (Unix tool), 397\nNetflix Chaos Monkey, 7, 280\nNetwork Attached Storage (NAS), 146, 398\nnetwork model, 36\nIndex | 577", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6cb767f9-eb9b-445b-a2b3-2dc877c4da9b": {"__data__": {"id_": "6cb767f9-eb9b-445b-a2b3-2dc877c4da9b", "embedding": null, "metadata": {"page_label": "578", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b92e74d-d69a-4e0d-9bc2-0cd9b49943bc", "node_type": "4", "metadata": {"page_label": "578", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "a3824f99502c5aa81b50a6d1918fbd2cc72b000564c9a098fe5afe5c4a25f2e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "graph databases versus, 60\nimperative query APIs, 46\nNetwork Time Protocol (see NTP)\nnetworks\ncongestion and queueing, 282\ndatacenter network topologies, 276\nfaults (see faults)\nlinearizability and network delays, 338\nnetwork partitions, 279, 337\ntimeouts and unbounded delays, 281\nnext-key locking, 260\nnodes (in graphs) (see vertices)\nnodes (processes), 556\nhandling outages in leader-based replica\u2010\ntion, 156\nsystem models for failure, 307\nnoisy neighbors, 284\nnonblocking atomic commit, 359\nnondeterministic operations\naccidental nondeterminism, 423\npartial failures in distributed systems, 275\nnonfunctional requirements, 22\nnonrepeatable reads, 238\n(see also read skew)\nnormalization (data representation), 33, 556\nexecuting joins, 39, 42, 403\nforeign key references, 231\nin systems of record, 386\nversus denormalization, 462\nNoSQL, 29, 499\ntransactions and, 223\nNotation3 (N3), 56\nnpm (package manager), 428\nNTP (Network Time Protocol), 287\naccuracy, 289, 293\nadjustments to monotonic clocks, 289\nmultiple server addresses, 306\nnumbers, in XML and JSON encodings, 114\nO\nobject-relational mapping (ORM) frameworks,\n30\nerror handling and aborted transactions,\n232\nunsafe read-modify-write cycle code, 244\nobject-relational mismatch, 29\nobserver pattern, 506\noffline systems, 390\n(see also batch processing)\nstateful, offline-capable clients, 170, 511\noffline-first applications, 511\noffsets\nconsumer offsets in partitioned logs, 449\nmessages in partitioned logs, 447\nOLAP (online analytic processing), 91, 556\ndata cubes, 102\nOLTP (online transaction processing), 90, 556\nanalytics queries versus, 411\nworkload characteristics, 253\none-to-many relationships, 30\nJSON representation, 32\nonline systems, 389\n(see also services)\nOozie (workflow scheduler), 402\nOpenAPI (service definition format), 133\nOpenStack\nNova (cloud infrastructure)\nuse of ZooKeeper, 370\nSwift (object storage), 398\noperability, 19\noperating systems versus databases, 499\noperation identifiers, 518, 522\noperational transformation, 174\noperators, 421\nflow of data between, 424\nin stream processing, 464\noptimistic concurrency control, 261\nOracle (database)\ndistributed transaction support, 361\nGoldenGate (change data capture), 161,\n170, 455\nlack of serializability, 226\nleader-based replication, 153\nmulti-table index cluster tables, 41\nnot preventing write skew, 248\npartitioned indexes, 209\nPL/SQL language, 255\npreventing lost updates, 245\nread committed isolation, 236\nReal Application Clusters (RAC), 330\nrecursive query support, 54\nsnapshot isolation support, 239, 242\nTimesTen (in-memory database), 89\nWAL-based replication, 160\nXML support, 30\nordering, 339-352\nby sequence numbers, 343-348\ncausal ordering, 339-343\n578 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7761c669-78f6-43a5-9bca-c6eb51c5c498": {"__data__": {"id_": "7761c669-78f6-43a5-9bca-c6eb51c5c498", "embedding": null, "metadata": {"page_label": "579", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b276a36-4177-4614-90f4-ced07f7eb874", "node_type": "4", "metadata": {"page_label": "579", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "eed352f38fc34d12faa1e973ddbdc4c5f28062fd99fc72cb26f7fa629a4f2935", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "partial order, 341\nlimits of total ordering, 493\ntotal order broadcast, 348-352\nOrleans (actor framework), 139\noutliers (response time), 14\nOz (programming language), 504\nP\npackage managers, 428, 505\npacket switching, 285\npackets\ncorruption of, 306\nsending via UDP, 442\nPageRank (algorithm), 49, 424\npaging (see virtual memory)\nParAccel (database), 93\nparallel databases (see massively parallel pro\u2010\ncessing)\nparallel execution\nof graph analysis algorithms, 426\nqueries in MPP databases, 216\nParquet (data format), 96, 131\n(see also column-oriented storage)\nuse in Hadoop, 414\npartial failures, 275, 310\nlimping, 311\npartial order, 341\npartitioning, 199-218, 556\nand replication, 200\nin batch processing, 429\nmulti-partition operations, 514\nenforcing constraints, 522\nsecondary index maintenance, 495\nof key-value data, 201-205\nby key range, 202\nskew and hot spots, 205\nrebalancing partitions, 209-214\nautomatic or manual rebalancing, 213\nproblems with hash mod N, 210\nusing dynamic partitioning, 212\nusing fixed number of partitions, 210\nusing N partitions per node, 212\nreplication and, 147\nrequest routing, 214-216\nsecondary indexes, 206-209\ndocument-based partitioning, 206\nterm-based partitioning, 208\nserial execution of transactions and, 255\nPaxos (consensus algorithm), 366\nballot number, 368\nMulti-Paxos (total order broadcast), 367\npercentiles, 14, 556\ncalculating efficiently, 16\nimportance of high percentiles, 16\nuse in service level agreements (SLAs), 15\nPercona XtraBackup (MySQL tool), 156\nperformance\ndescribing, 13\nof distributed transactions, 360\nof in-memory databases, 89\nof linearizability, 338\nof multi-leader replication, 169\nperpetual inconsistency, 525\npessimistic concurrency control, 261\nphantoms (transaction isolation), 250\nmaterializing conflicts, 251\npreventing, in serializability, 259\nphysical clocks (see clocks)\npickle (Python), 113\nPig (dataflow language), 419, 427\nreplicated joins, 409\nskewed joins, 407\nworkflows, 403\nPinball (workflow scheduler), 402\npipelined execution, 423\nin Unix, 394\npoint in time, 287\npolyglot persistence, 29\npolystores, 501\nPostgreSQL (database)\nBDR (multi-leader replication), 170\ncausal ordering of writes, 177\nBottled Water (change data capture), 455\nBucardo (trigger-based replication), 161,\n173\ndistributed transaction support, 361\nforeign data wrappers, 501\nfull text search support, 490\nleader-based replication, 153\nlog sequence number, 156\nMVCC implementation, 239, 241\nPL/pgSQL language, 255\nPostGIS geospatial indexes, 87\npreventing lost updates, 245\npreventing write skew, 248, 261\nread committed isolation, 236\nrecursive query support, 54\nrepresenting graphs, 51\nIndex | 579", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "000bc57d-2008-4cd9-a075-e6202829f8f8": {"__data__": {"id_": "000bc57d-2008-4cd9-a075-e6202829f8f8", "embedding": null, "metadata": {"page_label": "580", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec61542d-8b98-4f03-8581-edc47443e7d7", "node_type": "4", "metadata": {"page_label": "580", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "b8cec29aa25e8a3b076ea3b26df9806ac68364c20fb2762aecf38b74db826838", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "serializable snapshot isolation (SSI), 261\nsnapshot isolation support, 239, 242\nWAL-based replication, 160\nXML and JSON support, 30, 42\npre-splitting, 212\nPrecision Time Protocol (PTP), 290\npredicate locks, 259\npredictive analytics, 533-536\namplifying bias, 534\nethics of (see ethics)\nfeedback loops, 536\npreemption\nof datacenter resources, 418\nof threads, 298\nPregel processing model, 425\nprimary keys, 85, 556\ncompound primary key (Cassandra), 204\nprimary-secondary replication (see leader-\nbased replication)\nprivacy, 536-543\nconsent and freedom of choice, 538\ndata as assets and power, 540\ndeleting data, 463\nethical considerations (see ethics)\nlegislation and self-regulation, 542\nmeaning of, 539\nsurveillance, 537\ntracking behavioral data, 536\nprobabilistic algorithms, 16, 466\nprocess pauses, 295-299\nprocessing time (of events), 469\nproducers (message streams), 440\nprogramming languages\ndataflow languages, 504\nfor stored procedures, 255\nfunctional reactive programming (FRP),\n504\nlogic programming, 504\nProlog (language), 61\n(see also Datalog)\npromises (asynchronous operations), 135\nproperty graphs, 50\nCypher query language, 52\nProtocol Buffers (data format), 117-121\nfield tags and schema evolution, 120\nprovenance of data, 531\npublish/subscribe model, 441\npublishers (message streams), 440\npunch card tabulating machines, 390\npure functions, 48\nputting computation near data, 400\nQ\nQpid (messaging), 444\nquality of service (QoS), 285\nQuantcast File System (distributed filesystem),\n398\nquery languages, 42-48\naggregation pipeline, 48\nCSS and XSL, 44\nCypher, 52\nDatalog, 60\nJuttle, 504\nMapReduce querying, 46-48\nrecursive SQL queries, 53\nrelational algebra and SQL, 42\nSPARQL, 59\nquery optimizers, 37, 427\nqueueing delays (networks), 282\nhead-of-line blocking, 15\nlatency and response time, 14\nqueues (messaging), 137\nquorums, 179-182, 556\nfor leaderless replication, 179\nin consensus algorithms, 368\nlimitations of consistency, 181-183, 334\nmaking decisions in distributed systems,\n301\nmonitoring staleness, 182\nmulti-datacenter replication, 184\nrelying on durability, 309\nsloppy quorums and hinted handoff, 183\nR\nR-trees (indexes), 87\nRabbitMQ (messaging), 137, 444\nleader-based replication, 153\nrace conditions, 225\n(see also concurrency)\navoiding with linearizability, 331\ncaused by dual writes, 452\ndirty writes, 235\nin counter increments, 235\nlost updates, 242-246\npreventing with event logs, 462, 507\npreventing with serializable isolation, 252\nwrite skew, 246-251\nRaft (consensus algorithm), 366\n580 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f712bb45-e9bb-4204-9e98-8700b7ad2a8a": {"__data__": {"id_": "f712bb45-e9bb-4204-9e98-8700b7ad2a8a", "embedding": null, "metadata": {"page_label": "581", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adeb9a86-3040-4f7e-aeff-6c399f3e1d05", "node_type": "4", "metadata": {"page_label": "581", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "ba8adc525aa7e62ff3809767a1ed01d0c35132d9ec21bdda14b0b2fedd0b70e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "sensitivity to network problems, 369\nterm number, 368\nuse in etcd, 353\nRAID (Redundant Array of Independent\nDisks), 7, 398\nrailways, schema migration on, 496\nRAMCloud (in-memory storage), 89\nranking algorithms, 424\nRDF (Resource Description Framework), 57\nquerying with SPARQL, 59\nRDMA (Remote Direct Memory Access), 276\nread committed isolation level, 234-237\nimplementing, 236\nmulti-version concurrency control\n(MVCC), 239\nno dirty reads, 234\nno dirty writes, 235\nread path (derived data), 509\nread repair (leaderless replication), 178\nfor linearizability, 335\nread replicas (see leader-based replication)\nread skew (transaction isolation), 238, 266\nas violation of causality, 340\nread-after-write consistency, 163, 524\ncross-device, 164\nread-modify-write cycle, 243\nread-scaling architecture, 161\nreads as events, 513\nreal-time\ncollaborative editing, 170\nnear-real-time processing, 390\n(see also stream processing)\npublish/subscribe dataflow, 513\nresponse time guarantees, 298\ntime-of-day clocks, 288\nrebalancing partitions, 209-214, 556\n(see also partitioning)\nautomatic or manual rebalancing, 213\ndynamic partitioning, 212\nfixed number of partitions, 210\nfixed number of partitions per node, 212\nproblems with hash mod N, 210\nrecency guarantee, 324\nrecommendation engines\nbatch process outputs, 412\nbatch workflows, 403, 420\niterative processing, 424\nstatistical and numerical algorithms, 428\nrecords, 399\nevents in stream processing, 440\nrecursive common table expressions (SQL), 54\nredelivery (messaging), 445\nRedis (database)\natomic operations, 243\ndurability, 89\nLua scripting, 255\nsingle-threaded execution, 253\nusage example, 4\nredundancy\nhardware components, 7\nof derived data, 386\n(see also derived data)\nReed\u2013Solomon codes (error correction), 398\nrefactoring, 22\n(see also evolvability)\nregions (partitioning), 199\nregister (data structure), 325\nrelational data model, 28-42\ncomparison to document model, 38-42\ngraph queries in SQL, 53\nin-memory databases with, 89\nmany-to-one and many-to-many relation\u2010\nships, 33\nmulti-object transactions, need for, 231\nNoSQL as alternative to, 29\nobject-relational mismatch, 29\nrelational algebra and SQL, 42\nversus document model\nconvergence of models, 41\ndata locality, 41\nrelational databases\neventual consistency, 162\nhistory, 28\nleader-based replication, 153\nlogical logs, 160\nphilosophy compared to Unix, 499, 501\nschema changes, 40, 111, 130\nstatement-based replication, 158\nuse of B-tree indexes, 80\nrelationships (see edges)\nreliability, 6-10, 489\nbuilding a reliable system from unreliable\ncomponents, 276\ndefined, 6, 22\nhardware faults, 7\nhuman errors, 9\nimportance of, 10\nof messaging systems, 442\nIndex | 581", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b38d4ef2-4cab-4935-b9ee-9050a6ef9f60": {"__data__": {"id_": "b38d4ef2-4cab-4935-b9ee-9050a6ef9f60", "embedding": null, "metadata": {"page_label": "582", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e81d1485-4097-498e-ae45-4dcca4c7a01f", "node_type": "4", "metadata": {"page_label": "582", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "89f89e682bda47a4e50bbca9ba38e0edf1d215bb24b08f3b38eec91566ea74ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "software errors, 8\nRemote Method Invocation (Java RMI), 134\nremote procedure calls (RPCs), 134-136\n(see also services)\nbased on futures, 135\ndata encoding and evolution, 136\nissues with, 134\nusing Avro, 126, 135\nusing Thrift, 135\nversus message brokers, 137\nrepeatable reads (transaction isolation), 242\nreplicas, 152\nreplication, 151-193, 556\nand durability, 227\nchain replication, 155\nconflict resolution and, 246\nconsistency properties, 161-167\nconsistent prefix reads, 165\nmonotonic reads, 164\nreading your own writes, 162\nin distributed filesystems, 398\nleaderless, 177-191\ndetecting concurrent writes, 184-191\nlimitations of quorum consistency,\n181-183, 334\nsloppy quorums and hinted handoff, 183\nmonitoring staleness, 182\nmulti-leader, 168-177\nacross multiple datacenters, 168, 335\nhandling write conflicts, 171-175\nreplication topologies, 175-177\npartitioning and, 147, 200\nreasons for using, 145, 151\nsingle-leader, 152-161\nfailover, 157\nimplementation of replication logs,\n158-161\nrelation to consensus, 367\nsetting up new followers, 155\nsynchronous versus asynchronous,\n153-155\nstate machine replication, 349, 452\nusing erasure coding, 398\nwith heterogeneous data systems, 453\nreplication logs (see logs)\nreprocessing data, 496, 498\n(see also evolvability)\nfrom log-based messaging, 451\nrequest routing, 214-216\napproaches to, 214\nparallel query execution, 216\nresilient systems, 6\n(see also fault tolerance)\nresponse time\nas performance metric for services, 13, 389\nguarantees on, 298\nlatency versus, 14\nmean and percentiles, 14\nuser experience, 15\nresponsibility and accountability, 535\nREST (Representational State Transfer), 133\n(see also services)\nRethinkDB (database)\ndocument data model, 31\ndynamic partitioning, 212\njoin support, 34, 42\nkey-range partitioning, 202\nleader-based replication, 153\nsubscribing to changes, 456\nRiak (database)\nBitcask storage engine, 72\nCRDTs, 174, 191\ndotted version vectors, 191\ngossip protocol, 216\nhash partitioning, 203-204, 211\nlast-write-wins conflict resolution, 186\nleaderless replication, 177\nLevelDB storage engine, 78\nlinearizability, lack of, 335\nmulti-datacenter support, 184\npreventing lost updates across replicas, 246\nrebalancing, 213\nsearch feature, 209\nsecondary indexes, 207\nsiblings (concurrently written values), 190\nsloppy quorums, 184\nring buffers, 450\nRipple (cryptocurrency), 532\nrockets, 10, 36, 305\nRocksDB (storage engine), 78\nleveled compaction, 79\nrollbacks (transactions), 222\nrolling upgrades, 8, 112\nrouting (see request routing)\nrow-oriented storage, 96\nrow-based replication, 160\nrowhammer (memory corruption), 529\nRPCs (see remote procedure calls)\n582 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebf7af3c-7272-4fb4-8ef1-a59e65eddde0": {"__data__": {"id_": "ebf7af3c-7272-4fb4-8ef1-a59e65eddde0", "embedding": null, "metadata": {"page_label": "583", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3631ed6d-7b27-46ce-beb3-4fc6b4b8447c", "node_type": "4", "metadata": {"page_label": "583", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "0a21361a0c6b5a08f1c02d5604a87514f76f9065d04fe1d62950e87f8f861c75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rubygems (package manager), 428\nrules (Datalog), 61\nS\nsafety and liveness properties, 308\nin consensus algorithms, 366\nin transactions, 222\nsagas (see compensating transactions)\nSamza (stream processor), 466, 467\nfault tolerance, 479\nstreaming SQL support, 466\nsandboxes, 9\nSAP HANA (database), 93\nscalability, 10-18, 489\napproaches for coping with load, 17\ndefined, 22\ndescribing load, 11\ndescribing performance, 13\npartitioning and, 199\nreplication and, 161\nscaling up versus scaling out, 146\nscaling out, 17, 146\n(see also shared-nothing architecture)\nscaling up, 17, 146\nscatter/gather approach, querying partitioned\ndatabases, 207\nSCD (slowly changing dimension), 476\nschema-on-read, 39\ncomparison to evolvable schema, 128\nin distributed filesystems, 415\nschema-on-write, 39\nschemaless databases (see schema-on-read)\nschemas, 557\nAvro, 122-127\nreader determining writer\u2019s schema, 125\nschema evolution, 123\ndynamically generated, 126\nevolution of, 496\naffecting application code, 111\ncompatibility checking, 126\nin databases, 129-131\nin message-passing, 138\nin service calls, 136\nflexibility in document model, 39\nfor analytics, 93-95\nfor JSON and XML, 115\nmerits of, 127\nschema migration on railways, 496\nThrift and Protocol Buffers, 117-121\nschema evolution, 120\ntraditional approach to design, fallacy in,\n462\nsearches\nbuilding search indexes in batch processes,\n411\nk-nearest neighbors, 429\non streams, 467\npartitioned secondary indexes, 206\nsecondaries (see leader-based replication)\nsecondary indexes, 85, 557\npartitioning, 206-209, 217\ndocument-partitioned, 206\nindex maintenance, 495\nterm-partitioned, 208\nproblems with dual writes, 452, 491\nupdating, transaction isolation and, 231\nsecondary sorts, 405\nsed (Unix tool), 392\nself-describing files, 127\nself-joins, 480\nself-validating systems, 530\nsemantic web, 57\nsemi-synchronous replication, 154\nsequence number ordering, 343-348\ngenerators, 294, 344\ninsufficiency for enforcing constraints, 347\nLamport timestamps, 345\nuse of timestamps, 291, 295, 345\nsequential consistency, 351\nserializability, 225, 233, 251-266, 557\nlinearizability versus, 329\npessimistic versus optimistic concurrency\ncontrol, 261\nserial execution, 252-256\npartitioning, 255\nusing stored procedures, 253, 349\nserializable snapshot isolation (SSI),\n261-266\ndetecting stale MVCC reads, 263\ndetecting writes that affect prior reads,\n264\ndistributed execution, 265, 364\nperformance of SSI, 265\npreventing write skew, 262-265\ntwo-phase locking (2PL), 257-261\nindex-range locks, 260\nperformance, 258\nSerializable (Java), 113\nIndex | 583", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6729502a-e921-4090-9281-21453d5fcef5": {"__data__": {"id_": "6729502a-e921-4090-9281-21453d5fcef5", "embedding": null, "metadata": {"page_label": "584", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65680103-60a1-43a6-8038-837d7875d63d", "node_type": "4", "metadata": {"page_label": "584", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "c7603c48ae72747d881d1e842653e95169b6fe07c6bb56b0875bf31c2d7a0bd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "serialization, 113\n(see also encoding)\nservice discovery, 135, 214, 372\nusing DNS, 216, 372\nservice level agreements (SLAs), 15\nservice-oriented architecture (SOA), 132\n(see also services)\nservices, 131-136\nmicroservices, 132\ncausal dependencies across services, 493\nloose coupling, 502\nrelation to batch/stream processors, 389,\n508\nremote procedure calls (RPCs), 134-136\nissues with, 134\nsimilarity to databases, 132\nweb services, 132, 135\nsession windows (stream processing), 472\n(see also windows)\nsessionization, 407\nsharding (see partitioning)\nshared mode (locks), 258\nshared-disk architecture, 146, 398\nshared-memory architecture, 146\nshared-nothing architecture, 17, 146-147, 557\n(see also replication)\ndistributed filesystems, 398\n(see also distributed filesystems)\npartitioning, 199\nuse of network, 277\nsharks\nbiting undersea cables, 279\ncounting (example), 46-48\nfinding (example), 42\nwebsite about (example), 44\nshredding (in relational model), 38\nsiblings (concurrent values), 190, 246\n(see also conflicts)\nsimilarity search\nedit distance, 88\ngenome data, 63\nk-nearest neighbors, 429\nsingle-leader replication (see leader-based rep\u2010\nlication)\nsingle-threaded execution, 243, 252\nin batch processing, 406, 421, 426\nin stream processing, 448, 463, 522\nsize-tiered compaction, 79\nskew, 557\nclock skew, 291-294, 334\nin transaction isolation\nread skew, 238, 266\nwrite skew, 246-251, 262-265\n(see also write skew)\nmeanings of, 238\nunbalanced workload, 201\ncompensating for, 205\ndue to celebrities, 205\nfor time-series data, 203\nin batch processing, 407\nslaves (see leader-based replication)\nsliding windows (stream processing), 472\n(see also windows)\nsloppy quorums, 183\n(see also quorums)\nlack of linearizability, 334\nslowly changing dimension (data warehouses),\n476\nsmearing (leap seconds adjustments), 290\nsnapshots (databases)\ncausal consistency, 340\ncomputing derived data, 500\nin change data capture, 455\nserializable snapshot isolation (SSI),\n261-266, 329\nsetting up a new replica, 156\nsnapshot isolation and repeatable read,\n237-242\nimplementing with MVCC, 239\nindexes and MVCC, 241\nvisibility rules, 240\nsynchronized clocks for global snapshots,\n294\nsnowflake schemas, 95\nSOAP, 133\n(see also services)\nevolvability, 136\nsoftware bugs, 8\nmaintaining integrity, 529\nsolid state drives (SSDs)\naccess patterns, 84\ndetecting corruption, 519, 530\nfaults in, 227\nsequential write throughput, 75\nSolr (search server)\nbuilding indexes in batch processes, 411\ndocument-partitioned indexes, 207\nrequest routing, 216\n584 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6d75f0a-9ead-4ef3-99c5-b37e1bbf3184": {"__data__": {"id_": "d6d75f0a-9ead-4ef3-99c5-b37e1bbf3184", "embedding": null, "metadata": {"page_label": "585", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59074bad-f21b-4fb0-8e24-f280fb071a9a", "node_type": "4", "metadata": {"page_label": "585", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "80a121914eef16e80b242a4d126f941742ab7075699d658410345bd61e407e04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "usage example, 4\nuse of Lucene, 79\nsort (Unix tool), 392, 394, 395\nsort-merge joins (MapReduce), 405\nSorted String Tables (see SSTables)\nsorting\nsort order in column storage, 99\nsource of truth (see systems of record)\nSpanner (database)\ndata locality, 41\nsnapshot isolation using clocks, 295\nTrueTime API, 294\nSpark (processing framework), 421-423\nbytecode generation, 428\ndataflow APIs, 427\nfault tolerance, 422\nfor data warehouses, 93\nGraphX API (graph processing), 425\nmachine learning, 428\nquery optimizer, 427\nSpark Streaming, 466\nmicrobatching, 477\nstream processing on top of batch process\u2010\ning, 495\nSPARQL (query language), 59\nspatial algorithms, 429\nsplit brain, 158, 557\nin consensus algorithms, 352, 367\npreventing, 322, 333\nusing fencing tokens to avoid, 302-304\nspreadsheets, dataflow programming capabili\u2010\nties, 504\nSQL (Structured Query Language), 21, 28, 43\nadvantages and limitations of, 416\ndistributed query execution, 48\ngraph queries in, 53\nisolation levels standard, issues with, 242\nquery execution on Hadoop, 416\nr\u00e9sum\u00e9 (example), 30\nSQL injection vulnerability, 305\nSQL on Hadoop, 93\nstatement-based replication, 158\nstored procedures, 255\nSQL Server (database)\ndata warehousing support, 93\ndistributed transaction support, 361\nleader-based replication, 153\npreventing lost updates, 245\npreventing write skew, 248, 257\nread committed isolation, 236\nrecursive query support, 54\nserializable isolation, 257\nsnapshot isolation support, 239\nT-SQL language, 255\nXML support, 30\nSQLstream (stream analytics), 466\nSSDs (see solid state drives)\nSSTables (storage format), 76-79\nadvantages over hash indexes, 76\nconcatenated index, 204\nconstructing and maintaining, 78\nmaking LSM-Tree from, 78\nstaleness (old data), 162\ncross-channel timing dependencies, 331\nin leaderless databases, 178\nin multi-version concurrency control, 263\nmonitoring for, 182\nof client state, 512\nversus linearizability, 324\nversus timeliness, 524\nstandbys (see leader-based replication)\nstar replication topologies, 175\nstar schemas, 93-95\nsimilarity to event sourcing, 458\nStar Wars analogy (event time versus process\u2010\ning time), 469\nstate\nderived from log of immutable events, 459\nderiving current state from the event log,\n458\ninterplay between state changes and appli\u2010\ncation code, 507\nmaintaining derived state, 495\nmaintenance by stream processor in stream-\nstream joins, 473\nobserving derived state, 509-515\nrebuilding after stream processor failure,\n478\nseparation of application code and, 505\nstate machine replication, 349, 452\nstatement-based replication, 158\nstatically typed languages\nanalogy to schema-on-write, 40\ncode generation and, 127\nstatistical and numerical algorithms, 428\nStatsD (metrics aggregator), 442\nstdin, stdout, 395, 396\nStellar (cryptocurrency), 532\nIndex | 585", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6715eaaf-5a68-4fb9-bca7-b6adebeeb0e9": {"__data__": {"id_": "6715eaaf-5a68-4fb9-bca7-b6adebeeb0e9", "embedding": null, "metadata": {"page_label": "586", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "788fe1cc-44ad-46ab-b7e0-8d390ceb0fea", "node_type": "4", "metadata": {"page_label": "586", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "fabf3ab4c4b6e5e6e8f09878fe27cf4a7a478297045705b547cf9d0eef22a27b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "stock market feeds, 442\nSTONITH (Shoot The Other Node In The\nHead), 158\nstop-the-world (see garbage collection)\nstorage\ncomposing data storage technologies,\n499-504\ndiversity of, in MapReduce, 415\nStorage Area Network (SAN), 146, 398\nstorage engines, 69-104\ncolumn-oriented, 95-101\ncolumn compression, 97-99\ndefined, 96\ndistinction between column families\nand, 99\nParquet, 96, 131\nsort order in, 99-100\nwriting to, 101\ncomparing requirements for transaction\nprocessing and analytics, 90-96\nin-memory storage, 88\ndurability, 227\nrow-oriented, 70-90\nB-trees, 79-83\ncomparing B-trees and LSM-trees, 83-85\ndefined, 96\nlog-structured, 72-79\nstored procedures, 161, 253-255, 557\nand total order broadcast, 349\npros and cons of, 255\nsimilarity to stream processors, 505\nStorm (stream processor), 466\ndistributed RPC, 468, 514\nTrident state handling, 478\nstraggler events, 470, 498\nstream processing, 464-481, 557\naccessing external services within job, 474,\n477, 478, 517\ncombining with batch processing\nlambda architecture, 497\nunifying technologies, 498\ncomparison to batch processing, 464\ncomplex event processing (CEP), 465\nfault tolerance, 476-479\natomic commit, 477\nidempotence, 478\nmicrobatching and checkpointing, 477\nrebuilding state after a failure, 478\nfor data integration, 494-498\nmaintaining derived state, 495\nmaintenance of materialized views, 467\nmessaging systems (see messaging systems)\nreasoning about time, 468-472\nevent time versus processing time, 469,\n477, 498\nknowing when window is ready, 470\ntypes of windows, 472\nrelation to databases (see streams)\nrelation to services, 508\nsearch on streams, 467\nsingle-threaded execution, 448, 463\nstream analytics, 466\nstream joins, 472-476\nstream-stream join, 473\nstream-table join, 473\ntable-table join, 474\ntime-dependence of, 475\nstreams, 440-451\nend-to-end, pushing events to clients, 512\nmessaging systems (see messaging systems)\nprocessing (see stream processing)\nrelation to databases, 451-464\n(see also changelogs)\nAPI support for change streams, 456\nchange data capture, 454-457\nderivative of state by time, 460\nevent sourcing, 457-459\nkeeping systems in sync, 452-453\nphilosophy of immutable events,\n459-464\ntopics, 440\nstrict serializability, 329\nstrong consistency (see linearizability)\nstrong one-copy serializability, 329\nsubjects, predicates, and objects (in triple-\nstores), 55\nsubscribers (message streams), 440\n(see also consumers)\nsupercomputers, 275\nsurveillance, 537\n(see also privacy)\nSwagger (service definition format), 133\nswapping to disk (see virtual memory)\nsynchronous networks, 285, 557\ncomparison to asynchronous networks, 284\nformal model, 307\nsynchronous replication, 154, 557\nchain replication, 155\n586 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eedc25c1-8879-4ee7-b69f-bfc15b49c2bc": {"__data__": {"id_": "eedc25c1-8879-4ee7-b69f-bfc15b49c2bc", "embedding": null, "metadata": {"page_label": "587", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "521a8690-5463-439b-8c35-4cb6b8345bc9", "node_type": "4", "metadata": {"page_label": "587", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "7288649ed1c4615aa88b4581857b86c7c719422656d62283cdc670a36b267b4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "conflict detection, 172\nsystem models, 300, 306-310\nassumptions in, 528\ncorrectness of algorithms, 308\nmapping to the real world, 309\nsafety and liveness, 308\nsystems of record, 386, 557\nchange data capture, 454, 491\ntreating event log as, 460\nsystems thinking, 536\nT\nt-digest (algorithm), 16\ntable-table joins, 474\nTableau (data visualization software), 416\ntail (Unix tool), 447\ntail vertex (property graphs), 51\nTajo (query engine), 93\nTandem NonStop SQL (database), 200\nTCP (Transmission Control Protocol), 277\ncomparison to circuit switching, 285\ncomparison to UDP, 283\nconnection failures, 280\nflow control, 282, 441\npacket checksums, 306, 519, 529\nreliability and duplicate suppression, 517\nretransmission timeouts, 284\nuse for transaction sessions, 229\ntelemetry (see monitoring)\nTeradata (database), 93, 200\nterm-partitioned indexes, 208, 217\ntermination (consensus), 365\nTerrapin (database), 413\nTez (dataflow engine), 421-423\nfault tolerance, 422\nsupport by higher-level tools, 427\nthrashing (out of memory), 297\nthreads (concurrency)\nactor model, 138, 468\n(see also message-passing)\natomic operations, 223\nbackground threads, 73, 85\nexecution pauses, 286, 296-298\nmemory barriers, 338\npreemption, 298\nsingle (see single-threaded execution)\nthree-phase commit, 359\nThrift (data format), 117-121\nBinaryProtocol, 118\nCompactProtocol, 119\nfield tags and schema evolution, 120\nthroughput, 13, 390\nTIBCO, 137\nEnterprise Message Service, 444\nStreamBase (stream analytics), 466\ntime\nconcurrency and, 187\ncross-channel timing dependencies, 331\nin distributed systems, 287-299\n(see also clocks)\nclock synchronization and accuracy, 289\nrelying on synchronized clocks, 291-295\nprocess pauses, 295-299\nreasoning about, in stream processors,\n468-472\nevent time versus processing time, 469,\n477, 498\nknowing when window is ready, 470\ntimestamp of events, 471\ntypes of windows, 472\nsystem models for distributed systems, 307\ntime-dependence in stream joins, 475\ntime-of-day clocks, 288\ntimeliness, 524\ncoordination-avoiding data systems, 528\ncorrectness of dataflow systems, 525\ntimeouts, 279, 557\ndynamic configuration of, 284\nfor failover, 158\nlength of, 281\ntimestamps, 343\nassigning to events in stream processing,\n471\nfor read-after-write consistency, 163\nfor transaction ordering, 295\ninsufficiency for enforcing constraints, 347\nkey range partitioning by, 203\nLamport, 345\nlogical, 494\nordering events, 291, 345\nTitan (database), 50\ntombstones, 74, 191, 456\ntopics (messaging), 137, 440\ntotal order, 341, 557\nlimits of, 493\nsequence numbers or timestamps, 344\ntotal order broadcast, 348-352, 493, 522\nconsensus algorithms and, 366-368\nIndex | 587", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b986c154-fd8a-4979-9aad-6e31a29f0336": {"__data__": {"id_": "b986c154-fd8a-4979-9aad-6e31a29f0336", "embedding": null, "metadata": {"page_label": "588", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29c94398-ca63-42fa-bdc2-da01c5a7d075", "node_type": "4", "metadata": {"page_label": "588", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "748072ebc0ab41a7224239dedfebce054530980f5697aae3c663aea23348dc40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "implementation in ZooKeeper and etcd, 370\nimplementing with linearizable storage, 351\nusing, 349\nusing to implement linearizable storage, 350\ntracking behavioral data, 536\n(see also privacy)\ntransaction coordinator (see coordinator)\ntransaction manager (see coordinator)\ntransaction processing, 28, 90-95\ncomparison to analytics, 91\ncomparison to data warehousing, 93\ntransactions, 221-267, 558\nACID properties of, 223\natomicity, 223\nconsistency, 224\ndurability, 226\nisolation, 225\ncompensating (see compensating transac\u2010\ntions)\nconcept of, 222\ndistributed transactions, 352-364\navoiding, 492, 502, 521-528\nfailure amplification, 364, 495\nin doubt/uncertain status, 358, 362\ntwo-phase commit, 354-359\nuse of, 360-361\nXA transactions, 361-364\nOLTP versus analytics queries, 411\npurpose of, 222\nserializability, 251-266\nactual serial execution, 252-256\npessimistic versus optimistic concur\u2010\nrency control, 261\nserializable snapshot isolation (SSI),\n261-266\ntwo-phase locking (2PL), 257-261\nsingle-object and multi-object, 228-232\nhandling errors and aborts, 231\nneed for multi-object transactions, 231\nsingle-object writes, 230\nsnapshot isolation (see snapshots)\nweak isolation levels, 233-251\npreventing lost updates, 242-246\nread committed, 234-238\ntransitive closure (graph algorithm), 424\ntrie (data structure), 88\ntriggers (databases), 161, 441\nimplementing change data capture, 455\nimplementing replication, 161\ntriple-stores, 55-59\nSPARQL query language, 59\ntumbling windows (stream processing), 472\n(see also windows)\nin microbatching, 477\ntuple spaces (programming model), 507\nTurtle (RDF data format), 56\nTwitter\nconstructing home timelines (example), 11,\n462, 474, 511\nDistributedLog (event log), 448\nFinagle (RPC framework), 135\nSnowflake (sequence number generator),\n294\nSummingbird (processing library), 497\ntwo-phase commit (2PC), 353, 355-359, 558\nconfusion with two-phase locking, 356\ncoordinator failure, 358\ncoordinator recovery, 363\nhow it works, 357\nissues in practice, 363\nperformance cost, 360\ntransactions holding locks, 362\ntwo-phase locking (2PL), 257-261, 329, 558\nconfusion with two-phase commit, 356\nindex-range locks, 260\nperformance of, 258\ntype checking, dynamic versus static, 40\nU\nUDP (User Datagram Protocol)\ncomparison to TCP, 283\nmulticast, 442\nunbounded datasets, 439, 558\n(see also streams)\nunbounded delays, 558\nin networks, 282\nprocess pauses, 296\nunbundling databases, 499-515\ncomposing data storage technologies,\n499-504\nfederation versus unbundling, 501\nneed for high-level language, 503\ndesigning applications around dataflow,\n504-509\nobserving derived state, 509-515\nmaterialized views and caching, 510\nmulti-partition data processing, 514\npushing state changes to clients, 512\n588 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "362643ab-8026-4a88-ad53-9c1803807039": {"__data__": {"id_": "362643ab-8026-4a88-ad53-9c1803807039", "embedding": null, "metadata": {"page_label": "589", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d299fd8f-72ea-4fc8-a8a8-82f0fff1328c", "node_type": "4", "metadata": {"page_label": "589", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "76c9b19aed89115d2efef011e5cccb93cdf45d025f22ef375a17fde22fed00f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "uncertain (transaction status) (see in doubt)\nuniform consensus, 365\n(see also consensus)\nuniform interfaces, 395\nunion type (in Avro), 125\nuniq (Unix tool), 392\nuniqueness constraints\nasynchronously checked, 526\nrequiring consensus, 521\nrequiring linearizability, 330\nuniqueness in log-based messaging, 522\nUnix philosophy, 394-397\ncommand-line batch processing, 391-394\nUnix pipes versus dataflow engines, 423\ncomparison to Hadoop, 413-414\ncomparison to relational databases, 499, 501\ncomparison to stream processing, 464\ncomposability and uniform interfaces, 395\nloose coupling, 396\npipes, 394\nrelation to Hadoop, 499\nUPDATE statement (SQL), 40\nupdates\npreventing lost updates, 242-246\natomic write operations, 243\nautomatically detecting lost updates, 245\ncompare-and-set operations, 245\nconflict resolution and replication, 246\nusing explicit locking, 244\npreventing write skew, 246-251\nV\nvalidity (consensus), 365\nvBuckets (partitioning), 199\nvector clocks, 191\n(see also version vectors)\nvectorized processing, 99, 428\nverification, 528-533\navoiding blind trust, 530\nculture of, 530\ndesigning for auditability, 531\nend-to-end integrity checks, 531\ntools for auditable data systems, 532\nversion control systems, reliance on immutable\ndata, 463\nversion vectors, 177, 191\ncapturing causal dependencies, 343\nversus vector clocks, 191\nVertica (database), 93\nhandling writes, 101\nreplicas using different sort orders, 100\nvertical scaling (see scaling up)\nvertices (in graphs), 49\nproperty graph model, 50\nViewstamped Replication (consensus algo\u2010\nrithm), 366\nview number, 368\nvirtual machines, 146\n(see also cloud computing)\ncontext switches, 297\nnetwork performance, 282\nnoisy neighbors, 284\nreliability in cloud services, 8\nvirtualized clocks in, 290\nvirtual memory\nprocess pauses due to page faults, 14, 297\nversus memory management by databases,\n89\nVisiCalc (spreadsheets), 504\nvnodes (partitioning), 199\nVoice over IP (VoIP), 283\nVoldemort (database)\nbuilding read-only stores in batch processes,\n413\nhash partitioning, 203-204, 211\nleaderless replication, 177\nmulti-datacenter support, 184\nrebalancing, 213\nreliance on read repair, 179\nsloppy quorums, 184\nVoltDB (database)\ncross-partition serializability, 256\ndeterministic stored procedures, 255\nin-memory storage, 89\noutput streams, 456\nsecondary indexes, 207\nserial execution of transactions, 253\nstatement-based replication, 159, 479\ntransactions in stream processing, 477\nW\nWAL (write-ahead log), 82\nweb services (see services)\nWeb Services Description Language (WSDL),\n133\nwebhooks, 443\nwebMethods (messaging), 137\nWebSocket (protocol), 512\nIndex | 589", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13747bef-1046-4d49-90d0-0af8047029f5": {"__data__": {"id_": "13747bef-1046-4d49-90d0-0af8047029f5", "embedding": null, "metadata": {"page_label": "590", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ed38320-35a0-4b26-9be7-8d5a4eef7519", "node_type": "4", "metadata": {"page_label": "590", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "f772d61c32c1d1ee1acb214da8e3228d043683fbbf132e5e3f2fad3fd563991e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "windows (stream processing), 466, 468-472\ninfinite windows for changelogs, 467, 474\nknowing when all events have arrived, 470\nstream joins within a window, 473\ntypes of windows, 472\nwinners (conflict resolution), 173\nWITH RECURSIVE syntax (SQL), 54\nworkflows (MapReduce), 402\noutputs, 411-414\nkey-value stores, 412\nsearch indexes, 411\nwith map-side joins, 410\nworking set, 393\nwrite amplification, 84\nwrite path (derived data), 509\nwrite skew (transaction isolation), 246-251\ncharacterizing, 246-251, 262\nexamples of, 247, 249\nmaterializing conflicts, 251\noccurrence in practice, 529\nphantoms, 250\npreventing\nin snapshot isolation, 262-265\nin two-phase locking, 259-261\noptions for, 248\nwrite-ahead log (WAL), 82, 159\nwrites (database)\natomic write operations, 243\ndetecting writes affecting prior reads, 264\npreventing dirty writes with read commit\u2010\nted, 235\nWS-* framework, 133\n(see also services)\nWS-AtomicTransaction (2PC), 355\nX\nXA transactions, 355, 361-364\nheuristic decisions, 363\nlimitations of, 363\nxargs (Unix tool), 392, 396\nXML\nbinary variants, 115\nencoding RDF data, 57\nfor application data, issues with, 114\nin relational databases, 30, 41\nXSL/XPath, 45\nY\nYahoo!\nPistachio (database), 461\nSherpa (database), 455\nYARN (job scheduler), 416, 506\npreemption of jobs, 418\nuse of ZooKeeper, 370\nZ\nZab (consensus algorithm), 366\nuse in ZooKeeper, 353\nZeroMQ (messaging library), 442\nZooKeeper (coordination service), 370-373\ngenerating fencing tokens, 303, 349, 370\nlinearizable operations, 333, 351\nlocks and leader election, 330\nservice discovery, 372\nuse for partition assignment, 215, 371\nuse of Zab algorithm, 349, 353, 366\n590 | Index", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f41b7af-e4e1-45bc-8d39-ee2ecf409200": {"__data__": {"id_": "6f41b7af-e4e1-45bc-8d39-ee2ecf409200", "embedding": null, "metadata": {"page_label": "591", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3197b74-fa14-40df-8acc-33bd968f10b6", "node_type": "4", "metadata": {"page_label": "591", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}, "hash": "d5b852a05c94517f2600ad04ca848bb33b86d890546fa78b4e04b6983ab8d06e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "About the Author\nMartin Kleppmann is a researcher in distributed systems at the University of Cam\u2010\nbridge, UK. Previously he was a software engineer and entrepreneur at internet com\u2010\npanies including LinkedIn and Rapportive, where he worked on large-scale data\ninfrastructure. In the process he learned a few things the hard way, and he hopes this\nbook will save you from repeating the same mistakes.\nMartin is a regular conference speaker, blogger, and open source contributor. He\nbelieves that profound technical ideas should be accessible to everyone, and that\ndeeper understanding will help us develop better software.\nColophon\nThe animal on the cover of Designing Data-Intensive Applications  is an Indian wild\nboar (Sus scrofa cristatus), a subspecies of wild boar found in India, Myanmar, Nepal,\nSri Lanka, and Thailand. They are distinctive from European boars in that they have\nhigher back bristles, no woolly undercoat, and a larger, straighter skull.\nThe Indian wild boar has a coat of gray or black hair, with stiff bristles running along\nthe spine. Males have protruding canine teeth (called tushes) that are used to fight\nwith rivals or fend off predators. Males are larger than females, but the species aver\u2010\nages 33\u201335 inches tall at the shoulder and 200\u2013300 pounds in weight. Their natural\npredators include bears, tigers, and various big cats.\nThese animals are nocturnal and omnivorous\u2014they eat a wide variety of things,\nincluding roots, insects, carrion, nuts, berries, and small animals. Wild boars are also\nknown to root through garbage and crop fields, causing a great deal of destruction\nand earning the enmity of farmers. They need to eat 4,000\u20134,500 calories a day. Boars\nhave a well-developed sense of smell, which helps them forage for underground plant\nmaterial and burrowing animals. However, their eyesight is poor.\nWild boars have long held significance in human culture. In Hindu lore, the boar is\nan avatar of the god Vishnu. In ancient Greek funerary monuments, it was a symbol\nof a gallant loser (in contrast to the victorious lion). Due to its aggression, it was\ndepicted on the armor and weapons of Scandinavian, Germanic, and Anglo-Saxon\nwarriors. In the Chinese zodiac, it symbolizes determination and impetuosity.\nMany of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com.\nThe cover image is from Shaw\u2019s Zoology. The cover fonts are URW Typewriter and\nGuardian Sans. The text font is Adobe Minion Pro; the font in diagrams is Adobe\nMyriad Pro; the heading font is Adobe Myriad Condensed; and the code font is Dal\u2010\nton Maag\u2019s Ubuntu Mono.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"158b4de7-8a7e-4aeb-b7bf-c5d216ed1a23": {"node_ids": ["da934f90-7e2a-4913-94f7-f4f693618e6a"], "metadata": {"page_label": "Cover", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f17a44af-b268-42bc-9cc7-b4d76930c2ab": {"node_ids": ["64c4f981-23df-4f51-8e58-9e7b062924b2"], "metadata": {"page_label": "BackCover", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "803f0f88-0565-4a83-9050-ce9030447b37": {"node_ids": ["5b27adc0-5f42-4f41-beee-07f2029f8e40"], "metadata": {"page_label": "i", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d962c8b5-54cc-4680-be29-9586eb3e7519": {"node_ids": ["e2814700-a0d0-4423-b231-7fc15bd83b17"], "metadata": {"page_label": "ii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8be8a3ec-7583-40d3-ac97-2a239839d10b": {"node_ids": ["561613d5-88da-4b28-843d-e6694db14db9"], "metadata": {"page_label": "iii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7278b6a0-f9c7-49bd-a0e3-deaffbf982e7": {"node_ids": ["f6be9233-0bee-466e-b872-30cd57e71566"], "metadata": {"page_label": "iv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "59e1a370-fd70-40e6-8186-1014e376068f": {"node_ids": ["da7b41a6-830f-4c6a-895c-1d1cfaf0c24a"], "metadata": {"page_label": "v", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "45466ecd-1ae1-48a2-90fd-2398a596695a": {"node_ids": ["6f11c00a-0d78-4788-a7ca-2f2ed2ee24e8"], "metadata": {"page_label": "vi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b5661d06-2aae-4f13-8056-4e2db3b68406": {"node_ids": ["c5d9c598-3a7e-4c64-889b-701c8c1d0a2c"], "metadata": {"page_label": "vii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "38b20bf6-887c-4e02-bb3b-ece2d2f397ce": {"node_ids": ["32a1f12f-099a-4716-bce1-d9e47a3e63d1"], "metadata": {"page_label": "viii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "53b4b53a-1edc-428e-a3d5-7e6fc768608e": {"node_ids": ["2aff21b3-b096-470d-beaf-771591da1674"], "metadata": {"page_label": "ix", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5ee656fb-8bfe-4b51-822d-4eb09a6d2e06": {"node_ids": ["b9d4215b-0830-4e7e-9fe9-f865f50c2640"], "metadata": {"page_label": "x", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "96780b3c-3e8a-49ae-85b9-10e089161a90": {"node_ids": ["33292fe8-84ee-4ad3-8b7e-3e2edba505ed"], "metadata": {"page_label": "xi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7f70a001-7676-4bd5-9b3d-aeaa32f2cf4e": {"node_ids": ["feb08288-990c-4c69-880e-27b35286a12e"], "metadata": {"page_label": "xii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c9ccaf6f-00ac-4d62-aad7-7f976149b58f": {"node_ids": ["c70be966-a9dc-4884-b846-69f450cc34c3"], "metadata": {"page_label": "xiii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "57b2c53d-1303-4874-a654-481e669c432c": {"node_ids": ["771adf29-419d-466e-b73c-400a09313b67"], "metadata": {"page_label": "xiv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4fd3a217-c3bc-47a7-a2cc-3660c00ad8dd": {"node_ids": ["ec261c0b-7e21-4e43-ae4a-e8c393e6a089"], "metadata": {"page_label": "xv", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e36b77e3-bf09-4bdd-9aed-9e9095f7e345": {"node_ids": ["afb1c572-c830-49e6-953c-91b5838bb6ce"], "metadata": {"page_label": "xvi", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "949be309-81f7-4b2d-b8fb-b6e1c8243b8a": {"node_ids": ["da232409-ed36-4e1b-9e7a-ee174dab1226"], "metadata": {"page_label": "xvii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "333b3eb1-c28f-40bd-92b5-e0e2fa35310d": {"node_ids": ["ecd6489a-f60b-4f7f-91c3-93c2c2fb22f4"], "metadata": {"page_label": "xviii", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dede16a6-53e1-44ac-8b35-395e6e4fbf07": {"node_ids": ["8f710380-b0c9-44d0-b96e-77bd42e5b0af"], "metadata": {"page_label": "xix", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2c728a22-0319-46d7-a8ba-9fd84fe074df": {"node_ids": ["276e76e1-f1dd-45a0-bea9-caf7cc81111a"], "metadata": {"page_label": "xx", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "175f7a36-82be-4a56-81c6-4679d8ad2aa4": {"node_ids": ["ecd76de6-5648-489f-a243-15c87e1cced5"], "metadata": {"page_label": "1", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "cefe1f2c-0a0c-4644-a493-f1785b91d7fa": {"node_ids": ["3574d821-e7e4-4d0e-9f7b-352aabc0a303"], "metadata": {"page_label": "2", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b3cdb196-2131-48e5-a27a-a6244c693ba5": {"node_ids": ["a005d3f4-4167-41ab-b374-ed60eda103cb"], "metadata": {"page_label": "3", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "800c85e3-444a-4195-a9de-d5aabbe2cc68": {"node_ids": ["94f1f223-a88a-4ad1-94bb-6700d9f4b46f"], "metadata": {"page_label": "4", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0c065824-2202-4ddc-b76e-8b087036ad99": {"node_ids": ["20fee458-a49a-4c7e-ba21-0e5ea8c8609e"], "metadata": {"page_label": "5", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e90b83f0-de99-49a7-b817-3e17ed0ea68f": {"node_ids": ["a1afc7f8-e083-414b-9da6-a028894cf4fc"], "metadata": {"page_label": "6", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5d5e453c-0641-47c2-aaf0-7d96236a2634": {"node_ids": ["8d6071a6-9c88-40da-89ed-b72ca452bc97"], "metadata": {"page_label": "7", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9975b226-070a-4ce9-8252-29c09b570218": {"node_ids": ["5c9bc4b4-f952-457c-9fc0-74541858758d"], "metadata": {"page_label": "8", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6ae163d3-8fb6-45ff-9723-3aead34f0341": {"node_ids": ["12e1958a-06c6-4067-8be1-9a189801485e"], "metadata": {"page_label": "9", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8f618ba9-af3d-40ac-9cf7-a150ad794244": {"node_ids": ["c40765cc-eb46-4419-b9d0-9d55e71571d2"], "metadata": {"page_label": "10", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3d84d26b-4153-49da-a775-5697c59a786e": {"node_ids": ["a04a40f0-fca5-43da-9033-4ee60387574f"], "metadata": {"page_label": "11", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8cde927c-fa2c-4f44-bbb3-117e7192373d": {"node_ids": ["cb32bdbd-f078-418e-a3b2-aa4e86a9b38d"], "metadata": {"page_label": "12", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6ddd9787-8809-4b44-83d8-73f4e4c605bb": {"node_ids": ["49c4f477-90d5-4d38-a73e-1c7874cc47f3"], "metadata": {"page_label": "13", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "82681f19-cfd0-4373-8c02-5a140c7d4416": {"node_ids": ["6d13c574-23b7-4096-a223-9732f4846259"], "metadata": {"page_label": "14", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "33c010c0-cde3-4d06-8dae-aa630c182a01": {"node_ids": ["df49c6f6-dd50-4d70-9128-2a075a8de127"], "metadata": {"page_label": "15", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ebca9fb4-f907-467b-a919-92a54c84d9ae": {"node_ids": ["c9ff7186-5177-4874-a324-5b1949726998"], "metadata": {"page_label": "16", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "026a0aff-325d-4816-a4c7-ac3210891bba": {"node_ids": ["fbcd03c3-2428-48a8-a09e-2d45e3f8c6d1"], "metadata": {"page_label": "17", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "65c8d38a-117b-4a97-b221-12ec138c1fe0": {"node_ids": ["71416b74-ecd1-4437-b035-685abbf85a6f"], "metadata": {"page_label": "18", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0521757f-09a7-4b3f-a7d5-4ed986a399fc": {"node_ids": ["e3098fd5-4a3f-44f0-92b6-4f521b827017"], "metadata": {"page_label": "19", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "78dc8c45-d28c-44dd-8bbe-b1ac778d145e": {"node_ids": ["ed6d9857-bf66-4f99-b70c-dd2e83e927c9"], "metadata": {"page_label": "20", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4b4da5b9-4280-4507-ae6b-0236588afcbb": {"node_ids": ["e2a5b2f4-0b4d-48fd-87fe-68fc06298680"], "metadata": {"page_label": "21", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "86f26097-22cc-419d-90db-a1d6c342bcf3": {"node_ids": ["505b222f-a61e-4bf8-828b-92566105bba7"], "metadata": {"page_label": "22", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a3907149-1b10-4755-9b11-62c174602753": {"node_ids": ["e2ad2570-f543-4a8e-807b-6d145ccac168"], "metadata": {"page_label": "23", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "53f880cd-cd8b-4f03-b685-8e26a3ca7dae": {"node_ids": ["251474d9-8435-431b-89fc-9e2d6cfac3b7"], "metadata": {"page_label": "24", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b47728d4-e10c-490a-9da9-712f188da937": {"node_ids": ["e2499712-9e36-4c91-a963-0eb58f9750a1"], "metadata": {"page_label": "25", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3a343cc4-40f9-437b-8868-bccb9e00f012": {"node_ids": ["4adafff2-cc87-42de-bd97-cb62071b6080"], "metadata": {"page_label": "26", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "accfafda-224c-4f91-a224-3ded2f116879": {"node_ids": ["0a7f0a37-cef9-4724-8a49-6a5490c29d32"], "metadata": {"page_label": "27", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1cad1385-7571-4f49-a8d3-aa0846d34b47": {"node_ids": ["c8456caa-1e90-468b-b26e-ddaeda24a612"], "metadata": {"page_label": "28", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ce0ab60f-d163-449b-b741-2eb1c7578f18": {"node_ids": ["54819b5a-2f66-4d5a-bb5d-cba105077fa3"], "metadata": {"page_label": "29", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ebf38025-9a55-4f3b-8b77-d934a8affa83": {"node_ids": ["b7a641ac-aecf-42c1-bc58-751b2b3cbde9"], "metadata": {"page_label": "30", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4b0b820e-2840-455b-88fe-8136912889f5": {"node_ids": ["ef1fc31f-7cdc-4696-9560-d4d585421c35"], "metadata": {"page_label": "31", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "eea40c6e-3053-4fa0-9ae7-4ce87614052a": {"node_ids": ["8fd81cef-b9fc-4bab-926f-63ea33a0c8cc"], "metadata": {"page_label": "32", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "62a15cf9-434d-4e3b-aa87-461f55460f67": {"node_ids": ["11f2614e-7e73-49e8-85e2-498ed3156c00"], "metadata": {"page_label": "33", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "07b91301-91b9-4d5d-bee0-114c22b370c6": {"node_ids": ["a4ca28c6-d6eb-4d68-80e9-a5920132d524"], "metadata": {"page_label": "34", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "196ab068-bce1-4c40-bd5c-a25420c412d4": {"node_ids": ["dfddffbb-e575-42ad-b1ac-4ad7a436df4a"], "metadata": {"page_label": "35", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "13a1e471-4782-488c-88ae-ae8a07d45805": {"node_ids": ["75ba5197-8f29-49ae-ae1c-c8b9079e39d0"], "metadata": {"page_label": "36", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4174e529-5bac-40a6-a42c-71adf86f2e2b": {"node_ids": ["76ed2928-83f4-4f51-8822-ebb7791cc63e"], "metadata": {"page_label": "37", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "498e5d68-001a-486e-945e-7a4144989417": {"node_ids": ["3ae4615e-e690-4b91-ae76-72549a4a0a46"], "metadata": {"page_label": "38", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "30c6e9df-7d12-4968-b529-45020c041df0": {"node_ids": ["1b2fa6b5-85bb-48b4-8cb9-71cea62e572b"], "metadata": {"page_label": "39", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2b3a19c5-1ef0-4e37-b166-9dc1db7d6a37": {"node_ids": ["3542ce5a-27a2-4979-91f6-7f0c3ff069bd"], "metadata": {"page_label": "40", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3332c923-efb8-4443-a217-d8d349fe2a6d": {"node_ids": ["8c762345-63b9-45d7-82bb-da58d3d1df34"], "metadata": {"page_label": "41", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "64a28575-6889-4150-a276-f2b9ca6e08d7": {"node_ids": ["9078716e-2a23-47d1-8f06-1504f8685bfe"], "metadata": {"page_label": "42", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bc608379-63df-4bb9-a540-7ade84f76ec3": {"node_ids": ["b712d5dc-fc0f-426a-a3a6-2199d6354fb3"], "metadata": {"page_label": "43", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e3a94c32-7875-476e-af9a-fd1f1a3ae230": {"node_ids": ["7335fe9c-efe6-4626-8a31-4d8027db5439"], "metadata": {"page_label": "44", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5c165f2d-39ac-4820-9d2f-05b833b94bc5": {"node_ids": ["36c08017-b2c6-436d-98c2-dac376ca0b05"], "metadata": {"page_label": "45", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "39b200c9-1f24-4ed5-8ab3-1d71e1cd8ccd": {"node_ids": ["10019a92-e280-4e2c-bc85-5c1a67dea947"], "metadata": {"page_label": "46", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "72638843-dc45-402d-9618-a3a03b43bf9e": {"node_ids": ["8c8a5d68-d477-4c0a-ad3c-7b3663d069d5"], "metadata": {"page_label": "47", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e8aa1a36-af32-4dd5-a440-42a0ffc1fa99": {"node_ids": ["0c66afff-e8e1-48fd-86b6-ee6e91d74bc0"], "metadata": {"page_label": "48", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bf5a425c-53a0-43a8-812a-14d1e8963cbb": {"node_ids": ["b14badcd-adb7-44e1-aa5e-3282ffb91522"], "metadata": {"page_label": "49", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "10fe7c4e-0bda-4546-abe0-4b71ad91472f": {"node_ids": ["7d90f68a-6203-4908-9625-587075dbff69"], "metadata": {"page_label": "50", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6b885a6a-9e08-44ed-a3aa-5aaca44778f1": {"node_ids": ["775a2ebc-0d43-4d2c-af8b-2af5666f7edd"], "metadata": {"page_label": "51", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4e62801d-7a35-4ff1-aa17-642319b35c71": {"node_ids": ["dbd5f333-e39e-4083-8f37-acdf35019a1d"], "metadata": {"page_label": "52", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2ff38da6-8d1f-4878-bc32-af2d9b203592": {"node_ids": ["3c38e9b5-212c-4c9b-a68c-7bd0c93dd8e5"], "metadata": {"page_label": "53", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a6527f2a-9a0c-425e-8591-58e749edeb0c": {"node_ids": ["2dd7cb9f-5eab-4f78-b56b-5f0fe115a0db"], "metadata": {"page_label": "54", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "67a82af4-295d-4baa-88b8-dc93e18b293b": {"node_ids": ["66c29b72-8350-431d-aece-5355ab7d69ec"], "metadata": {"page_label": "55", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "47957cf9-b8c1-4fce-9d12-1767f143269e": {"node_ids": ["42f679b8-42d0-4ab7-9a90-4aeadfb05f06"], "metadata": {"page_label": "56", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "85c5bd4e-946d-4cd8-a653-51fec6567eb5": {"node_ids": ["5fbf53b6-7261-46f2-a5ae-a7f1aca25ebf"], "metadata": {"page_label": "57", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d22e775e-a19e-4e7f-b00e-06df2a4a163a": {"node_ids": ["cdbb1031-2d5f-4f47-97b9-bfb06d6cdb5b"], "metadata": {"page_label": "58", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0f92a0d6-1409-4419-a9c9-9ebed828fea4": {"node_ids": ["52dc0ab1-6f61-4e06-9298-0cca6ab08fa5"], "metadata": {"page_label": "59", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "58a41555-a20a-42a9-9b7b-9f4ab164f94f": {"node_ids": ["251ada01-d425-451f-a112-79422e782dad"], "metadata": {"page_label": "60", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1fb6742f-57e9-41d5-8c22-274b9d3c669d": {"node_ids": ["2e8b47db-184c-4cf3-b0e4-57b0110ff489"], "metadata": {"page_label": "61", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bd6c2965-d1ba-482e-aee1-51aaa8fca60b": {"node_ids": ["2bdf1dc6-99d4-45f5-83b4-ebc18ecda7c9"], "metadata": {"page_label": "62", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "75fe0343-1e02-4bcc-81bb-85c584e638dd": {"node_ids": ["059042a4-b304-419e-8331-d94647c3e944"], "metadata": {"page_label": "63", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "59494823-b19d-403b-ad05-1360189fd0e7": {"node_ids": ["c24b5add-3b39-4c58-94af-e7a178e4cff1"], "metadata": {"page_label": "64", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2d0fecaa-1342-4773-9153-ce1f0673bb71": {"node_ids": ["1bf032ee-2faf-4376-8694-8fa7854745cd"], "metadata": {"page_label": "65", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a0202601-d35a-4778-97e8-1416cb1d40b0": {"node_ids": ["515847bb-a4cb-49e0-9d2b-3d71bb4a486d"], "metadata": {"page_label": "66", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "47496805-f1bd-4ac5-9b88-61e65eab326f": {"node_ids": ["87e16443-3e79-4601-bb84-50c6cbfc0139"], "metadata": {"page_label": "67", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8016b10d-8686-4b02-aabf-7264c1e5dd51": {"node_ids": ["e2e3068a-e3cf-4456-8edd-365a3f0f9296"], "metadata": {"page_label": "68", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "23d8dc60-6c7d-444b-bd39-f2695060e598": {"node_ids": ["d666d74e-6203-4dfa-a846-13a88a489434"], "metadata": {"page_label": "69", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c15c24fc-48d4-4048-8a31-5b13804b3eac": {"node_ids": ["6cde527f-9cab-454b-bf45-3e622a7976e8"], "metadata": {"page_label": "70", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2d95fa0b-5348-4e94-9d2b-ec09375e4c0a": {"node_ids": ["57d88b82-34ef-4dca-bf49-47cf0f71e03e"], "metadata": {"page_label": "71", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "29ce8a71-ebdf-4fe4-90a7-df77cadc0597": {"node_ids": ["c9d594cc-8ba0-4445-850f-49779355b3df"], "metadata": {"page_label": "72", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "491ea2be-1772-4dce-a89d-bca86f9f0a1b": {"node_ids": ["a4dcdf71-a18a-4029-a1cc-b5a45c22fcaa"], "metadata": {"page_label": "73", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "73f18ff8-6c82-4fec-9751-419c75de6f77": {"node_ids": ["11f1b5b1-a4df-432f-bad0-08991e0e0f9d"], "metadata": {"page_label": "74", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "874ecc9e-2088-40a3-8c9b-a0149c34740b": {"node_ids": ["9a3961d5-07b0-4472-b2e0-c27b83ed1d05"], "metadata": {"page_label": "75", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "537bc11f-d29d-40de-973f-98eaf9b3e3bf": {"node_ids": ["cb42dbde-a4c4-4548-b94d-be9f51b6f48b"], "metadata": {"page_label": "76", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6476d238-12ee-4cf3-9af7-69a88bfcd53f": {"node_ids": ["62fbab34-2776-4dc8-a1d5-edca9b67141c"], "metadata": {"page_label": "77", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "52691c9d-5a8d-49b2-88b6-bf5d28fff13f": {"node_ids": ["e9037245-9f85-4d43-802b-92141bc9dc7b"], "metadata": {"page_label": "78", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "362504aa-2f4a-460d-b953-e469544af73b": {"node_ids": ["8e0ef7ca-e028-4fec-b94e-82efdaa58695"], "metadata": {"page_label": "79", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d08ab820-f6d7-4188-9b8a-3d4b18ad6fc2": {"node_ids": ["16ed9bfb-cb76-4bd1-b6c2-6013827f9e0a"], "metadata": {"page_label": "80", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "33be2da0-97eb-4239-9ff7-d7d9e5a8de60": {"node_ids": ["38faacd5-3f7a-46ad-b06c-3068129eb40b"], "metadata": {"page_label": "81", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "749d3916-1057-456f-b57e-a87090801b78": {"node_ids": ["a460fc7c-baf3-41a4-b636-73848f105665"], "metadata": {"page_label": "82", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "879c0ea2-9517-4705-ad8e-e422d4f0c535": {"node_ids": ["c0815e79-e88e-4800-9399-feb5f7747389"], "metadata": {"page_label": "83", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5f18ff42-25b2-4874-b9c1-711b6730375d": {"node_ids": ["b10d7e57-0256-411f-80ef-5cdf0f6bbb37"], "metadata": {"page_label": "84", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "21d53f44-d41a-43a1-943c-f709044405f4": {"node_ids": ["e001bf3c-f75c-4cb8-b2db-ecdc44e18bc0"], "metadata": {"page_label": "85", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7c087c30-a793-40a8-98b7-f2b2709f5e77": {"node_ids": ["74a863ca-a564-46a5-a900-9cb8332a30cb"], "metadata": {"page_label": "86", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "77cf12e8-62e4-4986-af61-c9c8c1c412a0": {"node_ids": ["1ccee254-1286-41ac-8fa1-1448015dd57d"], "metadata": {"page_label": "87", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d92d8bdb-8825-4a32-801b-f4742dcb6c39": {"node_ids": ["bc55e92f-4ed7-4ad8-b5a2-dc697a698b37"], "metadata": {"page_label": "88", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "83d23e43-aee3-46b0-8eab-920371d96849": {"node_ids": ["3bac6414-d57f-419d-bb1c-04f016018f4d"], "metadata": {"page_label": "89", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d6624e4f-7dbd-4716-b413-cd66f0055a69": {"node_ids": ["a4e76447-c943-4995-986f-4fbe37003ffc"], "metadata": {"page_label": "90", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "951c4c49-2d38-4604-a115-0f8dff56ed80": {"node_ids": ["0b12f8a0-7c7a-484d-8de8-4c7a1e66ec4f"], "metadata": {"page_label": "91", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "73b29697-d1cb-4732-a6d9-815a63da0d05": {"node_ids": ["0250f6cd-9ffc-4d0a-8061-7d4677fda721"], "metadata": {"page_label": "92", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "25ec912a-b6f7-45d4-b7d3-b03cefd03e07": {"node_ids": ["121ee863-9238-4561-99cd-46b3adbcd66e"], "metadata": {"page_label": "93", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "eb1b3245-8f85-498b-8c99-acfb26d5c969": {"node_ids": ["9dd69bfe-0179-4c4b-984a-59f9361b9a22"], "metadata": {"page_label": "94", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dfd9bfb1-094d-4f66-be09-a912dcd9a4dc": {"node_ids": ["ce78b252-4ee3-4359-8dc7-e588c899ce7c"], "metadata": {"page_label": "95", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "59e17e04-9380-4f53-9ac7-e9f2a0385485": {"node_ids": ["315d6c44-089f-4c97-a8df-c067e81bdbfe"], "metadata": {"page_label": "96", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4c8ef361-db9c-4853-a030-522ec6535aee": {"node_ids": ["94475ae2-be3f-4584-933d-a4ecd97b0bbe"], "metadata": {"page_label": "97", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d001133d-911d-4af4-9a0a-7ba952670862": {"node_ids": ["479f0d68-dbf1-47a8-bf24-fe666cdbd224"], "metadata": {"page_label": "98", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4f1b0618-4642-4184-9aeb-70e4323fa0fd": {"node_ids": ["28385301-280e-422e-b172-34d08728c15a"], "metadata": {"page_label": "99", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dc798519-767a-430c-a81a-dc7c36638aa6": {"node_ids": ["8d0639f9-d01e-4ed8-97bf-b4ed07d0bddf"], "metadata": {"page_label": "100", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "124b4c50-fdd5-4ebe-856d-839d9f1538b1": {"node_ids": ["ffdb2205-9058-41a4-b42b-663cdeae02d5"], "metadata": {"page_label": "101", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7fb080f3-6d65-484c-9181-880033258288": {"node_ids": ["3eb283ba-a6d0-4c5a-ad84-d7b83a5371bc"], "metadata": {"page_label": "102", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3302a6dd-22f8-4137-8488-8a627e163b02": {"node_ids": ["2ff95447-7a05-4d05-b932-2af443623148"], "metadata": {"page_label": "103", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f8a9a860-2380-4bd7-842d-71fe3fdb8f6c": {"node_ids": ["da43aa1a-7381-4b58-9b35-6520c780183f"], "metadata": {"page_label": "104", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "12290d7f-3841-4666-a602-db5ddba1c7c2": {"node_ids": ["803a94af-7bfd-438a-932e-914b7fdaa30d"], "metadata": {"page_label": "105", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8de76ca1-1478-4cf3-b08c-af18a5ae2ff6": {"node_ids": ["61d4e394-6479-469b-9d78-479679e71c70"], "metadata": {"page_label": "106", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "63fa029d-cf64-41f9-bd0d-d4bfc46210db": {"node_ids": ["f251fcfa-904b-4e5d-846e-0e3fd9e9c99b"], "metadata": {"page_label": "107", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "35dfe2e9-f655-49f3-9b1c-fc17d09e9a01": {"node_ids": ["d261c2a7-41d1-4a96-83ed-20c73daf661f"], "metadata": {"page_label": "108", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "eda0303e-0beb-49be-8d08-3ee5d9c8aea6": {"node_ids": ["e2955a4c-b889-4a6c-b478-078006eb11c5"], "metadata": {"page_label": "109", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b73c1a03-565b-4223-baa0-5d9d0417dcc7": {"node_ids": ["48b3a700-4052-4141-a777-b7423a9f8998"], "metadata": {"page_label": "110", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "345a8a3d-4e15-4d26-82b8-ec307d36e7ba": {"node_ids": ["b8261a33-323c-45fe-a8dd-40033faaf376"], "metadata": {"page_label": "111", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "eed8740c-838b-4ea7-8d9e-24766f5de35a": {"node_ids": ["575fec38-86d0-4072-8f68-bf6c5817834e"], "metadata": {"page_label": "112", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e186f9ad-7aeb-4bb8-839d-39009f37fbd4": {"node_ids": ["22500a9d-7f9d-4f63-bc64-e47857a76df4"], "metadata": {"page_label": "113", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4a598210-0d54-44a9-9502-ec10dfed78e1": {"node_ids": ["cc4ee4f3-4aa7-4647-a69c-47593e9970cf"], "metadata": {"page_label": "114", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0936adf8-4efe-4e42-a51a-30e6254e307d": {"node_ids": ["4794e613-bb4b-48c4-b6a9-21d09533b7a5"], "metadata": {"page_label": "115", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1c9b9f51-7f61-466d-b092-3e61f64ca6e4": {"node_ids": ["e111f854-1f76-4cc5-b8d4-f25ddd2c98e2"], "metadata": {"page_label": "116", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f63d7ccc-a7dd-4fea-9c51-4d82b8b42c59": {"node_ids": ["549c8289-fcb9-48f2-ae17-8a1aa78c18f5"], "metadata": {"page_label": "117", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c2ae9afc-ed09-4c1e-a1bf-ee09e7130048": {"node_ids": ["7792c5ed-984f-42d6-b4b3-05d9d506e0af"], "metadata": {"page_label": "118", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3ea71c8f-e875-4f17-9a99-d87468767bd7": {"node_ids": ["afdcc41b-4d19-47b7-9cec-407925fa2ec6"], "metadata": {"page_label": "119", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "88dbeb51-81c5-46a7-8629-780d2e47b287": {"node_ids": ["77bd090e-0514-4d8e-980b-de696eb48a71"], "metadata": {"page_label": "120", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f6e43daa-7ddc-4891-a997-03a0736954bc": {"node_ids": ["bb14270a-374d-41aa-994b-f95444f807e2"], "metadata": {"page_label": "121", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "60f0ca57-3352-4ff5-86df-948bd5358ff7": {"node_ids": ["a13a39c5-021e-47ce-86c7-6669dece0b96"], "metadata": {"page_label": "122", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "95b20aef-433d-4a25-874a-ebe075d8b4b5": {"node_ids": ["745238bb-3789-4c80-afd6-4360f2a4edaf"], "metadata": {"page_label": "123", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c4c09667-167e-4ce0-8055-26b96db88d24": {"node_ids": ["02db40c4-fbf8-4be8-b877-95f6f6677d42"], "metadata": {"page_label": "124", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c0037b18-1040-4e5d-ad92-d82bc18c1858": {"node_ids": ["c8e13435-2962-42f5-8193-0b3979e8dfd0"], "metadata": {"page_label": "125", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0b90b9ca-ce91-44cc-a62a-2d3c5b43aa71": {"node_ids": ["6b395a0c-ae9c-4b6c-ae07-641d93c5681a"], "metadata": {"page_label": "126", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d66b00b1-e2da-4ec0-8683-65c1b2ea9b5b": {"node_ids": ["1555d286-0a8e-42b2-8551-a3c3443bc9fc"], "metadata": {"page_label": "127", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8c559614-084b-43ff-9cfc-7ead488ee6cc": {"node_ids": ["93bd6082-5127-4f7c-86ac-ca615f90047f"], "metadata": {"page_label": "128", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4ce965e6-4a71-4430-a8ef-acb6044f9a42": {"node_ids": ["24ec6315-6c4c-4ca0-97e4-8d51191731b8"], "metadata": {"page_label": "129", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "67076165-aa9b-411b-9022-003bf9b5318a": {"node_ids": ["67d26cdb-8f65-42aa-8921-1204673e96c0"], "metadata": {"page_label": "130", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "af130a67-c49e-43bf-9b58-50a950c1ff08": {"node_ids": ["6e930f6d-603c-4e78-abd2-61b3fc0385e4"], "metadata": {"page_label": "131", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "eb13fee7-e50f-4d0d-9f2a-4ba29c7bc046": {"node_ids": ["66329508-b3e0-4025-8e62-5fa7fd9cd628"], "metadata": {"page_label": "132", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e73266c0-a56a-46f5-aca6-5ae3e27fa699": {"node_ids": ["54fac147-2b7d-4043-9fce-fdfb8d1128f9"], "metadata": {"page_label": "133", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0e8b2615-740c-4739-9cad-9d4bc88a465b": {"node_ids": ["fcff0f9c-6df2-447e-aef0-7b572c15d24c"], "metadata": {"page_label": "134", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "714d2d41-4756-48a3-86a5-64b8f6f54d97": {"node_ids": ["07ecd657-02aa-4eaa-9692-67144c0ad1e1"], "metadata": {"page_label": "135", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "db0cebe4-09ec-4fcf-9523-bb6f5fe36294": {"node_ids": ["fb60e919-c254-4cd3-8f0c-21441eee671c"], "metadata": {"page_label": "136", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d35650d0-7ec1-403f-b4fc-8b39878e25f7": {"node_ids": ["a89947bb-55f3-4541-96b1-0302003c95ef"], "metadata": {"page_label": "137", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "75584461-ea9d-4cc5-92f7-49cc25cdd0e9": {"node_ids": ["53373e60-a5fe-4ab1-b552-6cbfa9f8ea4a"], "metadata": {"page_label": "138", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "85b9f8fe-8a24-432d-b7db-a45a78223a81": {"node_ids": ["46bac0e0-f70c-4c81-86a2-1dbf66d1127d"], "metadata": {"page_label": "139", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "46b57060-f7ac-4571-a868-c6e91749782b": {"node_ids": ["13df25be-addf-4d90-adf8-c00769ded84d"], "metadata": {"page_label": "140", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "eb09f4e1-d0af-4f9d-9124-6ef7f7280147": {"node_ids": ["089f6d1c-aa88-442a-a81a-38c9a3f9b675"], "metadata": {"page_label": "141", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "919841c8-0688-4df9-acc7-e86a7f2ae6d8": {"node_ids": ["f94e1014-f7be-4e97-b2e5-8029c6d82de0"], "metadata": {"page_label": "142", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "03f2be03-5506-49cd-b146-24fcbe01f957": {"node_ids": ["f2cde020-300d-4fbd-aab6-9e7ca7c1c4de"], "metadata": {"page_label": "143", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b40ad265-d181-4cef-a479-eaec0cd332c6": {"node_ids": ["f265f3e8-dd1c-4ba1-8040-a76fd1de876c"], "metadata": {"page_label": "144", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "593a8ed3-5fa6-493c-b5ad-513122c2e444": {"node_ids": ["d11501cf-dc33-4b0b-a783-87d9d2fff5da"], "metadata": {"page_label": "145", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "95768b69-1d3e-4936-b120-59e38eabf579": {"node_ids": ["b590a616-c38e-48ee-ad45-fe1849e1500f"], "metadata": {"page_label": "146", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8b6f7b7b-52dc-477e-ae8b-97277996c19d": {"node_ids": ["dd639d1d-f3a2-4074-aada-647a6da30957"], "metadata": {"page_label": "147", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "195164b3-d902-4e34-ab9c-488282e01f97": {"node_ids": ["689277d6-465d-42ba-9d9f-c32d41e7bf3f"], "metadata": {"page_label": "148", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c921a356-bdc5-4096-942b-d82378df18b4": {"node_ids": ["d844c2cd-126c-4f79-9fa0-70802e22894c"], "metadata": {"page_label": "149", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8e8bb51e-8cbe-4634-ac6c-98bd7211de12": {"node_ids": ["8949d0cb-c011-4dbb-9327-1e40d32eb000"], "metadata": {"page_label": "150", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b38ed499-9796-446d-ad50-333c5414f666": {"node_ids": ["b1c16112-5316-45ce-9559-a73cf5a73dc2"], "metadata": {"page_label": "151", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1d2bf0bf-8cb5-4476-aae5-3b1d76c63966": {"node_ids": ["3af368f9-a3e5-4145-97d9-90efff86a7ff"], "metadata": {"page_label": "152", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9131b24f-6d97-4bd8-a5a0-355b25b4db76": {"node_ids": ["538ebd86-7ef1-4052-8b60-0156284a8acc"], "metadata": {"page_label": "153", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bec3700a-1655-4ff5-9f03-c325b45911aa": {"node_ids": ["42f75abf-b44e-4618-8e63-ccc94cad1d8a"], "metadata": {"page_label": "154", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b31b14b2-30c3-4d47-ae4d-c5fa2eeef456": {"node_ids": ["f9ed0a3e-b517-4224-8883-e2cea1a0296a"], "metadata": {"page_label": "155", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a7a48cd7-8ab2-4550-9ebf-8f7baa43058a": {"node_ids": ["a6dca04c-7115-4346-af1b-ca27ec7d12af"], "metadata": {"page_label": "156", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b2c8bbd8-bcad-479e-816e-199bff35e224": {"node_ids": ["06591038-061d-4627-92a0-45a02758b81b"], "metadata": {"page_label": "157", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5605c88d-26f9-4dd9-ae25-29adf6c839c9": {"node_ids": ["cc09b4e5-fd95-44fb-8eda-33365f120b39"], "metadata": {"page_label": "158", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "de4c517d-cab1-4e88-9e98-32bd65f36774": {"node_ids": ["1be1323b-77fb-4bc0-ba12-80dfce49d216"], "metadata": {"page_label": "159", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d5b9410c-6060-461f-80c9-1a079d431dff": {"node_ids": ["d03c1fc6-30d7-4b3b-a4b6-9c979fd2ce56"], "metadata": {"page_label": "160", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9ab61b82-4266-45c7-a720-45f93c985cf6": {"node_ids": ["5995a41e-4da1-4b08-b385-db4e083057ac"], "metadata": {"page_label": "161", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0f778f31-41aa-4d34-a48e-9e3b4cf2a339": {"node_ids": ["a7d43dc1-4b4c-4e91-9985-48825f58d304"], "metadata": {"page_label": "162", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b6911630-7e84-41ec-8d57-941199bb45ea": {"node_ids": ["2e95e850-091a-4987-a3cb-f0c51a045e15"], "metadata": {"page_label": "163", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "843e01eb-cd13-46f1-9318-12dab5e46341": {"node_ids": ["8e7f94ab-4aa8-4f02-8675-1c62edda9592"], "metadata": {"page_label": "164", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f9f956fb-1413-4767-b5c3-3c2a1ea68001": {"node_ids": ["9a3e0d76-d20c-42ab-8265-d6d8e1a86b88"], "metadata": {"page_label": "165", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "11e11b6a-99b7-4449-9e82-3691af2e3ae6": {"node_ids": ["63d9fd76-b0f9-41f3-9bbc-8b495ea19308"], "metadata": {"page_label": "166", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1b5388e7-9b69-478c-9d74-5deece8363bd": {"node_ids": ["8633b959-2abd-4958-9d23-3321ddd50ec6"], "metadata": {"page_label": "167", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "59c620fa-c73e-44a5-8f52-bcc51834e567": {"node_ids": ["369a1355-c2b2-4637-a58b-52ff004c673e"], "metadata": {"page_label": "168", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "384aa737-d647-432e-a3e6-687a03a38726": {"node_ids": ["51c4f586-792b-4d4a-a513-d2d145c65833"], "metadata": {"page_label": "169", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7505b684-5e5d-4c1c-a0dd-c21bb425195d": {"node_ids": ["2e53834e-91c3-4f62-8285-2d0b345de351"], "metadata": {"page_label": "170", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "95399a02-52c2-46bb-8915-1ae0772afbb1": {"node_ids": ["008e0dfd-2f90-48f7-a1c1-0357f872896f"], "metadata": {"page_label": "171", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8a0593a2-5395-4885-b6a2-735b2177eb55": {"node_ids": ["040ea64c-368e-4513-baf7-2ed2063a6898"], "metadata": {"page_label": "172", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d6d5b278-e94f-4115-8c0f-1bdf21f37202": {"node_ids": ["8ce47405-cd92-4f1c-946d-e335e1b80d5f"], "metadata": {"page_label": "173", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "483ae7a9-035f-4a75-a3e1-fbe28e3109a8": {"node_ids": ["bef59ae7-e996-4b0e-b713-20c54b3cc47d"], "metadata": {"page_label": "174", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "504a606a-c84d-49ae-8b52-47873bd0cb30": {"node_ids": ["61444208-b3a9-4ac7-b4bc-8970843cb03b"], "metadata": {"page_label": "175", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d45a1def-af70-4f6a-8155-9f98e7e5ca47": {"node_ids": ["4068028f-2ed8-4790-b6ca-a65a277cb6e8"], "metadata": {"page_label": "176", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2a9c34b7-7bfb-4d69-82cd-92ea5ce978ad": {"node_ids": ["eec6fcbe-c5bb-4104-838c-f384f5a7f3c9"], "metadata": {"page_label": "177", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b7389cd1-68cc-4697-b77c-eb76d0ddcba1": {"node_ids": ["6dcb78a9-b7e0-4b39-bd02-ca962a5f6b47"], "metadata": {"page_label": "178", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d52ea854-7804-4c8a-81d1-d3545322ed7a": {"node_ids": ["824e6936-163e-4fe3-9f48-0a4219069a0e"], "metadata": {"page_label": "179", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "00f86e02-8911-4ead-8cc7-13ffb0337b37": {"node_ids": ["b2309a15-2930-4f20-869a-c695476365c0"], "metadata": {"page_label": "180", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "05ea65a2-5eeb-4a89-b854-7eafe2180cf9": {"node_ids": ["88cb5cb9-9069-444d-9a07-3fb26d80a2f7"], "metadata": {"page_label": "181", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "034d0216-d8c6-4196-9f1e-82d36a15ad8c": {"node_ids": ["b3d89ad7-ef1b-4122-be2c-abd4aced6d83"], "metadata": {"page_label": "182", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8c4fab6e-1390-46bd-a4d6-2fd115973002": {"node_ids": ["115be742-f81f-41c1-9074-bb633d086ace"], "metadata": {"page_label": "183", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0dbcf08e-010c-4586-abb4-bc3f8e1b7972": {"node_ids": ["36e511cf-918e-4171-af39-9dbe13e45955"], "metadata": {"page_label": "184", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "26bcc9c5-b2c5-439a-bcd5-59f0d7b6314e": {"node_ids": ["df2aaf5c-c017-4f00-809a-350b95f2122e"], "metadata": {"page_label": "185", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dadd8d88-008b-4a35-b3ce-9c1d6a089e5c": {"node_ids": ["0cdd17b2-cdf0-4c0f-b8af-8d185635be8f"], "metadata": {"page_label": "186", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e23216a8-c2b3-421d-a992-d9008946d8e6": {"node_ids": ["8d6de103-3582-4e1a-9d46-94ab401ddaf4"], "metadata": {"page_label": "187", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8f549920-ed69-436a-9b02-bef620480b9d": {"node_ids": ["3ffee36e-827f-4d1d-af4b-9cfeb18d25ff"], "metadata": {"page_label": "188", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "388cb89f-2a82-46df-a496-441d6df6e64c": {"node_ids": ["764c594e-5898-4204-8d91-d03a863677c9"], "metadata": {"page_label": "189", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2c683c1c-e3e0-4220-932a-51301c1b07e4": {"node_ids": ["147f5959-c780-4186-94c9-afa4d07738cb"], "metadata": {"page_label": "190", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a5561d24-1aca-4b2b-b322-dd5ea7226001": {"node_ids": ["c912a3d4-3a4e-4b91-b3d5-701f6386ebed"], "metadata": {"page_label": "191", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "79809051-c57e-4978-bfee-b8fc6ca852b1": {"node_ids": ["0e32ae60-6dbc-4500-a4cb-838898df0327"], "metadata": {"page_label": "192", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8d237f0d-f526-4cdd-a73f-52644e70bd0b": {"node_ids": ["5392ead7-bccc-443c-8969-14c5599cda9d"], "metadata": {"page_label": "193", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b2bd46fd-96a9-4435-bd28-c07e274925c1": {"node_ids": ["f95ce866-380c-4f98-be2f-abb011bcf3e7"], "metadata": {"page_label": "194", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "017d6d9c-ed45-4169-95a5-59ccd6a35de0": {"node_ids": ["88d8b002-84a7-4dc6-82ab-73c1837b6d86"], "metadata": {"page_label": "195", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1aff4c98-de25-4051-800d-7b57d2ae66ab": {"node_ids": ["7dcc5b5f-efa4-4281-b101-f3f4b0f9c4d3"], "metadata": {"page_label": "196", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e514d2da-7c09-41be-b0a7-46492578fed4": {"node_ids": ["006f11ba-5d3e-4943-8360-5998e1e44070"], "metadata": {"page_label": "197", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c6aad94f-5348-48cd-b55f-99dca572b39e": {"node_ids": ["4696ebd6-50ec-4f02-9aa4-bfac0f275a0e"], "metadata": {"page_label": "198", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e3ec671b-22ae-47e2-bb4b-fda3fc2fcfe0": {"node_ids": ["55d98d56-baf8-4f8a-a8e7-fa5399ad62fd"], "metadata": {"page_label": "199", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2d55a1f2-1d5b-467e-92ec-332f0adc5247": {"node_ids": ["3edc449f-3b74-4248-9c91-942de2c19985"], "metadata": {"page_label": "200", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f372b5ec-85ea-449c-9e78-4f9d9ed6f908": {"node_ids": ["65cf5177-e10b-4eef-b04a-e8b698530545"], "metadata": {"page_label": "201", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9bce9237-fefe-40d4-a719-c3f2e335cbf8": {"node_ids": ["2113153b-ccac-4a32-a2b6-1ab608e64121"], "metadata": {"page_label": "202", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1818347e-aa20-4baa-a8b6-c64d667cc55e": {"node_ids": ["83e9b5ba-d9ee-416e-9033-823f6e7f1772"], "metadata": {"page_label": "203", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d3cf5a39-ad8a-4105-abd2-261a9d97db68": {"node_ids": ["3dc37a75-3b4a-4252-ae20-bafaae4d5922"], "metadata": {"page_label": "204", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3c384dbd-602b-40bb-b715-e2917af954e6": {"node_ids": ["5186f191-47f2-472d-94ef-6795f19b7e3c"], "metadata": {"page_label": "205", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d641463c-1bef-4129-a1d2-fcca7062f840": {"node_ids": ["15bd0a88-87eb-4559-8c40-ef0c0f6b540d"], "metadata": {"page_label": "206", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b3d6b3e9-d57e-47e8-b6ca-d763851c7509": {"node_ids": ["9105c188-1b32-4f20-ad2c-c9c006c8bc54"], "metadata": {"page_label": "207", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0ae2a488-63eb-424d-a1bb-1bb5fc66abb8": {"node_ids": ["affee9d9-3086-4424-a1f8-29050ac1aec7"], "metadata": {"page_label": "208", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ae70fef7-8300-4626-9199-205f444fe720": {"node_ids": ["22070ba0-eb69-4e64-897e-a5d04a38f9df"], "metadata": {"page_label": "209", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a63d4b5a-cd72-4508-bd4e-3bab18a77731": {"node_ids": ["faae5c7d-8997-4203-9de1-90d49e1e96f4"], "metadata": {"page_label": "210", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5f282b10-aaf4-4c9e-a7ea-ba2fbe971e76": {"node_ids": ["b3f78a50-c1e4-4c71-bdb5-fa0b9317d810"], "metadata": {"page_label": "211", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b73212b3-5dbc-4053-8fd5-6ec06b7ea30f": {"node_ids": ["f2531531-5bb9-43ad-8619-b2f88c53f3ee"], "metadata": {"page_label": "212", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b5fe18cb-8e22-4ebc-b789-315eb7098b4a": {"node_ids": ["ecd41e61-2c04-40db-942a-8dd73f3e5da7"], "metadata": {"page_label": "213", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4f8184cb-8aaa-4941-9835-ca2235fc1473": {"node_ids": ["c737e3c2-b6a6-4b8c-9a28-98c814539daa"], "metadata": {"page_label": "214", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "55e64ba1-3626-4432-9553-4a725df12809": {"node_ids": ["9fd34988-116e-47c8-a946-ec2634a1b4a4"], "metadata": {"page_label": "215", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d4d55bfe-7b55-41c1-b30b-0a02e339b3e9": {"node_ids": ["9487f11c-0654-459d-81fe-c5d8df12c1d7"], "metadata": {"page_label": "216", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ddea5aad-c97b-4e50-823c-791b6faea3a6": {"node_ids": ["f88ee966-7ef3-43f0-bd85-05f3ad109188"], "metadata": {"page_label": "217", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "807f5503-36e6-4f89-a513-2bbfbeaba8c5": {"node_ids": ["f6d648ac-d93b-42dc-9a73-e0ee9a0db712"], "metadata": {"page_label": "218", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "12d93309-5e41-4d01-9f3f-025abf8714aa": {"node_ids": ["a49adf62-2366-47cf-a697-10914acbedb3"], "metadata": {"page_label": "219", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7e02434b-e831-4ec2-807a-1f1eee14ba9f": {"node_ids": ["18613643-c614-40ff-a3a8-83e90f90f38f"], "metadata": {"page_label": "220", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4b60672d-8eb8-43c9-b9de-d82902f8df0f": {"node_ids": ["56904f13-9976-4cc6-a96e-af0458a62978"], "metadata": {"page_label": "221", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dfebe986-0279-4307-aa33-6308ee7cd67f": {"node_ids": ["86b31c19-7569-4742-a08a-7a1c8d3ab429"], "metadata": {"page_label": "222", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "86ffb901-20ea-4f43-a051-a25a91325180": {"node_ids": ["fb4c5da9-5bc6-4962-ac8f-39ed2346b3e7"], "metadata": {"page_label": "223", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "91cc06b7-7f6d-4bc6-92e5-e9ec0d649a14": {"node_ids": ["a9255cc5-dffa-4b58-91c7-cf72f0e70c6e"], "metadata": {"page_label": "224", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5d3fd03e-4fd8-44d8-b7ef-393253e96071": {"node_ids": ["bb120704-295b-4610-84b0-b95237f733b0"], "metadata": {"page_label": "225", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9438b325-5078-4032-ab75-23aee677f786": {"node_ids": ["4249d453-86c0-4995-a9f4-e129492ca7a3"], "metadata": {"page_label": "226", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a9b1a814-2bd1-467e-a2f8-90eea72933a2": {"node_ids": ["c56b5adb-907c-4ff7-8828-346b3d673581"], "metadata": {"page_label": "227", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "db1a1697-d138-4b24-9e51-4abbeb69f6e5": {"node_ids": ["05c1489e-26a6-4244-8dc5-b4ca413a5eee"], "metadata": {"page_label": "228", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "078ecd0c-4fe1-44ee-ad86-5bb08b4acd9e": {"node_ids": ["1ee830aa-719a-4435-a416-0aefc36799de"], "metadata": {"page_label": "229", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3a7c549f-524d-4575-9fcc-ef049f75fbe7": {"node_ids": ["1aa05958-52f8-4180-84ad-d857fb4dd308"], "metadata": {"page_label": "230", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dc153b65-8fe2-4645-88a1-dcd4f44b9695": {"node_ids": ["d276dec9-25b1-410f-8b3c-4ebadbd2bfcd"], "metadata": {"page_label": "231", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "439bd428-700d-4a3c-b961-ea2246b1e6b3": {"node_ids": ["e1acd896-ee09-476d-b84c-cf7a59921b16"], "metadata": {"page_label": "232", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "609dc07e-9863-456e-8249-5e6d274ad36c": {"node_ids": ["ae9dbe8f-7a9e-426c-aa60-042e77642367"], "metadata": {"page_label": "233", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9bb75d5e-fa94-47d1-b1fc-6778db6f4dcd": {"node_ids": ["dbc44fc3-af1b-41f3-bf69-f7a4f75e8d04"], "metadata": {"page_label": "234", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dc8b4ab0-8511-4f24-a921-7cbf29a03d9f": {"node_ids": ["4a47d7bf-0fba-4daa-beab-5f5ebfe5e4b1"], "metadata": {"page_label": "235", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a4ab655f-0313-44ca-ad7c-a45e7c2cf1e0": {"node_ids": ["e098c751-b2f4-4cf2-a488-789e07a1c972"], "metadata": {"page_label": "236", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5e8d4c61-96a6-4108-b38e-c57469a5e0d2": {"node_ids": ["95858d7e-43f8-4e2d-954c-18862952ba72"], "metadata": {"page_label": "237", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "344fb342-dcd1-41fc-8635-c0b301550fa2": {"node_ids": ["f2b0bbdc-8a96-4162-8786-a4591581bac0"], "metadata": {"page_label": "238", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "faff5807-f23f-4e13-ab66-53e25c1bb2df": {"node_ids": ["8d022fd9-104a-440c-ad86-b091a596086e"], "metadata": {"page_label": "239", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "faa21e24-2335-4321-8210-7f4d58b666b1": {"node_ids": ["b76295c3-ef69-4aa7-a5d8-a2c2e30941f6"], "metadata": {"page_label": "240", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6c0350b0-7685-43e6-bcd0-765f0a95c059": {"node_ids": ["00452fac-97c6-4f8e-8310-76792f535037"], "metadata": {"page_label": "241", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "54c98d2f-258a-4d13-a0cd-a7f9fb29d5e7": {"node_ids": ["d94fde1a-35ef-423a-88a9-6ca42a583763"], "metadata": {"page_label": "242", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "17c8aff2-c905-4674-98cd-f21304fe72a4": {"node_ids": ["fe2931b8-7128-456f-91d3-13dac568976c"], "metadata": {"page_label": "243", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "37142b41-f8f0-4d73-beff-2c98ebb4e9b2": {"node_ids": ["4829a8b8-3d2a-4292-a4bb-c5724289c1a3"], "metadata": {"page_label": "244", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f64c40cb-e6a8-4648-b6ec-2747dffc2df9": {"node_ids": ["21a90c2d-d3e8-4a24-adf1-4b8469c01d68"], "metadata": {"page_label": "245", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9e97586c-d232-4cec-831c-c5e0892d54a4": {"node_ids": ["3f68c1d7-2890-4f89-a558-cc651cf5ffc4"], "metadata": {"page_label": "246", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0868aa4e-7f15-4586-bbca-f7524768c0bd": {"node_ids": ["0bee69d8-a2ff-44d7-9cae-e951fa0522c3"], "metadata": {"page_label": "247", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5425a85e-b5ad-4dd6-953e-464a22a789ad": {"node_ids": ["f59a7c48-6005-4c00-947c-c86209f8c0e6"], "metadata": {"page_label": "248", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "25d505de-fcd3-4b28-b6c5-c61c3059ccf7": {"node_ids": ["54c3160d-3a0d-4b48-af46-83cddb434a3c"], "metadata": {"page_label": "249", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0f4c1cd9-a48c-49d5-a4cd-9da444550806": {"node_ids": ["df0c5bbe-7b64-42a7-a671-efcd977d61af"], "metadata": {"page_label": "250", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "692e4d1a-9030-4f99-97c5-d881fdd6131f": {"node_ids": ["35627666-0443-4c6a-8fdb-924f311fc955"], "metadata": {"page_label": "251", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "efd61919-c942-4d87-9e00-f944444079ef": {"node_ids": ["938fcc53-d73f-46a3-96b1-4853fd35995f"], "metadata": {"page_label": "252", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "26e77fce-5744-440a-839e-633e524ad6b9": {"node_ids": ["e467cecd-0de3-44b5-8349-db9ded548315"], "metadata": {"page_label": "253", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "86643081-82a7-408c-ba32-a5e5badd9400": {"node_ids": ["1f11fa67-2ca5-457d-bea1-fc701c92c0bc"], "metadata": {"page_label": "254", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "aafe8ec4-cf51-4792-b20c-6d0042ba4a13": {"node_ids": ["c5165da8-806d-46f6-bdca-65328e624bf5"], "metadata": {"page_label": "255", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ffd9175d-2101-4323-a1cf-1487f936a32a": {"node_ids": ["ba431e41-e6cf-4b36-a38c-50b0c043c7fc"], "metadata": {"page_label": "256", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "43d79507-4bfd-4089-b9da-cfca60c33aa1": {"node_ids": ["82f2bae5-5b11-47b2-bbb4-41c14a938318"], "metadata": {"page_label": "257", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "72f75733-9d8e-4758-95ad-ef2bc7cfcab0": {"node_ids": ["71adcf2a-c930-43cf-8fbc-d9a834f0f6fb"], "metadata": {"page_label": "258", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "297a91aa-162b-478d-bd1b-0e14265aa459": {"node_ids": ["82acb866-e5f8-4f75-988b-3b880b9630ff"], "metadata": {"page_label": "259", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a458f7be-3ba2-4ac7-a214-36d7e556600f": {"node_ids": ["1d90d14d-3730-42a5-86d6-e3b8702dab5c"], "metadata": {"page_label": "260", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0cc3edc6-2603-4fdf-934a-f264119a3e9b": {"node_ids": ["72510c96-12a9-430c-87c4-9e59db09b427"], "metadata": {"page_label": "261", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "90c905e1-5ea6-446c-b280-4061f7830663": {"node_ids": ["6ed4b873-65a2-48dc-b542-f3045d6830a4"], "metadata": {"page_label": "262", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6794b52d-e16b-472e-ac32-00d8f30f0870": {"node_ids": ["b9a8f1fe-216d-44b2-a28d-3d14c6337ace"], "metadata": {"page_label": "263", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b65208a1-da2e-44d9-b943-449420968989": {"node_ids": ["ee5a8d66-02f1-41fa-941d-cf0c7ed87e44"], "metadata": {"page_label": "264", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "17433cef-5ad8-4e43-b827-07d57be06b58": {"node_ids": ["c2579d4d-3b93-4e6b-9e49-30f33ba36e68"], "metadata": {"page_label": "265", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f084b401-eb4c-4664-968c-ecff59af92d2": {"node_ids": ["f76d9530-6870-449a-8783-cfa87f7b6128"], "metadata": {"page_label": "266", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a1984c0f-7d75-46a0-b135-b7d25cebc7cc": {"node_ids": ["41c1494d-0a71-453a-8e2b-abea48e25e7e"], "metadata": {"page_label": "267", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "24f49a3e-a823-43a1-8e11-4eb53efcd002": {"node_ids": ["dbcb5c31-13fd-47e3-bd4a-9012d36ae7ef"], "metadata": {"page_label": "268", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9b0bd982-d606-4186-9c30-2700f52e4f78": {"node_ids": ["3353eae1-aad4-426c-ad6b-e6d47a175c73"], "metadata": {"page_label": "269", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5062fc32-8fcc-4657-8dd9-32aa692b11a0": {"node_ids": ["27678ab8-265a-4ec6-bf1f-447e3fbf91d8"], "metadata": {"page_label": "270", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1d50e18f-ab9b-496e-b029-a462597b901e": {"node_ids": ["808f8330-1680-4df8-9a21-d6b448342624"], "metadata": {"page_label": "271", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6f0c8856-047f-44ad-b6a1-c854209e5789": {"node_ids": ["1a276039-1ee1-4a89-96a1-a62f2d63774a"], "metadata": {"page_label": "272", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a901a4b1-b8a5-4ae0-8c4c-2a48d5d7b51d": {"node_ids": ["be61191d-5935-42e9-9ee6-d26361fef5b8"], "metadata": {"page_label": "273", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3cf8bd25-df63-4537-bd80-9fafe18103fa": {"node_ids": ["5dbfb414-0379-46dd-9df6-7dc965d13c1d"], "metadata": {"page_label": "274", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d7029599-2a8b-4350-aa5a-429aa86669be": {"node_ids": ["f06cc0b9-76ca-4eb4-bb50-2cb06877f298"], "metadata": {"page_label": "275", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "afebad18-6729-4eb7-aa94-e1a408169132": {"node_ids": ["f9fde751-018b-4a76-bfdf-00a515b5869b"], "metadata": {"page_label": "276", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1922822c-7f61-41e8-882c-d455ea7dd004": {"node_ids": ["87974162-00de-46bf-a9f8-68c1899f7827"], "metadata": {"page_label": "277", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "06f08744-8672-4555-93ff-8b27740a7105": {"node_ids": ["590ec620-cc35-48f4-8cfb-f4bcfc8ccd50"], "metadata": {"page_label": "278", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e8369408-a7a0-46cb-9641-fe7b484dfb39": {"node_ids": ["91b6c310-4a2e-4115-88e2-238b54ae9168"], "metadata": {"page_label": "279", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6463ed32-d90e-4ae3-abde-fa5cfcfc1305": {"node_ids": ["5f9f1759-3369-4930-a4c4-7cfcd1be5491"], "metadata": {"page_label": "280", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "08274d27-5b3c-4259-85d1-00e40ab9e6db": {"node_ids": ["193abb13-9ee6-4158-bd4c-fc39ad25fef6"], "metadata": {"page_label": "281", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c16c5a69-4a10-455c-8808-8cfdb8891d86": {"node_ids": ["0bca4f73-72cd-4e1b-ad78-0a9a2006c8ee"], "metadata": {"page_label": "282", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6bc6e1ea-2743-49a3-8414-fff1a177faeb": {"node_ids": ["5fbfd3be-e56a-4cad-9d14-6aa3f12caed7"], "metadata": {"page_label": "283", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "76901f59-946a-4a22-ad06-71d826f3bdb7": {"node_ids": ["12a8b523-f14d-44a7-8a7a-53b31a4ce63f"], "metadata": {"page_label": "284", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "280c5167-53c5-4c84-ae47-536a354972ab": {"node_ids": ["03c3b789-8e17-4bcb-a94a-85322c763587"], "metadata": {"page_label": "285", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b6689f3b-3150-45b0-852c-663332516c6b": {"node_ids": ["a448399b-1559-461e-b941-b19d6b026eae"], "metadata": {"page_label": "286", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6fe95aaa-bd99-4cad-bd5f-b5171fdf5793": {"node_ids": ["05b54e45-8cfe-4aff-b6e0-02a481436b76"], "metadata": {"page_label": "287", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "285e2dbb-5bba-47eb-826a-aa493ce6035e": {"node_ids": ["ce5d2775-f66e-4983-a866-f41d5dc1538e"], "metadata": {"page_label": "288", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "fba866dc-60b3-4fad-b3de-64ec560e7a0e": {"node_ids": ["d64137af-b224-46f8-8af9-6acdf6ab2acd"], "metadata": {"page_label": "289", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3d919f57-9e0c-4cb2-ab1a-a404211b3290": {"node_ids": ["72b76ca3-008b-4646-a14b-ec8c164ca5ce"], "metadata": {"page_label": "290", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a1a825b3-44fe-429f-848a-655dc6e4c2df": {"node_ids": ["5b3675f7-9549-4639-81e6-7dd966756621"], "metadata": {"page_label": "291", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "870e626b-c339-4276-beb3-5c1352e52297": {"node_ids": ["3bdfa28f-bc01-46f4-9d17-1c6765b0f4bd"], "metadata": {"page_label": "292", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0ed1ed0e-1db8-4616-b55b-d5e6628152f4": {"node_ids": ["efe245cc-3a9b-4cef-8a8f-1921c3b8b737"], "metadata": {"page_label": "293", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4964c129-c94a-456e-916a-33e51414535b": {"node_ids": ["dc44001f-1e98-4112-bc79-b1168a49692f"], "metadata": {"page_label": "294", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8c17705d-01dd-4013-b142-da1c13c79ed6": {"node_ids": ["61a19777-6ed6-4461-8c50-d307c1671ccb"], "metadata": {"page_label": "295", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "64fffc0d-b0ac-4198-a2b3-fc555b5d8eee": {"node_ids": ["b915999c-4e19-4802-952f-e65440abe990"], "metadata": {"page_label": "296", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7ec1b229-ea8d-49e2-810f-43774a11dadc": {"node_ids": ["baaaaa1c-c866-4041-96f1-1a997847bfff"], "metadata": {"page_label": "297", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8326b6d6-b9da-4823-bb9a-6206d306e680": {"node_ids": ["fab27331-15dc-44f1-af30-9157c91fdd2e"], "metadata": {"page_label": "298", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d3dd3086-f639-4b29-b85f-44b42d7311b8": {"node_ids": ["736b478b-e213-46db-8762-8c59c6ca6f03"], "metadata": {"page_label": "299", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "53b432d7-598d-4280-bb3e-bd446d7cc82e": {"node_ids": ["ba643ffe-56d7-482b-a16b-4b2b5a64d3c1"], "metadata": {"page_label": "300", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3337c29f-6428-4c88-bba8-26230533f988": {"node_ids": ["767f35b3-1c06-4823-97d6-44da033cfdd0"], "metadata": {"page_label": "301", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "da4f8257-8431-4e24-8e8d-8189011b9d3e": {"node_ids": ["1706a084-50d7-4251-8447-5dbd68d7f39f"], "metadata": {"page_label": "302", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "805aea54-bc7d-433e-90be-4d8e421a1b25": {"node_ids": ["cb03cd18-f380-4d13-bce7-64ee7c5004f9"], "metadata": {"page_label": "303", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dd11957a-d0e9-46f1-a2de-544a1101cb70": {"node_ids": ["1e4eccff-29f1-4199-ac74-481319a7caab"], "metadata": {"page_label": "304", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e011e8d8-0f71-4918-b56e-0cf1f9df4e58": {"node_ids": ["8f71136d-ff0e-4237-b864-c311d4bb8d9d"], "metadata": {"page_label": "305", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "cc5b80f8-5814-414b-8e4d-4a3056d6b9f3": {"node_ids": ["91bc95a3-761e-4105-813e-9e8b1c726b8a"], "metadata": {"page_label": "306", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b7185b27-654a-45aa-80e2-62b8cd9e817e": {"node_ids": ["d134199f-e7f6-4340-abef-50cb140fa5e8"], "metadata": {"page_label": "307", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7da5b45b-7598-45d9-998d-2531afcfb62b": {"node_ids": ["0f152fba-89e1-4850-b131-ff401ccedb7e"], "metadata": {"page_label": "308", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2ee31911-576c-44eb-a412-8552d0ffb9c9": {"node_ids": ["2c0a0407-70ed-4ed3-9599-749d72a747e6"], "metadata": {"page_label": "309", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3f512eb2-6e49-49e9-b014-22933af6692b": {"node_ids": ["30a61c3b-d1ad-4c13-98f5-091a416ede63"], "metadata": {"page_label": "310", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "802d2266-c773-464d-b233-cec2ba0e113f": {"node_ids": ["6c603a64-d67d-4d8e-a0d2-d7f0f3e269d0"], "metadata": {"page_label": "311", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "aa37e8b2-13a0-4eb4-98b4-c84698883a2c": {"node_ids": ["c6d59a26-7c14-4944-add8-2246b00b3e88"], "metadata": {"page_label": "312", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3f9f5dc8-7542-499b-a32f-e9f3fc19cd02": {"node_ids": ["a2716131-b820-4f98-a85e-1927b0ed2e9f"], "metadata": {"page_label": "313", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e301a2a6-da2f-4aa7-b180-c9e47279cad3": {"node_ids": ["5acfb37e-18a6-4b68-83c1-e3171489a851"], "metadata": {"page_label": "314", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "90da39fc-2439-4d31-958f-025eb82fee62": {"node_ids": ["5d8c94c5-b90e-495e-b1d4-bd2e353e331b"], "metadata": {"page_label": "315", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4294a845-4635-4233-9e19-aa7d389dce76": {"node_ids": ["1330881a-3351-46b7-b477-b51928cd296b"], "metadata": {"page_label": "316", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2c45232a-fe8b-4edd-a0b7-20fadcd4f31f": {"node_ids": ["efa279e0-8fda-4507-8c15-cdc87bb6b7c5"], "metadata": {"page_label": "317", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "77802f9b-ccc2-44ef-9254-8700cc5a8c4b": {"node_ids": ["b21eba9d-611c-4381-9dee-81a4d4ff5698"], "metadata": {"page_label": "318", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4abe58bf-6574-4b43-8a84-bd5af396f967": {"node_ids": ["e7149c2a-4357-40fe-8c8b-c1da6ec5d906"], "metadata": {"page_label": "319", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "065d5d0c-83ba-429e-bb5f-98493f53da54": {"node_ids": ["fdae96b2-0a05-4c06-a0fa-b9a658a8d8af"], "metadata": {"page_label": "320", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "696d5cd1-317e-41a2-b635-958f4ac7a36c": {"node_ids": ["6765d998-408f-49ad-be32-71a543e161de"], "metadata": {"page_label": "321", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a0a95923-7449-46b7-8ecc-0066c504ec5e": {"node_ids": ["8c06d128-0917-4be4-9a5d-6669b3b77e9a"], "metadata": {"page_label": "322", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9ec43339-e699-40ca-ba21-a5b5428a097b": {"node_ids": ["b3068b87-d308-45dd-b31d-ee3cd7ca0df1"], "metadata": {"page_label": "323", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f1a57b63-993f-4ce6-b6ec-86385572464f": {"node_ids": ["b1230c82-b0f9-4148-a2ee-966d9fed23d6"], "metadata": {"page_label": "324", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "29d45a3f-475e-4287-93cd-c7a0cf01754b": {"node_ids": ["cb4d2a41-4910-47ba-b5ba-19da953b6faa"], "metadata": {"page_label": "325", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d9f7158e-35f8-4d8e-bb54-e30a6ea0b366": {"node_ids": ["5f7379fd-fe10-4967-bcee-62283fb9710a"], "metadata": {"page_label": "326", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c8de2432-1598-47fa-a541-d114fdaa0991": {"node_ids": ["0b023ad1-0939-46ea-8ae5-a86560934aa6"], "metadata": {"page_label": "327", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "cdb01ea5-7da9-47a0-998f-79c69f9f42c1": {"node_ids": ["2c289531-325b-4a8c-a56c-e87d089d6b48"], "metadata": {"page_label": "328", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ca285005-75a7-4c7e-b2b5-46e62f3530de": {"node_ids": ["6d193a44-3bd4-4d2b-b3f2-fa01f864d0e6"], "metadata": {"page_label": "329", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6be3961a-8656-4d71-86da-09c94d726dfc": {"node_ids": ["35ce4b15-151b-4e46-abf5-389ffea5e772"], "metadata": {"page_label": "330", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "99e4b054-2d92-4585-b778-a7ca930409c5": {"node_ids": ["b62a8b44-85e1-4e30-8642-2b024d45c432"], "metadata": {"page_label": "331", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3d8a5ef6-a6bc-4498-a913-a6ae3d1c049a": {"node_ids": ["29cd7e4f-9cc2-4f00-9ad3-3a29340e8820"], "metadata": {"page_label": "332", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7683e053-388f-42b1-98b3-b6d1eb9d9636": {"node_ids": ["6ee6a0b2-e5ca-486a-821c-8190075c4704"], "metadata": {"page_label": "333", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e4048736-6fb3-4e67-8272-69948092b646": {"node_ids": ["c5584b9d-f38f-4fad-9da1-5c77e9e93ff7"], "metadata": {"page_label": "334", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7c02a25a-494f-43c6-a565-446fb458179d": {"node_ids": ["a7920269-c0f8-4305-8d4a-369714e6dd2a"], "metadata": {"page_label": "335", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6bf757dc-44bc-414b-ba9b-2eb96f452623": {"node_ids": ["a8606b33-d7dd-4791-b8b6-f45bd5d87a1a"], "metadata": {"page_label": "336", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6aa317e1-ae0b-4749-a6db-c673a5e7b5a4": {"node_ids": ["77662751-1b92-4b0d-8337-091d61726928"], "metadata": {"page_label": "337", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "61c9d363-6a29-41fd-b772-95058130ac42": {"node_ids": ["cb46fb0c-ac37-48aa-a599-7c648d41fcd0"], "metadata": {"page_label": "338", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3129500b-7b60-4971-9871-95973d3d53e0": {"node_ids": ["94bcc9be-4361-420c-b54a-aa6e38ac0cf7"], "metadata": {"page_label": "339", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5544f9d2-9915-4226-abf0-c3ee6454199e": {"node_ids": ["b75bab2f-74b5-4bc7-93f6-625322a8abca"], "metadata": {"page_label": "340", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "287fdc18-ac7c-488d-9a6b-63a934d73724": {"node_ids": ["3e610af4-060d-4aa2-b17a-7d7c2645c1ea"], "metadata": {"page_label": "341", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "03d2d7df-6831-485d-bdcd-76b4ec0bfb08": {"node_ids": ["2bdee4a0-2a66-402f-bfe4-692ad4db56b7"], "metadata": {"page_label": "342", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2af1954a-ab08-44b4-8593-3b714352b5dd": {"node_ids": ["58e4d1a4-7c18-4c48-81c5-643a9f102d6b"], "metadata": {"page_label": "343", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "cf0d539a-a3bf-4b8f-b151-88cba71d43b6": {"node_ids": ["008ecae1-2464-43ff-8fc7-a122a427c509"], "metadata": {"page_label": "344", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "01073416-15d2-4ed3-9569-c3a7758c2fbf": {"node_ids": ["ef29320a-0e48-4d9c-adb9-2d1af822929a"], "metadata": {"page_label": "345", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "054e3658-ddc7-49ec-810d-a6de0f47add0": {"node_ids": ["80d4f7d8-cc79-40ec-8432-a39c92f93e90"], "metadata": {"page_label": "346", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bd6e70eb-c1a9-4d6e-8a91-e9a0845d9ee7": {"node_ids": ["1724835e-dc0b-417e-95f7-6ada304ee40f"], "metadata": {"page_label": "347", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "95dc23d6-cde0-4dba-8678-b0dec0c87ff1": {"node_ids": ["f941bcff-6544-4e72-b05f-35614512cf60"], "metadata": {"page_label": "348", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e25f08ce-c3aa-4165-a5f5-bc0a06378411": {"node_ids": ["f5636584-a70f-4a40-b689-d1e51459e6fa"], "metadata": {"page_label": "349", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4a5e6544-5550-4555-85be-40e06f5acc5a": {"node_ids": ["3417aa77-c8dd-4474-abdf-469a528017a0"], "metadata": {"page_label": "350", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bcad1dc8-f51f-4443-9685-1add2b409bf5": {"node_ids": ["1605ec7e-f09e-4ca4-a5b2-f20e9cbe68d6"], "metadata": {"page_label": "351", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "75efab75-6103-4a86-a65a-95b3f5f6803c": {"node_ids": ["f10aaa10-f44c-4783-81e2-c5785357f2d6"], "metadata": {"page_label": "352", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f3bca2b0-a931-4def-a018-aaa835af6265": {"node_ids": ["2bcbf27c-3aa3-47f1-88a5-60118d64f2d2"], "metadata": {"page_label": "353", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4dea3af1-89f0-4cff-9fe0-e62c4e20be25": {"node_ids": ["d79deba2-1ac3-4483-878e-50dd552007db"], "metadata": {"page_label": "354", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "84ad2d4d-3861-4c5b-ab04-05459126052d": {"node_ids": ["c10d0bf3-73f0-482b-9af2-f77b19642dfe"], "metadata": {"page_label": "355", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c5ad3909-7bc2-4fbf-b861-0ed12ef9cf7a": {"node_ids": ["d05cf104-5ec3-4ae0-a7ba-4ac2e6c77bfb"], "metadata": {"page_label": "356", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a7bd547d-9bdc-41b4-bd95-b27fba8ebf35": {"node_ids": ["2329e98c-13e8-401c-bcab-451ba37cc0fa"], "metadata": {"page_label": "357", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2900c8c7-cf10-455b-9d3a-3370dad43dac": {"node_ids": ["646b4931-b8e6-41af-9fa4-a865e5db6bab"], "metadata": {"page_label": "358", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6a96b9b3-1198-446e-b8d9-6255eebfb3be": {"node_ids": ["3ff5d943-cefc-4ee9-947e-b1b9299382a9"], "metadata": {"page_label": "359", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "97eb87f5-8826-4de8-93ba-962fba5e0936": {"node_ids": ["741b1bd2-52d7-468b-980a-f9423a4e8741"], "metadata": {"page_label": "360", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "af67df2a-d9d9-4735-8d02-691dbb378710": {"node_ids": ["7f280e60-6e62-4f83-9bac-8166db5aea09"], "metadata": {"page_label": "361", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0921d14d-082c-4af2-b132-e553b872873e": {"node_ids": ["f8cbc429-ae87-408c-8c1f-447823b54de9"], "metadata": {"page_label": "362", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ae0da04c-ccf6-4d95-9801-a037af371b77": {"node_ids": ["7d08237d-c0a9-41ee-97c3-24aff7f0e0ba"], "metadata": {"page_label": "363", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "df11fac4-a4f4-4a88-80c5-b88544cd60b5": {"node_ids": ["9bdd4460-fcef-4e78-8b98-6c06985ba9d5"], "metadata": {"page_label": "364", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "addf1f2d-18a5-486d-b004-995e5ed621ce": {"node_ids": ["a8006abb-fa9a-4244-bb3a-02f8f54f1a8d"], "metadata": {"page_label": "365", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0c6532e6-85bb-49fb-9e38-c7e180fae61a": {"node_ids": ["5237fc7f-0bcd-4349-82da-3c48080a3796"], "metadata": {"page_label": "366", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "45f61d53-fdd2-45e0-b68b-43316ab82c64": {"node_ids": ["b29f464c-64fe-4b0d-8776-724b1858c4ef"], "metadata": {"page_label": "367", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bea78591-7b96-4433-9d19-b7fe32c97193": {"node_ids": ["7381cfad-2e9d-4f55-a90d-73941c0dcd75"], "metadata": {"page_label": "368", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9c855298-579f-4f17-b6be-3cf70cd81aa5": {"node_ids": ["d94f311a-1ee0-4696-b08a-86c6f63fab4b"], "metadata": {"page_label": "369", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "54f93970-accf-4166-ae70-b619f6fab915": {"node_ids": ["ab66a745-7c35-493d-afa7-e4ba7e39a67c"], "metadata": {"page_label": "370", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "adf2fa83-68e0-4d59-a937-052769668a2b": {"node_ids": ["fd94e5c3-da99-43fd-bcbe-8c84bcf690fd"], "metadata": {"page_label": "371", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7dd88692-0b1e-45b3-88c4-0ccb4fbaeeae": {"node_ids": ["6c2671cf-f32b-42ad-8070-c7682a496ffa"], "metadata": {"page_label": "372", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6e80d002-31b9-401d-aabe-ab84c20d52ab": {"node_ids": ["40efb552-d122-4550-ad61-3f6fbeac35ae"], "metadata": {"page_label": "373", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a22269d0-f9ea-4172-a34e-41b479aacce3": {"node_ids": ["40770dcc-da68-4882-b16d-8cd4915f1f37"], "metadata": {"page_label": "374", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e4621f15-7b4d-4bd3-8711-6dbad76f7731": {"node_ids": ["1008ba1b-3c00-4d20-814b-9ff89e276f63"], "metadata": {"page_label": "375", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f8d59f6e-509c-48ef-8805-84c3cfa94d23": {"node_ids": ["ed553707-91e6-439a-8b34-1704a58f40b9"], "metadata": {"page_label": "376", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "80e080f4-3428-4c42-ba30-5a7c9d710361": {"node_ids": ["1b19bf71-d287-4428-bc19-debdbe537176"], "metadata": {"page_label": "377", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c1b36253-3fdb-4b05-9ed9-ac2f7a5fb212": {"node_ids": ["354f0f70-2e2f-4d67-86fe-f76b33ee59fc"], "metadata": {"page_label": "378", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "32076889-1c93-4889-b785-b1203477eab4": {"node_ids": ["1027363b-a0bf-4b1f-8574-5c9ca5167056"], "metadata": {"page_label": "379", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3311af09-25a9-43a2-8f45-c4edf1d0a72e": {"node_ids": ["19221bb1-c45d-40b5-b096-257cbc71b5cc"], "metadata": {"page_label": "380", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1a06ebf6-0823-4253-bc6c-7edc3aa13df6": {"node_ids": ["9d339f5e-b6c8-4665-8625-6fa639d47f9f"], "metadata": {"page_label": "381", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8c440d27-4be8-4565-b4bb-f853c17399df": {"node_ids": ["99fab3bf-7a6e-4401-8a15-d83176c447a6"], "metadata": {"page_label": "382", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f719e8f8-2109-4fc1-a46c-145ac5021b50": {"node_ids": ["75fdd03e-9023-4afc-b987-f30603e6fb4f"], "metadata": {"page_label": "383", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7055f343-6d9f-4afc-a566-1fe19af54aa3": {"node_ids": ["d9e27290-8b7b-408e-98f3-b3a22ed3067d"], "metadata": {"page_label": "384", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "28772a37-b87f-4a6f-ad17-d1081cd2900b": {"node_ids": ["e02410c2-077a-4f3f-88d6-cfebbcf3209d"], "metadata": {"page_label": "385", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3e1a3ecb-9dae-4847-965d-598c86a43865": {"node_ids": ["57b84501-5d6a-455e-84e6-3832a8625a32"], "metadata": {"page_label": "386", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0842b10d-a6fa-4dc2-b017-5b14af8da28c": {"node_ids": ["8a6c33ac-27fb-4402-acde-0644490055e1"], "metadata": {"page_label": "387", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3dc193bd-bba1-4922-8a69-fdcd6642501b": {"node_ids": ["734e1815-3c0d-41f2-91f4-28b6d6926b03"], "metadata": {"page_label": "388", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4389bd7a-45a2-497a-8acb-3175ba8ef500": {"node_ids": ["65365ce8-d0bc-48b3-bf6d-a11936fc4520"], "metadata": {"page_label": "389", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "cf2a28be-b1bc-4be7-a797-b894e1a9e90c": {"node_ids": ["aac8f3ed-b1fa-4cad-af85-41e34aaa5e28"], "metadata": {"page_label": "390", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "684cb30f-884f-4c91-a90e-11279924a187": {"node_ids": ["cdbac4ef-2d1b-4d49-b450-70c5a84972f1"], "metadata": {"page_label": "391", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1bbe6e75-21e8-4dcc-95be-cba9027560b7": {"node_ids": ["0af58246-6f30-40b0-b928-d8f576b481d7"], "metadata": {"page_label": "392", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2e8afd3e-c770-415a-838c-60b15b8e1b78": {"node_ids": ["01d2fab5-48d8-4ab4-b338-4e3f86d2f0c0"], "metadata": {"page_label": "393", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9566c3f6-07c9-40a0-9738-9c200d9de840": {"node_ids": ["cc466f1c-52b3-45c1-982d-dd6e97acf071"], "metadata": {"page_label": "394", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9a76d14f-c044-4894-a9ae-b358f3e8b501": {"node_ids": ["7b064d87-5acd-4449-badd-4dc0f9d0dc98"], "metadata": {"page_label": "395", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5672d9cb-a86b-4556-ac6c-ea32f804b33e": {"node_ids": ["b101ca7d-68b7-4e28-91ee-5a53ddb45242"], "metadata": {"page_label": "396", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f11afa31-403d-4507-907c-10cd9b848534": {"node_ids": ["b9d3953e-843e-4c75-a6ea-3aac1268744f"], "metadata": {"page_label": "397", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7d53a1cb-7313-430c-b35f-76e44bfe5a0c": {"node_ids": ["df58522f-9c7b-481d-9d6b-1d281d2f164c"], "metadata": {"page_label": "398", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "98137909-efe0-49e2-95a9-82cab4f68d19": {"node_ids": ["53d2dab7-4d80-4ffd-a1e8-ea6811484742"], "metadata": {"page_label": "399", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a0a8ae2d-72c8-4247-8b63-60632d8febcc": {"node_ids": ["c61f8765-247b-495b-a57d-cf6a1d4526e8"], "metadata": {"page_label": "400", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "311873fc-c662-42de-a871-fde4739a3499": {"node_ids": ["ce3ceb0a-2bac-4c67-bd77-fa348e11c68c"], "metadata": {"page_label": "401", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b4b40766-a5ea-486a-941c-f53d4e5c928f": {"node_ids": ["13052a63-5af9-45aa-bb9f-961019e1cd87"], "metadata": {"page_label": "402", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dbd1239c-7778-443c-b15d-589a0124e1da": {"node_ids": ["97d643d7-8421-4827-b893-0a1f77d074a2"], "metadata": {"page_label": "403", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4ccfaec2-a9b1-4d76-87f8-dae6ba181373": {"node_ids": ["a7cb41ea-a9c9-4b4f-8631-aa80efe3783c"], "metadata": {"page_label": "404", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6fc83394-5da9-4f8a-84eb-0148832b3c55": {"node_ids": ["556eeec3-fe84-4ccf-862a-8d1762daa777"], "metadata": {"page_label": "405", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b2de1ac1-0d16-453e-841a-20d054c85f8b": {"node_ids": ["a0d440cb-b701-4c96-8515-1982e0b47d61"], "metadata": {"page_label": "406", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5a800053-18d3-4b3f-9ae6-2b083f34a392": {"node_ids": ["6e58aeaa-8a6a-406a-bd5b-adaa0e9207a8"], "metadata": {"page_label": "407", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a713f84e-7055-42f0-a623-2ff9653d3ed4": {"node_ids": ["3b458f0e-76d5-49c2-a651-dadd129a24e4"], "metadata": {"page_label": "408", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9b1cc965-6768-4867-9a27-cfc85f41de16": {"node_ids": ["ac576e8f-b787-4ff3-bd54-ca75ee8d8d0f"], "metadata": {"page_label": "409", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0bd41b34-a1d2-49a6-b1b8-83ea4323ba5d": {"node_ids": ["7934df17-32bf-4175-83d0-d97a66c8d31a"], "metadata": {"page_label": "410", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c7a5009b-c4a1-420b-8a9c-e3ca17a7bba4": {"node_ids": ["7036e587-75ea-4984-b7d2-8ca4613c5a27"], "metadata": {"page_label": "411", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ddec3343-bef6-4269-b65e-e767294dc820": {"node_ids": ["cd87e09a-5461-405e-a0a4-c6dc70bbd0c6"], "metadata": {"page_label": "412", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1b142e80-6c3c-4b3f-88f7-2b38b6d9f797": {"node_ids": ["ddfd46c7-a179-427c-b09d-2f8914c12272"], "metadata": {"page_label": "413", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "162430e6-5ea3-4f9a-85f4-5d89a5385c8e": {"node_ids": ["140882aa-ffda-4d6d-a5ac-529bccd29869"], "metadata": {"page_label": "414", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "141c6cc9-ab51-4ded-8fe2-cac73bc41f75": {"node_ids": ["7bf33fbd-e388-440b-99a6-6ff3f3bf58a9"], "metadata": {"page_label": "415", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a513c3fe-3f32-42db-b7bd-f3d23a9fc402": {"node_ids": ["e87f6f78-accf-457c-ae9e-131b41f98e0c"], "metadata": {"page_label": "416", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a1a3367d-07f0-4546-84da-b64d32f5628d": {"node_ids": ["39f3e16d-771f-42f1-9d2a-af6eacd98d2b"], "metadata": {"page_label": "417", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f04e1ae7-0130-4947-927f-b0daf049078d": {"node_ids": ["e0fda21f-51fb-4cd6-b2f0-7f18c703df39"], "metadata": {"page_label": "418", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "aa41a984-149b-411e-a492-2ff95084de3f": {"node_ids": ["ecbfd787-41da-465f-b804-15509f7fa6e7"], "metadata": {"page_label": "419", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f9de3e58-a408-4d1d-b213-c6f2e02648af": {"node_ids": ["029844a3-5ba5-448e-b565-a09d87e3ebbc"], "metadata": {"page_label": "420", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3813571e-45be-49ca-90b3-54b9b4ab303d": {"node_ids": ["988a997a-689f-4dfe-9c55-e47e7ddd8ba7"], "metadata": {"page_label": "421", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b1aa33c2-fce2-425d-82a6-352a0eed3b42": {"node_ids": ["55b3d3da-97b5-40b8-9231-ffcead09b3ac"], "metadata": {"page_label": "422", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e7457fbf-ec07-4c45-959b-e13f8e0968f9": {"node_ids": ["3c451913-c2ef-4c58-b637-ea8b046115d1"], "metadata": {"page_label": "423", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ec2ad01f-991e-4a1b-acac-7109e76e8137": {"node_ids": ["0efc5463-0d2c-449a-83e1-83d1d90a0a50"], "metadata": {"page_label": "424", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "46f07b8b-004b-4221-8799-c9224afb7e58": {"node_ids": ["d8d69b19-ca7f-4b14-a616-2074968b4ef2"], "metadata": {"page_label": "425", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d90a2264-16d4-468b-9ae4-806739cca1b2": {"node_ids": ["82d17465-b07b-4f14-8f21-17d721755217"], "metadata": {"page_label": "426", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5827da53-13ed-415b-b577-ca6514d5b27f": {"node_ids": ["db83a839-6773-40e5-a08d-80fac0b13b12"], "metadata": {"page_label": "427", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "365bcf18-7006-498b-9573-a2c1c1405f7e": {"node_ids": ["2bfd7494-d6b3-4934-84b7-e113c354d575"], "metadata": {"page_label": "428", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8210b0c6-b22d-4537-bbd3-72854118a090": {"node_ids": ["b4c1e291-9e9b-4726-924a-831a589db950"], "metadata": {"page_label": "429", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "934eb9d2-92c3-4f32-b94b-7d6e7428cf41": {"node_ids": ["60b89021-aaec-4a2b-8367-03cf30ddcd0d"], "metadata": {"page_label": "430", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "14773323-390a-4edd-9ff0-adc79735bac4": {"node_ids": ["ed136d59-d9c5-42f3-abe0-60b7078ac7b7"], "metadata": {"page_label": "431", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a99d9ab7-1192-4469-a10c-332562365e28": {"node_ids": ["87f141bc-a64b-4028-be2b-47877aabfd6a"], "metadata": {"page_label": "432", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "84d75a17-6aba-4591-86fb-bc5d19de2eb0": {"node_ids": ["f69ba5ef-80d2-4ca1-ad81-72281e925876"], "metadata": {"page_label": "433", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "fa5099dd-36e0-4dc9-ab6c-546dd398816e": {"node_ids": ["dcc86546-203e-42da-9cff-182bba8ec427"], "metadata": {"page_label": "434", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7ee217a0-d49c-4749-8e89-5697487841db": {"node_ids": ["6ee6e46c-6950-42e9-851f-643cf8674306"], "metadata": {"page_label": "435", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2e1f0b58-7bb1-414c-b934-c5850fe5bb4a": {"node_ids": ["f435471e-2824-4ac1-8c03-5a47da58fddf"], "metadata": {"page_label": "436", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "416d6001-a1eb-44bd-b44c-853d0dbd64b0": {"node_ids": ["cf28b57e-ce26-4f2e-b50c-f748dda3373e"], "metadata": {"page_label": "437", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b9eab09e-4d9f-45b7-bb51-ea4432e9b921": {"node_ids": ["5de6c55c-4b3b-4038-a391-6c0a21ace325"], "metadata": {"page_label": "438", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5b2022a6-0497-44a4-a48d-d2e6146eaa10": {"node_ids": ["8fa2451c-b3e3-4970-9d44-7dceb51f15a3"], "metadata": {"page_label": "439", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9d534fcc-46ec-440c-8a4f-19cc53ae45a5": {"node_ids": ["b1616702-c8c5-4580-9b05-79363c1e6f05"], "metadata": {"page_label": "440", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "84a15434-eb42-4ff2-bfa6-9e1039197932": {"node_ids": ["ab385e4c-eba5-4a35-bd7b-6fea18681725"], "metadata": {"page_label": "441", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bf89a88f-b93b-4995-8968-125b7cb57ead": {"node_ids": ["c745b5e9-7026-4801-816b-e929e2df29fe"], "metadata": {"page_label": "442", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8e7cd6b6-9acf-47b8-8558-24cbde3b5e21": {"node_ids": ["c30524f2-5f2d-4a87-9d26-b3c8765c588c"], "metadata": {"page_label": "443", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5ede781f-94cc-4ec1-a0ac-79734c148224": {"node_ids": ["eb8740b2-e499-4281-88d5-dd19f2741edf"], "metadata": {"page_label": "444", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b06ace18-964e-4f5b-a098-c0a394666dc9": {"node_ids": ["53ca8140-9c62-42dc-8964-d43bd3bf4920"], "metadata": {"page_label": "445", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1b346ba6-c764-432b-9006-7a833ff633d0": {"node_ids": ["ed63a022-efee-4971-a6f0-4b87082346d2"], "metadata": {"page_label": "446", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0a68d8ca-712f-400f-b87f-270898937ad7": {"node_ids": ["6a78d55d-52b7-4e7d-9655-2fd9b8116163"], "metadata": {"page_label": "447", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6ba51c12-507c-4e81-9f86-55b53a2f7f69": {"node_ids": ["89240719-6ec7-4128-8c1a-6355ff0a4feb"], "metadata": {"page_label": "448", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "cd2be06c-6fe0-4823-8285-44c5689b8258": {"node_ids": ["6e8a07ce-5d0f-43d9-9043-757eab459f51"], "metadata": {"page_label": "449", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "968c7f06-2151-472f-8649-79563383d892": {"node_ids": ["5239067f-3e5d-4801-815f-fe47d16250dc"], "metadata": {"page_label": "450", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a8695a0c-5403-4b4a-b00c-8b131d041388": {"node_ids": ["8f7b017b-c969-4cbc-9554-79ba8e21acfd"], "metadata": {"page_label": "451", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "77511991-81d6-4abd-b506-97de6f31b92b": {"node_ids": ["b5d83bff-764e-411f-ab0a-64414049d7a7"], "metadata": {"page_label": "452", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1c595d90-86f7-4117-9d03-12cebbd672fa": {"node_ids": ["bec6f598-c082-49a6-b5d0-0423c575fd00"], "metadata": {"page_label": "453", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "be45592f-7e7f-4683-9a84-d84d94fbfa9a": {"node_ids": ["b73a98d1-c0fe-4ddb-80b7-855974b95bf0"], "metadata": {"page_label": "454", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7e14ee7e-a8dc-4a4f-914e-f5418c518135": {"node_ids": ["102e125c-f6fa-4863-a495-82a084834b7f"], "metadata": {"page_label": "455", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b137b1d1-76eb-4c28-8474-f9d9cb9fb11f": {"node_ids": ["8f4d2df9-ea45-4708-ae79-3290a72a4090"], "metadata": {"page_label": "456", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4738e651-60f3-49d9-9f0e-c6e80fc363a0": {"node_ids": ["7ba336cd-3f58-4c84-b1e4-3eb60aa766f3"], "metadata": {"page_label": "457", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f07ef0e3-bac2-4112-a550-4aa731fd8236": {"node_ids": ["d183b6af-427e-46d4-a96e-03a5fadc083b"], "metadata": {"page_label": "458", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d4b6e2d4-c5e9-412c-a263-4dd7ac67138e": {"node_ids": ["7917cce0-f165-406e-885e-7760e9931f5b"], "metadata": {"page_label": "459", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "268e3d23-d175-460f-9b0c-3c63224b0b97": {"node_ids": ["fdf849e5-e62f-4641-8a6a-bb69fcc12901"], "metadata": {"page_label": "460", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "fe45250b-25e0-46f3-be4b-8b26d7ee8be7": {"node_ids": ["b4421a52-79e9-4458-aeec-6b001af320fc"], "metadata": {"page_label": "461", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a33f9f31-d173-4f26-ab64-b78e454e6e7b": {"node_ids": ["3afa05b5-d65a-4d39-99bd-e883b621ef19"], "metadata": {"page_label": "462", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "759b35bf-9700-4feb-a710-198a75996616": {"node_ids": ["a4242c3d-88b9-418d-acfe-7d17143e2b90"], "metadata": {"page_label": "463", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0cda95b9-3300-4296-bd13-f0ce7b223d37": {"node_ids": ["a01450af-e5ee-48b7-ada1-b13bda805a30"], "metadata": {"page_label": "464", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e0325c47-3f7e-46e9-aaca-b5618ebf055c": {"node_ids": ["79322ace-b4a6-4be2-9497-cf78ef7d56b3"], "metadata": {"page_label": "465", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "556f65ab-88fb-4aba-8e8c-7829a9de24a6": {"node_ids": ["4f6b9974-7da9-4a32-baaf-db0f144776d8"], "metadata": {"page_label": "466", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "dea88df1-71db-48ff-861f-cfd6d384bd06": {"node_ids": ["ef0c9b42-428d-40ac-8f9c-4802e8f3fcb2"], "metadata": {"page_label": "467", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "89a5411c-aa01-4c2b-9d00-04d430ac929b": {"node_ids": ["9c775082-e439-4cc5-a189-e99bb59ce3cd"], "metadata": {"page_label": "468", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4fae5da1-37ad-4a3c-8804-fdd107bbcb39": {"node_ids": ["443f7b96-1b65-475f-92b1-e44745626f13"], "metadata": {"page_label": "469", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "cb91518e-ccca-4a97-8afa-b65ddedc40c8": {"node_ids": ["c539421d-cfc5-43ca-bf63-bf4c9d11ca5e"], "metadata": {"page_label": "470", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3e74ee4f-2c37-4c3a-9094-67291e7aed0c": {"node_ids": ["f086129b-5cd9-4e3d-a8b4-c81808701ddb"], "metadata": {"page_label": "471", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e6ed040d-4599-4f63-9713-21643c08d14b": {"node_ids": ["6ccebcd0-69bd-4d59-a113-7fa96f57b959"], "metadata": {"page_label": "472", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2507874c-147f-4ba2-b52d-0bce0275324f": {"node_ids": ["21ac0cbf-9094-4a7e-8934-d0791f6423fd"], "metadata": {"page_label": "473", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "aae6d196-35aa-4723-a43d-12ee2e12dad4": {"node_ids": ["f2537d46-2f96-46f8-b062-a2039c1d5119"], "metadata": {"page_label": "474", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "885ccbb4-34ac-456b-87b4-03cf1b17db25": {"node_ids": ["7454ecad-a503-4d2b-aee3-3ce634036818"], "metadata": {"page_label": "475", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ff564d44-d7e6-474f-abba-fd234ef7df00": {"node_ids": ["ba0d06b2-5e73-4c31-b4f0-1a885f68c384"], "metadata": {"page_label": "476", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5e22e822-31c6-4813-82e3-3b0dd629067a": {"node_ids": ["4de2793c-4712-4165-b783-bd094e6b0811"], "metadata": {"page_label": "477", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "13238669-c93a-448f-8db4-b4ad15071394": {"node_ids": ["b0f19198-d87a-430f-a53f-a4059a0884c3"], "metadata": {"page_label": "478", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9fc8790d-3459-4432-9a9a-2dc4513c3c3f": {"node_ids": ["dc295ccb-67b5-456f-b56d-bf9f2c0bbdfa"], "metadata": {"page_label": "479", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b3ae67c8-10a6-492d-98fd-caf2df38d93a": {"node_ids": ["3613c13b-e0a7-4228-ab85-a7b88f3c4a83"], "metadata": {"page_label": "480", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2c973714-1327-4f17-a8ad-3e6bbbe6c8c7": {"node_ids": ["b86e8f6a-376a-465d-a70a-b301ced72c89"], "metadata": {"page_label": "481", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2463e0f9-f295-4325-9a22-a2489ed55899": {"node_ids": ["ad71549c-8290-491a-b8fa-de15fab0163b"], "metadata": {"page_label": "482", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8aa29d93-878f-441e-a49d-089ece12d7b0": {"node_ids": ["498cc788-2eb3-4341-9088-ddd75b485c07"], "metadata": {"page_label": "483", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8d290785-3a12-4b06-a821-4e7adf4d3912": {"node_ids": ["099df565-1fea-455b-b414-774a18f0c18c"], "metadata": {"page_label": "484", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4aed5dff-ca5c-43dd-bde8-1c4102935cbf": {"node_ids": ["28070354-b27b-4996-a730-3c5cf87ac7d1"], "metadata": {"page_label": "485", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6c80f8f5-2c34-444f-9cd2-a89f9e0e7de9": {"node_ids": ["6149301d-90d4-4cba-8598-d905bf9a2832"], "metadata": {"page_label": "486", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "22110560-cf2e-47d5-9618-c6eb6b2be016": {"node_ids": ["16960e18-b9fd-43ad-953b-cbaf0367dc4f"], "metadata": {"page_label": "487", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9d027828-b439-4704-b10d-9ef1051c3c31": {"node_ids": ["a7771cfb-4128-4267-a213-5f1727df684c"], "metadata": {"page_label": "488", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7bb47ef6-b3b7-45ff-9da9-479566d83fd7": {"node_ids": ["eb026f77-7839-4e0e-9942-4429f1794509"], "metadata": {"page_label": "489", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b3e6e739-1c89-4f43-8ce7-77ea9514c8cd": {"node_ids": ["7d869ef0-6dfa-4a09-8809-bdb98780f251"], "metadata": {"page_label": "490", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c08874de-13ed-4097-ba87-dff6e43fb537": {"node_ids": ["e900c715-67be-4e10-9a4d-5ce5da0ac1c3"], "metadata": {"page_label": "491", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8705eb3a-1f5f-4bad-9654-edfbb8f88068": {"node_ids": ["25b1798f-e663-45c0-bfb4-e86489c1c23f"], "metadata": {"page_label": "492", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ae7cf2de-36d9-4a7a-9387-66cb81cb2068": {"node_ids": ["8daf03cd-cf7b-44ca-9550-0ae9615f1b3a"], "metadata": {"page_label": "493", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2bcde958-2fa2-4bd8-88e1-f60f9966fa94": {"node_ids": ["32493781-67e8-4b03-a60f-ce207c00c59d"], "metadata": {"page_label": "494", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2daa6184-1982-45da-bfee-483064f41798": {"node_ids": ["b70d3a3b-8ff7-44a2-a798-a7dc9336a65b"], "metadata": {"page_label": "495", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "66cdc326-000e-4d3d-acfe-9d46440b8b04": {"node_ids": ["41ec6ef9-0334-4155-ad11-49b5d38229a9"], "metadata": {"page_label": "496", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ed03184b-4e03-462b-9ec0-193a951c8487": {"node_ids": ["cedde4ec-54e2-4f3f-8a31-eb8a74ce9679"], "metadata": {"page_label": "497", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "65421512-2153-418f-a356-5f80f4d63fdc": {"node_ids": ["d3a0c1ed-8ee5-4625-8db9-35e595ad7fb0"], "metadata": {"page_label": "498", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bf6206f9-d55a-4a4f-9245-71b7ee4a85a6": {"node_ids": ["faad3477-2005-4101-bcdf-585338b2e818"], "metadata": {"page_label": "499", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "fe9ef192-dd43-40d4-9660-73382dbb3fa4": {"node_ids": ["7f820a50-6787-4f35-82fe-a091f80c4be4"], "metadata": {"page_label": "500", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3f655cb7-3c82-4f42-af76-6975240df378": {"node_ids": ["5ce76664-0e88-49f7-983e-dc41346532a4"], "metadata": {"page_label": "501", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0984b0e1-8825-4bab-a777-5b25903afb80": {"node_ids": ["c3b2b703-04ff-4c73-95ba-ac95076fc688"], "metadata": {"page_label": "502", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "16dabdeb-269c-4d97-b25d-1602538fe2ae": {"node_ids": ["09258b97-4b8b-4c6f-82d5-1ee07e793cdb"], "metadata": {"page_label": "503", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1e73ea35-227f-4865-afba-b5325315169b": {"node_ids": ["e611c3de-0710-4a63-a007-e0da60b5d1c0"], "metadata": {"page_label": "504", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "54cee247-9a64-4b4a-9526-013be196d873": {"node_ids": ["5180069a-c02c-4941-baa4-2494b5c06d85"], "metadata": {"page_label": "505", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6abe6038-5b67-4bf4-bb6e-47c4a3e0f197": {"node_ids": ["ba971737-4ff0-409c-a5a8-f748ef1739ec"], "metadata": {"page_label": "506", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f53dcc64-eb02-4465-b9ed-0625a409b519": {"node_ids": ["7c3793f3-be04-484d-b65e-3383693709dd"], "metadata": {"page_label": "507", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "681c7631-815a-4a12-affe-c1cdfff0ab8f": {"node_ids": ["524ee179-6372-4b35-aa27-a5e3eff6a904"], "metadata": {"page_label": "508", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b50947cc-b197-4b41-a716-a767606c578f": {"node_ids": ["63012630-b566-458e-9929-8448e34b2e16"], "metadata": {"page_label": "509", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1240fab6-21e1-4f4c-9dbe-a28e69663082": {"node_ids": ["cf3f82fc-54fc-4c25-917e-c595a64c6f7a"], "metadata": {"page_label": "510", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "60e0c99c-3ccb-4d86-9315-ce2b8bece4af": {"node_ids": ["c750e3b0-1d8a-4cec-ad1a-4a4469dae57d"], "metadata": {"page_label": "511", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e8c598b6-7706-4683-9167-3704832bf3c4": {"node_ids": ["96aac05d-08a0-487d-98f1-333a5f568531"], "metadata": {"page_label": "512", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9fa80710-f940-4a97-b983-d00dfb02adb5": {"node_ids": ["e492b05b-1a3f-41aa-af6b-efe4cfd30074"], "metadata": {"page_label": "513", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "11c9715d-dc9d-40a0-86f4-4aa11d06ac66": {"node_ids": ["99f7e23e-947a-4cd3-807a-953a2c398f84"], "metadata": {"page_label": "514", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ac15fe04-145d-4f53-ac4c-7f00a54a4cf3": {"node_ids": ["6074dd95-2f2e-4524-9356-6ddb60a80f6e"], "metadata": {"page_label": "515", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f2e4f884-77fd-49e8-a4c5-c53f6fb4bdb5": {"node_ids": ["e0467500-8fc9-4bf0-8068-df0e643ab802"], "metadata": {"page_label": "516", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "eb48d5c0-da60-4aa4-8734-06294983fb1c": {"node_ids": ["23113c86-7fd9-48ca-9a1a-f9497804b1e8"], "metadata": {"page_label": "517", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c2201a65-c24f-4b78-9c57-543a46df7785": {"node_ids": ["120dfe82-aa7f-4acc-b3b7-57b7c1565bee"], "metadata": {"page_label": "518", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "70070691-a812-46e1-820a-72b8d72a191c": {"node_ids": ["22d1546d-3d7d-417b-8930-82e7d47fa038"], "metadata": {"page_label": "519", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6bfd0d76-acb5-4a41-8f60-295f26f0ba1a": {"node_ids": ["495b45b5-7118-4a83-a7a1-bc424043c912"], "metadata": {"page_label": "520", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1e65ec52-ec77-4f0b-b62d-36ec7a0b7aa3": {"node_ids": ["c5109382-b8b9-4fc7-a197-78d2cd5b9034"], "metadata": {"page_label": "521", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "f27bd5ff-f5f6-4e01-9e67-5ec2795a3dd4": {"node_ids": ["5521481e-9acf-40ee-b452-96a6dcdf80ca"], "metadata": {"page_label": "522", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "82b6e5e3-0b77-4480-9b8f-ecf6fac3e528": {"node_ids": ["8d8c0d28-0a09-467c-bbaf-6134ee16c6a0"], "metadata": {"page_label": "523", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6fc1f10d-3311-47cb-b4e6-d101dec4e87e": {"node_ids": ["8009b1c1-c8bb-47cc-b01a-9f0b6b7701f6"], "metadata": {"page_label": "524", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d7738e21-a5b4-416b-8377-e673f3f8f5f6": {"node_ids": ["9a9fce72-0464-4609-895b-c98cf4727c4a"], "metadata": {"page_label": "525", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "80f0582b-5836-4bdc-81a1-01afee9c253a": {"node_ids": ["19151ddf-e2ce-4d90-937c-fd7c96b4cc83"], "metadata": {"page_label": "526", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c6ce5391-1394-43ec-87de-97316d6008ea": {"node_ids": ["65fbcd57-1afc-4fb5-82af-dca8c8e21b47"], "metadata": {"page_label": "527", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "435f96e8-f705-466b-8e3c-a20eba1a0ab6": {"node_ids": ["e6a3b9c3-cd81-4cb1-8c36-06a89b633131"], "metadata": {"page_label": "528", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a270f694-a4c1-44c4-9b76-b362fdd7be7a": {"node_ids": ["a0489b91-cdd9-47d3-b3dc-b41210749bbe"], "metadata": {"page_label": "529", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0e697497-e117-450e-96db-567cc1fd941f": {"node_ids": ["69e4c1eb-c844-4b9f-8309-7ffcc57469c5"], "metadata": {"page_label": "530", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "632b6f14-3965-45d9-a77d-e92181f5bb8c": {"node_ids": ["85481bec-fe3a-4fe6-9272-b86ab47179d4"], "metadata": {"page_label": "531", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8262359e-29ed-4003-bae3-37dc012c80a6": {"node_ids": ["fd25fc63-4d0b-4d29-9670-a6119124598a"], "metadata": {"page_label": "532", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "18624e86-96dd-4ca1-80fd-cbf4788d4b7d": {"node_ids": ["5685792d-17fb-4614-88c2-68520751abc8"], "metadata": {"page_label": "533", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "b3808f54-f124-4d0c-ab1f-618eda95593a": {"node_ids": ["eda916be-b392-4bc8-a356-0c80f1e044d6"], "metadata": {"page_label": "534", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "1f1c5137-1bbc-4a40-bd25-db7996926143": {"node_ids": ["3f10ee03-a0b6-4b02-af48-1848d629639a"], "metadata": {"page_label": "535", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3e90d93d-5469-493c-ac99-d2cd7e80f725": {"node_ids": ["059b6400-033b-4b11-a0d0-8dd0c02ca619"], "metadata": {"page_label": "536", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "bda2501a-784e-484b-a1df-bfdea29358a2": {"node_ids": ["339907ab-f39a-4eed-a7d6-e86106dcdd6f"], "metadata": {"page_label": "537", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7cc10ee2-2872-43b0-a6dc-059019cbfc7c": {"node_ids": ["c1442b64-4520-4316-be19-b90d40b2b9a8"], "metadata": {"page_label": "538", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "985383d5-8688-4d9c-92c5-0e27ee0b0112": {"node_ids": ["317777d7-9742-47af-be50-825be566268e"], "metadata": {"page_label": "539", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3ec9c0c5-b5f6-4e0a-96e3-4cdff960da1f": {"node_ids": ["183d2e7a-c302-4afd-aaff-eeae2ff3f3dd"], "metadata": {"page_label": "540", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "de17f2e6-650c-4858-9fa3-216f1eb249e7": {"node_ids": ["6d19626d-b338-4847-b1ca-120e1e30f76e"], "metadata": {"page_label": "541", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d98de347-796a-46d5-9155-f3aaa2f3a921": {"node_ids": ["29cc148e-8e91-435d-be63-cabb63a006f0"], "metadata": {"page_label": "542", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6eeeb315-2816-4bf0-92b5-f5d92eed548b": {"node_ids": ["5b4da239-7fd8-4aba-b908-dec4cb4d6342"], "metadata": {"page_label": "543", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a4d3d7bd-e13d-4f3a-a5cc-c53e97a1b090": {"node_ids": ["344780e3-f4d3-42d0-aa2b-974f955db383"], "metadata": {"page_label": "544", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "570bc122-9b98-430e-9c45-81f50eca26e4": {"node_ids": ["26ff6cb2-ae2a-4d1b-b08d-c68aa1fe6638"], "metadata": {"page_label": "545", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "85a17d89-f7f9-4f69-a1e4-4a7f196609a6": {"node_ids": ["b08a5680-2817-4cd5-a233-dfb3b9b478f7"], "metadata": {"page_label": "546", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9836168d-450a-4873-be3c-64d13d4707c9": {"node_ids": ["f35fdf52-7d2f-4503-8483-1845fdf8fae3"], "metadata": {"page_label": "547", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "8861b292-6a4e-462f-bf12-ebeaf3cb795b": {"node_ids": ["be7a4b96-5689-46c4-a7a4-75d3bc7a8256"], "metadata": {"page_label": "548", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "19053491-90c1-4900-b3df-cec4d042a334": {"node_ids": ["30b28b21-5eb9-4624-9dbe-d444ad80fd0e"], "metadata": {"page_label": "549", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "85fc3415-8fbd-4e4d-9f77-7ed86812eba3": {"node_ids": ["7026a587-e6d6-4d8d-a05a-8504c68e6ccf"], "metadata": {"page_label": "550", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "7be73b89-e40e-4d0a-bcdf-76a5aa7b79cb": {"node_ids": ["29b07c41-2f88-478b-8082-927a1713397b"], "metadata": {"page_label": "551", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "02648872-446a-41e0-a655-eca95044163c": {"node_ids": ["79c924ef-ba3d-4498-b7e1-9423edddcf29"], "metadata": {"page_label": "552", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "87aa9734-c741-46ea-844c-5abe89d376b7": {"node_ids": ["6b0821c1-ad26-4d07-b0a2-0b75dd5800ab"], "metadata": {"page_label": "553", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "703cc9b9-e2b0-48a5-95cc-1ea42d282b60": {"node_ids": ["20552f36-f7b5-4058-8e0a-ba0045d371c7"], "metadata": {"page_label": "554", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6935f477-88a3-44dd-98aa-f92408087518": {"node_ids": ["d6ac57a4-6f8f-4044-b86f-bd34617c1ba1"], "metadata": {"page_label": "555", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3509afe1-f76a-4f0b-adfa-80cffbcd902c": {"node_ids": ["b7a9e2b4-7d94-438f-962f-69543f8ab415"], "metadata": {"page_label": "556", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "9718f708-f716-43c9-aef5-3d79f9a41216": {"node_ids": ["7a81e309-8882-4fa3-8116-4e30fcccda52"], "metadata": {"page_label": "557", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "6d8940cf-4a56-40c8-a14d-65b8e7a452c6": {"node_ids": ["0c426982-35e8-4e5a-b4fc-a57f032d2b34"], "metadata": {"page_label": "558", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "18692ed0-051a-40f8-9c9d-5b4f5776e51d": {"node_ids": ["e11c3d95-5be7-4553-89e4-df6aae1257c5"], "metadata": {"page_label": "559", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "c93bc452-1746-464d-a1d6-1aca61539cf8": {"node_ids": ["63361d68-0270-4fc7-a375-1b33379c6b44"], "metadata": {"page_label": "560", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "891e1a39-3797-4686-aac2-2324a61fac11": {"node_ids": ["447de163-9c35-42be-93c3-c6ca5ef9f0f4"], "metadata": {"page_label": "561", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a3e1383c-3df4-4f5a-851c-fc3f75c8497e": {"node_ids": ["caf2b763-2722-4f06-a202-dc42883dbb8b"], "metadata": {"page_label": "562", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "506e2a77-43ad-4b8c-aeaa-5b97f719292e": {"node_ids": ["5417d6fd-3a96-4069-b649-74028b7231ca"], "metadata": {"page_label": "563", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d33cdea7-d2d2-4530-95b1-a6c2bddeb32b": {"node_ids": ["5bbb5831-61f8-4342-80c3-08839d713697"], "metadata": {"page_label": "564", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e16615fa-db1b-4c60-9dfd-398cb857304f": {"node_ids": ["49d5413a-8404-4664-b614-78e0837cb644"], "metadata": {"page_label": "565", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e74c6fc9-111f-44b9-a5c1-040c52410f6c": {"node_ids": ["8ce3a23c-aa96-46ec-bf3e-ced02a058149"], "metadata": {"page_label": "566", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d93faed3-ba2e-4915-8026-8365d2d14423": {"node_ids": ["90f6b2b6-a419-414c-8145-6f27d67a3aef"], "metadata": {"page_label": "567", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "48477fcc-691f-463f-b45b-2849a53d6aa5": {"node_ids": ["a9091752-0abf-46a7-865a-43ed73a2a757"], "metadata": {"page_label": "568", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "80287f50-da5d-4e30-bcc2-86fa7f642a27": {"node_ids": ["aa55c503-693b-471c-b732-65d2c1616ae1"], "metadata": {"page_label": "569", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3e1922ff-baa8-48e7-b731-94c1daca510b": {"node_ids": ["c6c9df40-ffc1-49dd-ad02-c73fd103b973"], "metadata": {"page_label": "570", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "36a7f4b5-af65-4b7e-a0ea-10d884b2f7b7": {"node_ids": ["f61bf414-8a45-4bd1-b582-744bb2e980c6"], "metadata": {"page_label": "571", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "63ee8a48-1f9a-4aa6-9ecf-14c016d732d2": {"node_ids": ["2c3011dd-0e32-4c59-be7c-eeace2eb27eb"], "metadata": {"page_label": "572", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "749e268c-6685-49d1-8911-06d614772d2c": {"node_ids": ["c7d44800-1881-4576-9697-6103d0371f4e"], "metadata": {"page_label": "573", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "0273c2ed-70a8-46c6-a209-dec0015be041": {"node_ids": ["1fb3d4c8-0a73-463b-9ae8-223de1733891"], "metadata": {"page_label": "574", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5c9636ae-360b-4680-a22c-b48120ff1284": {"node_ids": ["168012dd-7ed7-4a11-a10d-aaf5e606b579"], "metadata": {"page_label": "575", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "4c2a7d84-7bb9-4173-aba2-b8fd1e180b45": {"node_ids": ["f32e30a1-40a6-4c2e-96b3-ed3533296b1f"], "metadata": {"page_label": "576", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "be85ea15-9607-435f-a738-b0994566d1c8": {"node_ids": ["e0437c4c-8716-4354-a64c-cac858c3935a"], "metadata": {"page_label": "577", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3b92e74d-d69a-4e0d-9bc2-0cd9b49943bc": {"node_ids": ["6cb767f9-eb9b-445b-a2b3-2dc877c4da9b"], "metadata": {"page_label": "578", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "2b276a36-4177-4614-90f4-ced07f7eb874": {"node_ids": ["7761c669-78f6-43a5-9bca-c6eb51c5c498"], "metadata": {"page_label": "579", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "ec61542d-8b98-4f03-8581-edc47443e7d7": {"node_ids": ["000bc57d-2008-4cd9-a075-e6202829f8f8"], "metadata": {"page_label": "580", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "adeb9a86-3040-4f7e-aeff-6c399f3e1d05": {"node_ids": ["f712bb45-e9bb-4204-9e98-8700b7ad2a8a"], "metadata": {"page_label": "581", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "e81d1485-4097-498e-ae45-4dcca4c7a01f": {"node_ids": ["b38d4ef2-4cab-4935-b9ee-9050a6ef9f60"], "metadata": {"page_label": "582", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "3631ed6d-7b27-46ce-beb3-4fc6b4b8447c": {"node_ids": ["ebf7af3c-7272-4fb4-8ef1-a59e65eddde0"], "metadata": {"page_label": "583", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "65680103-60a1-43a6-8038-837d7875d63d": {"node_ids": ["6729502a-e921-4090-9281-21453d5fcef5"], "metadata": {"page_label": "584", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "59074bad-f21b-4fb0-8e24-f280fb071a9a": {"node_ids": ["d6d75f0a-9ead-4ef3-99c5-b37e1bbf3184"], "metadata": {"page_label": "585", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "788fe1cc-44ad-46ab-b7e0-8d390ceb0fea": {"node_ids": ["6715eaaf-5a68-4fb9-bca7-b6adebeeb0e9"], "metadata": {"page_label": "586", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "521a8690-5463-439b-8c35-4cb6b8345bc9": {"node_ids": ["eedc25c1-8879-4ee7-b69f-bfc15b49c2bc"], "metadata": {"page_label": "587", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "29c94398-ca63-42fa-bdc2-da01c5a7d075": {"node_ids": ["b986c154-fd8a-4979-9aad-6e31a29f0336"], "metadata": {"page_label": "588", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "d299fd8f-72ea-4fc8-a8a8-82f0fff1328c": {"node_ids": ["362643ab-8026-4a88-ad53-9c1803807039"], "metadata": {"page_label": "589", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "5ed38320-35a0-4b26-9be7-8d5a4eef7519": {"node_ids": ["13747bef-1046-4d49-90d0-0af8047029f5"], "metadata": {"page_label": "590", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}, "a3197b74-fa14-40df-8acc-33bd968f10b6": {"node_ids": ["6f41b7af-e4e1-45bc-8d39-ee2ecf409200"], "metadata": {"page_label": "591", "file_name": "Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_path": "/Users/josephmuller/Dev/chats/designing-data-intensive-applications/data/Designing Data Intensive Applications - Martin Kleppmann.pdf", "file_type": "application/pdf", "file_size": 24975901, "creation_date": "2025-01-26", "last_modified_date": "2025-01-26"}}}}